0    Professor Bath Philip    Nottingham,University of    None    None    Assessment of modern machine learning methods and conventional statistical regression techniques in diagnosis and prediction of outcome after acute stroke using big data    None    British Heart Foundation    None    145079.0GBP
1    Professor Jayson Gordon    University of Manchester    2005-01-01    2008-06-30    The human serum metabolome in health and disease    Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high- resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.    Biotechnology and Biological Sciences Research Council    LINK project    168047.0GBP
2    Professor Rubinsztein David    University of Cambridge    2011-10-12    2013-10-11    IDENTIFICATION OF GENERIC SUPRESSORS OF PROTEINOPATHIES    Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high- resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.    Medical Research Council    Research Grant    407427.0GBP
3    Professor Herrick Ariane    University of Manchester    2019-09-01    2022-08-05    Development of a measuring app for finger lesions as an outcome measure for systemic sclerosis-related digital ulceration (SALVE: Scleroderma App for Lesion VErification)’    Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high- resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.    Versus Arthritis    Full Application Treatment    216773.0GBP
4    Professor Kell Douglas    The University of Manchester    2019-09-01    2022-08-05    The Rothamsted Metabolomics Centre (MeT-RO)    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20482)    Biotechnology and Biological Sciences Research Council    Full Application Treatment    134363.0GBP
5    Professor Collins Andrew Richard    University of Southampton    2011-10-01    2014-09-30    Using machine learning methods to characterise the role of genetic factors in early onset breast cancer    Currently only about 30% of the genetic contribution to breast cancer risk has been identified. These risk factors comprise a small number of rare and moderate penetrance disease genes along with common variants identified through genome-wide association studies. Some of the 'missing heritability' is likely to remain within the phenotype: focus on 'simple' case and control disease phenotypes may be less powerful than testing for genes underlying disease sub-types. Furthermore, analytical methods have not adequately modelled interactions between genes, phenotypic variables and other factors. Finally, a proportion of genetic variation is likely to arise through rarer moderate penetrance genes that have not been tested for through association studies, but justify the effort towards sequencing. This studentship develops novel analyses in a large early onset breast cancer phenotype and genotype sample. Through collaborative supervision between the Southampton Schools of Medical and Mathematics, machine learning methods will be developed, evaluated and employed for the analyses, thereby avoiding the limitations of conventional parametric models. A particular focus will be the characterisation of the genetic basis of breast cancer sub-phenotypes (including for example, categories based on tumour grade, type and tumour biomarkers). Modelling the role of genetic variation using SNP genotype data to identify novel genetic variation underlying early onset disease is the basis of the studentship. An understanding of the role of this variation on breast cancer phenotypes is essential for the development of novel therapies in the future.    Breast Cancer Now    PhD    61400.0GBP
6    Professor Ananiadou Sophia    The University of Manchester    2011-10-01    2014-09-30    Tools for the text mining-based visualisation of the provenance of biochemical networks    Systems biology is concerned with the modelling, visualisation and analysis of biochemical networks in which, for instance, metabolites are 'linked' by arrows representing the enzymes which turn one molecule into another or which are modified by particular substances. SBML provides a computer- readable 'standard' for describing such biochemical or signalling networks. However, these diagrams (and thus the SBML models) are divorced from the scientific evidence on which they are based, represented by the scientific literature (and increasingly by online databases). In order to overcome the problems of reading the burgeoning scientific literature, we shall deploy Text Mining TM. TM involves named entity recognition (i.e. semantic annotation of enzymes, metabolites, etc) and information extraction (i.e. relationship extraction between named entities). An important part of this proposal is to find solutions for the terminology problem in systems biology, by developing techniques for recognising synonym terms.Based on our efficient parsing techniques, we shall extract relationships between entities that will form the basis by which we shall can discover, index, store and display the scientific evidence for such linkages. The selection of the most pertinent relationships will be performed using our preferred methods of advanced machine learning (Support Vector Machines and Genetic Programming). The overall aim of the project is thus to develop and deploy the necessary TM tools and to use them to display the different relationships to the user together with the literature from which they have been extracted. The different types (and strength) of evidence for these interactions will then be visualised directly and linked to a dynamic website of the literature. This will thus give users a direct linkage between the systems biology diagrams encoded in (an advanced form of) SBML and the scientific evidence for them. Where available, linkages to kinetic data will also be made.    Biotechnology and Biological Sciences Research Council    PhD    549458.0GBP
7    Professor Kell Douglas    The University of Manchester    2011-10-01    2014-09-30    Tools for the text mining-based visualisation of the provenance of biochemical networks    Systems biology is concerned with the modelling, visualisation and analysis of biochemical networks in which, for instance, metabolites are 'linked' by arrows representing the enzymes which turn one molecule into another or which are modified by particular substances. SBML provides a computer- readable 'standard' for describing such biochemical or signalling networks. However, these diagrams (and thus the SBML models) are divorced from the scientific evidence on which they are based, represented by the scientific literature (and increasingly by online databases). In order to overcome the problems of reading the burgeoning scientific literature, we shall deploy Text Mining TM. TM involves named entity recognition (i.e. semantic annotation of enzymes, metabolites, etc) and information extraction (i.e. relationship extraction between named entities). An important part of this proposal is to find solutions for the terminology problem in systems biology, by developing techniques for recognising synonym terms.Based on our efficient parsing techniques, we shall extract relationships between entities that will form the basis by which we shall can discover, index, store and display the scientific evidence for such linkages. The selection of the most pertinent relationships will be performed using our preferred methods of advanced machine learning (Support Vector Machines and Genetic Programming). The overall aim of the project is thus to develop and deploy the necessary TM tools and to use them to display the different relationships to the user together with the literature from which they have been extracted. The different types (and strength) of evidence for these interactions will then be visualised directly and linked to a dynamic website of the literature. This will thus give users a direct linkage between the systems biology diagrams encoded in (an advanced form of) SBML and the scientific evidence for them. Where available, linkages to kinetic data will also be made.    Biotechnology and Biological Sciences Research Council    PhD    549458.0GBP
8    Professor Kell Douglas    The University of Manchester    2011-10-01    2014-09-30    Inference and learning in machine vision    Systems biology is concerned with the modelling, visualisation and analysis of biochemical networks in which, for instance, metabolites are 'linked' by arrows representing the enzymes which turn one molecule into another or which are modified by particular substances. SBML provides a computer- readable 'standard' for describing such biochemical or signalling networks. However, these diagrams (and thus the SBML models) are divorced from the scientific evidence on which they are based, represented by the scientific literature (and increasingly by online databases). In order to overcome the problems of reading the burgeoning scientific literature, we shall deploy Text Mining TM. TM involves named entity recognition (i.e. semantic annotation of enzymes, metabolites, etc) and information extraction (i.e. relationship extraction between named entities). An important part of this proposal is to find solutions for the terminology problem in systems biology, by developing techniques for recognising synonym terms.Based on our efficient parsing techniques, we shall extract relationships between entities that will form the basis by which we shall can discover, index, store and display the scientific evidence for such linkages. The selection of the most pertinent relationships will be performed using our preferred methods of advanced machine learning (Support Vector Machines and Genetic Programming). The overall aim of the project is thus to develop and deploy the necessary TM tools and to use them to display the different relationships to the user together with the literature from which they have been extracted. The different types (and strength) of evidence for these interactions will then be visualised directly and linked to a dynamic website of the literature. This will thus give users a direct linkage between the systems biology diagrams encoded in (an advanced form of) SBML and the scientific evidence for them. Where available, linkages to kinetic data will also be made.    Biotechnology and Biological Sciences Research Council    PhD    549458.0GBP
9    Professor Wernisch Lorenz    University of Cambridge    2016-12-01    2019-03-31    Statistical bioinformatics and genetics    Systems biology is concerned with the modelling, visualisation and analysis of biochemical networks in which, for instance, metabolites are 'linked' by arrows representing the enzymes which turn one molecule into another or which are modified by particular substances. SBML provides a computer- readable 'standard' for describing such biochemical or signalling networks. However, these diagrams (and thus the SBML models) are divorced from the scientific evidence on which they are based, represented by the scientific literature (and increasingly by online databases). In order to overcome the problems of reading the burgeoning scientific literature, we shall deploy Text Mining TM. TM involves named entity recognition (i.e. semantic annotation of enzymes, metabolites, etc) and information extraction (i.e. relationship extraction between named entities). An important part of this proposal is to find solutions for the terminology problem in systems biology, by developing techniques for recognising synonym terms.Based on our efficient parsing techniques, we shall extract relationships between entities that will form the basis by which we shall can discover, index, store and display the scientific evidence for such linkages. The selection of the most pertinent relationships will be performed using our preferred methods of advanced machine learning (Support Vector Machines and Genetic Programming). The overall aim of the project is thus to develop and deploy the necessary TM tools and to use them to display the different relationships to the user together with the literature from which they have been extracted. The different types (and strength) of evidence for these interactions will then be visualised directly and linked to a dynamic website of the literature. This will thus give users a direct linkage between the systems biology diagrams encoded in (an advanced form of) SBML and the scientific evidence for them. Where available, linkages to kinetic data will also be made.    Medical Research Council    Unit    549458.0GBP
10    Professor Wernisch Lorenz    Rothamsted Research    2016-12-01    2019-03-31    The Rothamsted Metabolomics Centre (MeT-RO)    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)    Biotechnology and Biological Sciences Research Council    Unit    1461648.0GBP
11    Professor Wernisch Lorenz    University College London    2016-12-01    2019-03-31    Prediction of protein-protein interaction hot spots using a combination of physics and machine learning    Protein-protein interactions are central to most biological processes, from signal transduction to immune response. Understanding these functional associations requires knowledge of the three-dimensional structure of the complex as this reveal the underlying molecular mechanism. However, determining experimentally the 3D structure of a protein complex present considerable difficulties. There is therefore a need for accurate and reliable computational methods. Several experiments have shown that protein interactions are critically dependent on just a few residues, or hot spots, at the binding interface. Hot spots make a dominant contribution to the free energy of binding and if mutated they can disrupt the interaction. In this project we aim to develop a computational method that can identify hot spot residues (and the contacts they form across the interface) in unbound proteins (i.e. without prior knowledge of the complex). This would significantly improve our ability at predicting the overall structure of the complex (the so-called docking problem). We plan to combine and integrate the basic energetic terms that contribute to the stability of protein complexes (e.g. van der Waals potential, hydrogen bonds,etc.) using state of the art machine learning techniques. In the first part of the project, we will develop a method to predict hot-spot residues at protein protein interfaces when the structure of complex is available. In the second part, we plan to systematically dock structural fragments of the two unbound proteins and test them for the presence of potential hot spots (using the classifier developed in the first part). Eventually, we will combine different sources of information (energetic, evolutionary and structural) to predict few important contacts across the interface of two proteins.    Biotechnology and Biological Sciences Research Council    Unit    310931.0GBP
12    Professor McKeigue Paul    University of Edinburgh    2009-03-16    2012-03-15    Development of Bayesian methods for genetic epidemiology    Protein-protein interactions are central to most biological processes, from signal transduction to immune response. Understanding these functional associations requires knowledge of the three-dimensional structure of the complex as this reveal the underlying molecular mechanism. However, determining experimentally the 3D structure of a protein complex present considerable difficulties. There is therefore a need for accurate and reliable computational methods. Several experiments have shown that protein interactions are critically dependent on just a few residues, or hot spots, at the binding interface. Hot spots make a dominant contribution to the free energy of binding and if mutated they can disrupt the interaction. In this project we aim to develop a computational method that can identify hot spot residues (and the contacts they form across the interface) in unbound proteins (i.e. without prior knowledge of the complex). This would significantly improve our ability at predicting the overall structure of the complex (the so-called docking problem). We plan to combine and integrate the basic energetic terms that contribute to the stability of protein complexes (e.g. van der Waals potential, hydrogen bonds,etc.) using state of the art machine learning techniques. In the first part of the project, we will develop a method to predict hot-spot residues at protein protein interfaces when the structure of complex is available. In the second part, we plan to systematically dock structural fragments of the two unbound proteins and test them for the presence of potential hot spots (using the classifier developed in the first part). Eventually, we will combine different sources of information (energetic, evolutionary and structural) to predict few important contacts across the interface of two proteins.    Medical Research Council    Research Grant    317659.0GBP
13    Professor Draper John    Aberystwyth University    2009-03-16    2012-03-15    Analysis of Magnaporthe grisea pathogenicity by insertion mutagenesis and hierarchical metabolomics    The project aims to utilise metabolomics approaches to identify metabolic processes associated with pathogenicity in the fungus Magnaporthe grisea, a major disease of a range of cereals and grasses. The genome sequence of the fungus has been determined and tools are available for generating targeted gene replacement mutants, studying gene expression using genome microarrays, and carrying out detailed cell biological studies of plant infection (for review see Talbot, 2003). M. grisea is being subjected to intensive functional genomics analysis, including large-scale insertion mutagenesis projects. To date, mutants affecting pathogenicity are almost without exception impaired in ability to form infection structures (appressoria) and penetrate host epidermal cells. However, in new mutant screens carried out at Exeter and elsewhere, several new classes of mutant are emerging where the timing of lesion formation and subsequent lesion expansion is impaired. We hypothesize that the corresponding genes may make important contributions to plant tissue colonization and disease symptom formation by M. grisea. Molecular genetic analysis of early-phase infection mutants in M. grisea have largely been carried out ex planta by germination of spores on inert plastic surfaces, providing large synchronous populations of infection structures for biochemical analysis. In parallel, we have developed an accurate sampling system for in planta infection sites based on microscopy and GFP-tagging of the pathogen, and by using such sampling approaches have shown that metabolomic fingerprinting and supervised data analysis can detect reproducible major changes in metabolome during lesion development. We will carry out detailed metabolome phenotyping of M. grisea in order to understand the precise roles of genes involved in the development of fungal infection structures using already available mutants. We will also refine and carry out a screen for mutants affected in the timing, rate of growth and sporulation of disease lesions. Mutants representative of different phenotypic classes will be inoculated onto hosts in controlled environments and lesion material collected at several time points. Metabolome analysis will follow a hierarchical procedure initiated with high-throughput, low-resolution ESI- MS fingerprinting (LTQ linear ion trap) and GC-tof-MS fingerprinting (LECO Pegasus II). Discrimination of appropriate sample combinations will be determined by supervised data analysis. If there is evidence for metabolome differences (e.g. comparing mutants with an isogenic wild type strain at the same stage of infection) then a further subset of the same samples will be subjected to ESI-FTMS fingerprinting to generate high resolution peak tables. Corresponding GC-tof-MS chromatograms will also be processed to deconvolute and annotate peaks for data mining. Explanatory metabolite signals between different sample classes will be determined using machine learning procedures. In ESI-MS data, high ranked m/z signals will be examined in chromatograms of further LC-FTMS analyses to predict the mass of possible parent ions which will be further fragmented to obtain MS(n) spectral data. Metabolite mass tables and spectral libraries will be searched for matches with spectra representing discriminatory peaks, and predicted metabolites will be quantified against standards using targeted GC-tof-MS or LC-MS as appropriate. Metabolome differences centred on specific metabolites in the various mutants will be used to determine areas of metabolism that may impact on fungal pathogenicity in future experiments. Parallel genetic analysis of insertional mutant collections of M. grisea will focus on those in which metabolome differences are apparent during plant tissue invasion. Gene isolation by inverse PCR, complementation and validation by targeted gene replacement experiments will be used to define genes associated with disease lesion formation by M. grisea.    Biotechnology and Biological Sciences Research Council    Research Grant    203644.0GBP
14    Professor Draper John    Aberystwyth University    2009-03-16    2012-03-15    The Rothamsted Metabolomics Centre (MeT-RO)    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20482 and MET20484)    Biotechnology and Biological Sciences Research Council    Research Grant    462262.0GBP
15    Prof Goodacre Roy    The University of Manchester    2009-03-16    2012-03-15    The human serum metabolome in health and disease    Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high-resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.    Biotechnology and Biological Sciences Research Council    Research Grant    1083263.0GBP
16    Professor Kell Douglas    The University of Manchester    2009-03-16    2012-03-15    The human serum metabolome in health and disease    Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high-resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.    Biotechnology and Biological Sciences Research Council    Research Grant    1083263.0GBP
17    Professor King Ross    Aberystwyth University    2009-03-16    2012-03-15    The Modelling Apprentice: A tool to aid the formation of cell signalling models    This proposal aims to exploit and enhance the newly developed software system Justaid" to create the "Modelling Apprentice", a tool to assist biological researchers interested in developing accurate and understandable mathematical models of cell signalling networks. Justaid is a general purpose programming assistant for scientific discovery that uses techniques from Qualitative Reasoning (QR), Machine Learning and User Interface design to to simplify the task of model construction, testing and updating. QR is chosen as the knowledge representation because domain knowledge can be acquired easily and quickly from experimental observations that lack precision. Machine Learning is used to determine whether a given model can fully explain a set of experimental observations, and suggest model updates, extending the explanatory power of the model to hitherto unexplained observations. Justaid has been designed to remove the need for the scientist to understand the underlying knowledge representation and machine learning, so that he/she can concentrate on the scientific domain rather than on mathematical concepts. This is achieved by an intuitive graphical user interface and a modular architecture whereby construction of a model for a new scientific domain simply involves creating a new Justaid library. The first task involves the construction of a Justaid library for cell signalling, using the yeast MAPK signalling pathway as a test case. This will involve expert biologists from Centre for Systems Biology, University of Cambridge and the SABR centre at the University of Aberdeen. The Modelling Apprentice and the new MAPK model will then be used by the biologists, in conjunction with the results of wet lab experiments, to test the utility an appropriateness of Justaid as a scientific discovery tool. The insight from these tests will be used to further improve the user interface design and model updating techniques of the Modelling Apprentice"    Biotechnology and Biological Sciences Research Council    Research Grant    99554.0GBP
18    Professor Stuart David    Diamond Light Source Ltd    2018-10-01    2021-09-30    SuRVoS Workbench: Enhanced machine learning for segmentation across structural biology    This proposal aims to exploit and enhance the newly developed software system Justaid" to create the "Modelling Apprentice", a tool to assist biological researchers interested in developing accurate and understandable mathematical models of cell signalling networks. Justaid is a general purpose programming assistant for scientific discovery that uses techniques from Qualitative Reasoning (QR), Machine Learning and User Interface design to to simplify the task of model construction, testing and updating. QR is chosen as the knowledge representation because domain knowledge can be acquired easily and quickly from experimental observations that lack precision. Machine Learning is used to determine whether a given model can fully explain a set of experimental observations, and suggest model updates, extending the explanatory power of the model to hitherto unexplained observations. Justaid has been designed to remove the need for the scientist to understand the underlying knowledge representation and machine learning, so that he/she can concentrate on the scientific domain rather than on mathematical concepts. This is achieved by an intuitive graphical user interface and a modular architecture whereby construction of a model for a new scientific domain simply involves creating a new Justaid library. The first task involves the construction of a Justaid library for cell signalling, using the yeast MAPK signalling pathway as a test case. This will involve expert biologists from Centre for Systems Biology, University of Cambridge and the SABR centre at the University of Aberdeen. The Modelling Apprentice and the new MAPK model will then be used by the biologists, in conjunction with the results of wet lab experiments, to test the utility an appropriateness of Justaid as a scientific discovery tool. The insight from these tests will be used to further improve the user interface design and model updating techniques of the Modelling Apprentice"    Wellcome Trust    Biomedical Resources Grant    605412.0GBP
19    Dr Falciani Francesco    University of Birmingham    2008-08-01    2011-11-30    Modelling cell-to-cell communication networks:an integrated approach to studying cell interactions during tissue angiogenesis.    The recent development of functional genomics technologies, particularly gene expression profiling, has provided the scientific community with the tools to characterize the molecular state of cells and tissues at a genomic level. The analysis of these datasets using statistical modelling and more advanced machine learning techniques designed to reconstruct and model the structure of biologically relevant pathways has contributed to reshaping biological investigation. Despite the paramount importance of this work, very little has been done to apply such network inference methodologies to reconstruct and model the structure of networks representing cell-to-cell communication events. The project we propose develops from preliminary work performed in the applicants research groups, aiming to develop the mathematical and experimental framework to infer the structure of regulatory pathways from both steady-state and temporal data. From an experimental prospective we will focus on the understanding of the general mechanisms involved in endothelia-tumor cell interactions. The project involves an initial component of genome-wide expression profiling and proteomics data generation followed by iterative cycles of computational modelling, hypothesis formulation and experimental verification using a set of state of the art gene modulation tools applicable to both in vitro and in vivo systems. Initially, we will identify informative genes and map these on known functional networks and protein-protein interaction networks in order to identify functionally interesting gene modules (for example derived from KEGG, BioGrid, REACTOME, etc). This modularization process will help in the selection of the genes to model using ARACNE, Bayesian networks and other similar computational approaches. The project will ultimately lead to the development of computational models representing cell-to-cell interaction networks from both time course and steady-state datasets. The analysis and Biological interpretation of these networks will allow us to develop hypothesis and experimentally verify them using gene manipulation techniques in both in vitro and in vivo systems. We expect this project to lead to the identification of regulatory circuits that are important in the interaction between endothelial and tumour cells during tumour growth and to a better understanding of the biological and molecular events associated to known factors involved in the tumour vascularization.    Cancer Research UK    Project Award    605412.0GBP
20    Dr Walsh Claire    University College London    2018-08-01    2021-07-31    Creating high fidelity digital tissue substrates for the development of non-invasive microstructural MRI    The recent development of functional genomics technologies, particularly gene expression profiling, has provided the scientific community with the tools to characterize the molecular state of cells and tissues at a genomic level. The analysis of these datasets using statistical modelling and more advanced machine learning techniques designed to reconstruct and model the structure of biologically relevant pathways has contributed to reshaping biological investigation. Despite the paramount importance of this work, very little has been done to apply such network inference methodologies to reconstruct and model the structure of networks representing cell-to-cell communication events. The project we propose develops from preliminary work performed in the applicants research groups, aiming to develop the mathematical and experimental framework to infer the structure of regulatory pathways from both steady-state and temporal data. From an experimental prospective we will focus on the understanding of the general mechanisms involved in endothelia-tumor cell interactions. The project involves an initial component of genome-wide expression profiling and proteomics data generation followed by iterative cycles of computational modelling, hypothesis formulation and experimental verification using a set of state of the art gene modulation tools applicable to both in vitro and in vivo systems. Initially, we will identify informative genes and map these on known functional networks and protein-protein interaction networks in order to identify functionally interesting gene modules (for example derived from KEGG, BioGrid, REACTOME, etc). This modularization process will help in the selection of the genes to model using ARACNE, Bayesian networks and other similar computational approaches. The project will ultimately lead to the development of computational models representing cell-to-cell interaction networks from both time course and steady-state datasets. The analysis and Biological interpretation of these networks will allow us to develop hypothesis and experimentally verify them using gene manipulation techniques in both in vitro and in vivo systems. We expect this project to lead to the identification of regulatory circuits that are important in the interaction between endothelial and tumour cells during tumour growth and to a better understanding of the biological and molecular events associated to known factors involved in the tumour vascularization.    Medical Research Council    Fellowship    289629.0GBP
21    Dr Hentges Kathryn    Manchester, University of    2018-08-01    2021-07-31    Identification of genes associated with cardiac development using "Machine Learning"    The recent development of functional genomics technologies, particularly gene expression profiling, has provided the scientific community with the tools to characterize the molecular state of cells and tissues at a genomic level. The analysis of these datasets using statistical modelling and more advanced machine learning techniques designed to reconstruct and model the structure of biologically relevant pathways has contributed to reshaping biological investigation. Despite the paramount importance of this work, very little has been done to apply such network inference methodologies to reconstruct and model the structure of networks representing cell-to-cell communication events. The project we propose develops from preliminary work performed in the applicants research groups, aiming to develop the mathematical and experimental framework to infer the structure of regulatory pathways from both steady-state and temporal data. From an experimental prospective we will focus on the understanding of the general mechanisms involved in endothelia-tumor cell interactions. The project involves an initial component of genome-wide expression profiling and proteomics data generation followed by iterative cycles of computational modelling, hypothesis formulation and experimental verification using a set of state of the art gene modulation tools applicable to both in vitro and in vivo systems. Initially, we will identify informative genes and map these on known functional networks and protein-protein interaction networks in order to identify functionally interesting gene modules (for example derived from KEGG, BioGrid, REACTOME, etc). This modularization process will help in the selection of the genes to model using ARACNE, Bayesian networks and other similar computational approaches. The project will ultimately lead to the development of computational models representing cell-to-cell interaction networks from both time course and steady-state datasets. The analysis and Biological interpretation of these networks will allow us to develop hypothesis and experimentally verify them using gene manipulation techniques in both in vitro and in vivo systems. We expect this project to lead to the identification of regulatory circuits that are important in the interaction between endothelial and tumour cells during tumour growth and to a better understanding of the biological and molecular events associated to known factors involved in the tumour vascularization.    British Heart Foundation    Fellowship    246410.0GBP
22    Professor Tomaszewski Maciej    Manchester, University of    2018-08-01    2021-07-31    The BHF-Turing Cardiovascular Data Science Awards (Second Call): Molecular causal networks of hypertension – a machine learning approach (joint funding with The Alan Turing Institute)    The recent development of functional genomics technologies, particularly gene expression profiling, has provided the scientific community with the tools to characterize the molecular state of cells and tissues at a genomic level. The analysis of these datasets using statistical modelling and more advanced machine learning techniques designed to reconstruct and model the structure of biologically relevant pathways has contributed to reshaping biological investigation. Despite the paramount importance of this work, very little has been done to apply such network inference methodologies to reconstruct and model the structure of networks representing cell-to-cell communication events. The project we propose develops from preliminary work performed in the applicants research groups, aiming to develop the mathematical and experimental framework to infer the structure of regulatory pathways from both steady-state and temporal data. From an experimental prospective we will focus on the understanding of the general mechanisms involved in endothelia-tumor cell interactions. The project involves an initial component of genome-wide expression profiling and proteomics data generation followed by iterative cycles of computational modelling, hypothesis formulation and experimental verification using a set of state of the art gene modulation tools applicable to both in vitro and in vivo systems. Initially, we will identify informative genes and map these on known functional networks and protein-protein interaction networks in order to identify functionally interesting gene modules (for example derived from KEGG, BioGrid, REACTOME, etc). This modularization process will help in the selection of the genes to model using ARACNE, Bayesian networks and other similar computational approaches. The project will ultimately lead to the development of computational models representing cell-to-cell interaction networks from both time course and steady-state datasets. The analysis and Biological interpretation of these networks will allow us to develop hypothesis and experimentally verify them using gene manipulation techniques in both in vitro and in vivo systems. We expect this project to lead to the identification of regulatory circuits that are important in the interaction between endothelial and tumour cells during tumour growth and to a better understanding of the biological and molecular events associated to known factors involved in the tumour vascularization.    British Heart Foundation    Fellowship    24434.0GBP
23    Professor Tomaszewski Maciej    The University of Manchester    2018-08-01    2021-07-31    Target practice: informatic and metabolomic assessment of biological network changes and of drug-cell interactions    There are many occasions where one may wish to know the site of interaction of an effector molecule with a complex biological system (i.e. network), typically by measuring changes in the accessible state variables. These are usually ill-conditioned problems, in the sense that many models can account for the observable data, and to make progress it is necessary to apply constraints and simplifications of various kinds. In contrast to cognate analyses of signalling and gene regulatory networks, the analysis of METABOLIC networks and their fluxes is attractive since they NECESSARILY possess stoichiometric and thermodynamic constraints, which are known, and measurement of the molecules they excrete as end products creates further constraints on the fluxes through the different parts of the network. Initially using baker's yeast as a model organism, we wish to demonstrate that this strategy does indeed work. The necessary simplifications include the use of mass action and lin-log kinetics, while we shall develop and exploit modern methods of multivariate statistical optimisation and machine learning for parameter estimation. These include multi-objective evolutionary algorithms, and the exploitation of probabilistic graphical methods and Gaussian process models. We shall initially develop and test these strategies in baker's yeast, Saccharomyces cerevisiae, since this is a well understood organism. However, our collaborative partner Unilever are extremely interested in Corynebacterium jeikeium, for which a genome sequence and network model exist, and using resources made available by them for this project we shall also exploit these methods in the analysis of metabolic fluxes in this organism. The deliverable will be a suite of novel methods with which to infer the site of action of any effector in a reasonably well understood metabolic network.    Biotechnology and Biological Sciences Research Council    Fellowship    1287697.0GBP
24    Professor Kell Douglas    The University of Manchester    2018-08-01    2021-07-31    Target practice: informatic and metabolomic assessment of biological network changes and of drug-cell interactions    There are many occasions where one may wish to know the site of interaction of an effector molecule with a complex biological system (i.e. network), typically by measuring changes in the accessible state variables. These are usually ill-conditioned problems, in the sense that many models can account for the observable data, and to make progress it is necessary to apply constraints and simplifications of various kinds. In contrast to cognate analyses of signalling and gene regulatory networks, the analysis of METABOLIC networks and their fluxes is attractive since they NECESSARILY possess stoichiometric and thermodynamic constraints, which are known, and measurement of the molecules they excrete as end products creates further constraints on the fluxes through the different parts of the network. Initially using baker's yeast as a model organism, we wish to demonstrate that this strategy does indeed work. The necessary simplifications include the use of mass action and lin-log kinetics, while we shall develop and exploit modern methods of multivariate statistical optimisation and machine learning for parameter estimation. These include multi-objective evolutionary algorithms, and the exploitation of probabilistic graphical methods and Gaussian process models. We shall initially develop and test these strategies in baker's yeast, Saccharomyces cerevisiae, since this is a well understood organism. However, our collaborative partner Unilever are extremely interested in Corynebacterium jeikeium, for which a genome sequence and network model exist, and using resources made available by them for this project we shall also exploit these methods in the analysis of metabolic fluxes in this organism. The deliverable will be a suite of novel methods with which to infer the site of action of any effector in a reasonably well understood metabolic network.    Biotechnology and Biological Sciences Research Council    Fellowship    1287697.0GBP
25    Professor McKeigue Paul    University of Edinburgh    2009-03-16    2012-03-15    Development of Bayesian methods for genetic epidemiology    None    Medical Research Council    Research Grant    317659.0GBP
26    Professor Low Nicola Minling    University of Berne    2017-11-01    2021-10-31    Zika virus: causality, open science and risks of emerging infectious diseases    BackgroundZika virus infection was established as a cause of congenital abnormalities, including microcephaly, and of Guillain-Barré syndrome during a Public Health Emergency of International Concern that the World Health Organization (WHO) announced in February 2016. The Public Health Emergency ended in November 2016 but substantial gaps remain in the causality framework of Zika complications, knowledge about population level susceptibility to Zika virus infection and the risks of the newly recognised route of sexual transmission of Zika virus. Objectives1. To produce a web platform that will allow the production and updating of living systematic reviews of evidence about Zika virus infection; 2. To estimate key parameters that will allow refined inferences about the sexual transmissibility of Zika virus in endemic and non-endemic settings; 3. To investigate the seroprevalence of antibodies to Zika virus in different geographic settings and to use seroprevalence data to allow estimation of the duration of immunity after Zika virus infection. Methods1. We will produce an open access web application to produce living systematic reviews that allow continual updating of evidence about causal associations between Zika virus and its complications, and emerging research questions. The application will automate searching and deduplication, use text mining and machine learning to assist screening and allow automated updates of review output for rapid publication. 2. We have developed a sexual transmission framework to identify key parameters needed to understand the potential for ongoing spread of Zika virus through sexual transmission. We will analyse data to determine the duration of persistence of Zika virus in semen, vaginal fluid, urine, breast milk and other bodily fluids. We will then use a transmission model to estimate the per sex act probability of Zika virus transmission. 3. We will use data from ongoing longitudinal studies and repeated cross-sectional studies in Nicaragua that will determine antibody levels to Zika virus using new diagnostic tests (taking into account exposure to dengue and chikungunya). We will apply “back-calculation” methods to determine the duration of immunity of Zika virus infection. We will also pilot a method for the collection and assessment of seroprevalence data collected in a range of settings that have experienced new Zika transmission since 2013 and where Zika is presumed to be endemic to improve understanding of population level susceptibility to Zika virus infection.Timeline: The project will last four yearsImportance and impactThis project has considerable importance for research on Zika virus infection and transmission. Whilst vaccine development is advancing rapidly, there are still important gaps in our knowledge about vulnerability to Zika virus in large proportion of the world’s population that lives in areas where Aedes mosquito vectors are distributed. The project objectives are aligned with the research agenda of the WHO and with international initiatives to increase capacity for preparedness for infectious disease pandemics. The outputs are therefore relevant to current research priorities. By working within a culture of open science and with the living systematic review network, our research outputs, including publications and software will be publicly available as quickly as possible.    Swiss National Science Foundation    Project funding (Div. I-III)    700000.0CHF
27    Professor Ananiadou Sophia    The University of Manchester    2017-11-01    2021-10-31    Tools for the text mining-based visualisation of the provenance of biochemical networks    Systems biology is concerned with the modelling, visualisation and analysis of biochemical networks in which, for instance, metabolites are 'linked' by arrows representing the enzymes which turn one molecule into another or which are modified by particular substances. SBML provides a computer- readable 'standard' for describing such biochemical or signalling networks. However, these diagrams (and thus the SBML models) are divorced from the scientific evidence on which they are based, represented by the scientific literature (and increasingly by online databases). In order to overcome the problems of reading the burgeoning scientific literature, we shall deploy Text Mining TM. TM involves named entity recognition (i.e. semantic annotation of enzymes, metabolites, etc) and information extraction (i.e. relationship extraction between named entities). An important part of this proposal is to find solutions for the terminology problem in systems biology, by developing techniques for recognising synonym terms.Based on our efficient parsing techniques, we shall extract relationships between entities that will form the basis by which we shall can discover, index, store and display the scientific evidence for such linkages. The selection of the most pertinent relationships will be performed using our preferred methods of advanced machine learning (Support Vector Machines and Genetic Programming). The overall aim of the project is thus to develop and deploy the necessary TM tools and to use them to display the different relationships to the user together with the literature from which they have been extracted. The different types (and strength) of evidence for these interactions will then be visualised directly and linked to a dynamic website of the literature. This will thus give users a direct linkage between the systems biology diagrams encoded in (an advanced form of) SBML and the scientific evidence for them. Where available, linkages to kinetic data will also be made.    Biotechnology and Biological Sciences Research Council    Project funding (Div. I-III)    549458.0GBP
28    Professor Kell Douglas    The University of Manchester    2017-11-01    2021-10-31    Tools for the text mining-based visualisation of the provenance of biochemical networks    Systems biology is concerned with the modelling, visualisation and analysis of biochemical networks in which, for instance, metabolites are 'linked' by arrows representing the enzymes which turn one molecule into another or which are modified by particular substances. SBML provides a computer- readable 'standard' for describing such biochemical or signalling networks. However, these diagrams (and thus the SBML models) are divorced from the scientific evidence on which they are based, represented by the scientific literature (and increasingly by online databases). In order to overcome the problems of reading the burgeoning scientific literature, we shall deploy Text Mining TM. TM involves named entity recognition (i.e. semantic annotation of enzymes, metabolites, etc) and information extraction (i.e. relationship extraction between named entities). An important part of this proposal is to find solutions for the terminology problem in systems biology, by developing techniques for recognising synonym terms.Based on our efficient parsing techniques, we shall extract relationships between entities that will form the basis by which we shall can discover, index, store and display the scientific evidence for such linkages. The selection of the most pertinent relationships will be performed using our preferred methods of advanced machine learning (Support Vector Machines and Genetic Programming). The overall aim of the project is thus to develop and deploy the necessary TM tools and to use them to display the different relationships to the user together with the literature from which they have been extracted. The different types (and strength) of evidence for these interactions will then be visualised directly and linked to a dynamic website of the literature. This will thus give users a direct linkage between the systems biology diagrams encoded in (an advanced form of) SBML and the scientific evidence for them. Where available, linkages to kinetic data will also be made.    Biotechnology and Biological Sciences Research Council    Project funding (Div. I-III)    549458.0GBP
29    Professor Kell Douglas    University College London    2017-11-01    2021-10-31    Cerebellum as a neuronal machine: Behavioural, electrophysiological and computational analysis of classical conditioning    The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified and accurately measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single- unit electrophysiology and computational methods. (Joint with BBS/B/1700X and BBS/B/17026).    Biotechnology and Biological Sciences Research Council    Project funding (Div. I-III)    330321.0GBP
30    Professor Kell Douglas    University of Cambridge    2017-11-01    2021-10-31    Cerebellum as a neuronal machine: Behavioural, electrophysiological and computational analysis of classical conditioning    The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single-unit electrophysiology and computational methods. (Joint with BBS/B/17026 and BBS/B/16984).    Biotechnology and Biological Sciences Research Council    Project funding (Div. I-III)    272021.0GBP
31    Professor Kell Douglas    University of Sheffield    2017-11-01    2021-10-31    Cerebellum as a neuronal machine: Behavioural, electrophysiological and computational analysis of classical conditioning    The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single-unit electrophysiology and computational methods. (Joint with BBS/B/1700X and BBS/B/16984).    Biotechnology and Biological Sciences Research Council    Project funding (Div. I-III)    184698.0GBP
32    Professor Bath Philip    Nottingham,University of    2017-11-01    2021-10-31    Assessment of modern machine learning methods and conventional statistical regression techniques in diagnosis and prediction of outcome after acute stroke using big data    The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single-unit electrophysiology and computational methods. (Joint with BBS/B/1700X and BBS/B/16984).    British Heart Foundation    Project funding (Div. I-III)    145079.0GBP
33    Professor Wardlaw Joanna    Edinburgh, University of    2017-11-01    2021-10-31    The BHF-Turing Cardiovascular Data Science Awards (Second Call): Uncovering retinal microvascular predictors of compromised brain haemodynamics in small vessel disease (joint funding with The Alan Turing Institute)    The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single-unit electrophysiology and computational methods. (Joint with BBS/B/1700X and BBS/B/16984).    British Heart Foundation    Project funding (Div. I-III)    66000.0GBP
34    Professor Wardlaw Joanna    Edinburgh, University of    2017-11-01    2021-10-31    Inference and learning in machine vision    The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single-unit electrophysiology and computational methods. (Joint with BBS/B/1700X and BBS/B/16984).    Biotechnology and Biological Sciences Research Council    Project funding (Div. I-III)    66000.0GBP
35    Professor Wernisch Lorenz    University of Cambridge    2016-12-01    2019-03-31    Statistical bioinformatics and genetics    The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single-unit electrophysiology and computational methods. (Joint with BBS/B/1700X and BBS/B/16984).    Medical Research Council    Unit    66000.0GBP
36    Professor Wernisch Lorenz    Rothamsted Research    2016-12-01    2019-03-31    The Rothamsted Metabolomics Centre (MeT-RO)    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)    Biotechnology and Biological Sciences Research Council    Unit    1461648.0GBP
37    Professor Razavi Reza    King's College London    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)    Wellcome Trust    Innovations Priority Project    4030521.01GBP
38    Dr Yacoub Sophie    Oxford University Clinical Research Unit    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)    Wellcome Trust    Innovations Priority Project    4030521.01GBP
39    Dr Thwaites Catherine    University of Oxford    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)    Wellcome Trust    Innovations Priority Project    4030521.01GBP
40    Dr Modat Marc    King's College London    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)    Wellcome Trust    Innovations Priority Project    4030521.01GBP
41    Prof Dondorp Arjen    University of Oxford    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)    Wellcome Trust    Innovations Priority Project    4030521.01GBP
42    Dr Nguyen Vinh Chau    Oxford University Clinical Research Unit    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)    Wellcome Trust    Innovations Priority Project    4030521.01GBP
43    Prof Karlen Walter    ETH Zurich    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)    Wellcome Trust    Innovations Priority Project    4030521.01GBP
44    Dr Georgiou Pantelis    Imperial College London    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)    Wellcome Trust    Innovations Priority Project    4030521.01GBP
45    Prof Denehy Linda    University of Melbourne    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)    Wellcome Trust    Innovations Priority Project    4030521.01GBP
46    Prof Clifton David    University of Oxford    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)    Wellcome Trust    Innovations Priority Project    4030521.01GBP
47    Prof Day Nicholas    University of Oxford    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)    Wellcome Trust    Innovations Priority Project    4030521.01GBP
48    Prof Thwaites Guy    University of Oxford    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)    Wellcome Trust    Innovations Priority Project    4030521.01GBP
49    Prof Leff Alexander    University College London    2016-12-01    2021-11-30    Digital neuro-interventions to enhance re-learning in patients with acquired and degenerative brain diseases    Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)    National Institute for Health Research (Department of Health)    Full Award    2074551.0GBP
50    Professor Hubbard Simon    None    None    None    Improved identification of proteins from fragment ion spectra using machine learning in proteomics    None    Biotechnology and Biological Sciences Research Council    None    None
51    Professor Macleod Malcolm    University of Edinburgh    2016-04-01    2018-03-31    Pilot Study of the utility of text mining and machine learning tools to accelerate systematic review and meta-analysis of findings of in vivo research    None    Wellcome Trust    Discretionary Award – Directorate    176397.0GBP
52    Professor McMahon Stephen    King's College London    2017-01-01    2021-12-31    Stratifying Chronic Pain Patients By Pathological Mechanism- A Multimodal Investigation Using Functional MRI, Psychometric And Clinical Assessment    None    Medical Research Council    Research Grant    2718044.0GBP
53    Professor Wernisch Lorenz    MRC Biostatistics Unit    2000-04-01    2016-11-30    Statistical bioinformatics and genetics    None    Medical Research Council    Unit    2718044.0GBP
54    Professor Macleod Malcolm    University of Edinburgh    2016-04-01    2018-03-31    Pilot study of the utility of text mining and machine learning tools to accelerate systematic review and meta-analysis of findings of in vivo research    None    Medical Research Council    Research Grant    352793.0GBP
55    Professor Macleod Malcolm    University of Leeds    2007-02-01    2010-09-30    Cognitive Systems Foresight: Human Attention and Machine Learning    None    Wellcome Trust    Project Grant    67918.0GBP
56    Professor Macleod Malcolm    University of Bristol    2007-02-01    2010-09-30    Cognitive Systems Foresight: Human Attention and Machine Learning    None    Wellcome Trust    Project Grant    67918.0GBP
57    Professor Crook Derrick    University of Oxford    2016-10-01    2020-10-01    Comprehensive Resistance Prediction for Tuberculosis: an International Consortium (CRyPTIC)    None    Wellcome Trust    Collaborative Award in Science    4095865.0GBP
58    Professor Crook Derrick    University College London    2016-10-01    2020-10-01    Cerebellum as a neuronal machine: Behavioural, electrophysiological and computational analysis of classical conditioning    The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified and accurately measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single- unit electrophysiology and computational methods. (Joint with BBS/B/1700X and BBS/B/17026).    Biotechnology and Biological Sciences Research Council    Collaborative Award in Science    330321.0GBP
59    Professor Crook Derrick    University of Cambridge    2016-10-01    2020-10-01    Cerebellum as a neuronal machine: Behavioural, electrophysiological and computational analysis of classical conditioning    The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single-unit electrophysiology and computational methods. (Joint with BBS/B/17026 and BBS/B/16984).    Biotechnology and Biological Sciences Research Council    Collaborative Award in Science    272021.0GBP
60    Professor Crook Derrick    University of Sheffield    2016-10-01    2020-10-01    Cerebellum as a neuronal machine: Behavioural, electrophysiological and computational analysis of classical conditioning    The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single-unit electrophysiology and computational methods. (Joint with BBS/B/1700X and BBS/B/16984).    Biotechnology and Biological Sciences Research Council    Collaborative Award in Science    184698.0GBP
61    Professor Sir Bodmer Walter    University of Oxford    2009-07-12    2016-07-11    Genetics of the people of the british isles and their faces.    We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.    Wellcome Trust    Programme Grant    2067797.0GBP
62    Professor Ciccarelli Olga    University College London    2019-01-01    2023-12-31    Predicting individual treatment responses towards personalised medicine in Multiple Sclerosis    We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.    National Institute for Health Research (Department of Health)    Full Grant    1853695.0GBP
63    Professor Stein Ken    University of Exeter    2019-02-01    2021-01-31    Use of simulation and machine learning to identify key levers for maximising the disability benefit of intravenous thrombolysis in acute stroke pathways    We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.    National Institute for Health Research (Department of Health)    Full Grant    329261.0GBP
64    Professor Rockall Andrea    The Royal Marsden NHS Foundation Trust    2018-06-01    2021-08-31    MAchine Learning In MyelomA Response (MALIMAR study): Development of machine learning support for reading whole body diffusion weighted magnetic resonance imaging (WB-MRI) in myeloma for the detection and quantification of the extent of disease before and after treatment    We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.    National Institute for Health Research (Department of Health)    Full Award    646787.62GBP
65    Dr Shallcross Laura    University College London    2017-02-01    2022-01-31    Precision antibiotic prescribing for urinary tract infection in hospital    We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.    National Institute for Health Research (Department of Health)    Full Award    1127093.0GBP
66    Dr Stivaros Stavros    The University of Manchester    2006-09-04    2009-09-03    Language Based Decision Support System For Treatment Planning In Patients With Sub-arachnoid Haemorrhage.    We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.    Medical Research Council    Fellowship    192024.0GBP
67    Professor Razavi Reza    King's College London    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.    Wellcome Trust    Innovations Priority Project    4030521.01GBP
68    Dr Yacoub Sophie    Oxford University Clinical Research Unit    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.    Wellcome Trust    Innovations Priority Project    4030521.01GBP
69    Dr Thwaites Catherine    University of Oxford    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.    Wellcome Trust    Innovations Priority Project    4030521.01GBP
70    Dr Modat Marc    King's College London    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.    Wellcome Trust    Innovations Priority Project    4030521.01GBP
71    Prof Dondorp Arjen    University of Oxford    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.    Wellcome Trust    Innovations Priority Project    4030521.01GBP
72    Dr Nguyen Vinh Chau    Oxford University Clinical Research Unit    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.    Wellcome Trust    Innovations Priority Project    4030521.01GBP
73    Prof Karlen Walter    ETH Zurich    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.    Wellcome Trust    Innovations Priority Project    4030521.01GBP
74    Dr Georgiou Pantelis    Imperial College London    2019-09-01    2022-12-31    Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings    We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.    Wellcome Trust    Innovations Priority Project    4030521.01GBP
75    Professor Gaunt Tom    University of Bristol    2013-06-01    2018-03-31    Data mining and bioinformatics cross-cutting theme    None    Medical Research Council    Unit    None
76    Professor Akerman Colin    University of Oxford    2014-06-01    2015-05-31    A biologically inspired algorithm for training deep neural networks    'In machine learning, deep neural networks are powerful computer-based models that use layers of computational units. Current commercial applications for these models include a wide array of software tasks such as image classification, identification of potential drugs, market predictions and speech recognition. Network models must be ‘trained’ using data, and their success hinges critically on the quality of the learning algorithm that is employed. We have recently discovered a novel, biologically inspired algorithm for training deep neural networks that is simpler to implement, more flexible and finds better solutions than existing techniques on bench-mark tests. Thus, our system has the potential to improve performance widely across the many fields that make use of machine learning in software tasks. Furthermore, the simplicity and flexibility of our method means that it could be more easily exploited in hardware devices such as mobile phones and cameras. The central aim of this proposal is to move our new algorithm to a stage where it is ready for commercialization. To do this we plan to accomplish two main areas of work. First, we will research the optimal way to employ our algorithm, establish its performance on a comprehensive set of industry-accepted bench-mark tasks, and compile our research into a manuscript for publication in a leading machine learning journal. Second, we will secure any arising intellectual property in line with the preliminary US patent application that we have already filed, assess application of the algorithm to the different commercial sectors identified through market research, and generate commercial interest in the technology through targeted marketing to relevant companies. This plan of work will confirm the innovation potential of our new algorithm and will establish the technical and commercial feasibility of our discovery.'    European Research Council    Proof-of-Concept Grant    146761.0EUR
77    Prof Al-Chalabi Ammar    King's College London    2018-02-01    2021-01-31    JPND Biological Resource Analysis to Identify New MEchanisms and phenotypes in Neurodegenerative Diseases (BRAIN-MEND)    'In machine learning, deep neural networks are powerful computer-based models that use layers of computational units. Current commercial applications for these models include a wide array of software tasks such as image classification, identification of potential drugs, market predictions and speech recognition. Network models must be ‘trained’ using data, and their success hinges critically on the quality of the learning algorithm that is employed. We have recently discovered a novel, biologically inspired algorithm for training deep neural networks that is simpler to implement, more flexible and finds better solutions than existing techniques on bench-mark tests. Thus, our system has the potential to improve performance widely across the many fields that make use of machine learning in software tasks. Furthermore, the simplicity and flexibility of our method means that it could be more easily exploited in hardware devices such as mobile phones and cameras. The central aim of this proposal is to move our new algorithm to a stage where it is ready for commercialization. To do this we plan to accomplish two main areas of work. First, we will research the optimal way to employ our algorithm, establish its performance on a comprehensive set of industry-accepted bench-mark tasks, and compile our research into a manuscript for publication in a leading machine learning journal. Second, we will secure any arising intellectual property in line with the preliminary US patent application that we have already filed, assess application of the algorithm to the different commercial sectors identified through market research, and generate commercial interest in the technology through targeted marketing to relevant companies. This plan of work will confirm the innovation potential of our new algorithm and will establish the technical and commercial feasibility of our discovery.'    Medical Research Council    Research Grant    549837.0GBP
78    Professor Lord Darzi Ara    Imperial College London    2017-04-01    2022-03-31    Cancer Research UK Imperial Centre    The Cancer Research UK Imperial Centre will enhance cancer prevention, expedite early cancer detection, and improve the precision of cancer treatments and outcomes for patients, by leveraging Imperial’s core strengths in engineering, technology, physical sciences, imaging and systems medicine. Our vision is to unify technologies and platforms and re-orientate them in a co-ordinated effort to tackle cancer treatment and prevention. We will focus on reducing the burden of cancer by: improving the identification of high-risk populations; capitalising on advances in metabolic phenotyping; developing novel screening tests; enhancing screening uptake; improving clinical decision-making through machine learning and digital health systems. Our efforts to improve precision of cancer care will lead to improvements in: tumour boundary identification; cancer resectability via medical robotics, augmented reality, and intraoperative tissue characterisation; identification of risk for relapse through lab-on-chip technologies enabling interrogation of single cells, cell free DNA, and microRNAs; prediction of stage and spread by exploiting changes in microbiome composition; treatment monitoring of response and resistance by imaging apoptosis and studying epigenetic reprogramming. The Cancer Research UK Imperial Centre and Imperial Experimental Cancer Research Centre will utilise a broad and unique set of strengths across engineering and the physical sciences, surgery, imaging and diagnostics - underpinned with high-quality clinical practice - to realise a transformative research programme aimed at improving cancer survival for patients and the public.    Cancer Research UK    Research Grant    549837.0GBP
79    Professor McKenna Stephen    University of Dundee    2010-05-01    2011-04-30    Microscopic Image Analysis for Cell Biology    The Cancer Research UK Imperial Centre will enhance cancer prevention, expedite early cancer detection, and improve the precision of cancer treatments and outcomes for patients, by leveraging Imperial’s core strengths in engineering, technology, physical sciences, imaging and systems medicine. Our vision is to unify technologies and platforms and re-orientate them in a co-ordinated effort to tackle cancer treatment and prevention. We will focus on reducing the burden of cancer by: improving the identification of high-risk populations; capitalising on advances in metabolic phenotyping; developing novel screening tests; enhancing screening uptake; improving clinical decision-making through machine learning and digital health systems. Our efforts to improve precision of cancer care will lead to improvements in: tumour boundary identification; cancer resectability via medical robotics, augmented reality, and intraoperative tissue characterisation; identification of risk for relapse through lab-on-chip technologies enabling interrogation of single cells, cell free DNA, and microRNAs; prediction of stage and spread by exploiting changes in microbiome composition; treatment monitoring of response and resistance by imaging apoptosis and studying epigenetic reprogramming. The Cancer Research UK Imperial Centre and Imperial Experimental Cancer Research Centre will utilise a broad and unique set of strengths across engineering and the physical sciences, surgery, imaging and diagnostics - underpinned with high-quality clinical practice - to realise a transformative research programme aimed at improving cancer survival for patients and the public.    Medical Research Council    Research Grant    92895.0GBP
80    Professor Michie Susan    University College London    2016-11-21    2020-11-20    The Human Behaviour-Change Project: Building the science of behaviour change for complex intervention development    The Cancer Research UK Imperial Centre will enhance cancer prevention, expedite early cancer detection, and improve the precision of cancer treatments and outcomes for patients, by leveraging Imperial’s core strengths in engineering, technology, physical sciences, imaging and systems medicine. Our vision is to unify technologies and platforms and re-orientate them in a co-ordinated effort to tackle cancer treatment and prevention. We will focus on reducing the burden of cancer by: improving the identification of high-risk populations; capitalising on advances in metabolic phenotyping; developing novel screening tests; enhancing screening uptake; improving clinical decision-making through machine learning and digital health systems. Our efforts to improve precision of cancer care will lead to improvements in: tumour boundary identification; cancer resectability via medical robotics, augmented reality, and intraoperative tissue characterisation; identification of risk for relapse through lab-on-chip technologies enabling interrogation of single cells, cell free DNA, and microRNAs; prediction of stage and spread by exploiting changes in microbiome composition; treatment monitoring of response and resistance by imaging apoptosis and studying epigenetic reprogramming. The Cancer Research UK Imperial Centre and Imperial Experimental Cancer Research Centre will utilise a broad and unique set of strengths across engineering and the physical sciences, surgery, imaging and diagnostics - underpinned with high-quality clinical practice - to realise a transformative research programme aimed at improving cancer survival for patients and the public.    Wellcome Trust    Collaborative Award in Science    3796005.0GBP
81    Prof. Dr. TRINKA Eugen    SALK    2011-11-01    2016-10-31    Physiological Markers for the Prognosis of Memory Decline    The Cancer Research UK Imperial Centre will enhance cancer prevention, expedite early cancer detection, and improve the precision of cancer treatments and outcomes for patients, by leveraging Imperial’s core strengths in engineering, technology, physical sciences, imaging and systems medicine. Our vision is to unify technologies and platforms and re-orientate them in a co-ordinated effort to tackle cancer treatment and prevention. We will focus on reducing the burden of cancer by: improving the identification of high-risk populations; capitalising on advances in metabolic phenotyping; developing novel screening tests; enhancing screening uptake; improving clinical decision-making through machine learning and digital health systems. Our efforts to improve precision of cancer care will lead to improvements in: tumour boundary identification; cancer resectability via medical robotics, augmented reality, and intraoperative tissue characterisation; identification of risk for relapse through lab-on-chip technologies enabling interrogation of single cells, cell free DNA, and microRNAs; prediction of stage and spread by exploiting changes in microbiome composition; treatment monitoring of response and resistance by imaging apoptosis and studying epigenetic reprogramming. The Cancer Research UK Imperial Centre and Imperial Experimental Cancer Research Centre will utilise a broad and unique set of strengths across engineering and the physical sciences, surgery, imaging and diagnostics - underpinned with high-quality clinical practice - to realise a transformative research programme aimed at improving cancer survival for patients and the public.    Austrian Science Fund FWF    Clinical Research    312295.2EUR
82    Univ.Prof. Dr. SPEICHER Michael    Medical University of Graz    2019-10-01    2022-09-30    Breast cancer liquid biopsy stratification    Breast cancer is the most common cancer in Austrian women. Estimation of prognosis and treatment strategies is increasingly being dependent on stratification of tumors into different entities or classes. Currently, clinical routine stratification of tumors is mostly based on hormone receptor, HER2 status, and estimation of proliferation. However, a more robust and objective classification of tumors can be achieved by elucidation of further biological properties, which is also of increasing significance, as novel anticancer therapies are based on biological mechanisms. Consequently, available information from molecular analyses is increasingly being implemented in routine diagnostic assays with the aim to improve stratification for optimal treatment selection. To date the most extensive molecular-based taxonomy of breast cancer has been achieved by a classification based on combining gene expression and somatic copy number alterations (SCNAs), referred to as integrative clusters. Tissue biopsies are the current gold standard to attain such a classification. However, they can often be difficult to obtain in the metastatic setting and are subject to sampling bias due to intratumor heterogeneity. “Liquid biopsies” are, among other analytes, based on the analysis of cell-free DNA (cfDNA) which contains circulating tumor DNA (ctDNA), i.e. DNA fragments shed from normal and tumor cells into the blood, in patients with cancer. cfDNA can be obtained minimally invasive with a blood draw, allows for the “real time” analysis of tumor DNA from the circulation, and blood samples can be repeated at any time point, which is especially important for monitoring response to therapy. Our group has extensive expertise in the analysis of cfDNA and has developed a plethora of approaches for ctDNA analysis. Recently, we have developed a new approach, which relates to nucleosome positions and gene expression. cfDNA fragments have been associated with the release of DNA from apoptotic cells after enzymatic processing and hence consist mainly of mono-nucleosomal DNA. By performing whole-genome sequencing of cfDNA we could demonstrate that at transcriptional start sites, the nucleosome occupancy results in different read-depth coverage patterns in expressed and silent genes. By employing machine learning for gene classification, we were able to classify genes in cells releasing their DNA into the circulation as expressed. Our main hypothesis is that integrative breast cancer clusters can be established from directly blood without the need for an invasive tissue biopsy. Hence, our aims include refining stratification of patients for an improved selection of treatment strategies. Furthermore, we will obtain novel insights into the biology of metastatic breast cancer, so that this project will have important implications for patients, clinical oncologists, pathologists, pharmacologists, and all basic researchers interested in cancer.    Austrian Science Fund FWF    Clinical Research    387166.38EUR
83    Dr Miller Crispin    University of Manchester    2016-10-18    2020-10-17    noncoding RNA derived classifiers as biomarkers of patient response to therapy    Breast cancer is the most common cancer in Austrian women. Estimation of prognosis and treatment strategies is increasingly being dependent on stratification of tumors into different entities or classes. Currently, clinical routine stratification of tumors is mostly based on hormone receptor, HER2 status, and estimation of proliferation. However, a more robust and objective classification of tumors can be achieved by elucidation of further biological properties, which is also of increasing significance, as novel anticancer therapies are based on biological mechanisms. Consequently, available information from molecular analyses is increasingly being implemented in routine diagnostic assays with the aim to improve stratification for optimal treatment selection. To date the most extensive molecular-based taxonomy of breast cancer has been achieved by a classification based on combining gene expression and somatic copy number alterations (SCNAs), referred to as integrative clusters. Tissue biopsies are the current gold standard to attain such a classification. However, they can often be difficult to obtain in the metastatic setting and are subject to sampling bias due to intratumor heterogeneity. “Liquid biopsies” are, among other analytes, based on the analysis of cell-free DNA (cfDNA) which contains circulating tumor DNA (ctDNA), i.e. DNA fragments shed from normal and tumor cells into the blood, in patients with cancer. cfDNA can be obtained minimally invasive with a blood draw, allows for the “real time” analysis of tumor DNA from the circulation, and blood samples can be repeated at any time point, which is especially important for monitoring response to therapy. Our group has extensive expertise in the analysis of cfDNA and has developed a plethora of approaches for ctDNA analysis. Recently, we have developed a new approach, which relates to nucleosome positions and gene expression. cfDNA fragments have been associated with the release of DNA from apoptotic cells after enzymatic processing and hence consist mainly of mono-nucleosomal DNA. By performing whole-genome sequencing of cfDNA we could demonstrate that at transcriptional start sites, the nucleosome occupancy results in different read-depth coverage patterns in expressed and silent genes. By employing machine learning for gene classification, we were able to classify genes in cells releasing their DNA into the circulation as expressed. Our main hypothesis is that integrative breast cancer clusters can be established from directly blood without the need for an invasive tissue biopsy. Hence, our aims include refining stratification of patients for an improved selection of treatment strategies. Furthermore, we will obtain novel insights into the biology of metastatic breast cancer, so that this project will have important implications for patients, clinical oncologists, pathologists, pharmacologists, and all basic researchers interested in cancer.    Prostate Cancer UK    PhD Studentships    139722.0GBP
84    Dr Miller Crispin    University of Leeds    2016-10-18    2020-10-17    BBSRC Research Development Fellowship: Dr D R Westhead. From gene functional association to biomolecular networks in parasites and plants    1) To create databases of gene functional associations based on a variety of data sources for Arabidopsis and for the Apicomplexan parasites, and make these accessible to the associated research communities as web services. 2) To develop, compare and optimise machine learning-based methods to combine evidence from the sources above in prediction problems related to biomolecular networks, including missing enzymes in metabolic networks. 3) To make the results methods and predicted biomolecular networks available to the research community as web services.    Biotechnology and Biological Sciences Research Council    PhD Studentships    93700.0GBP
85    Dr Mourao-Miranda Janaina    University College London    2014-09-15    2020-03-14    Learning from neuroimaging and clinical data: a multiple-source machine learning approach for mental health disorders    This proposal aims to develop Multiple-Source Machine Learning models to investigate complex relationships between multivariate measures of brain anatomy or function (e.g. functional/structural Magnetic Resonance Imaging) and multidimensional descriptions of the mental health disorder and individual differences (e.g. clinical assessments, personality traits). Neuroimaging and machine learning techniques show potential as tools to identify biological measures that may help diagnosis and prognosi s of mental health disorders. However, so far, most of the studies using these techniques have focused on binary classification problems using a single imaging modality, i.e. they summarize the clinical assessment into a single measure and the output of the models is limited to a probability value and in most cases a binary decision (patients/healthy control). Although these studies represent an important advance in the field, they do not enable patient stratification and provide limited informa tion about the underlying biological mechanisms of the diseases. Considering the complexity of mental health disorders, it is potentially beneficial to embed a multidimensional description of the disorder into the models. The aim of this proposal is to move away from treating neuroimaging-based diagnosis as a binary classification problem towards models that: (i) are able to merge multi-modal neuroimaging and clinical information for diagnoses and prognoses of psychiatric disorders; (ii) can im prove stratification of patients with mental health disorders (e.g. identify subgroups of patients for helping treatment allocation or illness course prediction); (iii) are able to deal with large multi-center datasets; (iv) provide insights about underlying biological mechanisms of the diseases (e.g. biological markers).    Wellcome Trust    Senior Research Fellowship Basic    1295649.0GBP
86    Prof. Yau Christopher    University of Birmingham    2017-10-09    2020-01-06    From single cells to populations: generalized pseudotime analysis to identify patient trajectories from cross-sectional data in cancer genomics    This proposal aims to develop Multiple-Source Machine Learning models to investigate complex relationships between multivariate measures of brain anatomy or function (e.g. functional/structural Magnetic Resonance Imaging) and multidimensional descriptions of the mental health disorder and individual differences (e.g. clinical assessments, personality traits). Neuroimaging and machine learning techniques show potential as tools to identify biological measures that may help diagnosis and prognosi s of mental health disorders. However, so far, most of the studies using these techniques have focused on binary classification problems using a single imaging modality, i.e. they summarize the clinical assessment into a single measure and the output of the models is limited to a probability value and in most cases a binary decision (patients/healthy control). Although these studies represent an important advance in the field, they do not enable patient stratification and provide limited informa tion about the underlying biological mechanisms of the diseases. Considering the complexity of mental health disorders, it is potentially beneficial to embed a multidimensional description of the disorder into the models. The aim of this proposal is to move away from treating neuroimaging-based diagnosis as a binary classification problem towards models that: (i) are able to merge multi-modal neuroimaging and clinical information for diagnoses and prognoses of psychiatric disorders; (ii) can im prove stratification of patients with mental health disorders (e.g. identify subgroups of patients for helping treatment allocation or illness course prediction); (iii) are able to deal with large multi-center datasets; (iv) provide insights about underlying biological mechanisms of the diseases (e.g. biological markers).    Medical Research Council    Research Grant    325589.0GBP
87    Prof. Yau Christopher    The University of Manchester    2017-10-09    2020-01-06    Novel force fields devised using machine learning    The potential energy functions used in biomoelcular modelling must be improved in order to secure its future predictive power and trust of the experimental community. In our previous work we replaced point charges by atomic multipole moments in order to improve the accuracy of the (short-range) electrostatic interaction. Quantum Chemical Topology (aka atoms in Molecules") uses molecular electron density to define atoms as finite volumes shaped by the molecular enviroment they occur in. In this proposal we focus on atomic polarisation, i.e. how the electron density of a topological atom fluctuates upon a change in nuclear positions of its neighbouring atoms. We do not follow an existing method (such as the Drude/shell model, electronegativity equalisation, effective polarisation, polarisable point dipoles, etc.). Instead we adopt a modern machine learning method called Genetic Programming to directly establish the link between an atom's electron density reponsing to a geometrical change in immediate environment. Summarising the proposed method, focus on a given atom in a molecule. Normal modes generate hundreds of molecular geometries, representing the ever changing environment of an atom, including bond length and angle variations, and conformational flexibility. Quantum Chemical Topology cuts this given atom out of each of the distorted molecular charge density and calculates the corresponding atomic multipole moments. The latter fluctuate in response to a change in molecular geometry and it is these data that the Genetic Programming algorithm is trained on. Our proposed method is totally new, which is why we need this feasible study."    Biotechnology and Biological Sciences Research Council    Research Grant    105463.0GBP
88    Professor Relton Caroline    University of Bristol    2013-06-01    2018-03-31    Epigenetic Epidemiology    The potential energy functions used in biomoelcular modelling must be improved in order to secure its future predictive power and trust of the experimental community. In our previous work we replaced point charges by atomic multipole moments in order to improve the accuracy of the (short-range) electrostatic interaction. Quantum Chemical Topology (aka atoms in Molecules") uses molecular electron density to define atoms as finite volumes shaped by the molecular enviroment they occur in. In this proposal we focus on atomic polarisation, i.e. how the electron density of a topological atom fluctuates upon a change in nuclear positions of its neighbouring atoms. We do not follow an existing method (such as the Drude/shell model, electronegativity equalisation, effective polarisation, polarisable point dipoles, etc.). Instead we adopt a modern machine learning method called Genetic Programming to directly establish the link between an atom's electron density reponsing to a geometrical change in immediate environment. Summarising the proposed method, focus on a given atom in a molecule. Normal modes generate hundreds of molecular geometries, representing the ever changing environment of an atom, including bond length and angle variations, and conformational flexibility. Quantum Chemical Topology cuts this given atom out of each of the distorted molecular charge density and calculates the corresponding atomic multipole moments. The latter fluctuate in response to a change in molecular geometry and it is these data that the Genetic Programming algorithm is trained on. Our proposed method is totally new, which is why we need this feasible study."    Medical Research Council    Unit    105463.0GBP
89    Professor Relton Caroline    Rothamsted Research    2013-06-01    2018-03-31    A systems approach to candidate gene and pathway identification    This project will develop integrated bioinformatics approaches for identifying new genes and pathways implicated in a range of biological processes relevant to plant pathology and crop science. This work will in part build on earlier research into data integration and graph-based analysis methods that have been used in the development of the ONDEX data integration framework for systems biology applications. Research efforts will be directed in the following areas: 1. Methods and software for integrating new sources of biological information including gene function, biochemical pathway, protein structure, gene network and other information (including that extracted from biological literature through text mining) will be developed in response to requirements derived from the biological applications to be studied. 2. Comparative genome analysis and visualisation methods will be developed to exploit the information from model organisms that have more complete genome and pathway information. These methods will be applied to the prediction of new candidate gene and pathway functions from partially or newly sequenced crop, pest and pathogen genomes. 3. Machine learning approaches (including graphical data analysis, data mining and visualisation) will be developed to extract new biological insights from integrated datasets with the aim of identifying novel interactions at the gene and pathway level for the purpose of predicting new gene function, annotation of emerging genomes and other 'omics resources. 4. Specialist databases of gene function (e.g. PHI-base) will be further developed in collaboration with biologists as resources for the research community and to underpin research in the above areas.    Biotechnology and Biological Sciences Research Council    Unit    761116.0GBP
90    Dr Bakal Chris    Institute of Cancer Research    2011-12-01    2014-11-30    Quantifying the relationship between cancer cell genotype and phenotype. Determination and validation of the gene expression changes that drive the specific morphological changes essential for breast cancer metastasis.    Background Despite the vast amount of data that have been generated in recent years on cancer cell genotypes, a significant challenge in the post-genomic era will be to understand how genomic alterations drive cellular phenotypes, such as the dramatic changes in morphogenesis that occur during metastasis. We have previously developed technologies to quantify the morphology of single cells and cell populations to precisely investigate changes in cell shape (Bakal et al., Science 2007). We are currently using these methods to quantify the morphology of a panel of breast cancer lines in 2D and 3D culture. Using these methods, we can also quantify hetereogeneity and identify morphologically distinct subpoluations within in isogenic cell lines. Genome-wide mRNA expression levels and genomic copy number variations have been determined for all the lines we are investigating by the Reis-Filho laboratory at the ICR/Breakthrough Breast Cancer Center. This allows us to perform statistical analysis to find patterns of gene expression that relate to quantifiable differences in cell shape. In addition, we are working in collaboration with The Cancer Genome Atlas Project to profile the genotypes and morphological phenotypes parallel following treatment with different small molecules. In this project, we aim to determine how genotypic alterations are linked to specific changes in morphological in cancer cells. Aims (1) To classify a panel of 20-40 breast cancer cell lines based on morphology and identify patterns of gene expression that are correlated with and/or control cell shape. (2) To perform quantitative morphometric analysis of cell invasion in 3D matrices. (3) To determine the whether highly invasive cells represent stable subpopulations within tumour cell lines. Methods In order to complete these studies we will use: - Automated high-throughput image acquisition and feature analysis of single cells in both 2D and 3D. The Bakal laboratory is equipped with two Opera microscopes, each capable of capturing over 100,000 unique images per day. - High-throughput analysis of 3D morphology. - Machine-learning methods to quantify and classify independent phenotypes. The ICR has recently acquired an Altix UV computer in order to facilitate this work. - Statistical analysis to determine correlations between gene expression, copy number and morphological features. - RNAi and small-molecule perturbation. The results of this work will be primarily used to identify and validate genes whose changes in expression are causal to the morphogenesis of metastatic cells.    Cancer Research UK    Biological Sciences Committee - Project Award    761116.0GBP
91    Dr. MÜLLER-MANG Christina    Medical University of Vienna    2010-12-01    2014-11-30    Computerized 3D pulmonary architecture analysis    Background Despite the vast amount of data that have been generated in recent years on cancer cell genotypes, a significant challenge in the post-genomic era will be to understand how genomic alterations drive cellular phenotypes, such as the dramatic changes in morphogenesis that occur during metastasis. We have previously developed technologies to quantify the morphology of single cells and cell populations to precisely investigate changes in cell shape (Bakal et al., Science 2007). We are currently using these methods to quantify the morphology of a panel of breast cancer lines in 2D and 3D culture. Using these methods, we can also quantify hetereogeneity and identify morphologically distinct subpoluations within in isogenic cell lines. Genome-wide mRNA expression levels and genomic copy number variations have been determined for all the lines we are investigating by the Reis-Filho laboratory at the ICR/Breakthrough Breast Cancer Center. This allows us to perform statistical analysis to find patterns of gene expression that relate to quantifiable differences in cell shape. In addition, we are working in collaboration with The Cancer Genome Atlas Project to profile the genotypes and morphological phenotypes parallel following treatment with different small molecules. In this project, we aim to determine how genotypic alterations are linked to specific changes in morphological in cancer cells. Aims (1) To classify a panel of 20-40 breast cancer cell lines based on morphology and identify patterns of gene expression that are correlated with and/or control cell shape. (2) To perform quantitative morphometric analysis of cell invasion in 3D matrices. (3) To determine the whether highly invasive cells represent stable subpopulations within tumour cell lines. Methods In order to complete these studies we will use: - Automated high-throughput image acquisition and feature analysis of single cells in both 2D and 3D. The Bakal laboratory is equipped with two Opera microscopes, each capable of capturing over 100,000 unique images per day. - High-throughput analysis of 3D morphology. - Machine-learning methods to quantify and classify independent phenotypes. The ICR has recently acquired an Altix UV computer in order to facilitate this work. - Statistical analysis to determine correlations between gene expression, copy number and morphological features. - RNAi and small-molecule perturbation. The results of this work will be primarily used to identify and validate genes whose changes in expression are causal to the morphogenesis of metastatic cells.    Austrian Science Fund FWF    Stand-Alone Project    761116.0GBP
92    Professor Ananiadou Sophia    The University of Manchester    2009-09-01    2012-08-31    Automated Biological Event Extraction from the Literature for Drug Discovery    In establishing drug target confidence, it is essential to have evidence of the type of relationship between the target and key protein-bioprocesses. However, the primary starting point for target choice, and the context for interpretation of all pre-clinical observations is the literature. Text mining (TM) is ideally suited to support the discovery of reliable drug targets. But for TM systems to help researchers understand the role proteins play in biological processes, they have to extract, normalise and identify the context of complex relationships between genes, diseases and their underlying bioprocesses. Our TM techniques will recognise diverse surface forms in text describing bioprocesses and will link them with events and the proteins associated with them. Our methods are based on a combination of advanced semantic text mining (deep parsing, named entity recognition) and machine learning techniques, as we shall automatically identify events (involving proteins) such as decrease [in concentration], phosphorylation, ubiquitination, etc. Bioprocesses such as angiogenesis are composed of individual events described in the literature. We propose to identify these bioprocesses automatically and to link them with the associated events. A combination of kernel methods with knowledge resources and annotated texts (evaluated by biologists) will be used to automatically learn how bioprocesses underlying higher level processes are linked with which events. We shall concentrate on angiogenesis as an example. We shall thereby produce and make available a text mining service for researchers working in drug discovery. Both the software tools used for event extraction as well as the annotated texts used for training purposes will be made available. Co-funded by EPSRC under the RCUK Cross-Council Funding Agreement.    Biotechnology and Biological Sciences Research Council    Industrial (IPA)    288468.0GBP
93    Dr Savage Richard Stephen    University of Warwick    2010-06-01    2014-05-31    Applications of probabilistic machine learning to medical biostatistics    In establishing drug target confidence, it is essential to have evidence of the type of relationship between the target and key protein-bioprocesses. However, the primary starting point for target choice, and the context for interpretation of all pre-clinical observations is the literature. Text mining (TM) is ideally suited to support the discovery of reliable drug targets. But for TM systems to help researchers understand the role proteins play in biological processes, they have to extract, normalise and identify the context of complex relationships between genes, diseases and their underlying bioprocesses. Our TM techniques will recognise diverse surface forms in text describing bioprocesses and will link them with events and the proteins associated with them. Our methods are based on a combination of advanced semantic text mining (deep parsing, named entity recognition) and machine learning techniques, as we shall automatically identify events (involving proteins) such as decrease [in concentration], phosphorylation, ubiquitination, etc. Bioprocesses such as angiogenesis are composed of individual events described in the literature. We propose to identify these bioprocesses automatically and to link them with the associated events. A combination of kernel methods with knowledge resources and annotated texts (evaluated by biologists) will be used to automatically learn how bioprocesses underlying higher level processes are linked with which events. We shall concentrate on angiogenesis as an example. We shall thereby produce and make available a text mining service for researchers working in drug discovery. Both the software tools used for event extraction as well as the annotated texts used for training purposes will be made available. Co-funded by EPSRC under the RCUK Cross-Council Funding Agreement.    Medical Research Council    Fellowship    351627.0GBP
94    Professor Gadegaard Nikolaj    University of Glasgow    2015-08-01    2020-07-31    Focal Adhesion Kinetics In nanosurface Recognition    The provision of advanced functional materials in the area of regenerative medicine and discovery applications depends on many different factors to provide the appropriate targeted function. As adherent cells also read their environment through substrate interactions there is a great interest in developing such substrates in a predictable manner. Their first point of contact is through their focal adhesions and it is also though them that forces are applied allowing the cell to migrate and establish cytoskeletal tension which in turn regulates cell function. The objective of this project is to investigate the cell-substrate interaction at the nanoscale and correlate that to the surface topography for predictable biomaterials. Through the application of state-of-the-art nanofabrication we will fabricate precise surface topographies with length scales comparable to the structural units found in the focal adhesions. The aim is to map and understand the topographical influence in the architectural arrangement of the proteins in the adhesions. Aided by high resolution microscopy we will classify cell types on different nanotopographies. Combining that information with machine learning, we will be able to gain information about cell characteristics from the rule set. That information can also be used in reverse to identify cell types with the previously defined characteristic. This approach is similar to face recognition seen on cameras and mobile phones. The proposed research project will not only provide insight to an area of biomaterials not previously explored, yet aim to provide a blueprint for future design of biomaterials.    European Research Council    Consolidator Grant    2128895.0EUR
95    Professor Gadegaard Nikolaj    Rothamsted Research    2015-08-01    2020-07-31    Integration of 'omics databases and novel approaches to data analysis and annotation    The objective is to develop a common data infrastructure for integrating data from relevant technology platforms and enable linkage with other 'omics (proteomics, metabolomics) data available in Rothamsted Research. These data will be combined with public domain sources of gene annotation and classification that will facilitate the analysis and interpretation of experimental results. The intial focus will be on interpretation of microarray data. This system will create a platform for the development of new methods for data analysis and gene annotation based on graph theoretic, statistical, machine learning and visualisation methods.    Biotechnology and Biological Sciences Research Council    Consolidator Grant    528470.0GBP
96    Dr Cader Zameel    University of Oxford    2017-03-01    2021-03-31    University of Oxford Momentum Award – Dementias    Research and drug discovery for Alzheimer’s disease (AD) is undergoing a minor crisis with a paucity of novel therapeutic approaches that is human evidence-based. Whilst, human genetic studies have identified AD associated variants important for CNS glial function, there is uncertainty over whether inflammation plays a beneficial or deleterious role. As a result, many pharma companies are hesitant before committing significant resources to this novel therapeutic opportunity. Our proposal is therefore focused on the scientific theme “Microglial interactions in Alzheimer’s Disease”. The indicative projects will leverage four key capabilities within the University of Oxford that represent untapped potential and are ripe for development for dementia research, (1) access to human primary CNS cell types from human tissue coupled with single cell analysis, (2) Human iPSC derived microglia, (3) High order imaging analysis of cellular phenotypes driven by machine learning, (4) Protein-protein interaction disruption to probe signalling pathways. The proposed projects approaches neuro-inflammation in AD through these technical capabilities and are supported by world-leading bioinformatics and unrivalled structural genomics consortium (SGC) pharmacological tools. These tools enable pharmacological pilot studies within the projects, which if successful, will more rapidly lead to drug discovery projects because of the enabling structural knowledge and chemical starting points held within the SGC. Momentum funds will support the recruitment of a new research leader from outside the UK, as well as supporting early career high flyers within Oxford. Our pharma partners and institution leverages the MRC funds with significant institutional funds, platforms, space and expertise totalling ~£2million (~£850K from the institution and $1.5million industry in-kind). We have developed strong partnerships with Oxford University Hospital NHS Trust and industry including world-leaders in single cell biology and dementia drug discovery. The Award will support the diversification of target identification and develop human in vitro models that are more likely to be predictive of clinical outcomes compared to current dominant murine models. Our approach will facilitate synergy between disciplines and maximise the contribution of the grant towards the acceleration of new dementia research in Oxford.    Medical Research Council    P&Cs    1005900.0GBP
97    Prof Sir Lovestone Simon    University of Oxford    2016-10-01    2021-09-30    Deep and Frequent Phenotyping; combinatorial biomarkers for dementia experimental medicine    Research and drug discovery for Alzheimer’s disease (AD) is undergoing a minor crisis with a paucity of novel therapeutic approaches that is human evidence-based. Whilst, human genetic studies have identified AD associated variants important for CNS glial function, there is uncertainty over whether inflammation plays a beneficial or deleterious role. As a result, many pharma companies are hesitant before committing significant resources to this novel therapeutic opportunity. Our proposal is therefore focused on the scientific theme “Microglial interactions in Alzheimer’s Disease”. The indicative projects will leverage four key capabilities within the University of Oxford that represent untapped potential and are ripe for development for dementia research, (1) access to human primary CNS cell types from human tissue coupled with single cell analysis, (2) Human iPSC derived microglia, (3) High order imaging analysis of cellular phenotypes driven by machine learning, (4) Protein-protein interaction disruption to probe signalling pathways. The proposed projects approaches neuro-inflammation in AD through these technical capabilities and are supported by world-leading bioinformatics and unrivalled structural genomics consortium (SGC) pharmacological tools. These tools enable pharmacological pilot studies within the projects, which if successful, will more rapidly lead to drug discovery projects because of the enabling structural knowledge and chemical starting points held within the SGC. Momentum funds will support the recruitment of a new research leader from outside the UK, as well as supporting early career high flyers within Oxford. Our pharma partners and institution leverages the MRC funds with significant institutional funds, platforms, space and expertise totalling ~£2million (~£850K from the institution and $1.5million industry in-kind). We have developed strong partnerships with Oxford University Hospital NHS Trust and industry including world-leaders in single cell biology and dementia drug discovery. The Award will support the diversification of target identification and develop human in vitro models that are more likely to be predictive of clinical outcomes compared to current dominant murine models. Our approach will facilitate synergy between disciplines and maximise the contribution of the grant towards the acceleration of new dementia research in Oxford.    Medical Research Council    Research Grant    6301078.0GBP
98    Professor Brohi Karim    Queen Mary University of London    2019-01-01    2021-12-31    Defining and predicting the innate immune response to critical injury    Research and drug discovery for Alzheimer’s disease (AD) is undergoing a minor crisis with a paucity of novel therapeutic approaches that is human evidence-based. Whilst, human genetic studies have identified AD associated variants important for CNS glial function, there is uncertainty over whether inflammation plays a beneficial or deleterious role. As a result, many pharma companies are hesitant before committing significant resources to this novel therapeutic opportunity. Our proposal is therefore focused on the scientific theme “Microglial interactions in Alzheimer’s Disease”. The indicative projects will leverage four key capabilities within the University of Oxford that represent untapped potential and are ripe for development for dementia research, (1) access to human primary CNS cell types from human tissue coupled with single cell analysis, (2) Human iPSC derived microglia, (3) High order imaging analysis of cellular phenotypes driven by machine learning, (4) Protein-protein interaction disruption to probe signalling pathways. The proposed projects approaches neuro-inflammation in AD through these technical capabilities and are supported by world-leading bioinformatics and unrivalled structural genomics consortium (SGC) pharmacological tools. These tools enable pharmacological pilot studies within the projects, which if successful, will more rapidly lead to drug discovery projects because of the enabling structural knowledge and chemical starting points held within the SGC. Momentum funds will support the recruitment of a new research leader from outside the UK, as well as supporting early career high flyers within Oxford. Our pharma partners and institution leverages the MRC funds with significant institutional funds, platforms, space and expertise totalling ~£2million (~£850K from the institution and $1.5million industry in-kind). We have developed strong partnerships with Oxford University Hospital NHS Trust and industry including world-leaders in single cell biology and dementia drug discovery. The Award will support the diversification of target identification and develop human in vitro models that are more likely to be predictive of clinical outcomes compared to current dominant murine models. Our approach will facilitate synergy between disciplines and maximise the contribution of the grant towards the acceleration of new dementia research in Oxford.    Medical Research Council    Research Grant    874563.0GBP
99    Prof Rappsilber Juri    University of Edinburgh    2019-10-09    2022-10-08    ProCos: Protein co-regulation scores as a new resource for systematic and large-scale protein function annotation    Research and drug discovery for Alzheimer’s disease (AD) is undergoing a minor crisis with a paucity of novel therapeutic approaches that is human evidence-based. Whilst, human genetic studies have identified AD associated variants important for CNS glial function, there is uncertainty over whether inflammation plays a beneficial or deleterious role. As a result, many pharma companies are hesitant before committing significant resources to this novel therapeutic opportunity. Our proposal is therefore focused on the scientific theme “Microglial interactions in Alzheimer’s Disease”. The indicative projects will leverage four key capabilities within the University of Oxford that represent untapped potential and are ripe for development for dementia research, (1) access to human primary CNS cell types from human tissue coupled with single cell analysis, (2) Human iPSC derived microglia, (3) High order imaging analysis of cellular phenotypes driven by machine learning, (4) Protein-protein interaction disruption to probe signalling pathways. The proposed projects approaches neuro-inflammation in AD through these technical capabilities and are supported by world-leading bioinformatics and unrivalled structural genomics consortium (SGC) pharmacological tools. These tools enable pharmacological pilot studies within the projects, which if successful, will more rapidly lead to drug discovery projects because of the enabling structural knowledge and chemical starting points held within the SGC. Momentum funds will support the recruitment of a new research leader from outside the UK, as well as supporting early career high flyers within Oxford. Our pharma partners and institution leverages the MRC funds with significant institutional funds, platforms, space and expertise totalling ~£2million (~£850K from the institution and $1.5million industry in-kind). We have developed strong partnerships with Oxford University Hospital NHS Trust and industry including world-leaders in single cell biology and dementia drug discovery. The Award will support the diversification of target identification and develop human in vitro models that are more likely to be predictive of clinical outcomes compared to current dominant murine models. Our approach will facilitate synergy between disciplines and maximise the contribution of the grant towards the acceleration of new dementia research in Oxford.    Wellcome Trust    Biomedical Resources Grant    237247.0GBP
100    Professor Lyons Ronan    Swansea University    2019-10-01    2022-03-31    Application of machine learning to discover new multimorbidity phenotypes associated with poorer outcomes    None    Medical Research Council    Research Grant    563132.0GBP
101    Univ.Prof. Dipl.Ing. Dr. TRAJANOSKI Zlatko    MEDIZINISCHE UNIVERSITAT INNSBRUCK    2018-10-01    2023-09-30    Enabling Precision Immuno-oncology in Colorectal cancer    Immunotherapy with checkpoints blockers is transforming the treatment of advanced cancers. Colorectal cancer (CRC), a cancer with 1.4 million new cases diagnosed annually worldwide, is refractory to immunotherapy (with the exception of a minority of tumors with microsatellite instability). This is somehow paradoxical as CRC is a cancer for which we have shown that it is under immunological control and that tumor infiltrating lymphocytes represent a strong independent predictor of survival. Thus, there is an urgent need to broaden the clinical benefits of immune checkpoint blockers to CRC by combining agents with synergistic mechanisms of action. An attractive approach to sensitize tumors to immunotherapy is to harness immunogenic effects induced by approved conventional or targeted agents. Here I propose a new paradigm to identify molecular determinants of resistance to immunotherapy and develop personalized in silico and in vitro models for predicting response to combination therapy in CRC. The EPIC concept is based on three pillars: 1) emphasis on antitumor T cell activity; 2) systematic interrogation of tumor-immune cell interactions using data-driven modeling and knowledge-based mechanistic modeling, and 3) generation of key quantitative data to train and validate algorithms using perturbation experiments with patient-derived tumor organoids and cutting-edge technologies for multidimensional profiling. We will investigate three immunomodulatory processes: 1) immunostimulatory effects of chemotherapeutics, 2) rewiring of signaling networks induced by targeted drugs and their interference with immunity, and 3) metabolic reprogramming of T cells to enhance antitumor immunity. The anticipated outcome of EPIC is a precision immuno-oncology platform that integrates tumor organoids with high-throughput and high-content data for testing drug combinations, and machine learning for making therapeutic recommendations for individual patients.    European Research Council    Advanced Grant    2460500.0EUR
102    Dr. GOEBL Werner    University of Music and Performing Arts Vienna    2012-11-01    2016-09-30    Performing Together:Synchronisation and Communication in Music Ensembles    Immunotherapy with checkpoints blockers is transforming the treatment of advanced cancers. Colorectal cancer (CRC), a cancer with 1.4 million new cases diagnosed annually worldwide, is refractory to immunotherapy (with the exception of a minority of tumors with microsatellite instability). This is somehow paradoxical as CRC is a cancer for which we have shown that it is under immunological control and that tumor infiltrating lymphocytes represent a strong independent predictor of survival. Thus, there is an urgent need to broaden the clinical benefits of immune checkpoint blockers to CRC by combining agents with synergistic mechanisms of action. An attractive approach to sensitize tumors to immunotherapy is to harness immunogenic effects induced by approved conventional or targeted agents. Here I propose a new paradigm to identify molecular determinants of resistance to immunotherapy and develop personalized in silico and in vitro models for predicting response to combination therapy in CRC. The EPIC concept is based on three pillars: 1) emphasis on antitumor T cell activity; 2) systematic interrogation of tumor-immune cell interactions using data-driven modeling and knowledge-based mechanistic modeling, and 3) generation of key quantitative data to train and validate algorithms using perturbation experiments with patient-derived tumor organoids and cutting-edge technologies for multidimensional profiling. We will investigate three immunomodulatory processes: 1) immunostimulatory effects of chemotherapeutics, 2) rewiring of signaling networks induced by targeted drugs and their interference with immunity, and 3) metabolic reprogramming of T cells to enhance antitumor immunity. The anticipated outcome of EPIC is a precision immuno-oncology platform that integrates tumor organoids with high-throughput and high-content data for testing drug combinations, and machine learning for making therapeutic recommendations for individual patients.    Austrian Science Fund FWF    Stand-Alone Project    351237.09EUR
103    Univ.Prof. Dr. SCHMIDT Reinhold    Medical University of Graz    2012-03-01    2016-02-29    Mechanisms of Small Vessel Related Brain Damage and Cognitive Impairment    Immunotherapy with checkpoints blockers is transforming the treatment of advanced cancers. Colorectal cancer (CRC), a cancer with 1.4 million new cases diagnosed annually worldwide, is refractory to immunotherapy (with the exception of a minority of tumors with microsatellite instability). This is somehow paradoxical as CRC is a cancer for which we have shown that it is under immunological control and that tumor infiltrating lymphocytes represent a strong independent predictor of survival. Thus, there is an urgent need to broaden the clinical benefits of immune checkpoint blockers to CRC by combining agents with synergistic mechanisms of action. An attractive approach to sensitize tumors to immunotherapy is to harness immunogenic effects induced by approved conventional or targeted agents. Here I propose a new paradigm to identify molecular determinants of resistance to immunotherapy and develop personalized in silico and in vitro models for predicting response to combination therapy in CRC. The EPIC concept is based on three pillars: 1) emphasis on antitumor T cell activity; 2) systematic interrogation of tumor-immune cell interactions using data-driven modeling and knowledge-based mechanistic modeling, and 3) generation of key quantitative data to train and validate algorithms using perturbation experiments with patient-derived tumor organoids and cutting-edge technologies for multidimensional profiling. We will investigate three immunomodulatory processes: 1) immunostimulatory effects of chemotherapeutics, 2) rewiring of signaling networks induced by targeted drugs and their interference with immunity, and 3) metabolic reprogramming of T cells to enhance antitumor immunity. The anticipated outcome of EPIC is a precision immuno-oncology platform that integrates tumor organoids with high-throughput and high-content data for testing drug combinations, and machine learning for making therapeutic recommendations for individual patients.    Austrian Science Fund FWF    International programmes    191457.0EUR
104    Dr Colwell Lucy    University of Cambridge    2019-10-01    2021-10-01    Next Generation Drug Discovery Enabled by Digital Molecular Technologies: Machine learning enabled high throughput synthesis to repurpose drugs and failed clinical candidates    Immunotherapy with checkpoints blockers is transforming the treatment of advanced cancers. Colorectal cancer (CRC), a cancer with 1.4 million new cases diagnosed annually worldwide, is refractory to immunotherapy (with the exception of a minority of tumors with microsatellite instability). This is somehow paradoxical as CRC is a cancer for which we have shown that it is under immunological control and that tumor infiltrating lymphocytes represent a strong independent predictor of survival. Thus, there is an urgent need to broaden the clinical benefits of immune checkpoint blockers to CRC by combining agents with synergistic mechanisms of action. An attractive approach to sensitize tumors to immunotherapy is to harness immunogenic effects induced by approved conventional or targeted agents. Here I propose a new paradigm to identify molecular determinants of resistance to immunotherapy and develop personalized in silico and in vitro models for predicting response to combination therapy in CRC. The EPIC concept is based on three pillars: 1) emphasis on antitumor T cell activity; 2) systematic interrogation of tumor-immune cell interactions using data-driven modeling and knowledge-based mechanistic modeling, and 3) generation of key quantitative data to train and validate algorithms using perturbation experiments with patient-derived tumor organoids and cutting-edge technologies for multidimensional profiling. We will investigate three immunomodulatory processes: 1) immunostimulatory effects of chemotherapeutics, 2) rewiring of signaling networks induced by targeted drugs and their interference with immunity, and 3) metabolic reprogramming of T cells to enhance antitumor immunity. The anticipated outcome of EPIC is a precision immuno-oncology platform that integrates tumor organoids with high-throughput and high-content data for testing drug combinations, and machine learning for making therapeutic recommendations for individual patients.    Wellcome Trust    Innovator Award: Digital Technologies    575926.0GBP
105    Dr Gaunt Matthew    University of Cambridge    2019-10-01    2021-10-01    Next Generation Drug Discovery Enabled by Digital Molecular Technologies: Machine learning enabled high throughput synthesis to repurpose drugs and failed clinical candidates    Immunotherapy with checkpoints blockers is transforming the treatment of advanced cancers. Colorectal cancer (CRC), a cancer with 1.4 million new cases diagnosed annually worldwide, is refractory to immunotherapy (with the exception of a minority of tumors with microsatellite instability). This is somehow paradoxical as CRC is a cancer for which we have shown that it is under immunological control and that tumor infiltrating lymphocytes represent a strong independent predictor of survival. Thus, there is an urgent need to broaden the clinical benefits of immune checkpoint blockers to CRC by combining agents with synergistic mechanisms of action. An attractive approach to sensitize tumors to immunotherapy is to harness immunogenic effects induced by approved conventional or targeted agents. Here I propose a new paradigm to identify molecular determinants of resistance to immunotherapy and develop personalized in silico and in vitro models for predicting response to combination therapy in CRC. The EPIC concept is based on three pillars: 1) emphasis on antitumor T cell activity; 2) systematic interrogation of tumor-immune cell interactions using data-driven modeling and knowledge-based mechanistic modeling, and 3) generation of key quantitative data to train and validate algorithms using perturbation experiments with patient-derived tumor organoids and cutting-edge technologies for multidimensional profiling. We will investigate three immunomodulatory processes: 1) immunostimulatory effects of chemotherapeutics, 2) rewiring of signaling networks induced by targeted drugs and their interference with immunity, and 3) metabolic reprogramming of T cells to enhance antitumor immunity. The anticipated outcome of EPIC is a precision immuno-oncology platform that integrates tumor organoids with high-throughput and high-content data for testing drug combinations, and machine learning for making therapeutic recommendations for individual patients.    Wellcome Trust    Innovator Award: Digital Technologies    575926.0GBP
106    Univ.Prof. Dipl.Ing. Dr. VINCZE Markus    Vienna University of Technology    2003-12-15    2006-11-14    Cognitive Vision - A Key Technology for Personal Assistance - Coordination Project    Immunotherapy with checkpoints blockers is transforming the treatment of advanced cancers. Colorectal cancer (CRC), a cancer with 1.4 million new cases diagnosed annually worldwide, is refractory to immunotherapy (with the exception of a minority of tumors with microsatellite instability). This is somehow paradoxical as CRC is a cancer for which we have shown that it is under immunological control and that tumor infiltrating lymphocytes represent a strong independent predictor of survival. Thus, there is an urgent need to broaden the clinical benefits of immune checkpoint blockers to CRC by combining agents with synergistic mechanisms of action. An attractive approach to sensitize tumors to immunotherapy is to harness immunogenic effects induced by approved conventional or targeted agents. Here I propose a new paradigm to identify molecular determinants of resistance to immunotherapy and develop personalized in silico and in vitro models for predicting response to combination therapy in CRC. The EPIC concept is based on three pillars: 1) emphasis on antitumor T cell activity; 2) systematic interrogation of tumor-immune cell interactions using data-driven modeling and knowledge-based mechanistic modeling, and 3) generation of key quantitative data to train and validate algorithms using perturbation experiments with patient-derived tumor organoids and cutting-edge technologies for multidimensional profiling. We will investigate three immunomodulatory processes: 1) immunostimulatory effects of chemotherapeutics, 2) rewiring of signaling networks induced by targeted drugs and their interference with immunity, and 3) metabolic reprogramming of T cells to enhance antitumor immunity. The anticipated outcome of EPIC is a precision immuno-oncology platform that integrates tumor organoids with high-throughput and high-content data for testing drug combinations, and machine learning for making therapeutic recommendations for individual patients.    Austrian Science Fund FWF    National Research Networks NFN    575926.0GBP
107    Professor Wood Stephen    University of Birmingham    2013-06-10    2018-12-31    Linear and non-linear brain changes over the transition to psychosis    Immunotherapy with checkpoints blockers is transforming the treatment of advanced cancers. Colorectal cancer (CRC), a cancer with 1.4 million new cases diagnosed annually worldwide, is refractory to immunotherapy (with the exception of a minority of tumors with microsatellite instability). This is somehow paradoxical as CRC is a cancer for which we have shown that it is under immunological control and that tumor infiltrating lymphocytes represent a strong independent predictor of survival. Thus, there is an urgent need to broaden the clinical benefits of immune checkpoint blockers to CRC by combining agents with synergistic mechanisms of action. An attractive approach to sensitize tumors to immunotherapy is to harness immunogenic effects induced by approved conventional or targeted agents. Here I propose a new paradigm to identify molecular determinants of resistance to immunotherapy and develop personalized in silico and in vitro models for predicting response to combination therapy in CRC. The EPIC concept is based on three pillars: 1) emphasis on antitumor T cell activity; 2) systematic interrogation of tumor-immune cell interactions using data-driven modeling and knowledge-based mechanistic modeling, and 3) generation of key quantitative data to train and validate algorithms using perturbation experiments with patient-derived tumor organoids and cutting-edge technologies for multidimensional profiling. We will investigate three immunomodulatory processes: 1) immunostimulatory effects of chemotherapeutics, 2) rewiring of signaling networks induced by targeted drugs and their interference with immunity, and 3) metabolic reprogramming of T cells to enhance antitumor immunity. The anticipated outcome of EPIC is a precision immuno-oncology platform that integrates tumor organoids with high-throughput and high-content data for testing drug combinations, and machine learning for making therapeutic recommendations for individual patients.    Medical Research Council    Research Grant    808010.0GBP
108    Professor Landray Martin    University of Oxford    2016-07-01    2019-03-31    Methodological innovation in large-scale epidemiology    Immunotherapy with checkpoints blockers is transforming the treatment of advanced cancers. Colorectal cancer (CRC), a cancer with 1.4 million new cases diagnosed annually worldwide, is refractory to immunotherapy (with the exception of a minority of tumors with microsatellite instability). This is somehow paradoxical as CRC is a cancer for which we have shown that it is under immunological control and that tumor infiltrating lymphocytes represent a strong independent predictor of survival. Thus, there is an urgent need to broaden the clinical benefits of immune checkpoint blockers to CRC by combining agents with synergistic mechanisms of action. An attractive approach to sensitize tumors to immunotherapy is to harness immunogenic effects induced by approved conventional or targeted agents. Here I propose a new paradigm to identify molecular determinants of resistance to immunotherapy and develop personalized in silico and in vitro models for predicting response to combination therapy in CRC. The EPIC concept is based on three pillars: 1) emphasis on antitumor T cell activity; 2) systematic interrogation of tumor-immune cell interactions using data-driven modeling and knowledge-based mechanistic modeling, and 3) generation of key quantitative data to train and validate algorithms using perturbation experiments with patient-derived tumor organoids and cutting-edge technologies for multidimensional profiling. We will investigate three immunomodulatory processes: 1) immunostimulatory effects of chemotherapeutics, 2) rewiring of signaling networks induced by targeted drugs and their interference with immunity, and 3) metabolic reprogramming of T cells to enhance antitumor immunity. The anticipated outcome of EPIC is a precision immuno-oncology platform that integrates tumor organoids with high-throughput and high-content data for testing drug combinations, and machine learning for making therapeutic recommendations for individual patients.    Medical Research Council    Unit    808010.0GBP
109    Professor Landray Martin    University of Leeds    2016-07-01    2019-03-31    BBSRC Research Development Fellowship: Dr D R Westhead. From gene functional association to biomolecular networks in parasites and plants    1) To create databases of gene functional associations based on a variety of data sources for Arabidopsis and for the Apicomplexan parasites, and make these accessible to the associated research communities as web services. 2) To develop, compare and optimise machine learning-based methods to combine evidence from the sources above in prediction problems related to biomolecular networks, including missing enzymes in metabolic networks. 3) To make the results methods and predicted biomolecular networks available to the research community as web services.    Biotechnology and Biological Sciences Research Council    Unit    93700.0GBP
110    Professor Landray Martin    The University of Manchester    2016-07-01    2019-03-31    Novel force fields devised using machine learning    The potential energy functions used in biomoelcular modelling must be improved in order to secure its future predictive power and trust of the experimental community. In our previous work we replaced point charges by atomic multipole moments in order to improve the accuracy of the (short-range) electrostatic interaction. Quantum Chemical Topology (aka atoms in Molecules") uses molecular electron density to define atoms as finite volumes shaped by the molecular enviroment they occur in. In this proposal we focus on atomic polarisation, i.e. how the electron density of a topological atom fluctuates upon a change in nuclear positions of its neighbouring atoms. We do not follow an existing method (such as the Drude/shell model, electronegativity equalisation, effective polarisation, polarisable point dipoles, etc.). Instead we adopt a modern machine learning method called Genetic Programming to directly establish the link between an atom's electron density reponsing to a geometrical change in immediate environment. Summarising the proposed method, focus on a given atom in a molecule. Normal modes generate hundreds of molecular geometries, representing the ever changing environment of an atom, including bond length and angle variations, and conformational flexibility. Quantum Chemical Topology cuts this given atom out of each of the distorted molecular charge density and calculates the corresponding atomic multipole moments. The latter fluctuate in response to a change in molecular geometry and it is these data that the Genetic Programming algorithm is trained on. Our proposed method is totally new, which is why we need this feasible study."    Biotechnology and Biological Sciences Research Council    Unit    105463.0GBP
111    Professor Relton Caroline    University of Bristol    2013-06-01    2018-03-31    Epigenetic Epidemiology    The potential energy functions used in biomoelcular modelling must be improved in order to secure its future predictive power and trust of the experimental community. In our previous work we replaced point charges by atomic multipole moments in order to improve the accuracy of the (short-range) electrostatic interaction. Quantum Chemical Topology (aka atoms in Molecules") uses molecular electron density to define atoms as finite volumes shaped by the molecular enviroment they occur in. In this proposal we focus on atomic polarisation, i.e. how the electron density of a topological atom fluctuates upon a change in nuclear positions of its neighbouring atoms. We do not follow an existing method (such as the Drude/shell model, electronegativity equalisation, effective polarisation, polarisable point dipoles, etc.). Instead we adopt a modern machine learning method called Genetic Programming to directly establish the link between an atom's electron density reponsing to a geometrical change in immediate environment. Summarising the proposed method, focus on a given atom in a molecule. Normal modes generate hundreds of molecular geometries, representing the ever changing environment of an atom, including bond length and angle variations, and conformational flexibility. Quantum Chemical Topology cuts this given atom out of each of the distorted molecular charge density and calculates the corresponding atomic multipole moments. The latter fluctuate in response to a change in molecular geometry and it is these data that the Genetic Programming algorithm is trained on. Our proposed method is totally new, which is why we need this feasible study."    Medical Research Council    Unit    105463.0GBP
112    Professor Relton Caroline    Rothamsted Research    2013-06-01    2018-03-31    A systems approach to candidate gene and pathway identification    This project will develop integrated bioinformatics approaches for identifying new genes and pathways implicated in a range of biological processes relevant to plant pathology and crop science. This work will in part build on earlier research into data integration and graph-based analysis methods that have been used in the development of the ONDEX data integration framework for systems biology applications. Research efforts will be directed in the following areas: 1. Methods and software for integrating new sources of biological information including gene function, biochemical pathway, protein structure, gene network and other information (including that extracted from biological literature through text mining) will be developed in response to requirements derived from the biological applications to be studied. 2. Comparative genome analysis and visualisation methods will be developed to exploit the information from model organisms that have more complete genome and pathway information. These methods will be applied to the prediction of new candidate gene and pathway functions from partially or newly sequenced crop, pest and pathogen genomes. 3. Machine learning approaches (including graphical data analysis, data mining and visualisation) will be developed to extract new biological insights from integrated datasets with the aim of identifying novel interactions at the gene and pathway level for the purpose of predicting new gene function, annotation of emerging genomes and other 'omics resources. 4. Specialist databases of gene function (e.g. PHI-base) will be further developed in collaboration with biologists as resources for the research community and to underpin research in the above areas.    Biotechnology and Biological Sciences Research Council    Unit    761116.0GBP
113    Dr Bakal Chris    Institute of Cancer Research    2011-12-01    2014-11-30    Quantifying the relationship between cancer cell genotype and phenotype. Determination and validation of the gene expression changes that drive the specific morphological changes essential for breast cancer metastasis.    Background Despite the vast amount of data that have been generated in recent years on cancer cell genotypes, a significant challenge in the post-genomic era will be to understand how genomic alterations drive cellular phenotypes, such as the dramatic changes in morphogenesis that occur during metastasis. We have previously developed technologies to quantify the morphology of single cells and cell populations to precisely investigate changes in cell shape (Bakal et al., Science 2007). We are currently using these methods to quantify the morphology of a panel of breast cancer lines in 2D and 3D culture. Using these methods, we can also quantify hetereogeneity and identify morphologically distinct subpoluations within in isogenic cell lines. Genome-wide mRNA expression levels and genomic copy number variations have been determined for all the lines we are investigating by the Reis-Filho laboratory at the ICR/Breakthrough Breast Cancer Center. This allows us to perform statistical analysis to find patterns of gene expression that relate to quantifiable differences in cell shape. In addition, we are working in collaboration with The Cancer Genome Atlas Project to profile the genotypes and morphological phenotypes parallel following treatment with different small molecules. In this project, we aim to determine how genotypic alterations are linked to specific changes in morphological in cancer cells. Aims (1) To classify a panel of 20-40 breast cancer cell lines based on morphology and identify patterns of gene expression that are correlated with and/or control cell shape. (2) To perform quantitative morphometric analysis of cell invasion in 3D matrices. (3) To determine the whether highly invasive cells represent stable subpopulations within tumour cell lines. Methods In order to complete these studies we will use: - Automated high-throughput image acquisition and feature analysis of single cells in both 2D and 3D. The Bakal laboratory is equipped with two Opera microscopes, each capable of capturing over 100,000 unique images per day. - High-throughput analysis of 3D morphology. - Machine-learning methods to quantify and classify independent phenotypes. The ICR has recently acquired an Altix UV computer in order to facilitate this work. - Statistical analysis to determine correlations between gene expression, copy number and morphological features. - RNAi and small-molecule perturbation. The results of this work will be primarily used to identify and validate genes whose changes in expression are causal to the morphogenesis of metastatic cells.    Cancer Research UK    Biological Sciences Committee - Project Award    761116.0GBP
114    Professor McKenna Stephen    University of Dundee    2010-05-01    2011-04-30    Microscopic Image Analysis for Cell Biology    Background Despite the vast amount of data that have been generated in recent years on cancer cell genotypes, a significant challenge in the post-genomic era will be to understand how genomic alterations drive cellular phenotypes, such as the dramatic changes in morphogenesis that occur during metastasis. We have previously developed technologies to quantify the morphology of single cells and cell populations to precisely investigate changes in cell shape (Bakal et al., Science 2007). We are currently using these methods to quantify the morphology of a panel of breast cancer lines in 2D and 3D culture. Using these methods, we can also quantify hetereogeneity and identify morphologically distinct subpoluations within in isogenic cell lines. Genome-wide mRNA expression levels and genomic copy number variations have been determined for all the lines we are investigating by the Reis-Filho laboratory at the ICR/Breakthrough Breast Cancer Center. This allows us to perform statistical analysis to find patterns of gene expression that relate to quantifiable differences in cell shape. In addition, we are working in collaboration with The Cancer Genome Atlas Project to profile the genotypes and morphological phenotypes parallel following treatment with different small molecules. In this project, we aim to determine how genotypic alterations are linked to specific changes in morphological in cancer cells. Aims (1) To classify a panel of 20-40 breast cancer cell lines based on morphology and identify patterns of gene expression that are correlated with and/or control cell shape. (2) To perform quantitative morphometric analysis of cell invasion in 3D matrices. (3) To determine the whether highly invasive cells represent stable subpopulations within tumour cell lines. Methods In order to complete these studies we will use: - Automated high-throughput image acquisition and feature analysis of single cells in both 2D and 3D. The Bakal laboratory is equipped with two Opera microscopes, each capable of capturing over 100,000 unique images per day. - High-throughput analysis of 3D morphology. - Machine-learning methods to quantify and classify independent phenotypes. The ICR has recently acquired an Altix UV computer in order to facilitate this work. - Statistical analysis to determine correlations between gene expression, copy number and morphological features. - RNAi and small-molecule perturbation. The results of this work will be primarily used to identify and validate genes whose changes in expression are causal to the morphogenesis of metastatic cells.    Medical Research Council    Research Grant    92895.0GBP
115    Dr O'Regan Declan    Imperial College London    2017-09-18    2020-09-17    Using machine learning to predict clinical outcomes in heart failure.    Background Despite the vast amount of data that have been generated in recent years on cancer cell genotypes, a significant challenge in the post-genomic era will be to understand how genomic alterations drive cellular phenotypes, such as the dramatic changes in morphogenesis that occur during metastasis. We have previously developed technologies to quantify the morphology of single cells and cell populations to precisely investigate changes in cell shape (Bakal et al., Science 2007). We are currently using these methods to quantify the morphology of a panel of breast cancer lines in 2D and 3D culture. Using these methods, we can also quantify hetereogeneity and identify morphologically distinct subpoluations within in isogenic cell lines. Genome-wide mRNA expression levels and genomic copy number variations have been determined for all the lines we are investigating by the Reis-Filho laboratory at the ICR/Breakthrough Breast Cancer Center. This allows us to perform statistical analysis to find patterns of gene expression that relate to quantifiable differences in cell shape. In addition, we are working in collaboration with The Cancer Genome Atlas Project to profile the genotypes and morphological phenotypes parallel following treatment with different small molecules. In this project, we aim to determine how genotypic alterations are linked to specific changes in morphological in cancer cells. Aims (1) To classify a panel of 20-40 breast cancer cell lines based on morphology and identify patterns of gene expression that are correlated with and/or control cell shape. (2) To perform quantitative morphometric analysis of cell invasion in 3D matrices. (3) To determine the whether highly invasive cells represent stable subpopulations within tumour cell lines. Methods In order to complete these studies we will use: - Automated high-throughput image acquisition and feature analysis of single cells in both 2D and 3D. The Bakal laboratory is equipped with two Opera microscopes, each capable of capturing over 100,000 unique images per day. - High-throughput analysis of 3D morphology. - Machine-learning methods to quantify and classify independent phenotypes. The ICR has recently acquired an Altix UV computer in order to facilitate this work. - Statistical analysis to determine correlations between gene expression, copy number and morphological features. - RNAi and small-molecule perturbation. The results of this work will be primarily used to identify and validate genes whose changes in expression are causal to the morphogenesis of metastatic cells.    British Heart Foundation    New Horizons Grant    297017.0GBP
116    Dr O'Regan Declan    Imperial College London    2017-09-18    2020-09-17    Machine learning to model disease mechanisms and predict outcomes in cardiomyopathy    Background Despite the vast amount of data that have been generated in recent years on cancer cell genotypes, a significant challenge in the post-genomic era will be to understand how genomic alterations drive cellular phenotypes, such as the dramatic changes in morphogenesis that occur during metastasis. We have previously developed technologies to quantify the morphology of single cells and cell populations to precisely investigate changes in cell shape (Bakal et al., Science 2007). We are currently using these methods to quantify the morphology of a panel of breast cancer lines in 2D and 3D culture. Using these methods, we can also quantify hetereogeneity and identify morphologically distinct subpoluations within in isogenic cell lines. Genome-wide mRNA expression levels and genomic copy number variations have been determined for all the lines we are investigating by the Reis-Filho laboratory at the ICR/Breakthrough Breast Cancer Center. This allows us to perform statistical analysis to find patterns of gene expression that relate to quantifiable differences in cell shape. In addition, we are working in collaboration with The Cancer Genome Atlas Project to profile the genotypes and morphological phenotypes parallel following treatment with different small molecules. In this project, we aim to determine how genotypic alterations are linked to specific changes in morphological in cancer cells. Aims (1) To classify a panel of 20-40 breast cancer cell lines based on morphology and identify patterns of gene expression that are correlated with and/or control cell shape. (2) To perform quantitative morphometric analysis of cell invasion in 3D matrices. (3) To determine the whether highly invasive cells represent stable subpopulations within tumour cell lines. Methods In order to complete these studies we will use: - Automated high-throughput image acquisition and feature analysis of single cells in both 2D and 3D. The Bakal laboratory is equipped with two Opera microscopes, each capable of capturing over 100,000 unique images per day. - High-throughput analysis of 3D morphology. - Machine-learning methods to quantify and classify independent phenotypes. The ICR has recently acquired an Altix UV computer in order to facilitate this work. - Statistical analysis to determine correlations between gene expression, copy number and morphological features. - RNAi and small-molecule perturbation. The results of this work will be primarily used to identify and validate genes whose changes in expression are causal to the morphogenesis of metastatic cells.    British Heart Foundation    New Horizons Grant    1011285.0GBP
117    Dr Swift Andrew    University of Sheffield    2019-10-01    2021-09-30    Developing a machine learning tool to improve prognostic and treatment response assessment on cardiac MRI data    Background Despite the vast amount of data that have been generated in recent years on cancer cell genotypes, a significant challenge in the post-genomic era will be to understand how genomic alterations drive cellular phenotypes, such as the dramatic changes in morphogenesis that occur during metastasis. We have previously developed technologies to quantify the morphology of single cells and cell populations to precisely investigate changes in cell shape (Bakal et al., Science 2007). We are currently using these methods to quantify the morphology of a panel of breast cancer lines in 2D and 3D culture. Using these methods, we can also quantify hetereogeneity and identify morphologically distinct subpoluations within in isogenic cell lines. Genome-wide mRNA expression levels and genomic copy number variations have been determined for all the lines we are investigating by the Reis-Filho laboratory at the ICR/Breakthrough Breast Cancer Center. This allows us to perform statistical analysis to find patterns of gene expression that relate to quantifiable differences in cell shape. In addition, we are working in collaboration with The Cancer Genome Atlas Project to profile the genotypes and morphological phenotypes parallel following treatment with different small molecules. In this project, we aim to determine how genotypic alterations are linked to specific changes in morphological in cancer cells. Aims (1) To classify a panel of 20-40 breast cancer cell lines based on morphology and identify patterns of gene expression that are correlated with and/or control cell shape. (2) To perform quantitative morphometric analysis of cell invasion in 3D matrices. (3) To determine the whether highly invasive cells represent stable subpopulations within tumour cell lines. Methods In order to complete these studies we will use: - Automated high-throughput image acquisition and feature analysis of single cells in both 2D and 3D. The Bakal laboratory is equipped with two Opera microscopes, each capable of capturing over 100,000 unique images per day. - High-throughput analysis of 3D morphology. - Machine-learning methods to quantify and classify independent phenotypes. The ICR has recently acquired an Altix UV computer in order to facilitate this work. - Statistical analysis to determine correlations between gene expression, copy number and morphological features. - RNAi and small-molecule perturbation. The results of this work will be primarily used to identify and validate genes whose changes in expression are causal to the morphogenesis of metastatic cells.    Wellcome Trust    Innovator Award: Digital Technologies    639873.0GBP
118    Dr Lu Haiping    University of Sheffield    2019-10-01    2021-09-30    Developing a machine learning tool to improve prognostic and treatment response assessment on cardiac MRI data    Background Despite the vast amount of data that have been generated in recent years on cancer cell genotypes, a significant challenge in the post-genomic era will be to understand how genomic alterations drive cellular phenotypes, such as the dramatic changes in morphogenesis that occur during metastasis. We have previously developed technologies to quantify the morphology of single cells and cell populations to precisely investigate changes in cell shape (Bakal et al., Science 2007). We are currently using these methods to quantify the morphology of a panel of breast cancer lines in 2D and 3D culture. Using these methods, we can also quantify hetereogeneity and identify morphologically distinct subpoluations within in isogenic cell lines. Genome-wide mRNA expression levels and genomic copy number variations have been determined for all the lines we are investigating by the Reis-Filho laboratory at the ICR/Breakthrough Breast Cancer Center. This allows us to perform statistical analysis to find patterns of gene expression that relate to quantifiable differences in cell shape. In addition, we are working in collaboration with The Cancer Genome Atlas Project to profile the genotypes and morphological phenotypes parallel following treatment with different small molecules. In this project, we aim to determine how genotypic alterations are linked to specific changes in morphological in cancer cells. Aims (1) To classify a panel of 20-40 breast cancer cell lines based on morphology and identify patterns of gene expression that are correlated with and/or control cell shape. (2) To perform quantitative morphometric analysis of cell invasion in 3D matrices. (3) To determine the whether highly invasive cells represent stable subpopulations within tumour cell lines. Methods In order to complete these studies we will use: - Automated high-throughput image acquisition and feature analysis of single cells in both 2D and 3D. The Bakal laboratory is equipped with two Opera microscopes, each capable of capturing over 100,000 unique images per day. - High-throughput analysis of 3D morphology. - Machine-learning methods to quantify and classify independent phenotypes. The ICR has recently acquired an Altix UV computer in order to facilitate this work. - Statistical analysis to determine correlations between gene expression, copy number and morphological features. - RNAi and small-molecule perturbation. The results of this work will be primarily used to identify and validate genes whose changes in expression are causal to the morphogenesis of metastatic cells.    Wellcome Trust    Innovator Award: Digital Technologies    639873.0GBP
119    Prof Jenkinson Mark    University of Oxford    2019-06-01    2024-05-31    Integrative imaging of brain structure and function in populations and individuals    Background Despite the vast amount of data that have been generated in recent years on cancer cell genotypes, a significant challenge in the post-genomic era will be to understand how genomic alterations drive cellular phenotypes, such as the dramatic changes in morphogenesis that occur during metastasis. We have previously developed technologies to quantify the morphology of single cells and cell populations to precisely investigate changes in cell shape (Bakal et al., Science 2007). We are currently using these methods to quantify the morphology of a panel of breast cancer lines in 2D and 3D culture. Using these methods, we can also quantify hetereogeneity and identify morphologically distinct subpoluations within in isogenic cell lines. Genome-wide mRNA expression levels and genomic copy number variations have been determined for all the lines we are investigating by the Reis-Filho laboratory at the ICR/Breakthrough Breast Cancer Center. This allows us to perform statistical analysis to find patterns of gene expression that relate to quantifiable differences in cell shape. In addition, we are working in collaboration with The Cancer Genome Atlas Project to profile the genotypes and morphological phenotypes parallel following treatment with different small molecules. In this project, we aim to determine how genotypic alterations are linked to specific changes in morphological in cancer cells. Aims (1) To classify a panel of 20-40 breast cancer cell lines based on morphology and identify patterns of gene expression that are correlated with and/or control cell shape. (2) To perform quantitative morphometric analysis of cell invasion in 3D matrices. (3) To determine the whether highly invasive cells represent stable subpopulations within tumour cell lines. Methods In order to complete these studies we will use: - Automated high-throughput image acquisition and feature analysis of single cells in both 2D and 3D. The Bakal laboratory is equipped with two Opera microscopes, each capable of capturing over 100,000 unique images per day. - High-throughput analysis of 3D morphology. - Machine-learning methods to quantify and classify independent phenotypes. The ICR has recently acquired an Altix UV computer in order to facilitate this work. - Statistical analysis to determine correlations between gene expression, copy number and morphological features. - RNAi and small-molecule perturbation. The results of this work will be primarily used to identify and validate genes whose changes in expression are causal to the morphogenesis of metastatic cells.    Wellcome Trust    Collaborative Award in Science    4106203.0GBP
120    Prof Beckmann Christian    Radboud Universiteit Nijmegen    2019-06-01    2024-05-31    Integrative imaging of brain structure and function in populations and individuals    Background Despite the vast amount of data that have been generated in recent years on cancer cell genotypes, a significant challenge in the post-genomic era will be to understand how genomic alterations drive cellular phenotypes, such as the dramatic changes in morphogenesis that occur during metastasis. We have previously developed technologies to quantify the morphology of single cells and cell populations to precisely investigate changes in cell shape (Bakal et al., Science 2007). We are currently using these methods to quantify the morphology of a panel of breast cancer lines in 2D and 3D culture. Using these methods, we can also quantify hetereogeneity and identify morphologically distinct subpoluations within in isogenic cell lines. Genome-wide mRNA expression levels and genomic copy number variations have been determined for all the lines we are investigating by the Reis-Filho laboratory at the ICR/Breakthrough Breast Cancer Center. This allows us to perform statistical analysis to find patterns of gene expression that relate to quantifiable differences in cell shape. In addition, we are working in collaboration with The Cancer Genome Atlas Project to profile the genotypes and morphological phenotypes parallel following treatment with different small molecules. In this project, we aim to determine how genotypic alterations are linked to specific changes in morphological in cancer cells. Aims (1) To classify a panel of 20-40 breast cancer cell lines based on morphology and identify patterns of gene expression that are correlated with and/or control cell shape. (2) To perform quantitative morphometric analysis of cell invasion in 3D matrices. (3) To determine the whether highly invasive cells represent stable subpopulations within tumour cell lines. Methods In order to complete these studies we will use: - Automated high-throughput image acquisition and feature analysis of single cells in both 2D and 3D. The Bakal laboratory is equipped with two Opera microscopes, each capable of capturing over 100,000 unique images per day. - High-throughput analysis of 3D morphology. - Machine-learning methods to quantify and classify independent phenotypes. The ICR has recently acquired an Altix UV computer in order to facilitate this work. - Statistical analysis to determine correlations between gene expression, copy number and morphological features. - RNAi and small-molecule perturbation. The results of this work will be primarily used to identify and validate genes whose changes in expression are causal to the morphogenesis of metastatic cells.    Wellcome Trust    Collaborative Award in Science    4106203.0GBP
121    Prof Miller Karla    University of Oxford    2019-06-01    2024-05-31    Integrative imaging of brain structure and function in populations and individuals    Background Despite the vast amount of data that have been generated in recent years on cancer cell genotypes, a significant challenge in the post-genomic era will be to understand how genomic alterations drive cellular phenotypes, such as the dramatic changes in morphogenesis that occur during metastasis. We have previously developed technologies to quantify the morphology of single cells and cell populations to precisely investigate changes in cell shape (Bakal et al., Science 2007). We are currently using these methods to quantify the morphology of a panel of breast cancer lines in 2D and 3D culture. Using these methods, we can also quantify hetereogeneity and identify morphologically distinct subpoluations within in isogenic cell lines. Genome-wide mRNA expression levels and genomic copy number variations have been determined for all the lines we are investigating by the Reis-Filho laboratory at the ICR/Breakthrough Breast Cancer Center. This allows us to perform statistical analysis to find patterns of gene expression that relate to quantifiable differences in cell shape. In addition, we are working in collaboration with The Cancer Genome Atlas Project to profile the genotypes and morphological phenotypes parallel following treatment with different small molecules. In this project, we aim to determine how genotypic alterations are linked to specific changes in morphological in cancer cells. Aims (1) To classify a panel of 20-40 breast cancer cell lines based on morphology and identify patterns of gene expression that are correlated with and/or control cell shape. (2) To perform quantitative morphometric analysis of cell invasion in 3D matrices. (3) To determine the whether highly invasive cells represent stable subpopulations within tumour cell lines. Methods In order to complete these studies we will use: - Automated high-throughput image acquisition and feature analysis of single cells in both 2D and 3D. The Bakal laboratory is equipped with two Opera microscopes, each capable of capturing over 100,000 unique images per day. - High-throughput analysis of 3D morphology. - Machine-learning methods to quantify and classify independent phenotypes. The ICR has recently acquired an Altix UV computer in order to facilitate this work. - Statistical analysis to determine correlations between gene expression, copy number and morphological features. - RNAi and small-molecule perturbation. The results of this work will be primarily used to identify and validate genes whose changes in expression are causal to the morphogenesis of metastatic cells.    Wellcome Trust    Collaborative Award in Science    4106203.0GBP
122    Prof Smith Stephen    University of Oxford    2019-06-01    2024-05-31    Integrative imaging of brain structure and function in populations and individuals    Background Despite the vast amount of data that have been generated in recent years on cancer cell genotypes, a significant challenge in the post-genomic era will be to understand how genomic alterations drive cellular phenotypes, such as the dramatic changes in morphogenesis that occur during metastasis. We have previously developed technologies to quantify the morphology of single cells and cell populations to precisely investigate changes in cell shape (Bakal et al., Science 2007). We are currently using these methods to quantify the morphology of a panel of breast cancer lines in 2D and 3D culture. Using these methods, we can also quantify hetereogeneity and identify morphologically distinct subpoluations within in isogenic cell lines. Genome-wide mRNA expression levels and genomic copy number variations have been determined for all the lines we are investigating by the Reis-Filho laboratory at the ICR/Breakthrough Breast Cancer Center. This allows us to perform statistical analysis to find patterns of gene expression that relate to quantifiable differences in cell shape. In addition, we are working in collaboration with The Cancer Genome Atlas Project to profile the genotypes and morphological phenotypes parallel following treatment with different small molecules. In this project, we aim to determine how genotypic alterations are linked to specific changes in morphological in cancer cells. Aims (1) To classify a panel of 20-40 breast cancer cell lines based on morphology and identify patterns of gene expression that are correlated with and/or control cell shape. (2) To perform quantitative morphometric analysis of cell invasion in 3D matrices. (3) To determine the whether highly invasive cells represent stable subpopulations within tumour cell lines. Methods In order to complete these studies we will use: - Automated high-throughput image acquisition and feature analysis of single cells in both 2D and 3D. The Bakal laboratory is equipped with two Opera microscopes, each capable of capturing over 100,000 unique images per day. - High-throughput analysis of 3D morphology. - Machine-learning methods to quantify and classify independent phenotypes. The ICR has recently acquired an Altix UV computer in order to facilitate this work. - Statistical analysis to determine correlations between gene expression, copy number and morphological features. - RNAi and small-molecule perturbation. The results of this work will be primarily used to identify and validate genes whose changes in expression are causal to the morphogenesis of metastatic cells.    Wellcome Trust    Collaborative Award in Science    4106203.0GBP
123    Prof Jbabdi Saad    University of Oxford    2019-06-01    2024-05-31    Integrative imaging of brain structure and function in populations and individuals    Background Despite the vast amount of data that have been generated in recent years on cancer cell genotypes, a significant challenge in the post-genomic era will be to understand how genomic alterations drive cellular phenotypes, such as the dramatic changes in morphogenesis that occur during metastasis. We have previously developed technologies to quantify the morphology of single cells and cell populations to precisely investigate changes in cell shape (Bakal et al., Science 2007). We are currently using these methods to quantify the morphology of a panel of breast cancer lines in 2D and 3D culture. Using these methods, we can also quantify hetereogeneity and identify morphologically distinct subpoluations within in isogenic cell lines. Genome-wide mRNA expression levels and genomic copy number variations have been determined for all the lines we are investigating by the Reis-Filho laboratory at the ICR/Breakthrough Breast Cancer Center. This allows us to perform statistical analysis to find patterns of gene expression that relate to quantifiable differences in cell shape. In addition, we are working in collaboration with The Cancer Genome Atlas Project to profile the genotypes and morphological phenotypes parallel following treatment with different small molecules. In this project, we aim to determine how genotypic alterations are linked to specific changes in morphological in cancer cells. Aims (1) To classify a panel of 20-40 breast cancer cell lines based on morphology and identify patterns of gene expression that are correlated with and/or control cell shape. (2) To perform quantitative morphometric analysis of cell invasion in 3D matrices. (3) To determine the whether highly invasive cells represent stable subpopulations within tumour cell lines. Methods In order to complete these studies we will use: - Automated high-throughput image acquisition and feature analysis of single cells in both 2D and 3D. The Bakal laboratory is equipped with two Opera microscopes, each capable of capturing over 100,000 unique images per day. - High-throughput analysis of 3D morphology. - Machine-learning methods to quantify and classify independent phenotypes. The ICR has recently acquired an Altix UV computer in order to facilitate this work. - Statistical analysis to determine correlations between gene expression, copy number and morphological features. - RNAi and small-molecule perturbation. The results of this work will be primarily used to identify and validate genes whose changes in expression are causal to the morphogenesis of metastatic cells.    Wellcome Trust    Collaborative Award in Science    4106203.0GBP
124    Prof Woolrich Mark    University of Oxford    2019-06-01    2024-05-31    Integrative imaging of brain structure and function in populations and individuals    Background Despite the vast amount of data that have been generated in recent years on cancer cell genotypes, a significant challenge in the post-genomic era will be to understand how genomic alterations drive cellular phenotypes, such as the dramatic changes in morphogenesis that occur during metastasis. We have previously developed technologies to quantify the morphology of single cells and cell populations to precisely investigate changes in cell shape (Bakal et al., Science 2007). We are currently using these methods to quantify the morphology of a panel of breast cancer lines in 2D and 3D culture. Using these methods, we can also quantify hetereogeneity and identify morphologically distinct subpoluations within in isogenic cell lines. Genome-wide mRNA expression levels and genomic copy number variations have been determined for all the lines we are investigating by the Reis-Filho laboratory at the ICR/Breakthrough Breast Cancer Center. This allows us to perform statistical analysis to find patterns of gene expression that relate to quantifiable differences in cell shape. In addition, we are working in collaboration with The Cancer Genome Atlas Project to profile the genotypes and morphological phenotypes parallel following treatment with different small molecules. In this project, we aim to determine how genotypic alterations are linked to specific changes in morphological in cancer cells. Aims (1) To classify a panel of 20-40 breast cancer cell lines based on morphology and identify patterns of gene expression that are correlated with and/or control cell shape. (2) To perform quantitative morphometric analysis of cell invasion in 3D matrices. (3) To determine the whether highly invasive cells represent stable subpopulations within tumour cell lines. Methods In order to complete these studies we will use: - Automated high-throughput image acquisition and feature analysis of single cells in both 2D and 3D. The Bakal laboratory is equipped with two Opera microscopes, each capable of capturing over 100,000 unique images per day. - High-throughput analysis of 3D morphology. - Machine-learning methods to quantify and classify independent phenotypes. The ICR has recently acquired an Altix UV computer in order to facilitate this work. - Statistical analysis to determine correlations between gene expression, copy number and morphological features. - RNAi and small-molecule perturbation. The results of this work will be primarily used to identify and validate genes whose changes in expression are causal to the morphogenesis of metastatic cells.    Wellcome Trust    Collaborative Award in Science    4106203.0GBP
125    Dr Purandare Nitin    The University of Manchester    2005-10-01    2009-03-31    The human serum metabolome in health and disease    Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high- resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.    Biotechnology and Biological Sciences Research Council    LINK project    197915.0GBP
126    Prof Miller Karla    University of Oxford    2016-10-01    2021-10-01    Linking MRI and microscopy for multi-scale neuroscience: Mechanisms, diagnostics and anatomy    Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high- resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.    Wellcome Trust    Senior Research Fellowship Basic    1793980.0GBP
127    Prof Miller Karla    University of Oxford    2010-05-01    2015-05-01    From Software Verification to Everyware Verification    In the words of Adam Greenfield, ?the age of ubiquitous computing is here: a computing without computers, where information processing has diffused into everyday life, and virtually disappeared from view?. Conventional hardware and software has evolved into ?everyware' sensor-enabled electronic devices, virtually invisible and wirelessly connected on which we increasingly often rely for everyday activities and access to services such as banking and healthcare. The key component of ?everyware' is embedded software, continuously interacting with its environment by means of sensors and actuators. Ubiquitous computing must deal with the challenges posed by the complex scenario of communities of ?everyware', in presence of environmental uncertainty and resource limitations, while at the same time aiming to meet high-level expectations of autonomous operation, predictability and robustness. This calls for the use of quantitative measures, stochastic modelling, discrete and continuous dynamics and goal-driven approaches, which the emerging quantitative software verification is unable to address at present. The central premise of the proposal is that there is a need for a paradigm shift in verification to enable ?everyware' verification, which can be achieved through a model-based approach that admits discrete and continuous dynamics, the replacement of offline methods with online techniques such as machine learning, and the use of game-theoretic and planning techniques. The project will significantly advance quantitative probabilistic verification in new and previously unexplored directions. I will lead a team of researchers investigating the fundamental principles of ?everyware' verification, development of algorithms and prototype implementations, and experimenting with case studies. I will also provide continued scientific leadership in the area of ubiquitous computing.    European Research Council    Advanced Grant    2060360.0EUR
128    Dr Mirams Gary    University of Oxford    2014-02-01    2019-02-01    Improving assessment of drug-induced cardiac risk with mathematical electrophysiology models.    Drug-induced cardiac arrhythmia is a leading cause of withdrawal of drugs from the market, and risk of this (both real and perceived) is one of the leading causes of attrition during compound development. The earliest cardiac safety test consists of measuring hERG channel blockade, but its predictive power for human clinical pro-arrhythmic risk is limited. My novel approach is to use experimental data on multiple ion-channel screens for a large number of drugs. In order to integrate this info rmation I will use, and develop further, computational models of cardiac cells and tissue. I will also harness machine learning methods to quantify the predictive power of in-silico markers for safety test results and pro-arrhythmic risk. This work will require a re-calibration of cardiac electrophysiology models, and the design of optimal experiments, to establish the ion-channel conductances in different species and cell types as accurately as possible. I will also extend the existing basic models of drug/ion-channel interaction to capture more subtle, yet perhaps crucial, effects, such as heart rate-dependent blockade. Computational models including pro-arrhythmic risk factors such as age, gender and disease will be developed and utilised to improve the understanding and prediction of susceptibility to drug-induced arrhythmias.    Wellcome Trust    Sir Henry Dale Fellowship    534961.0GBP
129    Professor Holmans Peter    Cardiff University    2006-09-01    2009-08-31    MSc in Bioinformatics    The Biostatistics and Bioinformatics Unit (BBU), in close collaboration with five internationally recognised cancer research groups with substantial Cancer Research UK support, requests funding to enable cancer researchers (or future cancer researchers) to study on an MSc course in bioinformatics. We seek this funding to enable highly motivated individuals to train in bioinformatics in order to boost the use of higher level bioinformatics in cancer research. The course will develop the appropriate complementary skills to provide students with the multidisciplinary bioinformatic skill set required to work effectively in the post genomic era in cancer research. The course contains core modules in computer science, statistics and the use of bioinformatics in the postgenomic era and specialist modules in databasing, machine learning and data mining, algorithmic aspects of sequence analysis and molecular modelling. Students complete a mini-research project as well as a full-time 3 month research project and the latter two components are embedded within the collaborating groups. We envisage individuals already in cancer research will obtain these bursaries in order to develop new skills but in some circumstances highly motivated individuals with no previous cancer research experience but with clear aspirations to move into cancer research may be accepted.    Cancer Research UK    Bursary    534961.0GBP
130    Professor Landray Martin    University of Oxford    2016-07-01    2019-03-31    Methodological innovation in large-scale epidemiology    The Biostatistics and Bioinformatics Unit (BBU), in close collaboration with five internationally recognised cancer research groups with substantial Cancer Research UK support, requests funding to enable cancer researchers (or future cancer researchers) to study on an MSc course in bioinformatics. We seek this funding to enable highly motivated individuals to train in bioinformatics in order to boost the use of higher level bioinformatics in cancer research. The course will develop the appropriate complementary skills to provide students with the multidisciplinary bioinformatic skill set required to work effectively in the post genomic era in cancer research. The course contains core modules in computer science, statistics and the use of bioinformatics in the postgenomic era and specialist modules in databasing, machine learning and data mining, algorithmic aspects of sequence analysis and molecular modelling. Students complete a mini-research project as well as a full-time 3 month research project and the latter two components are embedded within the collaborating groups. We envisage individuals already in cancer research will obtain these bursaries in order to develop new skills but in some circumstances highly motivated individuals with no previous cancer research experience but with clear aspirations to move into cancer research may be accepted.    Medical Research Council    Unit    534961.0GBP
131    Professor Kaban Ata    University of Birmingham    2008-09-22    2009-09-21    Generative-discriminative hybrids for disease prediction and cell communication modelling    The Biostatistics and Bioinformatics Unit (BBU), in close collaboration with five internationally recognised cancer research groups with substantial Cancer Research UK support, requests funding to enable cancer researchers (or future cancer researchers) to study on an MSc course in bioinformatics. We seek this funding to enable highly motivated individuals to train in bioinformatics in order to boost the use of higher level bioinformatics in cancer research. The course will develop the appropriate complementary skills to provide students with the multidisciplinary bioinformatic skill set required to work effectively in the post genomic era in cancer research. The course contains core modules in computer science, statistics and the use of bioinformatics in the postgenomic era and specialist modules in databasing, machine learning and data mining, algorithmic aspects of sequence analysis and molecular modelling. Students complete a mini-research project as well as a full-time 3 month research project and the latter two components are embedded within the collaborating groups. We envisage individuals already in cancer research will obtain these bursaries in order to develop new skills but in some circumstances highly motivated individuals with no previous cancer research experience but with clear aspirations to move into cancer research may be accepted.    Medical Research Council    Research Grant    99349.0GBP
132    Dr Casals-Pascual Climent    University of Oxford    2008-10-01    2012-09-30    Pathobiological classification of severe malaria based on an integrated approach of high-throughput proteomics and syste    The Biostatistics and Bioinformatics Unit (BBU), in close collaboration with five internationally recognised cancer research groups with substantial Cancer Research UK support, requests funding to enable cancer researchers (or future cancer researchers) to study on an MSc course in bioinformatics. We seek this funding to enable highly motivated individuals to train in bioinformatics in order to boost the use of higher level bioinformatics in cancer research. The course will develop the appropriate complementary skills to provide students with the multidisciplinary bioinformatic skill set required to work effectively in the post genomic era in cancer research. The course contains core modules in computer science, statistics and the use of bioinformatics in the postgenomic era and specialist modules in databasing, machine learning and data mining, algorithmic aspects of sequence analysis and molecular modelling. Students complete a mini-research project as well as a full-time 3 month research project and the latter two components are embedded within the collaborating groups. We envisage individuals already in cancer research will obtain these bursaries in order to develop new skills but in some circumstances highly motivated individuals with no previous cancer research experience but with clear aspirations to move into cancer research may be accepted.    Medical Research Council    Fellowship    921960.0GBP
133    Dr Mourao-Miranda Janaina    University College London    2009-06-15    2014-09-14    A machine learning approach to the analysis of psychiatric neuroimaging data.    The World Health Organization (WHO) states that one in four people experience a mental health problem during their life. The development of technologies for assisting psychiatric diagnosis and predicting treatment responses is extremely relevant to avoid inefficient treatment and to promote mental health. The purpose of this research is to develop models and tools for the application of novel machine learning techniques to the analysis of brain scan data to assist in the understanding, diagnosis and prognosis of psychiatric disorders. Key Goals: 1. Development of new machine learning techniques for the analysis of brain scans including: - Probabilistic classifiers to predict group membership (e.g. patients vs. controls and - responders vs. non-responders); - Hypothesis-driven models of brain alterations in psychiatric disorders (e.g. schizophrenia, depression); - New strategies to extract useful information from neuroimaging data (i.e. feature selection approaches). 2. Developme nt of multi-modal classification integrating data from different techniques (e.g. fMRI, MRI, EEG, genetic data). 3. Validation of the developed approaches using a variety of data sets. 4. Translation of the validated approaches into clinical practice, i.e. development of a toolbox that could be used for aid diagnosis of psychiatric disorders and other clinical/outcome goals.    Wellcome Trust    Research Career Development Fellowship    619620.0GBP
134    Professor Ananiadou Sophia    The University of Manchester    2014-03-31    2017-09-30    Supporting Evidence-based Public Health Interventions using Text Mining    The World Health Organization (WHO) states that one in four people experience a mental health problem during their life. The development of technologies for assisting psychiatric diagnosis and predicting treatment responses is extremely relevant to avoid inefficient treatment and to promote mental health. The purpose of this research is to develop models and tools for the application of novel machine learning techniques to the analysis of brain scan data to assist in the understanding, diagnosis and prognosis of psychiatric disorders. Key Goals: 1. Development of new machine learning techniques for the analysis of brain scans including: - Probabilistic classifiers to predict group membership (e.g. patients vs. controls and - responders vs. non-responders); - Hypothesis-driven models of brain alterations in psychiatric disorders (e.g. schizophrenia, depression); - New strategies to extract useful information from neuroimaging data (i.e. feature selection approaches). 2. Developme nt of multi-modal classification integrating data from different techniques (e.g. fMRI, MRI, EEG, genetic data). 3. Validation of the developed approaches using a variety of data sets. 4. Translation of the validated approaches into clinical practice, i.e. development of a toolbox that could be used for aid diagnosis of psychiatric disorders and other clinical/outcome goals.    Medical Research Council    Research Grant    655668.0GBP
135    Prof Jenkinson Mark    University of Oxford    2019-06-01    2024-05-31    Integrative imaging of brain structure and function in populations and individuals    The World Health Organization (WHO) states that one in four people experience a mental health problem during their life. The development of technologies for assisting psychiatric diagnosis and predicting treatment responses is extremely relevant to avoid inefficient treatment and to promote mental health. The purpose of this research is to develop models and tools for the application of novel machine learning techniques to the analysis of brain scan data to assist in the understanding, diagnosis and prognosis of psychiatric disorders. Key Goals: 1. Development of new machine learning techniques for the analysis of brain scans including: - Probabilistic classifiers to predict group membership (e.g. patients vs. controls and - responders vs. non-responders); - Hypothesis-driven models of brain alterations in psychiatric disorders (e.g. schizophrenia, depression); - New strategies to extract useful information from neuroimaging data (i.e. feature selection approaches). 2. Developme nt of multi-modal classification integrating data from different techniques (e.g. fMRI, MRI, EEG, genetic data). 3. Validation of the developed approaches using a variety of data sets. 4. Translation of the validated approaches into clinical practice, i.e. development of a toolbox that could be used for aid diagnosis of psychiatric disorders and other clinical/outcome goals.    Wellcome Trust    Collaborative Award in Science    4106203.0GBP
136    Prof Beckmann Christian    Radboud Universiteit Nijmegen    2019-06-01    2024-05-31    Integrative imaging of brain structure and function in populations and individuals    The World Health Organization (WHO) states that one in four people experience a mental health problem during their life. The development of technologies for assisting psychiatric diagnosis and predicting treatment responses is extremely relevant to avoid inefficient treatment and to promote mental health. The purpose of this research is to develop models and tools for the application of novel machine learning techniques to the analysis of brain scan data to assist in the understanding, diagnosis and prognosis of psychiatric disorders. Key Goals: 1. Development of new machine learning techniques for the analysis of brain scans including: - Probabilistic classifiers to predict group membership (e.g. patients vs. controls and - responders vs. non-responders); - Hypothesis-driven models of brain alterations in psychiatric disorders (e.g. schizophrenia, depression); - New strategies to extract useful information from neuroimaging data (i.e. feature selection approaches). 2. Developme nt of multi-modal classification integrating data from different techniques (e.g. fMRI, MRI, EEG, genetic data). 3. Validation of the developed approaches using a variety of data sets. 4. Translation of the validated approaches into clinical practice, i.e. development of a toolbox that could be used for aid diagnosis of psychiatric disorders and other clinical/outcome goals.    Wellcome Trust    Collaborative Award in Science    4106203.0GBP
137    Prof Miller Karla    University of Oxford    2019-06-01    2024-05-31    Integrative imaging of brain structure and function in populations and individuals    The World Health Organization (WHO) states that one in four people experience a mental health problem during their life. The development of technologies for assisting psychiatric diagnosis and predicting treatment responses is extremely relevant to avoid inefficient treatment and to promote mental health. The purpose of this research is to develop models and tools for the application of novel machine learning techniques to the analysis of brain scan data to assist in the understanding, diagnosis and prognosis of psychiatric disorders. Key Goals: 1. Development of new machine learning techniques for the analysis of brain scans including: - Probabilistic classifiers to predict group membership (e.g. patients vs. controls and - responders vs. non-responders); - Hypothesis-driven models of brain alterations in psychiatric disorders (e.g. schizophrenia, depression); - New strategies to extract useful information from neuroimaging data (i.e. feature selection approaches). 2. Developme nt of multi-modal classification integrating data from different techniques (e.g. fMRI, MRI, EEG, genetic data). 3. Validation of the developed approaches using a variety of data sets. 4. Translation of the validated approaches into clinical practice, i.e. development of a toolbox that could be used for aid diagnosis of psychiatric disorders and other clinical/outcome goals.    Wellcome Trust    Collaborative Award in Science    4106203.0GBP
138    Prof Smith Stephen    University of Oxford    2019-06-01    2024-05-31    Integrative imaging of brain structure and function in populations and individuals    The World Health Organization (WHO) states that one in four people experience a mental health problem during their life. The development of technologies for assisting psychiatric diagnosis and predicting treatment responses is extremely relevant to avoid inefficient treatment and to promote mental health. The purpose of this research is to develop models and tools for the application of novel machine learning techniques to the analysis of brain scan data to assist in the understanding, diagnosis and prognosis of psychiatric disorders. Key Goals: 1. Development of new machine learning techniques for the analysis of brain scans including: - Probabilistic classifiers to predict group membership (e.g. patients vs. controls and - responders vs. non-responders); - Hypothesis-driven models of brain alterations in psychiatric disorders (e.g. schizophrenia, depression); - New strategies to extract useful information from neuroimaging data (i.e. feature selection approaches). 2. Developme nt of multi-modal classification integrating data from different techniques (e.g. fMRI, MRI, EEG, genetic data). 3. Validation of the developed approaches using a variety of data sets. 4. Translation of the validated approaches into clinical practice, i.e. development of a toolbox that could be used for aid diagnosis of psychiatric disorders and other clinical/outcome goals.    Wellcome Trust    Collaborative Award in Science    4106203.0GBP
139    Prof Jbabdi Saad    University of Oxford    2019-06-01    2024-05-31    Integrative imaging of brain structure and function in populations and individuals    The World Health Organization (WHO) states that one in four people experience a mental health problem during their life. The development of technologies for assisting psychiatric diagnosis and predicting treatment responses is extremely relevant to avoid inefficient treatment and to promote mental health. The purpose of this research is to develop models and tools for the application of novel machine learning techniques to the analysis of brain scan data to assist in the understanding, diagnosis and prognosis of psychiatric disorders. Key Goals: 1. Development of new machine learning techniques for the analysis of brain scans including: - Probabilistic classifiers to predict group membership (e.g. patients vs. controls and - responders vs. non-responders); - Hypothesis-driven models of brain alterations in psychiatric disorders (e.g. schizophrenia, depression); - New strategies to extract useful information from neuroimaging data (i.e. feature selection approaches). 2. Developme nt of multi-modal classification integrating data from different techniques (e.g. fMRI, MRI, EEG, genetic data). 3. Validation of the developed approaches using a variety of data sets. 4. Translation of the validated approaches into clinical practice, i.e. development of a toolbox that could be used for aid diagnosis of psychiatric disorders and other clinical/outcome goals.    Wellcome Trust    Collaborative Award in Science    4106203.0GBP
140    Prof Woolrich Mark    University of Oxford    2019-06-01    2024-05-31    Integrative imaging of brain structure and function in populations and individuals    The World Health Organization (WHO) states that one in four people experience a mental health problem during their life. The development of technologies for assisting psychiatric diagnosis and predicting treatment responses is extremely relevant to avoid inefficient treatment and to promote mental health. The purpose of this research is to develop models and tools for the application of novel machine learning techniques to the analysis of brain scan data to assist in the understanding, diagnosis and prognosis of psychiatric disorders. Key Goals: 1. Development of new machine learning techniques for the analysis of brain scans including: - Probabilistic classifiers to predict group membership (e.g. patients vs. controls and - responders vs. non-responders); - Hypothesis-driven models of brain alterations in psychiatric disorders (e.g. schizophrenia, depression); - New strategies to extract useful information from neuroimaging data (i.e. feature selection approaches). 2. Developme nt of multi-modal classification integrating data from different techniques (e.g. fMRI, MRI, EEG, genetic data). 3. Validation of the developed approaches using a variety of data sets. 4. Translation of the validated approaches into clinical practice, i.e. development of a toolbox that could be used for aid diagnosis of psychiatric disorders and other clinical/outcome goals.    Wellcome Trust    Collaborative Award in Science    4106203.0GBP
141    Dr Robinson Emma    King's College London    2019-06-01    2024-05-31    Integrative imaging of brain structure and function in populations and individuals    The World Health Organization (WHO) states that one in four people experience a mental health problem during their life. The development of technologies for assisting psychiatric diagnosis and predicting treatment responses is extremely relevant to avoid inefficient treatment and to promote mental health. The purpose of this research is to develop models and tools for the application of novel machine learning techniques to the analysis of brain scan data to assist in the understanding, diagnosis and prognosis of psychiatric disorders. Key Goals: 1. Development of new machine learning techniques for the analysis of brain scans including: - Probabilistic classifiers to predict group membership (e.g. patients vs. controls and - responders vs. non-responders); - Hypothesis-driven models of brain alterations in psychiatric disorders (e.g. schizophrenia, depression); - New strategies to extract useful information from neuroimaging data (i.e. feature selection approaches). 2. Developme nt of multi-modal classification integrating data from different techniques (e.g. fMRI, MRI, EEG, genetic data). 3. Validation of the developed approaches using a variety of data sets. 4. Translation of the validated approaches into clinical practice, i.e. development of a toolbox that could be used for aid diagnosis of psychiatric disorders and other clinical/outcome goals.    Wellcome Trust    Collaborative Award in Science    4106203.0GBP
142    Dr Robinson Emma    Royal Holloway, Univ Of London    2019-06-01    2024-05-31    Development of a graph-theoretic approach to predict protein function by integrating large scale heterogeneous data    Statistically sound large-scale protein function prediction can be obtained only by integrating evidence from different sources. Functional inference methods that exploit biological networks topologies offer good performance. But so far such methods are limited in the type of data they can integrate, while methods that can integrate a greater variety of data do not take advantage of the networks' topologies. I propose a general method that can integrate essentially any data type available taking into account the intrinsic structure of each data type: it uses graph-theoretic methods to produce functional evidence from network data, and it integrates it with evidence from one-dimensional information using machine learning techniques. Defining function in terms of the Gene Ontology, I shall collect datasets for S. cerevisiae, C. elegans, D. melanogaster, A. thaliana, H. sapiens. Algorithm development and testing will be done on S. cerevisiae. I shall then verify how these methods transfer to the other organisms. Performance on these organisms will be evaluated in silico", by means of test sets. The approach will also be tested "in vivo" by predicting the Biological Process for a group of MAP kinases that belong to the signalling pathways of A. thaliana. These predictions will be tested through functional assays: 1. an RNAi screen and quantitative measurements of MAPK signalling outputs, MAPK activities and promoter activations in cultured Arabidopsis cells 2. quantitative phenotypic tests for selected phenotypes in cell differentiation (e.g. stomata development) and stress responses. I shall design and implement stand-alone and web-based software tools incorporating the algorithms developed. These will enable the biologist to easily apply the algorithms through a user-friendly interface; visualization tools will make the functional inference process transparent to the user. All these tools will be made freely available to the scientific community."    Biotechnology and Biological Sciences Research Council    Collaborative Award in Science    419814.0GBP
143    Professor Relton Caroline    University of Bristol    2018-04-01    2023-03-31    Epigenetic Epidemiology    Statistically sound large-scale protein function prediction can be obtained only by integrating evidence from different sources. Functional inference methods that exploit biological networks topologies offer good performance. But so far such methods are limited in the type of data they can integrate, while methods that can integrate a greater variety of data do not take advantage of the networks' topologies. I propose a general method that can integrate essentially any data type available taking into account the intrinsic structure of each data type: it uses graph-theoretic methods to produce functional evidence from network data, and it integrates it with evidence from one-dimensional information using machine learning techniques. Defining function in terms of the Gene Ontology, I shall collect datasets for S. cerevisiae, C. elegans, D. melanogaster, A. thaliana, H. sapiens. Algorithm development and testing will be done on S. cerevisiae. I shall then verify how these methods transfer to the other organisms. Performance on these organisms will be evaluated in silico", by means of test sets. The approach will also be tested "in vivo" by predicting the Biological Process for a group of MAP kinases that belong to the signalling pathways of A. thaliana. These predictions will be tested through functional assays: 1. an RNAi screen and quantitative measurements of MAPK signalling outputs, MAPK activities and promoter activations in cultured Arabidopsis cells 2. quantitative phenotypic tests for selected phenotypes in cell differentiation (e.g. stomata development) and stress responses. I shall design and implement stand-alone and web-based software tools incorporating the algorithms developed. These will enable the biologist to easily apply the algorithms through a user-friendly interface; visualization tools will make the functional inference process transparent to the user. All these tools will be made freely available to the scientific community."    Medical Research Council    Unit    1254000.0GBP
144    Dr Ballester Aristin Pedro    EMBL - European Bioinformatics Institute    2010-07-01    2014-06-30    New computational methods for protein function prediction using structural, binding and sequence data    Statistically sound large-scale protein function prediction can be obtained only by integrating evidence from different sources. Functional inference methods that exploit biological networks topologies offer good performance. But so far such methods are limited in the type of data they can integrate, while methods that can integrate a greater variety of data do not take advantage of the networks' topologies. I propose a general method that can integrate essentially any data type available taking into account the intrinsic structure of each data type: it uses graph-theoretic methods to produce functional evidence from network data, and it integrates it with evidence from one-dimensional information using machine learning techniques. Defining function in terms of the Gene Ontology, I shall collect datasets for S. cerevisiae, C. elegans, D. melanogaster, A. thaliana, H. sapiens. Algorithm development and testing will be done on S. cerevisiae. I shall then verify how these methods transfer to the other organisms. Performance on these organisms will be evaluated in silico", by means of test sets. The approach will also be tested "in vivo" by predicting the Biological Process for a group of MAP kinases that belong to the signalling pathways of A. thaliana. These predictions will be tested through functional assays: 1. an RNAi screen and quantitative measurements of MAPK signalling outputs, MAPK activities and promoter activations in cultured Arabidopsis cells 2. quantitative phenotypic tests for selected phenotypes in cell differentiation (e.g. stomata development) and stress responses. I shall design and implement stand-alone and web-based software tools incorporating the algorithms developed. These will enable the biologist to easily apply the algorithms through a user-friendly interface; visualization tools will make the functional inference process transparent to the user. All these tools will be made freely available to the scientific community."    Medical Research Council    Fellowship    400905.0GBP
145    Professor Lyons Ronan    Swansea University    2019-10-01    2022-03-31    Application of machine learning to discover new multimorbidity phenotypes associated with poorer outcomes    Statistically sound large-scale protein function prediction can be obtained only by integrating evidence from different sources. Functional inference methods that exploit biological networks topologies offer good performance. But so far such methods are limited in the type of data they can integrate, while methods that can integrate a greater variety of data do not take advantage of the networks' topologies. I propose a general method that can integrate essentially any data type available taking into account the intrinsic structure of each data type: it uses graph-theoretic methods to produce functional evidence from network data, and it integrates it with evidence from one-dimensional information using machine learning techniques. Defining function in terms of the Gene Ontology, I shall collect datasets for S. cerevisiae, C. elegans, D. melanogaster, A. thaliana, H. sapiens. Algorithm development and testing will be done on S. cerevisiae. I shall then verify how these methods transfer to the other organisms. Performance on these organisms will be evaluated in silico", by means of test sets. The approach will also be tested "in vivo" by predicting the Biological Process for a group of MAP kinases that belong to the signalling pathways of A. thaliana. These predictions will be tested through functional assays: 1. an RNAi screen and quantitative measurements of MAPK signalling outputs, MAPK activities and promoter activations in cultured Arabidopsis cells 2. quantitative phenotypic tests for selected phenotypes in cell differentiation (e.g. stomata development) and stress responses. I shall design and implement stand-alone and web-based software tools incorporating the algorithms developed. These will enable the biologist to easily apply the algorithms through a user-friendly interface; visualization tools will make the functional inference process transparent to the user. All these tools will be made freely available to the scientific community."    Medical Research Council    Research Grant    563132.0GBP
146    Dr O'Regan Declan    Imperial College London    2017-09-18    2020-09-17    Using machine learning to predict clinical outcomes in heart failure.    Statistically sound large-scale protein function prediction can be obtained only by integrating evidence from different sources. Functional inference methods that exploit biological networks topologies offer good performance. But so far such methods are limited in the type of data they can integrate, while methods that can integrate a greater variety of data do not take advantage of the networks' topologies. I propose a general method that can integrate essentially any data type available taking into account the intrinsic structure of each data type: it uses graph-theoretic methods to produce functional evidence from network data, and it integrates it with evidence from one-dimensional information using machine learning techniques. Defining function in terms of the Gene Ontology, I shall collect datasets for S. cerevisiae, C. elegans, D. melanogaster, A. thaliana, H. sapiens. Algorithm development and testing will be done on S. cerevisiae. I shall then verify how these methods transfer to the other organisms. Performance on these organisms will be evaluated in silico", by means of test sets. The approach will also be tested "in vivo" by predicting the Biological Process for a group of MAP kinases that belong to the signalling pathways of A. thaliana. These predictions will be tested through functional assays: 1. an RNAi screen and quantitative measurements of MAPK signalling outputs, MAPK activities and promoter activations in cultured Arabidopsis cells 2. quantitative phenotypic tests for selected phenotypes in cell differentiation (e.g. stomata development) and stress responses. I shall design and implement stand-alone and web-based software tools incorporating the algorithms developed. These will enable the biologist to easily apply the algorithms through a user-friendly interface; visualization tools will make the functional inference process transparent to the user. All these tools will be made freely available to the scientific community."    British Heart Foundation    New Horizons Grant    297017.0GBP
147    Dr O'Regan Declan    Imperial College London    2017-09-18    2020-09-17    Machine learning to model disease mechanisms and predict outcomes in cardiomyopathy    Statistically sound large-scale protein function prediction can be obtained only by integrating evidence from different sources. Functional inference methods that exploit biological networks topologies offer good performance. But so far such methods are limited in the type of data they can integrate, while methods that can integrate a greater variety of data do not take advantage of the networks' topologies. I propose a general method that can integrate essentially any data type available taking into account the intrinsic structure of each data type: it uses graph-theoretic methods to produce functional evidence from network data, and it integrates it with evidence from one-dimensional information using machine learning techniques. Defining function in terms of the Gene Ontology, I shall collect datasets for S. cerevisiae, C. elegans, D. melanogaster, A. thaliana, H. sapiens. Algorithm development and testing will be done on S. cerevisiae. I shall then verify how these methods transfer to the other organisms. Performance on these organisms will be evaluated in silico", by means of test sets. The approach will also be tested "in vivo" by predicting the Biological Process for a group of MAP kinases that belong to the signalling pathways of A. thaliana. These predictions will be tested through functional assays: 1. an RNAi screen and quantitative measurements of MAPK signalling outputs, MAPK activities and promoter activations in cultured Arabidopsis cells 2. quantitative phenotypic tests for selected phenotypes in cell differentiation (e.g. stomata development) and stress responses. I shall design and implement stand-alone and web-based software tools incorporating the algorithms developed. These will enable the biologist to easily apply the algorithms through a user-friendly interface; visualization tools will make the functional inference process transparent to the user. All these tools will be made freely available to the scientific community."    British Heart Foundation    New Horizons Grant    1011285.0GBP
148    Dr Swift Andrew    University of Sheffield    2019-10-01    2021-09-30    Developing a machine learning tool to improve prognostic and treatment response assessment on cardiac MRI data    Statistically sound large-scale protein function prediction can be obtained only by integrating evidence from different sources. Functional inference methods that exploit biological networks topologies offer good performance. But so far such methods are limited in the type of data they can integrate, while methods that can integrate a greater variety of data do not take advantage of the networks' topologies. I propose a general method that can integrate essentially any data type available taking into account the intrinsic structure of each data type: it uses graph-theoretic methods to produce functional evidence from network data, and it integrates it with evidence from one-dimensional information using machine learning techniques. Defining function in terms of the Gene Ontology, I shall collect datasets for S. cerevisiae, C. elegans, D. melanogaster, A. thaliana, H. sapiens. Algorithm development and testing will be done on S. cerevisiae. I shall then verify how these methods transfer to the other organisms. Performance on these organisms will be evaluated in silico", by means of test sets. The approach will also be tested "in vivo" by predicting the Biological Process for a group of MAP kinases that belong to the signalling pathways of A. thaliana. These predictions will be tested through functional assays: 1. an RNAi screen and quantitative measurements of MAPK signalling outputs, MAPK activities and promoter activations in cultured Arabidopsis cells 2. quantitative phenotypic tests for selected phenotypes in cell differentiation (e.g. stomata development) and stress responses. I shall design and implement stand-alone and web-based software tools incorporating the algorithms developed. These will enable the biologist to easily apply the algorithms through a user-friendly interface; visualization tools will make the functional inference process transparent to the user. All these tools will be made freely available to the scientific community."    Wellcome Trust    Innovator Award: Digital Technologies    639873.0GBP
149    Dr Lu Haiping    University of Sheffield    2019-10-01    2021-09-30    Developing a machine learning tool to improve prognostic and treatment response assessment on cardiac MRI data    Statistically sound large-scale protein function prediction can be obtained only by integrating evidence from different sources. Functional inference methods that exploit biological networks topologies offer good performance. But so far such methods are limited in the type of data they can integrate, while methods that can integrate a greater variety of data do not take advantage of the networks' topologies. I propose a general method that can integrate essentially any data type available taking into account the intrinsic structure of each data type: it uses graph-theoretic methods to produce functional evidence from network data, and it integrates it with evidence from one-dimensional information using machine learning techniques. Defining function in terms of the Gene Ontology, I shall collect datasets for S. cerevisiae, C. elegans, D. melanogaster, A. thaliana, H. sapiens. Algorithm development and testing will be done on S. cerevisiae. I shall then verify how these methods transfer to the other organisms. Performance on these organisms will be evaluated in silico", by means of test sets. The approach will also be tested "in vivo" by predicting the Biological Process for a group of MAP kinases that belong to the signalling pathways of A. thaliana. These predictions will be tested through functional assays: 1. an RNAi screen and quantitative measurements of MAPK signalling outputs, MAPK activities and promoter activations in cultured Arabidopsis cells 2. quantitative phenotypic tests for selected phenotypes in cell differentiation (e.g. stomata development) and stress responses. I shall design and implement stand-alone and web-based software tools incorporating the algorithms developed. These will enable the biologist to easily apply the algorithms through a user-friendly interface; visualization tools will make the functional inference process transparent to the user. All these tools will be made freely available to the scientific community."    Wellcome Trust    Innovator Award: Digital Technologies    639873.0GBP
150    Professor Williams Steven    King's College London    2012-08-06    2014-07-05    Cerebral Blood Flow Imaging - Towards an Efficient, Automated Assay of Ongoing Pain and its Treatment    None    Medical Research Council    Research Grant    394246.0GBP
151    Prof. Dehaene Stanislas Pierre Joseph    French Alternative Energies and Atomic Energy Commission    2016-10-01    2021-09-30    The cerebral representation of sequences and roles : investigating the origins of human uniqueness.    What are the origins of humans’ remarkable capacities to grasp, memorize, and produce complex sequences and rules, as manifested in language and mathematics? During its evolution, the human brain may have acquired a capacity to represent nested rules, based in part on the expansion of circuits involving the inferior frontal gyrus. This hypothesis will be tested using behavioral measures, functional MRI, magneto-encephalography (MEG), electro-corticography (ECOG) and machine learning techniques in human and non-human primates tested in identical paradigms. (1) We will design a hierarchy of non-linguistic visual and auditory sequences that place increasing demands on abstract rule coding.(2) Behavioral studies of pointing time and eye tracking will investigate the memory for such sequences in human adults, children, and macaque monkeys, and their extrapolation to future items. (3) Functional MRI, MEG, and ECOG will probe the localization, time course, and neural coding of such non-linguistic sequences in human adults. (4) In the same subjects, we will investigate the representation of linguistic and mathematical structures and determine if they involve the same areas and coding principles. (5) We will also record fMRI and ECOG responses to this hierarchy of non-linguistic sequences in macaque monkeys, in search of both correspondences and sharp differences with humans. (6) The same non-linguistic materials will be used in fMRI and EEG studies of human children and infants. Our hypothesis predicts that human children may perform better than adult monkeys. (7) We will formulate and test mathematical models that propose that the human brain “compresses” incoming sequences using nested rules (Kolmogorov complexity), uses predictive codes to anticipate on future inputs, and encodes syntax via tensor-product representations. The results will clarify the brain mechanisms of human language and abstraction abilities, and shed light on their ontogeny and phylogeny.    European Research Council    Advanced Grant    2499747.0EUR
152    Dr Fu Cynthia    King's College London    2009-04-15    2010-04-14    Application of conformal predictors to functional magnetic resonance imaging research    What are the origins of humans’ remarkable capacities to grasp, memorize, and produce complex sequences and rules, as manifested in language and mathematics? During its evolution, the human brain may have acquired a capacity to represent nested rules, based in part on the expansion of circuits involving the inferior frontal gyrus. This hypothesis will be tested using behavioral measures, functional MRI, magneto-encephalography (MEG), electro-corticography (ECOG) and machine learning techniques in human and non-human primates tested in identical paradigms. (1) We will design a hierarchy of non-linguistic visual and auditory sequences that place increasing demands on abstract rule coding.(2) Behavioral studies of pointing time and eye tracking will investigate the memory for such sequences in human adults, children, and macaque monkeys, and their extrapolation to future items. (3) Functional MRI, MEG, and ECOG will probe the localization, time course, and neural coding of such non-linguistic sequences in human adults. (4) In the same subjects, we will investigate the representation of linguistic and mathematical structures and determine if they involve the same areas and coding principles. (5) We will also record fMRI and ECOG responses to this hierarchy of non-linguistic sequences in macaque monkeys, in search of both correspondences and sharp differences with humans. (6) The same non-linguistic materials will be used in fMRI and EEG studies of human children and infants. Our hypothesis predicts that human children may perform better than adult monkeys. (7) We will formulate and test mathematical models that propose that the human brain “compresses” incoming sequences using nested rules (Kolmogorov complexity), uses predictive codes to anticipate on future inputs, and encodes syntax via tensor-product representations. The results will clarify the brain mechanisms of human language and abstraction abilities, and shed light on their ontogeny and phylogeny.    Medical Research Council    Research Grant    99973.0GBP
153    Dr Campbell Ian    University of Bristol    2008-10-01    2012-04-30    siRNA target selection in cancer research using machine learning techniques    We wil use advanced methods from machine learning and statistics, together with prior biological knowledge, to rank candidates for gene expression knockdown using short interfering RNAs. These targets could lead through to potential therapeutic targets for further study.    Cancer Research UK    Project Award    99973.0GBP
154    Prof. Yau Christopher    The University of Manchester    2020-01-07    2020-10-08    From single cells to populations: generalized pseudotime analysis to identify patient trajectories from cross-sectional data in cancer genomics    We wil use advanced methods from machine learning and statistics, together with prior biological knowledge, to rank candidates for gene expression knockdown using short interfering RNAs. These targets could lead through to potential therapeutic targets for further study.    Medical Research Council    Research Grant    117835.0GBP
155    Prof. Yau Christopher    University of Edinburgh    2012-10-01    2017-10-01    Machine learning for computational science:statistical and formal modelling of biological systems    Computational modelling is changing the face of science. Many complex systems can be understood as embodied computational systems performing distributed computations on a massive scale. Biology is the discipline where these ideas find their most natural application: cells can be viewed as input/ output devices, with proteins and organelles behaving as finite state machines performing distributed computations inside the cell. This led to the influential framework of cell as computation, and the successful deployment of formal verification and analysis on models of biological systems. This paradigm shift in our understanding of biology has been possible due to the increasingly quantitative experimental techniques being developed in experimental biology. Formal modelling techniques, however, do not have mechanisms to directly include the information obtained from experimental observations in a statistically consistent way. This difficulty in relating the experimental and theoretical developments in biology is a central problem: without incorporating observations, it is extremely difficult to obtain reliable parametrisations of models. More importantly, it is impossible to assess the confidence of model predictions. This means that the central scientific task of falsifying hypotheses cannot be performed in a statistically meaningful way, and that it is very difficult to employ model predictions to rationally plan novel experiments.In this project we will build and develop machine learning tools for continuous time stochastic processes to obtain a principled treatment of the uncertainty at every step of the modelling pipeline. We will use and extend probabilistic programming languages to fully automate the inference tasks, and link to advanced modelling languages to allow formal analysis tools to be deployed in a data modelling framework. We will pursue twoapplications to fundamental problems in systems biology, guaranteeing impact on exciting scientific questions.    European Research Council    Starting Grant    1421944.0EUR
156    Dr Myers Nicholas    University of Oxford    2017-01-01    2021-01-01    Dynamic cortical networks for cognitive flexibility    Computational modelling is changing the face of science. Many complex systems can be understood as embodied computational systems performing distributed computations on a massive scale. Biology is the discipline where these ideas find their most natural application: cells can be viewed as input/ output devices, with proteins and organelles behaving as finite state machines performing distributed computations inside the cell. This led to the influential framework of cell as computation, and the successful deployment of formal verification and analysis on models of biological systems. This paradigm shift in our understanding of biology has been possible due to the increasingly quantitative experimental techniques being developed in experimental biology. Formal modelling techniques, however, do not have mechanisms to directly include the information obtained from experimental observations in a statistically consistent way. This difficulty in relating the experimental and theoretical developments in biology is a central problem: without incorporating observations, it is extremely difficult to obtain reliable parametrisations of models. More importantly, it is impossible to assess the confidence of model predictions. This means that the central scientific task of falsifying hypotheses cannot be performed in a statistically meaningful way, and that it is very difficult to employ model predictions to rationally plan novel experiments.In this project we will build and develop machine learning tools for continuous time stochastic processes to obtain a principled treatment of the uncertainty at every step of the modelling pipeline. We will use and extend probabilistic programming languages to fully automate the inference tasks, and link to advanced modelling languages to allow formal analysis tools to be deployed in a data modelling framework. We will pursue twoapplications to fundamental problems in systems biology, guaranteeing impact on exciting scientific questions.    Wellcome Trust    Sir Henry Wellcome Postdoctoral Fellowship    250000.0GBP
157    Professor Dr Kirchhof Paulus    Birmingham, University of    2017-01-01    2021-01-01    Defining clusters of patients with atrial fibrillation at risk of heart failure and death    Computational modelling is changing the face of science. Many complex systems can be understood as embodied computational systems performing distributed computations on a massive scale. Biology is the discipline where these ideas find their most natural application: cells can be viewed as input/ output devices, with proteins and organelles behaving as finite state machines performing distributed computations inside the cell. This led to the influential framework of cell as computation, and the successful deployment of formal verification and analysis on models of biological systems. This paradigm shift in our understanding of biology has been possible due to the increasingly quantitative experimental techniques being developed in experimental biology. Formal modelling techniques, however, do not have mechanisms to directly include the information obtained from experimental observations in a statistically consistent way. This difficulty in relating the experimental and theoretical developments in biology is a central problem: without incorporating observations, it is extremely difficult to obtain reliable parametrisations of models. More importantly, it is impossible to assess the confidence of model predictions. This means that the central scientific task of falsifying hypotheses cannot be performed in a statistically meaningful way, and that it is very difficult to employ model predictions to rationally plan novel experiments.In this project we will build and develop machine learning tools for continuous time stochastic processes to obtain a principled treatment of the uncertainty at every step of the modelling pipeline. We will use and extend probabilistic programming languages to fully automate the inference tasks, and link to advanced modelling languages to allow formal analysis tools to be deployed in a data modelling framework. We will pursue twoapplications to fundamental problems in systems biology, guaranteeing impact on exciting scientific questions.    British Heart Foundation    Sir Henry Wellcome Postdoctoral Fellowship    73801.0GBP
158    Dr Belgrave Danielle    Imperial College London    2015-10-01    2018-09-30    Unified probabilistic latent variable modelling strategies to accelerate endotype discovery in longitudinal studies    Computational modelling is changing the face of science. Many complex systems can be understood as embodied computational systems performing distributed computations on a massive scale. Biology is the discipline where these ideas find their most natural application: cells can be viewed as input/ output devices, with proteins and organelles behaving as finite state machines performing distributed computations inside the cell. This led to the influential framework of cell as computation, and the successful deployment of formal verification and analysis on models of biological systems. This paradigm shift in our understanding of biology has been possible due to the increasingly quantitative experimental techniques being developed in experimental biology. Formal modelling techniques, however, do not have mechanisms to directly include the information obtained from experimental observations in a statistically consistent way. This difficulty in relating the experimental and theoretical developments in biology is a central problem: without incorporating observations, it is extremely difficult to obtain reliable parametrisations of models. More importantly, it is impossible to assess the confidence of model predictions. This means that the central scientific task of falsifying hypotheses cannot be performed in a statistically meaningful way, and that it is very difficult to employ model predictions to rationally plan novel experiments.In this project we will build and develop machine learning tools for continuous time stochastic processes to obtain a principled treatment of the uncertainty at every step of the modelling pipeline. We will use and extend probabilistic programming languages to fully automate the inference tasks, and link to advanced modelling languages to allow formal analysis tools to be deployed in a data modelling framework. We will pursue twoapplications to fundamental problems in systems biology, guaranteeing impact on exciting scientific questions.    Medical Research Council    Fellowship    260595.0GBP
159    Dr Bentley Paul    Imperial College London    2015-11-01    2019-04-30    Decision-assist software for management of acute ischaemic stroke using brain-imaging machine-learning    Computational modelling is changing the face of science. Many complex systems can be understood as embodied computational systems performing distributed computations on a massive scale. Biology is the discipline where these ideas find their most natural application: cells can be viewed as input/ output devices, with proteins and organelles behaving as finite state machines performing distributed computations inside the cell. This led to the influential framework of cell as computation, and the successful deployment of formal verification and analysis on models of biological systems. This paradigm shift in our understanding of biology has been possible due to the increasingly quantitative experimental techniques being developed in experimental biology. Formal modelling techniques, however, do not have mechanisms to directly include the information obtained from experimental observations in a statistically consistent way. This difficulty in relating the experimental and theoretical developments in biology is a central problem: without incorporating observations, it is extremely difficult to obtain reliable parametrisations of models. More importantly, it is impossible to assess the confidence of model predictions. This means that the central scientific task of falsifying hypotheses cannot be performed in a statistically meaningful way, and that it is very difficult to employ model predictions to rationally plan novel experiments.In this project we will build and develop machine learning tools for continuous time stochastic processes to obtain a principled treatment of the uncertainty at every step of the modelling pipeline. We will use and extend probabilistic programming languages to fully automate the inference tasks, and link to advanced modelling languages to allow formal analysis tools to be deployed in a data modelling framework. We will pursue twoapplications to fundamental problems in systems biology, guaranteeing impact on exciting scientific questions.    National Institute for Health Research (Department of Health)    Full Award    525360.0GBP
160    Dr Bentley Paul    Eberhard Karls Universität Tübingen    2013-04-01    2018-04-01    Language Evolution: The Empirical Turn    This proposal describes a highly interdisciplinary approach to the empirical study of cultural language evolution. It draws on ideas and methods from *historical linguistics and typology*, *natural language processing*, *biology*, *bioinformatics*, *computer science*, and *statistics*.The computer aided study of cultural language evolution has seen a tremendous upturn over the past fifteen years. This comprises both model-driven approaches - studying the consequences of design assumptions regarding language production, comprehension, and learning for their long-term population-wide consequences - and data-driven approaches that employ algorithmic techniques from bioinformatics to recover otherwise inaccessible information about language history. At the current junction, the field faces two challenges:- The specifics of language evolution - which includes parallels with but also key differences to biological evolution - require central attention.- Model-driven and data-driven approaches need to inform each other to achieve explanatory power and to assess the statistical significance of the findings.The project will establish a radically data-oriented framework for the study of language evolution. This includes three aspects:- replacing the off-the-shelf tools from bioinformatics that are currently in use in computational language classification by linguistically informed algorithms, esp.\ *multiple sequence alignment techniques*,- identifying characteristic traits of language evolution via *exploratory data analysis*, guided by the theory of *complex systems* and employing cutting-edge methods from *machine learning* such as *kernel methods* and *causal inference*, and- developing, implementing and testing models of language evolution that correctly predict the *statistical fingerprints of language evolution*, i.e. pay sufficient attention to the domain specific features of language evolution that have no counterpart in biological evolution.    European Research Council    Advanced Grant    2003580.0EUR
161    Dr Kolehmainen Niina    Newcastle University    2020-05-01    2020-10-31    Understanding early life determinants and mechanisms to preventing life course multimorbidity    This proposal describes a highly interdisciplinary approach to the empirical study of cultural language evolution. It draws on ideas and methods from *historical linguistics and typology*, *natural language processing*, *biology*, *bioinformatics*, *computer science*, and *statistics*.The computer aided study of cultural language evolution has seen a tremendous upturn over the past fifteen years. This comprises both model-driven approaches - studying the consequences of design assumptions regarding language production, comprehension, and learning for their long-term population-wide consequences - and data-driven approaches that employ algorithmic techniques from bioinformatics to recover otherwise inaccessible information about language history. At the current junction, the field faces two challenges:- The specifics of language evolution - which includes parallels with but also key differences to biological evolution - require central attention.- Model-driven and data-driven approaches need to inform each other to achieve explanatory power and to assess the statistical significance of the findings.The project will establish a radically data-oriented framework for the study of language evolution. This includes three aspects:- replacing the off-the-shelf tools from bioinformatics that are currently in use in computational language classification by linguistically informed algorithms, esp.\ *multiple sequence alignment techniques*,- identifying characteristic traits of language evolution via *exploratory data analysis*, guided by the theory of *complex systems* and employing cutting-edge methods from *machine learning* such as *kernel methods* and *causal inference*, and- developing, implementing and testing models of language evolution that correctly predict the *statistical fingerprints of language evolution*, i.e. pay sufficient attention to the domain specific features of language evolution that have no counterpart in biological evolution.    Medical Research Council    Research Grant    99268.0GBP
162    Dr Kolehmainen Niina    Eidgenössische Technische Hochschule Zürich    2014-09-01    2019-09-01    Land-Climate Interactions: Constraints for Droughts and Heatwaves in a Changing Climate    Land-climate interactions mediated through soil moisture and vegetation play a critical role in the climate system, in particular for the occurrence of extreme events such as droughts and heatwaves. They are, however, poorly constrained in current Earth System Models (ESMs), leading to large uncertainties in climate projections. These uncertainties affect the quality and accuracy of projections of temperature, water availability, and carbon concentrations, as well as that of projected impacts on agriculture, ecosystems, and health. In the past years, in-situ and remote sensing-based datasets of soil moisture, evapotranspiration, and energy and carbon fluxes have become increasingly available, providing untapped potential for reducing associated uncertainties in current climate models. The DROUGHT-HEAT project aims at innovatively exploiting these new information sources in order to 1) derive observations-based diagnostics to quantify and isolate the role of land-climate interactions in past extreme events ("Diagnostic Atlas"), 2) evaluate and improve current ESMs and constrain climate-change projections using the derived diagnostics, and 3) apply the newly gained knowledge to frontier developments in the attribution of climate extremes to land processes and their mitigation through "land geoengineering". The DROUGHT-HEAT project integrates the newest land observational datasets with the latest stream of ESMs. Novel methodologies will be applied to extract functional relationships from the data, and identify key gaps in the ESMs' representation of underlying processes. These will build on physically-based relationships, machine learning tools, and model calibration. In addition, they will encompass the mapping and merging of derived diagnostics in space and time to reduce "blank spaces" in the datasets. The project is unprecedented in its breadth and scope and will allow a major breakthrough in our understanding of the processes leading to heatwaves and droughts.    European Research Council    Consolidator Grant    1952285.0EUR
163    Dr Chan Laureen Lui Yan    Cardiff University    2010-09-01    2011-08-31    MSc in Bioinformatics and Genetic Epidemiology & Bioinformatics    The aim of the MSc Bioinformatics programme is to gain skills in computational power to investigate genetic epidemiology and post-genomics biology. The exponential growth of epigenetics and genomic data facilitates innovative approaches to investigate risk of having cancer and underlying mechanisms. As some genes play a dual role in cancer and obesity, association between cancer and obesity can be assessed. I have reported that Bitter melon possesses anti-diabetic and anti-obesity effects. Recent studies revealed its anti-tumour property. It was shown that Bitter melon decreased O6-methylguanine DNA adduct in colonic mucosa. It is hypothesized that localization of CpG island in genes and hence mechanisms could substantiate effects of Bitter melon in obesity-related cancer. Besides, maternal nutrition remodels underlying mechanism of obesity which may exaggerate the growth of tumours and then hinder treatment outcome. Genetics and epigenetics may be clues to delineate mechanisms of obesity-related cancer. The effects of diet on the obesity-cancer association could be investigated in network pathway analysis. Genetic epidemiology and artificial biological models can be established to study time-dependent effects. Having been trained in machine learning, pattern recognition, genetic epidemiology, likelihood and quantitative trait analysis, cutting edge methods could be used to further investigate these polygenic diseases.    Cancer Research UK    Bursary    1952285.0EUR
164    Dr Nangalia Vishal    University College London    2013-06-03    2016-08-02    Machine Learning - Early Warning System (ML-EWS)    The aim of the MSc Bioinformatics programme is to gain skills in computational power to investigate genetic epidemiology and post-genomics biology. The exponential growth of epigenetics and genomic data facilitates innovative approaches to investigate risk of having cancer and underlying mechanisms. As some genes play a dual role in cancer and obesity, association between cancer and obesity can be assessed. I have reported that Bitter melon possesses anti-diabetic and anti-obesity effects. Recent studies revealed its anti-tumour property. It was shown that Bitter melon decreased O6-methylguanine DNA adduct in colonic mucosa. It is hypothesized that localization of CpG island in genes and hence mechanisms could substantiate effects of Bitter melon in obesity-related cancer. Besides, maternal nutrition remodels underlying mechanism of obesity which may exaggerate the growth of tumours and then hinder treatment outcome. Genetics and epigenetics may be clues to delineate mechanisms of obesity-related cancer. The effects of diet on the obesity-cancer association could be investigated in network pathway analysis. Genetic epidemiology and artificial biological models can be established to study time-dependent effects. Having been trained in machine learning, pattern recognition, genetic epidemiology, likelihood and quantitative trait analysis, cutting edge methods could be used to further investigate these polygenic diseases.    Medical Research Council    Fellowship    260560.0GBP
165    Dr Hayes Joseph    University College London    2019-01-01    2022-01-01    Personalising the pharmacological treatment of bipolar disorder    The aim of the MSc Bioinformatics programme is to gain skills in computational power to investigate genetic epidemiology and post-genomics biology. The exponential growth of epigenetics and genomic data facilitates innovative approaches to investigate risk of having cancer and underlying mechanisms. As some genes play a dual role in cancer and obesity, association between cancer and obesity can be assessed. I have reported that Bitter melon possesses anti-diabetic and anti-obesity effects. Recent studies revealed its anti-tumour property. It was shown that Bitter melon decreased O6-methylguanine DNA adduct in colonic mucosa. It is hypothesized that localization of CpG island in genes and hence mechanisms could substantiate effects of Bitter melon in obesity-related cancer. Besides, maternal nutrition remodels underlying mechanism of obesity which may exaggerate the growth of tumours and then hinder treatment outcome. Genetics and epigenetics may be clues to delineate mechanisms of obesity-related cancer. The effects of diet on the obesity-cancer association could be investigated in network pathway analysis. Genetic epidemiology and artificial biological models can be established to study time-dependent effects. Having been trained in machine learning, pattern recognition, genetic epidemiology, likelihood and quantitative trait analysis, cutting edge methods could be used to further investigate these polygenic diseases.    Wellcome Trust    Clinical Research Career Development Fellowship    460704.0GBP
166    Dr Hayes Joseph    Foundation For Research And Technology - Hellas    2012-10-01    2017-10-01    Dissecting the Role of Dendrites in Memory    Understanding the rules and mechanisms underlying memory formation, storage and retrieval is a grand challenge in neuroscience. In light of cumulating evidence regarding non-linear dendritic events (dendritic-spikes, branch strength potentiation, temporal sequence detection etc) together with activity-dependent rewiring of the connection matrix, the classical notion of information storage via Hebbian-like changes in synaptic connections is inadequate. While more recent plasticity theories consider non-linear dendritic properties, a unifying theory of how dendrites are utilized to achieve memory coding, storing and/or retrieval is cruelly missing. Using computational models, we will simulate memory processes in three key brain regions: the hippocampus, the amygdala and the prefrontal cortex. Models will incorporate biologically constrained dendrites and state-of-the-art plasticity rules and will span different levels of abstraction, ranging from detailed biophysical single neurons and circuits to integrate-and-fire networks and abstract theoretical models. Our main goal is to dissect the role of dendrites in information processing and storage across the three different regions by systematically altering their anatomical, biophysical and plasticity properties. Findings will further our understanding of the fundamental computations supported by these structures and how these computations, reinforced by plasticity mechanisms, sub-serve memory formation and associated dysfunctions, thus opening new avenues for hypothesis driven experimentation and development of novel treatments for memory-related diseases. Identification of dendrites as the key processing units across brain regions and complexity levels will lay the foundations for a new era in computational and experimental neuroscience and serve as the basis for groundbreaking advances in the robotics and artificial intelligence fields while also having a large impact on the machine learning community.    European Research Council    Starting Grant    1398000.0EUR
167    Professor Collier Nigel    University of Cambridge    2012-10-01    2017-10-01    SIPHS: Semantic interpretation of personal health messages for generating public health summaries    Open online data such as microblogs and discussion board messages have the potential to be an incredibly valuable source of information about health in populations. Such data has been rapidly growing, is low cost, real-time and seems likely to cover a significant proportion of the demographic. To take two examples, PatientsLikeMe has enjoyed 10% growth and now has over 200,000 users covering over 1500 health conditions; the generic Twitter service is expanding at a rate of 30% annually with over 200 million active users. Going beyond simple keyword search and harnessing this data for public health represents both an opportunity and a challenge to natural language processing (NLP). This fellowship proposal is about helping health experts leverage social media for their own clinical and scientific studies through automatic techniques that encode messages according to a machine understandable semantic representation. There are three major challenges this project seeks to address: (1) knowledge brokering: to develop algorithms to identify and code the informal descriptions of conditions, treatments, medications, behaviours and attitudes to standard ontologies such as the UMLS; (2) knowledge management: to create a structured resource of patient vocabulary used in blog texts and link it to existing coding systems; and (3) adding insight to evidence: to work with domain experts to utilize the coded information to automatically generate meaningful summaries for follow up investigation. At the technological level the fellowship seeks to pioneer new methods for NLP and machine learning (ML). Social media remains a challenging area for NLP for a variety of reasons: short de-contextualised messages, high levels of ambiguity/out of vocabulary words, use of slang and an evolving vocabulary, as well as inherent bias towards sensational topics. The fellowship seeks to harness the progress made so far in NLP for social media analysis in the commercial domain and develop it further to provide meaningful public health evidence. One key aspect not previously addressed is in the clinical coding of patient messages. Although knowledge brokering systems exist for clinical and scientific texts (e.g. the NLM's MetaMap), their performance on social media messages has been poor. The fellowship will utilise the rich availability of ontological resources in biomedicine together with ML on annotated message data to disambiguate informal language. Research will also aim to understanding the communicative function of messages, for example whether the message reports direct experience or is related to news, humour or marketing. If these problems are successfully overcome an important barrier to data integration with other types of clinical data will be removed. The advantage of providing health coding for social media reports is its potential for studying very-large scale cohorts and also in real-time early alerting of aberrations. In the fellowship I will research the potential for multi-variate time series alerting from semantically coded features, working with domain experts to evaluate across a range of metrics (e.g. sensitivity, timeliness, false alerting rates). A variety of approaches will be explored to generate real time risk summaries across social media sources. Two real-world applications have been chosen to take this forwards: early alerting for Adverse drug reactions (ADRs) and Infectious disease surveillance (IDS). Project outcomes will include fundamental technologies as well as open source algorithms, data sets and ontology. An exciting aspect of this fellowship is inter-disciplinary collaboration across stakeholders at all levels: scientists, public health experts and industry. Finally, participation will be opened up to the international community through the release of open source data. Colleagues working on social media technologies will be invited to participate in discussions with users at a new challenge evaluation workshop.    UK Research and Innovation    Starting Grant    971954.0GBP
168    Doctor Dobson Richard    King's College London    2013-10-01    2016-09-30    Creating an early diagnostic blood test for Alzheimer's Disease    In this PhD we aim to validate, refine and extend a panel of biomarkers of early Alzheimer's disease pathology. One of the earliest known markers of Alzheimer's disease pathology is the level of amyloid beta in the brain; another early marker is hippocampal volume. Our previous work has shown that the level of some proteins in plasma associate with brain amyloid burden across independent cohorts and proteomic platforms. In this study these plasma protein markers will be validated, and novel plasma protein markers of brain amyloid burden identified. In addition, inexpensive and non-invasive multi-modal biomarkers of early Alzheimer's disease pathology will be investigated for the first time. Data on the level of plasma proteins, and/or other blood analytes or cognitive measures, will be compared to brain amyloid burden and/or hippocampal volume as derived from brain scans. Regression, classification and machine learning approaches will be used to study the ability of these biomarkers to predict brain amyloid burden or hippocampal atrophy. Cross-validation will be used to test the robustness of these biomarkers over datasets derived from independent cohorts and proteomic platforms. These results will reveal the clinical utility of inexpensive and non-invasive biomarkers of early Alzheimer's disease, a potentially transformative technology.    Alzheimer's Society    PhD Studentship    79457.0GBP
169    Dr Bottle Alex    Imperial College London    2010-09-01    2014-02-28    Can valid and practical risk-prediction or casemix adjustment models, including adjustment for co-morbidity, be generated from English hospital administrative data (Hospital Episode Statistics)?    In this PhD we aim to validate, refine and extend a panel of biomarkers of early Alzheimer's disease pathology. One of the earliest known markers of Alzheimer's disease pathology is the level of amyloid beta in the brain; another early marker is hippocampal volume. Our previous work has shown that the level of some proteins in plasma associate with brain amyloid burden across independent cohorts and proteomic platforms. In this study these plasma protein markers will be validated, and novel plasma protein markers of brain amyloid burden identified. In addition, inexpensive and non-invasive multi-modal biomarkers of early Alzheimer's disease pathology will be investigated for the first time. Data on the level of plasma proteins, and/or other blood analytes or cognitive measures, will be compared to brain amyloid burden and/or hippocampal volume as derived from brain scans. Regression, classification and machine learning approaches will be used to study the ability of these biomarkers to predict brain amyloid burden or hippocampal atrophy. Cross-validation will be used to test the robustness of these biomarkers over datasets derived from independent cohorts and proteomic platforms. These results will reveal the clinical utility of inexpensive and non-invasive biomarkers of early Alzheimer's disease, a potentially transformative technology.    National Institute for Health Research (Department of Health)    Full Grant    400921.33GBP
170    Professor Koh Dow-Mu    The Royal Marsden NHS Foundation Trust    2017-01-02    2020-01-01    Advanced computer diagnostics for whole body magnetic resonance imaging to improve management of patients with metastatic bone cancer    In this PhD we aim to validate, refine and extend a panel of biomarkers of early Alzheimer's disease pathology. One of the earliest known markers of Alzheimer's disease pathology is the level of amyloid beta in the brain; another early marker is hippocampal volume. Our previous work has shown that the level of some proteins in plasma associate with brain amyloid burden across independent cohorts and proteomic platforms. In this study these plasma protein markers will be validated, and novel plasma protein markers of brain amyloid burden identified. In addition, inexpensive and non-invasive multi-modal biomarkers of early Alzheimer's disease pathology will be investigated for the first time. Data on the level of plasma proteins, and/or other blood analytes or cognitive measures, will be compared to brain amyloid burden and/or hippocampal volume as derived from brain scans. Regression, classification and machine learning approaches will be used to study the ability of these biomarkers to predict brain amyloid burden or hippocampal atrophy. Cross-validation will be used to test the robustness of these biomarkers over datasets derived from independent cohorts and proteomic platforms. These results will reveal the clinical utility of inexpensive and non-invasive biomarkers of early Alzheimer's disease, a potentially transformative technology.    National Institute for Health Research (Department of Health)    Full Award    1142228.0GBP
171    Dr Gibbons Chris    University of Cambridge    2017-10-01    2018-09-30    Development and testing of an intelligent system for the personalised reporting and evaluation of patient-reported data (INSPiRED).    In this PhD we aim to validate, refine and extend a panel of biomarkers of early Alzheimer's disease pathology. One of the earliest known markers of Alzheimer's disease pathology is the level of amyloid beta in the brain; another early marker is hippocampal volume. Our previous work has shown that the level of some proteins in plasma associate with brain amyloid burden across independent cohorts and proteomic platforms. In this study these plasma protein markers will be validated, and novel plasma protein markers of brain amyloid burden identified. In addition, inexpensive and non-invasive multi-modal biomarkers of early Alzheimer's disease pathology will be investigated for the first time. Data on the level of plasma proteins, and/or other blood analytes or cognitive measures, will be compared to brain amyloid burden and/or hippocampal volume as derived from brain scans. Regression, classification and machine learning approaches will be used to study the ability of these biomarkers to predict brain amyloid burden or hippocampal atrophy. Cross-validation will be used to test the robustness of these biomarkers over datasets derived from independent cohorts and proteomic platforms. These results will reveal the clinical utility of inexpensive and non-invasive biomarkers of early Alzheimer's disease, a potentially transformative technology.    National Institute for Health Research (Department of Health)    Full award    115285.0GBP
172    Dr Gibbons Chris    Università degli Studi di Trento    2014-08-01    2019-08-01    Transfer Learning within and between brains    The neural bases of adaptive behavior in social environments are far from being understood. We propose to use both computational and neuroscientific methodologies to provide new and more accurate models of learning in interactive settings. The long-term objective is to develop a neural theory of learning: a mathematical framework that describes the computations mediating social learning in terms of neural signals, structures and plasticity. We plan to develop a model of adaptive learning based on three basic principles: (1) the observation of the outcome of un-chosen options improves the decisions taken in the learning process, (2) learning can be transferred from one domain to another, and (3) learning can be transferred from one agent to another (i. e. social learning). In all three cases, humans appear able to construct and transfer knowledge from sources other than their own direct experience, an underappreciated though we believe critical aspect of learning. Our approach will combine neural and behavioral data with computational models of learning. The hypotheses will be formalized into machine learning algorithms and neural networks of "regret" learning, to quantify the evolution of the learning computations on a trial-by-trial basis from the sequence of stimuli, choices and outcomes. The existence and accuracy of the predicted computations will be then tested on neural signals recorded with functional magnetic resonance imaging (fMRI). The potential findings of this project could lead us to suggest general principles of social learning, and we will be able to measure and model neural activation to show those general principles in action. In addition, our results could have important implications into policy-making - by revealing what type of information agents are naturally inclined to better learn from - and clinical practice - by outlining potential diagnostic procedures and behavioral therapies for disorders affecting social behavior.    European Research Council    Consolidator Grant    1999998.0EUR
173    Dr Sproul Duncan    University of Edinburgh    2016-09-01    2022-08-31    The impact of genetic interactions on DNA methylation in breast cancer    Background:Epigenetic dysfunction is a near universal feature of carcinogenesis. DNA methylation alterations affect key breast cancer genes such as BRCA1 suggesting their importance to carcinogenesis and progression. However, the mechanisms underpinning such alterations in DNA methylation are unknown, so their significance remains unclear.My recent work suggests epigenetic marks are strongly programmed by the underlying DNA sequence. This means that sequence variants are likely to be an important and underexplored influence on breast cancer epigenomes.Aims:This project aims to understand the role of the genome in programming the epigenetic mark DNA methylation in breast cancer through two aims.1. Define the extent to which germ-line and somatic DNA sequence variants modulate local levels of DNA methylation in breast tumours in vivo.2. Understand how the effects of regulatory variants on DNA methylation and gene expression interact and how such variants impact on clinical phenotypes.Methods:I will elucidate the frequency to which germ-line and somatic sequence variants affect DNA methylation in breast cancer through machine-learning analyses of HER2-driven mouse mammary tumours and clinical high-grade luminal breast tumours. The interaction between variant-modulated alterations in DNA methylation and gene expression will be tested using genome editing technologies and the promoter of BRCA1 as a paradigm. The phenotypic impact of BRCA1 regulatory mutations will be assessed through engineering into mice. These synergistic approaches reinforce one another to uncover the global impact of sequence variation on DNA methylation, the mechanistic details underlying these effects and their clinical relevance. How the results of this research will be used:This project uses state-of-the-art computational and laboratory approaches to leverage genetics towards tackling our lack of understanding regarding the impact of DNA sequence variation on epigenetic alterations in cancer. In doing so, it impacts on our understanding of the regulation of a key breast and ovarian cancer risk gene, BRCA1, and provides new approaches to delineate the contribution of regulatory variants to carcinogenesis and progression.    Cancer Research UK    NIC - Career Development Fellowship    1999998.0EUR
174    Dr Overton Ian    University of Edinburgh    2012-04-01    2017-07-31    Computational studies of phenotypic plasticity in development, metastasis and drug response; towards new clinical tools    Background:Epigenetic dysfunction is a near universal feature of carcinogenesis. DNA methylation alterations affect key breast cancer genes such as BRCA1 suggesting their importance to carcinogenesis and progression. However, the mechanisms underpinning such alterations in DNA methylation are unknown, so their significance remains unclear.My recent work suggests epigenetic marks are strongly programmed by the underlying DNA sequence. This means that sequence variants are likely to be an important and underexplored influence on breast cancer epigenomes.Aims:This project aims to understand the role of the genome in programming the epigenetic mark DNA methylation in breast cancer through two aims.1. Define the extent to which germ-line and somatic DNA sequence variants modulate local levels of DNA methylation in breast tumours in vivo.2. Understand how the effects of regulatory variants on DNA methylation and gene expression interact and how such variants impact on clinical phenotypes.Methods:I will elucidate the frequency to which germ-line and somatic sequence variants affect DNA methylation in breast cancer through machine-learning analyses of HER2-driven mouse mammary tumours and clinical high-grade luminal breast tumours. The interaction between variant-modulated alterations in DNA methylation and gene expression will be tested using genome editing technologies and the promoter of BRCA1 as a paradigm. The phenotypic impact of BRCA1 regulatory mutations will be assessed through engineering into mice. These synergistic approaches reinforce one another to uncover the global impact of sequence variation on DNA methylation, the mechanistic details underlying these effects and their clinical relevance. How the results of this research will be used:This project uses state-of-the-art computational and laboratory approaches to leverage genetics towards tackling our lack of understanding regarding the impact of DNA sequence variation on epigenetic alterations in cancer. In doing so, it impacts on our understanding of the regulation of a key breast and ovarian cancer risk gene, BRCA1, and provides new approaches to delineate the contribution of regulatory variants to carcinogenesis and progression.    Medical Research Council    Unit    1999998.0EUR
175    Prof Miller Karla    University of Oxford    2016-10-01    2021-10-01    Linking MRI and microscopy for multi-scale neuroscience: Mechanisms, diagnostics and anatomy    None    Wellcome Trust    Senior Research Fellowship Basic    1793980.0GBP
176    Dr Batada Nizar    University of Edinburgh    2017-05-01    2019-05-01    QMAT-seq: A Novel CRISPR/Cas9 based assay for studying DNA repair associated mutations    Mutations are a root cause of cancer. In skin and lung cancers, they often result from exposure to environmental mutagens such as cigarette smoke or UV light; however, in other cancers, their causes remain unexplained. Restoration of DNA-repair in mice with BRCA1-deficiency prevents tumour formation revealing that defective DNA-repair contributes to cancer. The Alternative-Non-Homologous-End-Joining (A-EJ) pathway has recently been discovered and shown to promote genome instability and therapy resistance in BRCA1-deficient cancers but only few components of A-EJ pathway are known. In addition the causes of their activation remain elusive. Inhibition of A-EJ could be an effective therapy for cancers associated with DNA repair deficiency. In pump-priming experiments that will establish proof of concept for my new laboratory in the UK, I will develop novel quantitative laboratory assay for A-EJ employing cutting-edge CRISPR/Cas9 genome editing, high-throughput sequencing and machine learning algorithm. My long-term goal is to elucidate A-EJ regulation, to identify new members of the A-EJ pathway and to define its mutational footprints that will serve as a biomarker of DNA repair deficiency. The outcomes of this seed funding will enable me to secure long-term funding to study A-EJ in cancer and its potential in cancer stratification and personalized therapy.    Wellcome Trust    Seed Award in Science    100000.0GBP
177    Dr Batada Nizar    University of Berne    2015-02-01    2018-01-31    INTACT - INTerstitial pneumonia pattern Analysis for CompuTer-aided diagnosis    Interstitial Lung Diseases (ILDs) is a group of more than 200 chronic lung disorders characterized by scarring and/or inflammation of the lung tissue that leads to respiratory failure. In most cases, ILDs have unknown causes so they are described as Idiopathic Interstitial Pneumonia (IIP). Clinical examination, chest radiography and Computed Tomography (CT) constitute the first steps of the ILD diagnosis procedure, however most forms of ILD require additional invasive histological confirmation which introduces potential risks for the patient and increases common health care costs. Although there is currently no permanent cure, the precise diagnosis of ILDs is crucial for achieving the optimal treatment as well as investigating novel therapies. Idiopathic Pulmonary Fibrosis (IPF) constitutes the most prevalent IIP and carries a rather poor prognosis. Non-Specific Interstitial Pneumonia (NSIP) is a pathological subtype of IIP that mimics IPF in its clinical presentation, having though a more favorable prognosis. The high prevalence of IPF and NSIP among the ILDs combined with their similar clinical/radiological manifestations and their rather different prognosis and treatment make the corresponding differential diagnosis both crucial and challenging.The aim of the proposed project is to develop a computational system that will assist clinicians with the diagnosis of ILDs while avoiding the dangerous, expensive and time-consuming invasive biopsies. The system will provide a computerized differential diagnosis, based on radiological data and clinical/biochemical markers and will focus on the discrimination between the IPF and the NSIP while keeping a generic architecture that could be expanded to most types of ILDs. The appropriate interpretation of the available radiological data combined with clinical/biochemical information can provide reliable diagnosis able to improve the diagnostic accuracy of the clinicians.The proposed system will consist of two major modules: the image analysis module and the diagnosis support module. The former will segment the lung parenchyma from the CT images and perform detection/classification of the existing ILD patterns. Then, the extent and distribution of the lung abnormalities will be quantified. The second module will take as input the image analysis results together with the available clinical/biochemical data and based on the existing medical knowledge and machine learning techniques will provide as output a ranked list indicating the most likely disease. New methods for describing the 2D and 3D texture of CT patterns will be proposed and evaluated comparatively to the well-established literature. Moreover, the most appropriate artificial intelligence tools will be determined for classifying the CT patterns and producing the final diagnosis. For the purposes of the project two databases will be used: the publicly available Talisman database by the University Hospital of Geneva and the Inselspital database containing complementary 3D data from Multiple Detector CT (MDCT) scanners which will be created within the framework of the project.The project will strengthen ILD lung CT-image analysis by introducing novel computer vision and machine learning techniques and evaluating comparatively the existing literature. The improvement of the clinicians’ diagnostic accuracy on ILDs will result in optimizing the corresponding treatment so the patient life will be prolonged and its quality will be improved. Furthermore, the research for novel treatments will be promoted. By achieving a reliable diagnosis based on CT, the patients will avoid the potential risks of bleeding and general anaesthesia as well as the high costs that come with a surgical biopsy.    Swiss National Science Foundation    Project funding (Div. I-III)    263604.0CHF
178    Dr Papoutsi Chrysanthi    University of Oxford    2017-09-01    2018-12-01    Digital health for patient safety: a case study in epilepsy care    This project will focus on the potential for machine learning to improve patient safety and will interrogate whether current ethical and regulatory frameworks adequately account for the role of artificial intelligence in healthcare. In contrast to prevailing conceptions of patient safety driven by biomedical and technocratic priorities, I will pursue an understanding of patient safety as an emergent, practical accomplishment between patients, lay carers and service providers. Methods will include a theory-driven review of the literature on patient safety and machine learning, followed by an empirical case study on assisted living technologies for people with epilepsy and their carers. Qualitative methods will be used, such as interviews, ethnographic field notes, multi-modal qualitative data and document analysis. Rich, methodologically-robust empirical data will be analysed by drawing on theoretical frameworks from medical sociology and science and technology studies, to extend previous theorisations of patient safety practices.This award will also contribute to transdisciplinary learning and knowledge exchange. Drawing on the research on assisted living technologies for epilepsy, a number of workshops will be organised between clinicians, computer and data scientists, technology developers, patient and carers, social scientists and health services researchers, charities and commercial organisations. This work will advance the field theoretically and will contribute to tangible changes in digital health technology development and co-production.    The Academy of Medical Sciences    Springboard - Health of the Public 2040 Round 1    49939.53GBP
179    Dr Keating Peter    University College London    2017-05-01    2019-05-01    Developmental impact of intermittent hearing loss on spatial hearing in noisy environments    I aim to understand how intermittent unilateral hearing loss affects complex aspects of auditory development, using spatial hearing in noisy environments as a model system. Problem: Otitis media is one of the most common diseases in young children, and can often produce an intermittent unilateral hearing loss. This can disrupt normal development and produce impairments that persist even after normal hearing is restored. Spatial hearing contributes to listening in noisy environments, and this important skill is particularly impaired by developmental hearing loss. However, the neurophysiological changes that produce these impairments are unknown.Research: To address this, I will induce an intermittent unilateral hearing loss in ferrets during development. I will then perform bilateral extracellular recordings in A1 as these animals perform a spatial sound segregation task, and use machine learning to link neural activity with behaviour. I will perform these experiments during periods of both hearing loss and normal hearing, and investigate whether these animals: (i) have adapted to the hearing loss, and (ii) are impaired even when the hearing loss is absent.Impact: By understanding how the brain adapts to hearing loss, we will be able to design better cochlear implants and hearing aids, and better train people to use these devices. By linking impairments with physiological changes in neuronal populations, we will gain insight into pathophysiological markers that could help identify at-risk children. This would facilitate earlier, and better targeted, clinical intervention, and help us prevent, detect, and reverse the underlying pathophysiology.    The Academy of Medical Sciences    Springboard Round 2    99612.0GBP
180    Dr Cusack Rhodri    MRC Cognition and Brain Sciences Unit    2008-04-01    2011-06-30    ACA4: Selective attention, short-term memory and the parietal lobe    I aim to understand how intermittent unilateral hearing loss affects complex aspects of auditory development, using spatial hearing in noisy environments as a model system. Problem: Otitis media is one of the most common diseases in young children, and can often produce an intermittent unilateral hearing loss. This can disrupt normal development and produce impairments that persist even after normal hearing is restored. Spatial hearing contributes to listening in noisy environments, and this important skill is particularly impaired by developmental hearing loss. However, the neurophysiological changes that produce these impairments are unknown.Research: To address this, I will induce an intermittent unilateral hearing loss in ferrets during development. I will then perform bilateral extracellular recordings in A1 as these animals perform a spatial sound segregation task, and use machine learning to link neural activity with behaviour. I will perform these experiments during periods of both hearing loss and normal hearing, and investigate whether these animals: (i) have adapted to the hearing loss, and (ii) are impaired even when the hearing loss is absent.Impact: By understanding how the brain adapts to hearing loss, we will be able to design better cochlear implants and hearing aids, and better train people to use these devices. By linking impairments with physiological changes in neuronal populations, we will gain insight into pathophysiological markers that could help identify at-risk children. This would facilitate earlier, and better targeted, clinical intervention, and help us prevent, detect, and reverse the underlying pathophysiology.    Medical Research Council    Unit    99612.0GBP
181    Dr Cusack Rhodri    University of Basel    2015-08-01    2018-07-31    Multiscale dynamics of dog rabies elimination    Multiscale dynamics of dog rabies elimination The aim of this project is to elucidate the contributions of population vaccination coverage and the vaccine immunity of individual dogs on the interruption of dog rabies transmission; determine the role of population density of dogs in the transmission of rabies; and identify the optimal frequency and coverage of vaccination campaigns. The proposed project will help define the most cost-effective dog mass vaccination strategies for rabies elimination in Africa and Asia. Rabies is a zoonotic disease that is responsible for substantial human mortality in Asia and Africa, but recent studies have suggested that elimination is possible. We hypothesize that the population level aspects of vaccination coverage contribute more to the dynamics of dog rabies elimination than the kinetics of protective antibodies within individual dogs; and that the transmission of dog rabies is density dependent below a threshold of 100 dogs per km2.Approach: In 2012 and 2013 we vaccinated 18,200 and 20,000 dogs in N’Djaména, Chad, reaching a population coverage of more than 70%. Dog rabies incidence dropped from one rabid dog per week prior to the mass vaccination to less than one rabid dog in eight months afterwards. The last rabid dog was recorded in January 2014. Because of the multiple scales (between dogs and within dogs) in rabies transmission and immune dynamics, this unique data set will be used for comparative mathematical modeling approaches with individual based (contact networks and machine learning) and population-based models. First, we use dog to dog and dog to human contact network data, collected by observational studies, to extrapolate individual dog contact networks to a citywide contact network as a basis for rabies transmission models. Next we will develop metapopulation models that extend an existing compartmental model into 4 or 5 interconnected sub-models, based on the spatial heterogeneity of dog populations in N’Djaména, with assumptions of both frequency-dependent and density-dependent contact rates. Finally we will develop an individual dog based machine learning model and simulate the kinetics of protective antibodies in new populations. The three sets of models will be calibrated to existing data from the previous vaccination campaigns and compared by goodness of fit measures that evaluate parsimony and determine best fitting models (and model assumptions). The models will be used to predict the effectiveness of vaccination campaigns at various frequencies and coverage levels in preventing rabies epidemics due to imported cases. These predictions can then be validated against numbers of any new dog and human rabies cases through the project duration. Expected results and societal impact: This project will generate new knowledge on dog rabies transmission dynamics and potential for elimination; provide advice on optimal vaccination strategies; and identify the most realistic and parsimonious models for the follow-up of forthcoming dog mass vaccination campaigns in Africa and Asia in the framework of the Global Alliance for Rabies Control (GARC).    Swiss National Science Foundation    Project funding (Div. I-III)    491517.0CHF
182    Dr Markowetz Florian    University of Cambridge    2014-03-01    2019-02-28    Computational Biology Laboratory    Recent advances in biotechnology have produced a wealth of genomic data, which capture a variety of complementary cellular features. While these data promise to yield key insights into molecular biology and medicine, much of the information present remains underutilized because of the lack of scalable approaches for detecting signals across large, diverse data sets. A proper framework for capturing these numerous snapshots of complementary phenomena under a variety of conditions can provide the holistic view necessary for developing relevant and precise systems-level hypotheses of new drug targets and disease mechanisms. My research is concerned with developing statistical and mathematical models of complex biological systems and analyzing large-scale molecular data. My research interests range from the analysis of microarray data in clinical settings to inference of cellular networks from high-throughput gene perturbation screens and integration of heterogeneous data sources using machine learning techniques and probabilistic graphical models Research Directions at the Cambridge Research Institute My goal is to continue research on computational and statistical analysis of high-throughput data. In my lab at the Institute, I aim to develop comprehensive descriptions of genetic systems of cellular controls, including: 1. regulation of cell differentiation and development, and 2. controls whose failure may lead to developmental defects or genetic disorders, such as cancer. Together with my group, I will investigate the structure and function of biological regulatory networks and their relationship to disease mechanisms and drug targets. This includes an investigation of topological and functional units in regulatory networks as well as time-dependent or condition-specific alterations. In the immediate future, my lab will work in collaboration with the Ponder and Caldas groups on projects in breast cancer genetics and genomics, which aim to identify key drivers of disease by integrating genome-wide data from SNP, array CGH, gene expression, and microRNA profiling studies.    Cancer Research UK    SEB - Institute Group Award    491517.0CHF
183    Dr Bizley Jennifer    University College London    2016-06-20    2016-08-12    Neural mechanisms underlying complex sound identification    The aim of the project is to better understand how brain processes complex sound information by measuring responses of cortical neurons (nerve cells) to artificial vowel sounds. The student will construct artificial network of neurons called the convolutional neural networks to make sense of sound identity from neural responses. Dataset has already been collected from 8 ferrets. Student has strong computational and programming skills and experience in programmes needed to analyse the data such as MATLAB and machine learning. Student will be supervised by Dr Stephen Town (Pauline Ashley grant holder) who collected the primary electrophysiology data. The work is feasible for a summer student as the primary data (neural responses to pure tones, single and multi-formant vowels in anesthetised and behaving ferrets) has already been collected and formatted for analysis. Dr Bizley has also developed some basic models with which to analyse data, and from which the student can build and develop his own ideas with support and assistance as necessary.    Action on Hearing Loss    Summer studentship    1600.0GBP
184    Prof. INDIVERI Giacomo    University Of Zurich    2017-09-01    2022-08-31    Neuromorphic Electronic Agents: from sensory processing to autonomous cognitive behavior    Neural networks and deep learning algorithms are currently achieving impressive state-of-the-art results. In parallel computational neuroscience has made tremendous progress with both theories of neural computation and with hardware implementations of dedicated brain-inspired computing platforms. However, despite this remarkable progress, today’s artificial systems are still not able to compete with biological ones in tasks that involve processing of sensory data acquired in real-time, in complex and uncertain settings. One of the reasons is that neural computation in biological systems is very different from the way today's computers operate: it is tightly linked to the properties of their computational embodiment, to the physics of their computing elements and to their temporal dynamics. Conventional computers on the other hand operate with mainly serial and synchronous logic gates, with functions that are decoupled from their hardware implementation, and with discretized and virtual time. In this project we will combine the recent advancements in machine learning and neural computation with the latest developments in neuromorphic computing technology to design autonomous systems that can express robust cognitive behavior while interacting with the environment, through the physics of their computing substrate. To achieve this we will embed in robotic platforms microelectronic neuromorphic processors and sensors that implement biophysically realistic neural computational primitives and dynamics. We will adopt active-sensing and on-line spike-based learning strategies, context and state-dependent computation, and probabilistic inference methods for "programming" these neuromorphic cognitive agents to solve challenging tasks in real-time. Our results will lead to compact low-power intelligent sensory-motor systems that will have a large impact on service and consumer robotics, Internet of Things, as well as prosthetics and personalized medicine.    European Research Council    Consolidator Grant    1999090.0EUR
185    Professor L. Barreto Mauricio    Fiocruz (Oswaldo Cruz Foundation)    2020-04-01    2021-09-30    The risk of a chronic clinical condition following a previous hospitalisation by a psychiatric disorder: a linkage nationwide study in Brazil    Neural networks and deep learning algorithms are currently achieving impressive state-of-the-art results. In parallel computational neuroscience has made tremendous progress with both theories of neural computation and with hardware implementations of dedicated brain-inspired computing platforms. However, despite this remarkable progress, today’s artificial systems are still not able to compete with biological ones in tasks that involve processing of sensory data acquired in real-time, in complex and uncertain settings. One of the reasons is that neural computation in biological systems is very different from the way today's computers operate: it is tightly linked to the properties of their computational embodiment, to the physics of their computing elements and to their temporal dynamics. Conventional computers on the other hand operate with mainly serial and synchronous logic gates, with functions that are decoupled from their hardware implementation, and with discretized and virtual time. In this project we will combine the recent advancements in machine learning and neural computation with the latest developments in neuromorphic computing technology to design autonomous systems that can express robust cognitive behavior while interacting with the environment, through the physics of their computing substrate. To achieve this we will embed in robotic platforms microelectronic neuromorphic processors and sensors that implement biophysically realistic neural computational primitives and dynamics. We will adopt active-sensing and on-line spike-based learning strategies, context and state-dependent computation, and probabilistic inference methods for "programming" these neuromorphic cognitive agents to solve challenging tasks in real-time. Our results will lead to compact low-power intelligent sensory-motor systems that will have a large impact on service and consumer robotics, Internet of Things, as well as prosthetics and personalized medicine.    Medical Research Council    P&Cs    161780.0GBP
186    Professor Ciccarelli Olga    Institute of Neurology, UCL    2019-01-01    2021-12-31    Assessing treatment responses using machine learning    Thirteen disease-modifying treatments (DMTs) are approved in the UK to reduce the risk of relapses. Patients can switch from one first-line DMT to a more effective medication, if they present with relapses. The consequences of this policy, that requires "failure" of DMTs before using another DMT are continuous relapses and disability accumulation. We cannot currently predict which DMT will work best for an individual patient. The goal of this project is to predict the individual treatment response in MS by translating machine learning from the computer science field into clinical practice. All patients (adults and children) currently on DMTs and due to start a treatment at UCLH NHS Trust and in the UK paediatric neuroinflammation centres will be studied. Demographic factors, diet quality and life-style, clinical scales, comorbidities, MRI scans and blood tests for safety data, neurofilaments-light levels, and genetic analysis will be collected. This information will be used to predict treatment response in each individual using a high-dimensional model. The model will be validated on independent, international cohorts. This project will bridge the gap between clinical trials, which focus on the "average" response to a therapy, to clinical practice, where the focus should be on the individual treatment response.    Multiple Sclerosis Society    Project grant application    355293.0GBP
187    Ms Bright Rebecca    Therapy Box Limited    2018-01-02    2019-01-01    ATLAS - AUTOMATED TRANSCRIPTION & LANGUAGE ANALYSIS SOFTWARE    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    National Institute for Health Research (Department of Health)    Full Award    149330.0GBP
188    Dr Barbosa da Silva Adriano    Queen Mary University of London    2018-03-22    2021-03-21    A translational data integration platform for the stratification of patients based on clinical, laboratory and magnetic resonances imaging    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    299527.0GBP
189    Dr Blackburn Ruth    University College London    2018-02-14    2021-11-13    Social contagion? : using data science to characterise the distribution and dispersion of health behaviours in adolescence    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    313016.0GBP
190    Dr Carson Jason    Swansea University    2018-02-14    2021-02-13    Non-invasive assessment and management of coronary heart disease - a translational, data driven approach    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    284559.0GBP
191    Dr Cole James    King's College London    2017-11-15    2019-10-06    Modelling brain ageing using neuroimaging to improve brain health in older adults    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    290658.0GBP
192    Dr Droop Alastair    Wellcome Trust Sanger Institute    2019-04-08    2021-02-13    Facilitating Deep Learning with Domain-Specific Knowledge    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    185651.0GBP
193    Dr Gurdasani Deepti    Wellcome Trust Sanger Institute    2018-02-14    2019-06-25    Predictive analytics of integrated genomic and clinical data using machine learning and complex statistical approaches    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    319508.0GBP
194    Dr Hall Benjamin    University of Cambridge    2018-12-01    2021-11-30    Deciphering and targeting cancer metabolism using executable models    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Research Grant    344485.0GBP
195    Dr Hassan Lamiece    The University of Manchester    2018-02-14    2021-02-13    Social listening: Applying natural language processing methods to social media data to yield actionable analytics for health and care services    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    300577.0GBP
196    Dr Lumbers Richard    University College London    2018-02-14    2021-02-13    'Unlocking therapeutic innovation in heart failure through genomic data science    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    313017.0GBP
197    Professor Tchanturia Kate    King's College London    2017-10-09    2019-10-08    The Triple A study (Adolescents with Anorexia and Autism): A search for biomarkers    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Research Grant    181988.0GBP
198    Professor Tchanturia Kate    King's College London    2019-10-01    2022-09-30    BiomaRkers for AnorexIa NErvosa and autism spectrum Disorders- longitudinal study    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Research Grant    496884.0GBP
199    Dr Verity Robert    Imperial College London    2016-04-01    2019-03-31    Genetic data as a signal of changing malaria transmission    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    281369.0GBP
200    Dr Wright David    Queen's University of Belfast    2018-02-14    2021-02-13    Data driven public health approaches for diabetic retinopathy and age-related macular degeneration    None    Medical Research Council    Fellowship    279158.0GBP
201    Professor Lorincz Attila    University of London    2016-06-01    2019-05-31    Development of a highly accurate DNA methylation classifier for prevalent and incident cervical pre-cancer    Background:Cervical cancer, caused by persistent infection with high risk (hr) HPV affects ~500,000 women globally and causes ~260,000 deaths annually. Although HPV immunisation has been successfully implemented the level of protection expected from current vaccines will be quite substantially incomplete and it will take several decades to see an effect of the new and improved nonavalent vaccine. A pressing need to combat cervical cancer through improved screening remains far into the future. Highly sensitive hrHPV testing is likely to become the dominant primary screen. However, hrHPV infection is common and only a fraction of women are at risk of developing cervical cancer. 40% of hrHPV+ women are cytology negative and triage by proposed adjunctive immunostaining tests such as p16 (often in conjunction with ki67) are insufficient. A molecular test is needed to more accurately identify clinically significant HPV infection. Aims:We aim to develop a DNA methylation (DNAm) biomarker panel that has the potential to be an excellent triage tool for hrHPV+ women and may become an integral part of the primary screening test for preventing cervical cancer.Methods:We propose to measure genome-scale human DNAm as well as DNAm of predefined sites in HPV16, 18, 31 and 33 in a set of 350 hrHPV+ women with normal cytology. This will be the largest such study to date. First, we will measure DNAm with reduced representation bisulfite sequencing followed by machine learning using MS-SPCA to identify ~100 sites which consistently appear in the best ranking models. Then, the best sites will be further sifted by additional multivariate data modelling to provide us with a minimum number of required classifier sites. Finally, these selected sites, presumably 10-20 sites, will be validated in a second set of 200 well characterised cervical samples.How the results of this research will be used:We will develop a significantly improved risk classification tool to triage hrHPV+ women to colposcopy. The new classifier must come close to the high sensitivity of current hrHPV tests (90-95%) but deliver a substantially higher specificity and PPV (both ~70%) than current molecular reflex tests (30-40% and 40-50% respectively). This new algorithm would allow more efficient utilization of colposcopy services while hrHPV+ women negative for the triage classifier could be followed up at suitably frequent intervals to safely catch most, if not all, triage false negatives.    Cancer Research UK    CRC - Biomarker Project Award    279158.0GBP
202    Professor Lorincz Attila    Department of Genetics Stanford University School of Medicine    2017-10-01    2019-03-31    Dissecting the regulatory landscape of the human genome    A large number of genetic variants linked to phenotypic changes are located in the non-coding regions of the genome, suggesting that their effects are most likely the result of changes in gene expression regulation. Expression quantitative trait locus (eQTL) analyses allow finding of regulatory genetic variants, which are linked to changes in gene expression. However, the regulatory elements perturbed by the genetic variants must be identified to fully understand these mechanisms. I propose to develop a computational framework to predict enhancers and their target genes based on genome-wide chromatin and gene expression data in an eQTL study setting. First, a set of high-confidence enhancers in human lympoblastoid cell lines (LCL) will be created from published literature and available databases. A machine-learning algorithm will then be trained based on posttranslational histone modifications, transcription factor binding and DNA sequence conservation at these enhancers. This model will then be used to predict genome-wide enhancers in additional 47 LCLs. Furthermore, data on detected eQTLs and dynamics in gene expression and chromatin variability will be used to associate the predicted enhancers to target genes, thus deciphering the cis-regulatory network around eQTL genes and pinpointing candidate regulatory elements that are perturbed by genetic variants.    Swiss National Science Foundation    Early Postdoc.Mobility    279158.0GBP
203    Professor Lorincz Attila    University of Zurich    2015-05-01    2018-04-30    HIV-1 Transmission in Switzerland: viral transmission traits, superinfection and drug resistance    This translational research project is a continuation of SNF 130865. We aim to unravel traits of HIV transmission in three conceptually different situations of high clinical relevance: transmission leading to de novo infection (Aim 1), spread of HIV between infected individuals (conferred to as superinfection (SI); Aim 2) and transmission of drug resistant virus (Aim 3). The proposed studies are based on two renowned longitudinal studies with extensive biobanks: The Swiss HIV Cohort and the Zurich Primary HIV-1 infection study (ZPHI). AIM 1. Here we will determine whether specific genotypic and phenotypic viral traits of transmitted/founder viruses (T/F) exist. Characterization of T/F viruses, the earliest virus population that emerges in a newly infected individual, is of high interest as understanding their transmis-sion pathways may open avenues for prophylaxis and treatment. Current data on T/F virus features led to intriguing in-sights, but no definitive picture of genetic and phenotypic traits of T/F viruses has emerged. A limitation of previous studies was that features of T/F viruses had to be compared to unrelated viruses from chronically infected patients as transmitting individuals were in most cases not identified. Here using phylogenetically linked T/F viruses and their transmitters we will be able to study phylogenetically linked transmitter-recipient pairs. Thus far we identified 79 acutely HIV infected (recipients) and 107 chronically infected individuals with genetically related HIV strains which are their likely transmitters. Plasma samples close to estimated time of infection will be chosen for virus genomic analysis from transmitters and recipients to retrieve full length HIV-genomes employing Next Generation Sequencing (NGS) followed by haplotype reconstruction. T/F virus will be compared to variants present in the transmitter regarding, length, specific mutations, deletions/insertions, glycosylation sites, charge and frequency to define whether stochastic or non-stochastic transmission occurs. If genotypic traits are identified, gene transfer and mutagenesis experiments will be performed followed by phenotypic analysis to discern if indeed signifying characteristics of T/F viruses exist. To this end we will compare transmitter and T/F viruses for features implicated in shaping transmission including replication capacity, sensitivity to neutralizing antibodies and entry inhibitors, entry kinetics and interferon sensitivity. AIM 2: Here we will systematically explore frequency and risk factors for HIV-1 superinfection in the SHCS and ZPHI. A first phylogenetic screen based on pol sequences has revealed 150-312 potential SI cases. By retrieving longitudinal plasma samples before patients went on antiretroviral therapy we will seek to confirm SI by full length NGS and define approximate time points of SI events. In a detailed virus genomic analysis we aim to reconstruct haplotypes, study recombination events. A specific emphasis of our studies is on determining risk factors for acquiring SI using detailed patient, disease and host genomic data available. Viral fitness of initial circulating and superinfecting strains will be computed using machine learning techniques. To test whether SI is prevented by neutralizing antibody responses, those will be compared among superinfected and non-superinfected individuals. Finally, viral setpoint of cases and controls will be compared to determine the impact of SI on disease progression. AIM 3: Here we will investigate transmission of HIV-1 drug resistance mutations (TDR) within the SHCS, specifically focussing on the non-B compared to the subtype B epidemic, newer drugs, and reversion rates of TDR. We will study TDR prevalence over time for B and non- B subtypes in all drug naives and, specifically, in recently infected individuals. We will particularly focus on newer drugs. In patients with TDR we will determine reversion rates for specific TDR and fitness cost by taking into account the pol genotypic backbone indepen-dently of TDR. Furthermore, by performing phylogenetic analysis we will study whether there are differences in transmission dynamics between B- and non-B subtype TDR. This analysis will reveal whether TDR in the non-B subtypes is increasing as would be expected due to the recent, wide spread roll out of antiretroviral drugs in developing countries.    Swiss National Science Foundation    Project funding (special)    834000.0CHF
204    Professor Lorincz Attila    University of Berne    2017-10-01    2020-09-30    Stroke treatment goes personalized: Gaining added diagnostic yield by computer-assisted treatment selection (the STRAY-CATS project)    Stroke is the second most frequent cause of death and a major cause of disability in industrial countries: in patients who survive, stroke is frequently associated with high socioeconomic costs due to persistent disability. In clinical practice, advanced neuroimaging techniques are increasingly employed for a quick, reliable diagnosis and stratification for therapy. Tissue-at-risk estimation is frequently performed by MRI, with the infarct core being identified as an area of restricted diffusion on diffusion-weighted magnetic resonance imaging (DWI-MRI). The surrounding severly hypoperfused and potentially salvageable tissue tissue (i.e. the “penumbra”) is characterized by its delay in arterial transit time using perfusion-weighted MRI. The clinical image interpretation is routinely performed as a visual analysis done by neuroradiologists and/or neurological stroke experts.There is class I evidence that intravenous thrombolysis is a safe and effective therapy within an estimated time frame of 4.5 h after stroke onset. Very recently, four prospective studies demonstrated the superiority of mechanical thrombectomy in proximal vessel occlusions within a time frame of 6 h after stroke onset. Mechanical thrombectomy has thus become the treatment option of choice to achieve an early and sustained revascularization of proximally occluded vessels in specialized stroke centres2. The recent advent of mechanical thrombectomy has now raised an urgent question that needs to be answered: “can we predict advantageous tissue survival if mechanical thrombectomy is successfully applied compared to the natural course of disease in the presence of sufficient vs. insufficient collaterals?” The availability of a safe, reproducible and reliable information about the expected tissue salvage would allow not only to select patients that would benefit from mechanical thrombectomy, it would further allow to select patients for revascularization in a time window that exceeds 6 h if sufficient collateral flow enables sustained tissue survival. It is essential that indicators for further success of endovascular therapy can be calculated as soon after admission as possible, in order to save as much of the brain tissue as possible: '*time is brain*'. Computer-assisted and automated tissue segregation of the infarct core and salvageable penumbra using compound information from multimodal MRI offers a novel and robust standardized solution to this problem: while simple thresholding based on perfusion and diffusion imaging provide only a crude estimate of the tissue at risk, machine-learning approaches based on multimodal MR data overcome the limited accuracy of linear analyses. The proposed machine-learning approach incorporates thus two separate goals, i) to quantify penumbral collateral flow in the acute emergency setting and ii) to identify fingerprints that disentangle salvageable vs. non-salvageable tissue based on machine learning in a “big data” approach based on multiparametric imaging. We will provide means for i) by transforming the interpretation of image features into an interpretation of the underlying stationary flow field. This will allow us to combine information of all available MR imaging data, to quantify the collateral blood flow in the individual patient before and during intervention, and to compare 4D flow patterns at a population level. We will ii) build on our existing predictive models of stroke outcome, incorporating FLAIR and SWI maps and making use of the 'big data' that have been acquired during the last years in more than 1000 patients that underwent intraarterial thrombolysis or thrombectomy. Our overall goal is to investigate if, given sufficient training data, predictive maps of the infarction can improve on the current 'penumbra' concept as a tool for identifying patients who will have a favourable response to reperfusion therapy.    Swiss National Science Foundation    Project funding (Div. I-III)    474000.0CHF
205    Professor Lorincz Attila    Other Hospitals    2017-06-01    2017-07-31    NEWS - Newborn Early Warning Signs    Sepsis and meningitis are major causes of morbidity and mortality in theneonatal population. Despite vigilant clinical assessment of infants in the neonatalintensive care unit (NICU), diagnosis of sepsis and necrotizing enterocolitis (NEC) oftendoes not occur until an infant has significant hemodynamic compromise. Analysis of heart rae variability becomes more and more relevant in predicting outcomes over short and long time. Thus decription of apnea, bradycardia and desaturation (ABD-events) are of major importance to predict future potentially life-threatening events. Combining biomarker screening with predictive monitoring of physiologic markers for assessing risk of sepsis or NEC and for detection of early-stage illness could prevent progression to severe illness and shock.The neonatal intensive care unit's database for surveillance and storage ofcardiorespiratory parameters (Clinisoft®), allows us to prospectively characterizedevelopment of ABD-events over time.To examine the contribution of infection, acute and chronic inflammation, indevelopment of cardiorespiratory dysfunction there are ongoing prospective studies ofterm and premature infants at the NICU and paediatric intensive care units at the Karolinska University Hospital Solna. There cardiorespiratory recordings (Clinisoft®) of ABD- events are correlated with routine inflammatory biomarkers & medical records. We then further examine how physiomarkers (ABD-events) & biomarkers may act as Newborn Early Warning Signs (NEWS) for infection, inflammation and need for increased therapeutic interventions. In this project, our objective is to develop machine learning based methods for using low frequency labeled data (Clinisoft) in conjunction with the high frequency unlabeled data. Using pattern recognition we develop better physiomarkers (cardiorespiratory indexes) as NEWS that will help to predict short-term outcome and enable rapid therapeutic interventions.    Swiss National Science Foundation    International short research visits    6300.0CHF
206    Professor Lorincz Attila    University of Zurich    2017-08-01    2020-07-31    Radiomics as biomarker in multi-modality treatment of locally advanced non-small cell lung cancer    Radiomics is defined as the use of quantitative image analysis algorithms for calculation of a comprehensive set of image phenotypes also called image biomarkers. In contrast to conventional radiological image analysis, this methodology is a) objective and observer-independent and b) allows for a comprehensive description of all available information within medical images. Radiomics is therefore considered as a potential addition to the efforts currently undertaken in the field of precision medicine: to achieve a comprehensive analysis of the cancer phenotype in order to adjust the treatment to the patient-individual cancer characteristics.The aim of this project is to establish and to use comprehensive radiomics analyses in CT and FDG-PET images for outcome modelling. Prior to all image analyses, the radiomics algorithms will be evaluated regarding their robustness against non-standardized image acquisition and image reconstruction parameters. In addition, a model for quantification of loco-regional tumor spread will be established and its prognostic and predictive value will be compared to the conventional staging system. Machine learning algorithms will be established to correlate the large amount of radiomics biomarkers with clinical outcome parameters. The radiomics methodology will be tested based on images acquired within the randomized SAKK 16-00 study of multi-modality treatment for locally advanced non-small cell lung cancer (NSCLC). Radiomics biomarkers of pre-treatment CT and FDG-PET images, or post induction therapy images and changes of radiomics parameters between these two time points will be analysed. Radiomics parameters will be correlated with available histo-pathological tumor characteristics, primary (event-free survival) and secondary (overall survival, response to induction therapy, failure pattern, pulmonary toxicity) study endpoints.It is the hypothesis, that radiomics analyses will enable us to build better and more accurate prognostic models than the ones currently based on the TNM system as well as predictive models for identification of patients, who might benefit from neoadjuvant radiochemotherapy instead of neoadjuvant chemotherapy, only.The radiomics methodology and the machine learning algorithms will be evaluated in cancer patients suffering from NSCLC, a disease with currently very poor prognosis. It is planned to evaluate radiomics and the machine learning algorithms in other cancer sites as well. Additionally, the application of radiomics and the machine learning algorithms are not restricted to Oncology but can maybe successfully used in other medical questions.    Swiss National Science Foundation    Project funding (Div. I-III)    427191.0CHF
207    Professor Lorincz Attila    Brain Imaging and EEG Laboratory San Francisco VA Medical Center University of California, San Francisco    2016-03-01    2017-08-31    Neural Oscillations as Predictors of Psychosis    Aims:This study will attempt to predict the transition to psychosis on a landmark sample whose size is unmatched by any other study in the world. The study design of the North American Prodrome Longitudinal Study 2 (NAPLS-2) allows to investigate brain activity in clinical high risk (CHR) individuals longitudinally (repeatedly over time) and observe whether the anomalies identified at baseline worsen over time thereby leading to psychosis. Owing to the rich and varied set of experimental paradigms assessed in NAPLS 2, the proposed project will allow identifying neurophysiological paradigms that are most sensitive to conversion to psychosis. These paradigms could subsequently be selectively implemented in early detection clinics in Switzerland.Methodology:We will assess 242 Healthy Controls (HC), 199 CHR individuals who did not convert to psychosis (CHR-NP) and 72 CHR individuals who converted to psychosis (CHR-P) on three different EEG paradigms collected as part of NAPLS-2. NAPLS-2 is a consortium of eight universities (including prestigious universities such as Harvard, Yale and UCLA) with the largest CHR sample in the field and allows for advanced analyses and statistical power that cannot be performed in single-site studies. In particular, we will assess neural oscillations during the (1) visual and (2) auditory oddball paradigms, along with the (3) mismatch negativity response. Analyses will include: assessing neural oscillations using time-frequency analyses, phase locking value, and lagged phase synchronization across frontal and temporal regions. We will also make use of advanced machine learning algorithms (artificial intelligence) to identify multivariate patterns of brain activity predictive of transition to psychosis.Hypotheses:•Compared to both CHR-NP and HC, CHR-P individuals will demonstrate altered theta activity during both the P3a and P3b response during context-updating processes elicited by oddball target and novel stimuli.•During the P3a and P3b response, CHR-P individuals will have reduced frontal-temporal phase synchronization of theta neural oscillations compared to both CHR-NP and HC due to lower grey matter volume in both of these brain regions.•The auditory oddball paradigm will yield markers more predictive of conversion to psychosis than the visual oddball paradigm.•Compared to both CHR-NP and HC, CHR-P individuals will demonstrate lower early theta activity during the MMN response that will be associated with lower MMN amplitude.•Following the deviant tone, CHR-P individuals will have reduced frontal-temporal phase synchronization of theta neural oscillations compared to both CHR-P and HC due to lower grey matter volume in both of these brain regions.•In CHR-P and CHR-NP individuals, aberrant frontal-temporal phase-synchronization and lower theta oscillations following deviant stimuli in the MMN paradigm will be associated with psychotic symptoms and neuropsychological deficits.•Variation in brain structural (acquired via MRI) and functional data (acquired via EEG) among CHR individuals will be predictive of the individualized transition to psychosis.Expected value of the project:Currently in Switzerland, about 32,000 people are affected by schizophrenia and the average cost per patient has been estimated to be about EUR 39,000 in the year 2012 alone. Therefore, the early detection of psychosis potentially leading to enhanced treatment approaches will not only benefit the national economy in the long run but also help the patients and their families to get ready for a possible transition to psychosis. An early detection could allow for an early intervention, that is, the patients could undergo mild treatments such as a psychological intervention or administration of low-dosage antipsychotic medication. This study is unique as it is the first to investigate, repeatedly over time, a rich and varied set of experimental paradigms on the largest sample of CHR individuals worldwide. The main findings could help clarify whether brain abnormalities identified at baseline worsen overtime and allow for the individualized prediction of transition to psychosis.    Swiss National Science Foundation    Early Postdoc.Mobility    427191.0CHF
208    Professor Lorincz Attila    University of Zurich    2012-01-01    2015-07-31    Developing Bayesian Networks as a tool for Zoonotic Systems Epidemiology    A primary objective of many zoonotic epidemiological studies is to investigate hypothesized relationships between covariates of interest, and one or more outcome variables, through analyses of appropriate data. Typically, the biological, epidemiology and behavioural processes which generated this data are highly complex, resulting in multiple correlations/dependencies between covariates and also between outcome variables. Standard epidemiological and statistical approaches cannot adequately describe such inter-dependent multi-factorial relationships. Bayesian Network (BN) modelling is a generic and well established data mining/machine learning methodology, which has been demonstrated in other fields of study to be ideally suited to such analyses. The accessibility, however, of this methodology to epidemiologists is severely limited due to the sheer breadth and diversity of zoonotic epidemiological data, which is outside the established application areas of BN modelling. Two key challenges exist, one technical and one epidemiological. Firstly, no appropriate software exists for fitting the types of BN models necessary for analysing zoonotic epidemiological data, where complexities such as grouped/overdispersed/correlated observations are ubiquitous. This project will develop easy-to-use software to allow ready access to BN modelling to epidemiological practitioners, which is essential in order to make the crucial transition from merely a technically attractive methodology, to an approach which is actually used in practice. Secondly, to demonstrate and promote the use of this methodology in zoonotic epidemiology, relevant and high quality exemplar case studies will be developed showing objectively, situations in which BN models can offer the most added value, relative to existing standard statistical and epidemiological methods. Through the project's collaborators a diverse range of zoonotic data are available for analyses, including cross-sectional and longitudinal studies of antimicrobial resistance in Escherichia coli in farmed pigs in Canada, along with a wide range of different parasite field studies including Echinococcus multilocularis, Taenia, Mesocestoides, Uncinaria and Toxocara collected across eastern Europe and central Asia. This project will deliver a range of peer-reviewed publications comprising both methods orientated papers, and pathogen focused studies. The final deliverable of the project will be to provide a valuable addition to the quantitative skills base within veterinary science, by providing PhD level training in applied computational epidemiology of direct relevance to both zoonotic and animal disease research.    Swiss National Science Foundation    Project funding (Div. I-III)    193569.0CHF
209    Professor Lorincz Attila    University of Zurich    2017-09-01    2021-08-31    Predicting outcome after stroke: take a look at the other side    Despite improvements in primary prophylaxis and acute recanalization treatments, stroke remains one of the leading causes of death and disability worldwide. In order to achieve the best outcome possible for the individual patient, therapies have to be administered rapidly. However, the longer the time from symptom onset, the lower the efficacy and the higher the risk of treatment side effects. Although brain imaging is the mainstay of acute stroke diagnostics, current imaging strategies that aim to predict therapeutic success or failure in acute stroke patients remain insufficient. Here, we propose a novel prediction approach that is based on immediate and long-term vascular adaptations affecting the contralateral side of stroke. From data obtained through animal models of stroke and imaging in patients with proximal vessel occlusions, contralateral cerebral blood flow (CBF) appears to be particularly suited to predict clinical benefit from recanalization therapies. Contralateral CBF and related perfusion parameters may indicate the ability of the individual to withstand longer durations of ischemia. In the experimental part of the project, we will use a thrombin-injection stroke model that does not artificially impact collateral supply. Reperfusion will be achieved through intravenous injection of recombinant tissue plasminogen activator (rtPA). To assess CBF and ischemic tissue damage, repeated magnetic resonance imaging (MRI) combined with positron emission tomography (PET) will be performed during ischemia, after reperfusion and in the chronic phase of stroke along with behavioral assessments. In the same stroke model, processes influencing CBF on the microvascular level will be directly observed using advanced optical imaging methods. Structural adaptations of the microvascular bed as well as gene and protein expression profiles contralateral to stroke will be analyzed in brain samples. In the clinical part of the project, we will analyze stroke patient imaging data in relation to clinical outcome. In addition to a traditional region-of interest (ROI) based analysis we will apply a novel, “unbiased” machine learning algorithm to extract relevant outcome predictors from patient imaging data.Using a translational approach we aim for a mechanistic characterization of the concept of contralateral flow changes in stroke, and will generate and apply imaging predictors to clinical patient data. With the multidisciplinary study proposed here, I envision i) to deepen the understanding of the basic mechanisms regulating brain perfusion after ischemic stroke, and ii) to improve therapeutic decision-making in acute stroke patients.    Swiss National Science Foundation    SNSF Professorships    1509881.0CHF
210    Professor Lorincz Attila    Department of Neurosurgery Stanford University    2017-11-01    2019-04-30    Behavioral Biomarker for Temporal Lobe Epilepsy    The mechanistic bases of temporal lobe epilepsy (TLE) are not well understood. A major impediment to progress towards evidence-based, rigorous preclinical, translational research into the TLE is that the experimental approach often requires labor-intensive 24/7 video-EEG monitoring of seizures and rests on the inherently subjective scoring of behavioral seizure phenotypes by human observers along the Racine scale.The aim of the proposed research is to explore the transformative potential of state of the art machine learning (ML)-assisted 3D video analysis to phenotype mice with acquired epilepsy in an automated and high-throughput manner (Aim 1) and to correlate TLE-specific neuronal circuit activity with subtle behavioral dynamics that are not captured by current technology (Aim 2). The central idea rests on the discovery that complex animal behaviors are structured in stereotyped modules (“syllables”) at sub-second timescales that are arranged according to specific rules (“grammar”) that can be detected without observer bias by a combined 3D imaging (Kinect™) and ML method. The ML-assisted 3D video analysis technology will be adopted for epilepsy research to test if there are TLE-specific behavioral syllables and associated transition probabilities that might not be discernable by a human observer. In addition, it will be determine if behavioral syllables in chronic TLE are invariant between animal models of TLE. In order to comply with current standards of research into TLE, the presence or absence of TLE will be determined using conventional 24/7 video-EEG (vEEG) recordings and analysis. Synchronous acquisition of vEEG and 3D video will be used to assess if aberrant neuronal network activity is associated with the expression of specific behavioral syllables. Intervene upon local seizures or interictal spikes using closed-loop optogenetics will help to verify if these TLE-specific behavioral syllables are related to the certain hippocampal ensemble activity.    Swiss National Science Foundation    Early Postdoc.Mobility    1509881.0CHF
211    Professor Schumann Gunter    King's College London    2016-10-01    2021-09-30    Brain network based stratification of mental illness    To reduce the burden of mental disorders it is a formidable aim to identify widely applicable disease markers based on neural processes, which predict psychopathology and allow for targeted interventions. We will generate a neurobehavioural framework for stratification of psychopathology by characterising links between network properties of brain function and structure and reinforcement–related behaviours, which are fundamental components of some of the most prevalent mental disorders, major depression, alcohol use disorder and ADHD. We will assess if network configurations define subtypes within and if they correspond to comorbidity across these diagnoses. We will identify discriminative data modalities and characterize predictors of future psychopathology. To identify specific neurobehavioural clusters we will carry out precision phenotyping of 900 patients with major depression, ADHD and alcohol use disorders and 300 controls, which we will investigate with innovative deep machine learning methods derived from artifical intelligence research. Development of these methods will optimize exploitation of a wide range of assessment modalities, including functional and structural neuroimaging, cognitive, emotional as well as environmental measures. The neurobehavioural clusters resulting from this analysis will be validated in a longitudinal population-based imaging genomics cohort, the IMAGEN sample of over 2000 participants spanning the period from adolescence to adulthood and integrated with information generated from genomic and imaging-genomic meta-analyses of >300.000 individuals. By targeting specific neural processes the resulting stratification markers will serve as paradigmatic examples for a diagnostic classification, which is based upon quantifiable neurobiological measures, thus enabling targetted early intervention, identification of novel pharmaceutical targets and the establishment of neurobehaviourally informed endpoints for clinical trials.    European Research Council    Advanced Grant    3394215.0GBP
212    Professor Goh Vicky    King's College London    2015-07-01    2018-06-30    Evaluation of treatment response and resistance in metastatic renal cell cancer using integrated positron emission tomography/magnetic resonance imaging (PET/MRI) with 18F-Fluorodeoxyglucose (FDG)    Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.    Cancer Research UK    SC - Biomarker Project Award    3394215.0GBP
213    Dr Cole James    University College London    2019-10-07    2021-03-31    Modelling brain ageing using neuroimaging to improve brain health in older adults    Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.    Medical Research Council    Fellowship    113938.0GBP
214    Dr Desrivieres Sylvane    King's College London    2019-09-01    2022-08-31    Establishing causal relationships between biopsychosocial predictors and correlates of eating disorders and their mediation by neural pathways    Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.    Medical Research Council    Research Grant    507739.0GBP
215    Dr Droop Alastair    University of Leeds    2018-02-14    2019-04-07    Facilitating Deep Learning with Domain-Specific Knowledge    Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.    Medical Research Council    Fellowship    286050.0GBP
216    Dr HUGHES DAVID    University of Liverpool    2017-11-15    2020-11-14    Variational Approximation Approaches for Efficient Clinical Predictions    Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.    Medical Research Council    Fellowship    257981.0GBP
217    Dr Repapi Emmanouela    University of Oxford    2018-04-01    2022-10-28    Novel methods for the integration of high dimensional single cell proteomic and RNA data to understand cell populations in development and disease.    Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.    Medical Research Council    Fellowship    326906.0GBP
218    Dr Scholl Jacqueline    University of Oxford    2016-07-01    2021-06-30    Understanding the neural and cognitive mechanisms of attributional styles and credit assignment in depression    Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.    Medical Research Council    Fellowship    460831.0GBP
219    Ms Suel Esra    Imperial College London    2018-02-14    2021-02-13    Application of deep learning to heterogeneous open data for measuring urban environment and health    Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.    Medical Research Council    Fellowship    331573.0GBP
220    Dr Wu Honghan    University of Edinburgh    2018-02-14    2021-02-13    Deriving an actionable patient phenome from healthcare data    Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.    Medical Research Council    Fellowship    315181.0GBP
221    Dr Gordon George    University of Cambridge    2017-04-01    2019-03-31    Shedding light on cancer using nanoplasmonic metapixels    Background: Histopathology of biopsied tissue is the gold standard for diagnosis and grading of most cancers but the biopsy process is traumatic, introduces delays and is not practical for widespread early screening. Advanced optical imaging modalities hold promise for faster and less invasive methods of diagnosis, grading and precision treatment of cancer, particularly for early detection and surgical tumour margin identification. Examples include fluorescence imaging, Raman spectroscopy and polarimetric imaging. Each modality interrogates a different set of tissue properties and provides limited discrimination, but when many of these are combined, a multimodal approach, high diagnostic accuracy is achieved. This is typically achieved in a research setting, as several instruments can be applied sequentially, but in routine clinical practice this introduces unacceptable costs in terms of the instruments needed and procedure time. Further, movement occurring between changes of modality makes co-registration of images inaccurate, limiting the ability to overlay multiple tissue properties for augmented real-time vision. Aims: We propose to develop a unified multimodal optical imaging paradigm that can control all properties of light: wavelength, spatial profile, polarisation and phase. By activating different properties of illumination light and detector filtering in quick succession, we aim to develop a single system that can be dynamically reconfigured to implement many different types of imaging in a compact, low-cost fashion with high co-registration between modalities. Methods: We plan to construct a prototype using a technology we have recently developed: nanoplasmonic metapixels. A metapixel is a structure built from metal nanostructures that are small enough (~30nm) that their geometry controls the wavelength, polarisation and phase of light. We can make large arrays of metapixels that can be incorporated into existing projectors (for illumination) and image sensors. How the results will be used: A successful prototype demonstration will pave the way for numerous new research avenues and commercialisation opportunities. The reduced time and cost of multimodal imaging could enable larger screening programmes for improved early detection. The highly multimodal co-registered datasets collected could be used to develop advanced machine learning algorithms for improving cancer diagnosis. Finally, the multimodal datasets could be visualised using augmented reality systems for real-time diagnosis and improved surgical tumour resection. There is a strong base of UK companies working on the latter two aspects, creating significant commercialisation opportunities. Together, these advances will lead to improved diagnosis, treatment and hence outcome for cancer patients.    Cancer Research UK    Pioneer Award Committee - Pioneer Award    315181.0GBP
222    Dr Angelopoulou Anastasia    University of Westminster    2018-10-01    2020-03-31    Automated Diagnostic Toolkit for Dementia in Ageing Deaf Users of British Sign Language (BSL)    Background: Histopathology of biopsied tissue is the gold standard for diagnosis and grading of most cancers but the biopsy process is traumatic, introduces delays and is not practical for widespread early screening. Advanced optical imaging modalities hold promise for faster and less invasive methods of diagnosis, grading and precision treatment of cancer, particularly for early detection and surgical tumour margin identification. Examples include fluorescence imaging, Raman spectroscopy and polarimetric imaging. Each modality interrogates a different set of tissue properties and provides limited discrimination, but when many of these are combined, a multimodal approach, high diagnostic accuracy is achieved. This is typically achieved in a research setting, as several instruments can be applied sequentially, but in routine clinical practice this introduces unacceptable costs in terms of the instruments needed and procedure time. Further, movement occurring between changes of modality makes co-registration of images inaccurate, limiting the ability to overlay multiple tissue properties for augmented real-time vision. Aims: We propose to develop a unified multimodal optical imaging paradigm that can control all properties of light: wavelength, spatial profile, polarisation and phase. By activating different properties of illumination light and detector filtering in quick succession, we aim to develop a single system that can be dynamically reconfigured to implement many different types of imaging in a compact, low-cost fashion with high co-registration between modalities. Methods: We plan to construct a prototype using a technology we have recently developed: nanoplasmonic metapixels. A metapixel is a structure built from metal nanostructures that are small enough (~30nm) that their geometry controls the wavelength, polarisation and phase of light. We can make large arrays of metapixels that can be incorporated into existing projectors (for illumination) and image sensors. How the results will be used: A successful prototype demonstration will pave the way for numerous new research avenues and commercialisation opportunities. The reduced time and cost of multimodal imaging could enable larger screening programmes for improved early detection. The highly multimodal co-registered datasets collected could be used to develop advanced machine learning algorithms for improving cancer diagnosis. Finally, the multimodal datasets could be visualised using augmented reality systems for real-time diagnosis and improved surgical tumour resection. There is a strong base of UK companies working on the latter two aspects, creating significant commercialisation opportunities. Together, these advances will lead to improved diagnosis, treatment and hence outcome for cancer patients.    The Dunhill Medical Trust    Research Project & Programme Grants    65173.09GBP
223    Dr Patwardhan Ardan    European Bioinformatics Institute    2018-10-01    2023-09-30    EMDB: dealing with the cryo-EM data deluge    Background: Histopathology of biopsied tissue is the gold standard for diagnosis and grading of most cancers but the biopsy process is traumatic, introduces delays and is not practical for widespread early screening. Advanced optical imaging modalities hold promise for faster and less invasive methods of diagnosis, grading and precision treatment of cancer, particularly for early detection and surgical tumour margin identification. Examples include fluorescence imaging, Raman spectroscopy and polarimetric imaging. Each modality interrogates a different set of tissue properties and provides limited discrimination, but when many of these are combined, a multimodal approach, high diagnostic accuracy is achieved. This is typically achieved in a research setting, as several instruments can be applied sequentially, but in routine clinical practice this introduces unacceptable costs in terms of the instruments needed and procedure time. Further, movement occurring between changes of modality makes co-registration of images inaccurate, limiting the ability to overlay multiple tissue properties for augmented real-time vision. Aims: We propose to develop a unified multimodal optical imaging paradigm that can control all properties of light: wavelength, spatial profile, polarisation and phase. By activating different properties of illumination light and detector filtering in quick succession, we aim to develop a single system that can be dynamically reconfigured to implement many different types of imaging in a compact, low-cost fashion with high co-registration between modalities. Methods: We plan to construct a prototype using a technology we have recently developed: nanoplasmonic metapixels. A metapixel is a structure built from metal nanostructures that are small enough (~30nm) that their geometry controls the wavelength, polarisation and phase of light. We can make large arrays of metapixels that can be incorporated into existing projectors (for illumination) and image sensors. How the results will be used: A successful prototype demonstration will pave the way for numerous new research avenues and commercialisation opportunities. The reduced time and cost of multimodal imaging could enable larger screening programmes for improved early detection. The highly multimodal co-registered datasets collected could be used to develop advanced machine learning algorithms for improving cancer diagnosis. Finally, the multimodal datasets could be visualised using augmented reality systems for real-time diagnosis and improved surgical tumour resection. There is a strong base of UK companies working on the latter two aspects, creating significant commercialisation opportunities. Together, these advances will lead to improved diagnosis, treatment and hence outcome for cancer patients.    Wellcome Trust    Biomedical Resources Grant    1291058.0GBP
224    Dr Bunz Mercedes    King's College London    2019-01-15    2020-07-14    Public data, private collaborator: Will machine learning relocate medical knowledge?    Background: Histopathology of biopsied tissue is the gold standard for diagnosis and grading of most cancers but the biopsy process is traumatic, introduces delays and is not practical for widespread early screening. Advanced optical imaging modalities hold promise for faster and less invasive methods of diagnosis, grading and precision treatment of cancer, particularly for early detection and surgical tumour margin identification. Examples include fluorescence imaging, Raman spectroscopy and polarimetric imaging. Each modality interrogates a different set of tissue properties and provides limited discrimination, but when many of these are combined, a multimodal approach, high diagnostic accuracy is achieved. This is typically achieved in a research setting, as several instruments can be applied sequentially, but in routine clinical practice this introduces unacceptable costs in terms of the instruments needed and procedure time. Further, movement occurring between changes of modality makes co-registration of images inaccurate, limiting the ability to overlay multiple tissue properties for augmented real-time vision. Aims: We propose to develop a unified multimodal optical imaging paradigm that can control all properties of light: wavelength, spatial profile, polarisation and phase. By activating different properties of illumination light and detector filtering in quick succession, we aim to develop a single system that can be dynamically reconfigured to implement many different types of imaging in a compact, low-cost fashion with high co-registration between modalities. Methods: We plan to construct a prototype using a technology we have recently developed: nanoplasmonic metapixels. A metapixel is a structure built from metal nanostructures that are small enough (~30nm) that their geometry controls the wavelength, polarisation and phase of light. We can make large arrays of metapixels that can be incorporated into existing projectors (for illumination) and image sensors. How the results will be used: A successful prototype demonstration will pave the way for numerous new research avenues and commercialisation opportunities. The reduced time and cost of multimodal imaging could enable larger screening programmes for improved early detection. The highly multimodal co-registered datasets collected could be used to develop advanced machine learning algorithms for improving cancer diagnosis. Finally, the multimodal datasets could be visualised using augmented reality systems for real-time diagnosis and improved surgical tumour resection. There is a strong base of UK companies working on the latter two aspects, creating significant commercialisation opportunities. Together, these advances will lead to improved diagnosis, treatment and hence outcome for cancer patients.    Wellcome Trust    Seed Award in H&SS    57307.0GBP
225    Dr Hennequin Guillaume    Department of Electrical Engineering University of Cambridge    2013-02-01    2014-07-31    Fast but not furious: rapid Bayesian inference in balanced cortical circuits    Sensory perception is fast in human, and nonetheless almost Bayes-optimal. I propose a theoretical approach to this puzzle in which circuit mechanisms for fast dynamics (which I have explored as part of my PhD thesis) will be related to the speed of perception, for wich the computational aspects will be cast in the framework of probabilistic Bayesian inference. The working hypothesis will be that the brain represents posterior uncertainty via sampling, for which the host laboratory has found experimental evidence recently. We will thus combine methods from machine learning and statistical physics to understand how inference via sampling can be performed in the cortex. The project has expected outcomes both in the field of machine learning and in neuroscience.    Swiss National Science Foundation    Fellowships for prospective researchers    None
226    Dr Churcher Thomas    Imperial College London    2017-03-01    2020-02-29    Near-Infrared Spectroscopy: A One-Stop-Shop for Mosquito Epidemiological Monitoring?    Sensory perception is fast in human, and nonetheless almost Bayes-optimal. I propose a theoretical approach to this puzzle in which circuit mechanisms for fast dynamics (which I have explored as part of my PhD thesis) will be related to the speed of perception, for wich the computational aspects will be cast in the framework of probabilistic Bayesian inference. The working hypothesis will be that the brain represents posterior uncertainty via sampling, for which the host laboratory has found experimental evidence recently. We will thus combine methods from machine learning and statistical physics to understand how inference via sampling can be performed in the cortex. The project has expected outcomes both in the field of machine learning and in neuroscience.    Medical Research Council    Research Grant    643682.0GBP
227    Dr Churcher Thomas    UNIVERSITAIR MEDISCH CENTRUM UTRECHT    2013-08-01    2018-08-01    Intracranial COnnection with Neural Networks for Enabling Communication in Total paralysis    iCONNECT aims to give severely paralyzed people the means to communicate by merely imagining to talk or make hand gestures. Imagining specific movements generates spatiotemporal patterns of neuronal activity in the brain which I intend to record and decode with an intracranial Brain-Computer Interface (BCI) system. Many people suffer from partial or full loss of control over their body due to stroke, disease or trauma, and this will increase with population ageing. With both duration and quality of life beyond 60 increasing in the western world, more and more people will suffer from the consequences of function loss (mostly stroke) with the prospect of living for decades with the handicap, and will stand to benefit from restorative technology that has yet to be developed. I believe that functionality can be restored with brain implants. My goal is to develop a BCI that can interpret activity patterns on the surface of the brain in real-time. For this we need to discover how the brain codes for (imagined) actions, how codes can be captured and decoded and how an intracranial BCI system impacts on a user. I will use state of the art techniques (7 Tesla MRI and electrocorticography, ECoG) to explore brain codes and develop decoding strategies. Interactions between user and implanted device will be studied in paralyzed people. I will directly link decoded movements to animated visual feedback of the same body part, expecting to induce a feeling of ownership of the animation, and thereby a sense of actual movement. This research is only possible because of the latest developments in imaging of human brain activity, machine learning techniques, and micro systems technology. My lab is unique in bringing together all these techniques. Success of the project will lead to deeper understanding of how sensorimotor functions are represented in the human brain. The ability to ?read' the brain will add a new dimension to the field of neural prosthetics.    European Research Council    Advanced Grant    2498829.0EUR
228    Ms Bright Rebecca    Therapy Box Limited    2018-01-02    2019-01-01    ATLAS - AUTOMATED TRANSCRIPTION & LANGUAGE ANALYSIS SOFTWARE    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    National Institute for Health Research (Department of Health)    Full Award    149330.0GBP
229    Dr Barbosa da Silva Adriano    Queen Mary University of London    2018-03-22    2021-03-21    A translational data integration platform for the stratification of patients based on clinical, laboratory and magnetic resonances imaging    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    299527.0GBP
230    Dr Blackburn Ruth    University College London    2018-02-14    2021-11-13    Social contagion? : using data science to characterise the distribution and dispersion of health behaviours in adolescence    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    313016.0GBP
231    Dr Carson Jason    Swansea University    2018-02-14    2021-02-13    Non-invasive assessment and management of coronary heart disease - a translational, data driven approach    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    284559.0GBP
232    Dr Cole James    King's College London    2017-11-15    2019-10-06    Modelling brain ageing using neuroimaging to improve brain health in older adults    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    290658.0GBP
233    Dr Droop Alastair    Wellcome Trust Sanger Institute    2019-04-08    2021-02-13    Facilitating Deep Learning with Domain-Specific Knowledge    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    185651.0GBP
234    Dr Gurdasani Deepti    Wellcome Trust Sanger Institute    2018-02-14    2019-06-25    Predictive analytics of integrated genomic and clinical data using machine learning and complex statistical approaches    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    319508.0GBP
235    Dr Hall Benjamin    University of Cambridge    2018-12-01    2021-11-30    Deciphering and targeting cancer metabolism using executable models    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Research Grant    344485.0GBP
236    Dr Hassan Lamiece    The University of Manchester    2018-02-14    2021-02-13    Social listening: Applying natural language processing methods to social media data to yield actionable analytics for health and care services    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    300577.0GBP
237    Dr Lumbers Richard    University College London    2018-02-14    2021-02-13    'Unlocking therapeutic innovation in heart failure through genomic data science    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    313017.0GBP
238    Professor Tchanturia Kate    King's College London    2017-10-09    2019-10-08    The Triple A study (Adolescents with Anorexia and Autism): A search for biomarkers    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Research Grant    181988.0GBP
239    Professor Tchanturia Kate    King's College London    2019-10-01    2022-09-30    BiomaRkers for AnorexIa NErvosa and autism spectrum Disorders- longitudinal study    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Research Grant    496884.0GBP
240    Dr Verity Robert    Imperial College London    2016-04-01    2019-03-31    Genetic data as a signal of changing malaria transmission    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    281369.0GBP
241    Dr Wright David    Queen's University of Belfast    2018-02-14    2021-02-13    Data driven public health approaches for diabetic retinopathy and age-related macular degeneration    ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.    Medical Research Council    Fellowship    279158.0GBP
242    Professor Lorincz Attila    University of London    2016-06-01    2019-05-31    Development of a highly accurate DNA methylation classifier for prevalent and incident cervical pre-cancer    Background:Cervical cancer, caused by persistent infection with high risk (hr) HPV affects ~500,000 women globally and causes ~260,000 deaths annually. Although HPV immunisation has been successfully implemented the level of protection expected from current vaccines will be quite substantially incomplete and it will take several decades to see an effect of the new and improved nonavalent vaccine. A pressing need to combat cervical cancer through improved screening remains far into the future. Highly sensitive hrHPV testing is likely to become the dominant primary screen. However, hrHPV infection is common and only a fraction of women are at risk of developing cervical cancer. 40% of hrHPV+ women are cytology negative and triage by proposed adjunctive immunostaining tests such as p16 (often in conjunction with ki67) are insufficient. A molecular test is needed to more accurately identify clinically significant HPV infection. Aims:We aim to develop a DNA methylation (DNAm) biomarker panel that has the potential to be an excellent triage tool for hrHPV+ women and may become an integral part of the primary screening test for preventing cervical cancer.Methods:We propose to measure genome-scale human DNAm as well as DNAm of predefined sites in HPV16, 18, 31 and 33 in a set of 350 hrHPV+ women with normal cytology. This will be the largest such study to date. First, we will measure DNAm with reduced representation bisulfite sequencing followed by machine learning using MS-SPCA to identify ~100 sites which consistently appear in the best ranking models. Then, the best sites will be further sifted by additional multivariate data modelling to provide us with a minimum number of required classifier sites. Finally, these selected sites, presumably 10-20 sites, will be validated in a second set of 200 well characterised cervical samples.How the results of this research will be used:We will develop a significantly improved risk classification tool to triage hrHPV+ women to colposcopy. The new classifier must come close to the high sensitivity of current hrHPV tests (90-95%) but deliver a substantially higher specificity and PPV (both ~70%) than current molecular reflex tests (30-40% and 40-50% respectively). This new algorithm would allow more efficient utilization of colposcopy services while hrHPV+ women negative for the triage classifier could be followed up at suitably frequent intervals to safely catch most, if not all, triage false negatives.    Cancer Research UK    CRC - Biomarker Project Award    279158.0GBP
243    Dr Braga Rodrigo    Imperial College London    2014-11-01    2018-10-31    Local functional architecture and individual differences in cognitive and clinical states.    Functional connectivity within and between large-scale brain networks is disrupted in a range of mental disorders. Current network-based explanations of mental disorders have led to ambiguous conclusions, with different clinical conditions being associated with disruption of the same functional networks. My recent work has shown that heteromodal regions of the cortex can be decomposed to reveal a 'local functional architecture' (LFA) of separable subregions with distinct activation timecourses. Exploring this deeper LFA-level could disambiguate the neural basis of a range of mental illnesses. I will develop a novel analysis technique for the study of individual differences in brain activity at the LFA-level. My first goal will be to optimise a group-wise analysis technique I have developed (2,3) for use within individual subjects. I will apply the technique to large fMRI databases to explore to what extent LFA-level features such as subregion size and functional connectivity with ot her brain structures are conserved across the population. I will then use machine-learning techniques to test which LFA features relate to individual variability in a range of cognitive and personality inventories. Having identified candidate LFA features, I will test for group-wise differences in LFA properties in two psychiatric populations, schizophrenia and unipolar depression.    Wellcome Trust    Sir Henry Wellcome Postdoctoral Fellowship    250000.0GBP
244    Prof Noble Alison    University of Oxford    2016-11-01    2021-10-31    Perception Ultrasound by Learning Sonographic Experience    PULSE will develop a new generation of ultrasound imaging capabilities to revolutionize the use of this low-cost and portable imaging technology across clinical medicine worldwide. The greatest barrier to the universal implementation of ultrasound (US) in clinical medicine today is the need to train sonographers to the highest level to ensure diagnostic images are of consistently high quality and fit for purpose. Unfortunately, the non-expert finds US images very difficult to interpret by eye alone. Perception Ultrasound by Learning Sonographic Experience (PULSE) is an innovative inter-disciplinary project designed to eliminate the need for highly skilled operators of the technology. It is motivated by the observation that sonographers find it easier to interpret their own scans than review those taken by others. The innovation in PULSE is to apply the latest ideas from machine learning and computer vision to build, from real world training video data, computational models that describe how an expert sonographer performs a diagnostic study of a subject from multiple perceptual cues. Novel machine-learning based computational models will be derived based on probe and eye motion tracking, image processing, and knowledge of how to interpret real-world clinical images and videos acquired to a standardised protocol. By building models that more closely mimic how a human makes decisions from US images we believe we will build considerably more powerful assistive interpretation methods than have previously been possible from still US images and videos alone. Software demonstrators will be developed and evaluated on real world obstetric US data in collaboration with clinical experts and novices to demonstrate the new approach and its potential to move routine US scanning services from hospitals into the community which would have clear economic, healthcare and social benefits across Europe and beyond.    European Research Council    Advanced Grant    2462015.0EUR
245    Professor Leech Robert    King's College London    2018-04-01    2020-09-30    A novel adaptive sampling technique for mapping brain function    PULSE will develop a new generation of ultrasound imaging capabilities to revolutionize the use of this low-cost and portable imaging technology across clinical medicine worldwide. The greatest barrier to the universal implementation of ultrasound (US) in clinical medicine today is the need to train sonographers to the highest level to ensure diagnostic images are of consistently high quality and fit for purpose. Unfortunately, the non-expert finds US images very difficult to interpret by eye alone. Perception Ultrasound by Learning Sonographic Experience (PULSE) is an innovative inter-disciplinary project designed to eliminate the need for highly skilled operators of the technology. It is motivated by the observation that sonographers find it easier to interpret their own scans than review those taken by others. The innovation in PULSE is to apply the latest ideas from machine learning and computer vision to build, from real world training video data, computational models that describe how an expert sonographer performs a diagnostic study of a subject from multiple perceptual cues. Novel machine-learning based computational models will be derived based on probe and eye motion tracking, image processing, and knowledge of how to interpret real-world clinical images and videos acquired to a standardised protocol. By building models that more closely mimic how a human makes decisions from US images we believe we will build considerably more powerful assistive interpretation methods than have previously been possible from still US images and videos alone. Software demonstrators will be developed and evaluated on real world obstetric US data in collaboration with clinical experts and novices to demonstrate the new approach and its potential to move routine US scanning services from hospitals into the community which would have clear economic, healthcare and social benefits across Europe and beyond.    Medical Research Council    Research Grant    456799.0GBP
246    Dipl.Ing. Dr. KRANNER Gerhard    Viscovery Software GmbH    2018-07-01    2021-06-30    SystemMedicine clinical decision support for COPD patients    Chronic obstructive pulmonary disease (COPD) claimed 3.2 million lives in 2015, making it the third cause of death worldwide. It is predicted to increase in coming years due to aging populations and thus constitutes an enormous socio-economic burden. Existing assessment strategies neglect the complex, multi-component, and heterogeneous pathophysiology, as well as manifold comorbidities (cardiovascular, metabolic etc.). Therefore, improved COPD diagnosis and classification constitutes an urgent medical need for improved and personalized prevention measures and treatments strategies. The main aim of our proposal is to develop a tool that will enable effective preventive measures and personalize treatment strategies for COPD. This transnational and interdisciplinary project combines clinical scientists, experimentalists, computational and systems biology researchers, as well as a medium sized company. We will develop a systems medicine model of COPD constructed on (i) machine learning clustering of two comprehensive patient cohorts (COSYCONET, CIRO) providing long-term clinical observations, systematic outcome evaluation, biomaterial collections, multiple laboratory measurements, and extensive imaging data of more than 6,000 patients, complemented by (ii) an iterative systems biology framework of modeling and experimental analysis. Based on this multi-scale systems medicine model, we will generate a novel Clinical Decision Support (CDS) software that can be implemented in the existing IT infrastructure of hospitals and private practices for routine application. As a prototypic demonstrator of applied systems medicine, our tool will enable i) individual and comprehensive treatment and prevention measures for COPD patients ii) significant reductions of socio-economic costs due to less mortality and disability. iii) novel insights in the dysregulation of metabolism, immunology and aging in COPD from the underlying model.    Austrian Science Fund FWF    Research Grant    246692.36EUR
247    Professor Pridmore Tony    University of Nottingham    2018-09-01    2022-08-31    PhenomUK - Crop Phenotyping: from Sensors to Knowledge    Chronic obstructive pulmonary disease (COPD) claimed 3.2 million lives in 2015, making it the third cause of death worldwide. It is predicted to increase in coming years due to aging populations and thus constitutes an enormous socio-economic burden. Existing assessment strategies neglect the complex, multi-component, and heterogeneous pathophysiology, as well as manifold comorbidities (cardiovascular, metabolic etc.). Therefore, improved COPD diagnosis and classification constitutes an urgent medical need for improved and personalized prevention measures and treatments strategies. The main aim of our proposal is to develop a tool that will enable effective preventive measures and personalize treatment strategies for COPD. This transnational and interdisciplinary project combines clinical scientists, experimentalists, computational and systems biology researchers, as well as a medium sized company. We will develop a systems medicine model of COPD constructed on (i) machine learning clustering of two comprehensive patient cohorts (COSYCONET, CIRO) providing long-term clinical observations, systematic outcome evaluation, biomaterial collections, multiple laboratory measurements, and extensive imaging data of more than 6,000 patients, complemented by (ii) an iterative systems biology framework of modeling and experimental analysis. Based on this multi-scale systems medicine model, we will generate a novel Clinical Decision Support (CDS) software that can be implemented in the existing IT infrastructure of hospitals and private practices for routine application. As a prototypic demonstrator of applied systems medicine, our tool will enable i) individual and comprehensive treatment and prevention measures for COPD patients ii) significant reductions of socio-economic costs due to less mortality and disability. iii) novel insights in the dysregulation of metabolism, immunology and aging in COPD from the underlying model.    Medical Research Council    Research Grant    528567.0GBP
248    Dr Nyrup Rune    University of Cambridge    2018-10-01    2020-03-31    Understanding Medical Black Boxes: A Philosophical Analysis of AI Explainability    Chronic obstructive pulmonary disease (COPD) claimed 3.2 million lives in 2015, making it the third cause of death worldwide. It is predicted to increase in coming years due to aging populations and thus constitutes an enormous socio-economic burden. Existing assessment strategies neglect the complex, multi-component, and heterogeneous pathophysiology, as well as manifold comorbidities (cardiovascular, metabolic etc.). Therefore, improved COPD diagnosis and classification constitutes an urgent medical need for improved and personalized prevention measures and treatments strategies. The main aim of our proposal is to develop a tool that will enable effective preventive measures and personalize treatment strategies for COPD. This transnational and interdisciplinary project combines clinical scientists, experimentalists, computational and systems biology researchers, as well as a medium sized company. We will develop a systems medicine model of COPD constructed on (i) machine learning clustering of two comprehensive patient cohorts (COSYCONET, CIRO) providing long-term clinical observations, systematic outcome evaluation, biomaterial collections, multiple laboratory measurements, and extensive imaging data of more than 6,000 patients, complemented by (ii) an iterative systems biology framework of modeling and experimental analysis. Based on this multi-scale systems medicine model, we will generate a novel Clinical Decision Support (CDS) software that can be implemented in the existing IT infrastructure of hospitals and private practices for routine application. As a prototypic demonstrator of applied systems medicine, our tool will enable i) individual and comprehensive treatment and prevention measures for COPD patients ii) significant reductions of socio-economic costs due to less mortality and disability. iii) novel insights in the dysregulation of metabolism, immunology and aging in COPD from the underlying model.    Wellcome Trust    Seed Award in H&SS    86561.0GBP
249    Mr Mishra Abhishek    University of Oxford    2018-10-01    2021-10-01    Delivering Care Through AI Systems    Chronic obstructive pulmonary disease (COPD) claimed 3.2 million lives in 2015, making it the third cause of death worldwide. It is predicted to increase in coming years due to aging populations and thus constitutes an enormous socio-economic burden. Existing assessment strategies neglect the complex, multi-component, and heterogeneous pathophysiology, as well as manifold comorbidities (cardiovascular, metabolic etc.). Therefore, improved COPD diagnosis and classification constitutes an urgent medical need for improved and personalized prevention measures and treatments strategies. The main aim of our proposal is to develop a tool that will enable effective preventive measures and personalize treatment strategies for COPD. This transnational and interdisciplinary project combines clinical scientists, experimentalists, computational and systems biology researchers, as well as a medium sized company. We will develop a systems medicine model of COPD constructed on (i) machine learning clustering of two comprehensive patient cohorts (COSYCONET, CIRO) providing long-term clinical observations, systematic outcome evaluation, biomaterial collections, multiple laboratory measurements, and extensive imaging data of more than 6,000 patients, complemented by (ii) an iterative systems biology framework of modeling and experimental analysis. Based on this multi-scale systems medicine model, we will generate a novel Clinical Decision Support (CDS) software that can be implemented in the existing IT infrastructure of hospitals and private practices for routine application. As a prototypic demonstrator of applied systems medicine, our tool will enable i) individual and comprehensive treatment and prevention measures for COPD patients ii) significant reductions of socio-economic costs due to less mortality and disability. iii) novel insights in the dysregulation of metabolism, immunology and aging in COPD from the underlying model.    Wellcome Trust    PhD Studentship in H&SS    140538.0GBP
250    Professor Zhang Daoqiang    Nanjing University of Aeronautics and Astronautics    2018-03-31    2021-02-28    Big-Data Driven Intelligent Analysis of High Dimensional Multimodal Neuroimaging Data and its Application to Brain Disease Diagnosis    Neuroimaging methods based on high dimensional multimodal structural and functional imaging data have been recently proposed for objective diagnosis of brain diseases such as Alzheimer’s disease and autism. However, it is challenging to deal with multimodal data, since 1) multimodal data are typically massive in dimensionality,2) the complete set of Training Programme multimodal data is often unavailable for each subject due to data loss during canning or storage, or due to different study designs in different institutes, and 3) the number of subjects is often much smaller than the dimensionality of the multimodal data. The goal of this project is to develop advanced machine learning methods to address all these challenges in intelligent analysis of multimodal neuroimaging data. Specifically, we will develop a robust ensemble learning method for hierarchical decision fusion from multiple multi-level classifiers, through a layer-by-layer and localˇto-global fashion, to address the first challenge of high dimensionality. Moreover, we will develop a unified framework for novel multi-task semi-supervised (or transfer) learning with multimodal imaging data, to jointly address the second and third challenges of missing data and insufficient training samples, respectively. To our knowledge, the existing neuroimaging analysis methods are not able to deal effectively with these two challenges. Finally, all these proposed methods will be tested and evaluated on real neuroimaging datasets. The overarching aim of this project is to develop, train and transfer news skills to China-PI (Zhang) and his group on 1) efficient analysis of high-dimensional neuroimaging data using current advances and new knowledge in big data analytics and data science, especially in the areas of parallel and distributed computing, data analytics/image pattern recognition; 2) and transferable skills such communication, leadership and project management skills.    The Academy of Medical Sciences    Newton Advanced Fellowship    108000.0GBP
251    Dr Bedford Jonathan    University of Oxford    2018-03-31    2021-02-28    New-onset atrial fibrillation in critically ill patients: risk factors and outcomes    Neuroimaging methods based on high dimensional multimodal structural and functional imaging data have been recently proposed for objective diagnosis of brain diseases such as Alzheimer’s disease and autism. However, it is challenging to deal with multimodal data, since 1) multimodal data are typically massive in dimensionality,2) the complete set of Training Programme multimodal data is often unavailable for each subject due to data loss during canning or storage, or due to different study designs in different institutes, and 3) the number of subjects is often much smaller than the dimensionality of the multimodal data. The goal of this project is to develop advanced machine learning methods to address all these challenges in intelligent analysis of multimodal neuroimaging data. Specifically, we will develop a robust ensemble learning method for hierarchical decision fusion from multiple multi-level classifiers, through a layer-by-layer and localˇto-global fashion, to address the first challenge of high dimensionality. Moreover, we will develop a unified framework for novel multi-task semi-supervised (or transfer) learning with multimodal imaging data, to jointly address the second and third challenges of missing data and insufficient training samples, respectively. To our knowledge, the existing neuroimaging analysis methods are not able to deal effectively with these two challenges. Finally, all these proposed methods will be tested and evaluated on real neuroimaging datasets. The overarching aim of this project is to develop, train and transfer news skills to China-PI (Zhang) and his group on 1) efficient analysis of high-dimensional neuroimaging data using current advances and new knowledge in big data analytics and data science, especially in the areas of parallel and distributed computing, data analytics/image pattern recognition; 2) and transferable skills such communication, leadership and project management skills.    National Institute for Health Research (Department of Health)    Newton Advanced Fellowship    367243.0GBP
252    Dr Bedford Jonathan    University of California-Santa Barbara    2020-05-15    2020-10-31    RAPID: COVIDGeoGraph ? A Geographically Integrated Cross-Domain Knowledge Graph for Studying Regional Disruptions    Office of the Director - This COVID-19 RAPID research program will develop a geographically integrated knowledge graph to support data scientists and decision-makers in industry and the government in taking region-specific steps towards reopening the country. Towards this goal, the project will combine data from themes as diverse as transportation, social distancing measures, demographic and environmental factors, as well as economic impacts. Knowledge graphs are contextualization technologies. They enable their users to gain a more holistic understanding of complex social and scientific questions by providing actionable insights from neighboring disciplines. For instance, making decisions about local economies and food systems require an understanding of the frequently changing social distancing measures, traffic control and restrictions including neighboring regions, demographic factors, and the percentage of recovered citizens. The project will work together with industry partners to understand how graphed knowledge can be utilized in their forecasting models. Finally, the project will reach out to other knowledge graphs to jointly form an open knowledge network related to COVID-19.<br/><br/> More technically, we will utilize a stack of open source technologies and international standards to develop a highly integrated Linked Data-based knowledge graph that combines cross-domain data across different geographic scales and types of places. We will utilize machine learning technologies to learn graph embeddings for predicting links within our graph as well as alignments to other graphs by using places such as cities or counties as points of integration. We will also provide the functionality to collaborate with the broader schema.org effort. We are particularly interested in studying how to represent and align COVID-19 data that is currently being reported at different geographic scales and aggregates, thereby fostering interoperability and breaking up data silos. We will work directly with our industry partners on engineering (spatially-explicit) features from our and external graphs. These features will be integrated into industry downstream forecasting models with a particular focus on food systems. Finally, We will utilize the expertise of our partners in creating visual data dashboards to better communicate our findings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    90658.0USD
253    Dr Bedford Jonathan    New York University    2020-05-01    2021-04-30    RAPID: Computational Modeling of Contact Density and Outbreak Estimation for COVID-19 Using Large-scale Geolocation Data from Mobile Devices    Engineering - The outbreak of COVID-19 has highlighted both the growing global risk of emerging pandemics and the urgent need for enhanced data-driven tools to identify, contain, and mitigate their effects, particularly in dense urban areas. There has been increasing attention given to locational data from smartphones as a way to enhance epidemiological modeling and predict outbreak progression, transmission, and exposure risk. When combined with artificial intelligence or machine learning algorithms, these high resolution data have the potential to vastly improve the granularity and precision of infection and hospitalization estimates. However, the use of locational data raises serious social, ethical, and technical challenges. Trade-offs between the potential public health benefits and the impacts for privacy and civil liberties have started to be debated in earnest within the context of the current pandemic, especially in light of increasing use of these data by private companies to promote targeted advertisements, evaluate retail consumer behavior, and model travel demand, among other applications. Furthermore, the use of these data in the public interest is undermined by an incomplete understanding of the representativeness and bias embedded in these data, particularly in relation to under-represented and vulnerable communities. What is not yet known is the extent of this bias in locational data and how the public health benefits of using these data diminish with spatial and temporal aggregation, which could help to minimize privacy concerns in the collection and use of these data. To address these questions, this project will develop computational models derived from large-scale locational data to (1) estimate the exposure density across a range of temporal (hourly, daily, etc.) and spatial (census block, neighborhood, etc.) scales, which will enable officials and researchers to evaluate and predict transmission rates in a particular area; (2) measure and evaluate the extent and effectiveness of social (physical) distancing efforts over time and comparatively within and across counties and cities, as well as understand the disparate impacts on vulnerable communities and populations; and (3) measure the extent of disease spread based on movement and travel patterns between neighborhoods and communities, which will support predictions of the spatial-temporal patterns of disease outbreak and identify ?at-risk? locations based on the aggregated mobility trajectories for areas were infections have been identified or suspected. <br/><br/>The project team is particularly concerned with how shelter-in-place orders and exposure risk disproportionately impact low-income and minority communities, and the implications of potential bias in locational data in assessing socioeconomic variations. The project will assess how the usefulness of these models for epidemiologists and public health officials varies with spatial aggregation (e.g. is neighborhood level data superior to county level data) and temporal aggregation (e.g. is a near-real-time model superior to daily or weekly timescales) and provide quantitative performance assessments that can be used for collective decision-making on the trade-offs between health benefits and privacy risk. Project outputs will be made open-source and publicly available as appropriate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    199958.0USD
254    Dr Bedford Jonathan    Columbia University    2020-05-15    2020-10-31    RAPID: COVID Information Commons (CIC)    Office of the Director - This project will create a COVID Information Commons (CIC) website to facilitate knowledge sharing and collaboration across various COVID research efforts, especially focusing on all the NSF-funded COVID Rapid Response Research (RAPID) projects. The CIC will serve as a resource for researchers as well as decision-makers from government, academia, not-for-profit and industry to leverage each other's findings, and invest in and accelerate the most promising research to mitigate the broad societal impacts of the COVID-19 pandemic. It will also serve as a model for integrated knowledge sharing and collaboration on other public health challenges, in benefit to society. Projects will be able to enter and publish information about their efforts in ways that are most relevant and user-friendly for a variety of potential stakeholders from academia, industry, government, and non-profit sectors. Information will be organized in multiple ways, for example, by research topics areas and by geography. In addition to information from NSF COVID-19 RAPID projects, the COVID Information Commons will incorporate coronavirus-related information from NSF Open Knowledge Network projects, as well as from other NSF research projects in general. <br/><br/>The COVID Information Commons will utilize information science methods to bring together information about the collection of COVID-19 RAPID projects funded by the National Science Foundation. A wide array of research efforts are underway to study the impacts of the pandemic in fields as far ranging as biophysics, social justice/inequity, behavioral science, public health, supply chains, and risk management. The CIC will semantically link information across projects to provide a more holistic view across distinct efforts, including efforts such as the COVID projects in the NSF Open Knowledge Network. The resulting, concise, curated, integrated resource will provide insight into NSF-funded COVID RAPID projects and facilitate collaborations among such efforts. These objectives will be achieved using information science approaches to 1) compile a comprehensive list of NSF COVID RAPID awards, along with relevant details for each project, 2) link to any publicly available data sets and data feeds, 3) organize the information and data feeds, for example, by categories of research areas and/or geography, using a meta-data schema developed for the resource and existing taxonomy and semantic frameworks; 4) design and develop a web portal to allow project teams to publish their data, or links to the data, and present project information in ways that are most relevant and user friendly for researchers in academia, industry, and government; 5) integrate the schema.org COVID-19 annotated data to enable more effective identification, retrieval, and integration of relevant data. A Minimum Viable Product for the website will be developed first, working with stakeholders in the community to prioritize features and add new functionality. In addition to the Information Commons, the project will also assess the effort and feasibility of implementing a data and model commons?to share datasets as well as data-driven models, such as machine learning models related to COVID-19.<br/><br/>This RAPID award is made by the Convergence Accelerator program in the Office of Integrative Activities with funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    200000.0USD
255    Dr Bedford Jonathan    Johns Hopkins University    2020-05-01    2021-04-30    RAPID: Prediction of Cardiac Dysfunction in COVID-19 Patients Using Machine Learning    Engineering - Recent reports demonstrate the critical influence of COVID-19 on the cardiovascular system, with up to 20% of COVID-19 patients suffering acute cardiac injury. Approaches to identify COVID-19 patients at risk for cardiac dysfunction have not yet been developed, and no alerting clinical parameters are available to address the impending decline of cardiac function and mortality. The goal of this project is to develop a machine learning approach to identify COVID-19 patients at risk for cardiac dysfunction and sudden cardiac death. Utilizing such an approach will provide early warning and enable the delivery of early goal-directed therapy, reducing mortality and optimizing allocation of resources. The machine learning classifier is to be distributed to any interested healthcare institution, to augment their ability to successfully treat patients. This project also provides fundamental new scientific knowledge: how COVID-19-related cardiac injury could result in cardiac dysfunction and sudden cardiac death. Such knowledge is of paramount importance in the fight against COVID-19 and the post-disease adverse effects on human health. <br/><br/>Features that will serve as input into the machine learning classifier will be extracted from both time series (ECG, cardiac-specific laboratory values, continuously-obtained vital signs) and imaging data (CT, echocardiography). Data will be collected from patients admitted to Johns Hopkins Hospital and Johns Hopkins Health System; other hospitals in the Chesapeake area; and potetially hospitals in NYC, with a confirmed diagnosis of COVID-19 based on nucleic acid or polymerase chain reaction testing. We will develop a time-varying risk score that will determine the posterior probability of hemodynamically-significant cardiac disease outcome within 24 hours of certain time points. For new patients, the model will be used to perform a baseline prediction which will be updated in a Bayesian fashion each time new data becomes available.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    195621.0USD
256    Dr Bedford Jonathan    Clark University    2020-05-15    2021-04-30    RAPID: Predicting Coronavirus Disease (COVID-19) Impact with Multiscale Contact and Transmission Mitigation    Mathematical and Physical Sciences - Nontechnical Abstract: <br/>The rapid spread of new coronavirus SARS-CoV-2, which causes Coronavirus Disease (covid-19), requires a multidisciplinary mitigation strategy from the clinical to physical host-to-host transmission modelling. Data is required on the transmission of pathogen carrying airborne mucosalivary droplets and aerosols generated during normal breathing, talking, sneezing, and coughing. Synthetic exhalations will be measured leveraging advanced prototyping to obtain data needed to model the spread of covid-19, and the efficacy of personal protection devices and face coverings fabricated with various weaves and materials will be tested. Physical data related to temperature, humidity, and airflow on survival and dispersion of exhalations will be obtained. The data will be integrated using supervised machine learning methods, mathematical network simulations, and epidemiological data to develop an individual-based method that can give pandemic management results. Physical data will be published on transmission rates, including wearing of personal protective equipment and face coverings with various weaves, to inform mitigation strategies to alleviate covid-19 pandemic. Interactive Web based resources will be used for immediate broad dissemination of data and learning outcomes on covid-19 to the public, in addition to peer reviewed publications and training post-doctoral and undergraduate researchers in methods leading to pandemic mitigation.<br/><br/>Technical Abstract:<br/>The mode of transmission and extent of environmental contaminations on the outbreak of the Coronavirus Disease 2019 (covid-19), while sharing features with severe acute respiratory syndrome and other infectious diseases, remains unknown. This project will address fundamental rheology-matched metrics of transport and survival of airborne exhalation droplets and aerosols that carry coronavirus and on surfaces needed as input parameters for modeling mitigation. Impact of personal protective equipment on individual prognosis, with physical data related to temperature, humidity, and airflow-dependent dispersion distance of pathogen bearing viscoelastic droplets corresponding to breathing, sneezing, and coughing, will be obtained. The impact of the measured transmission rates on the spread and recurrence will be investigated with epidemiological data integrated with deep learning to implement a scalable, individual-based, stochastic, spatial model. Resulting peer-reviewed publications will serve as trusted source for calculation of covid-19 transmissibility and personal protection strategies. Post-doctoral and undergraduate researchers versed in fluid dynamics, soft matter physics, and network simulations will be trained toward mitigating infectious disease spread.<br/><br/>This Rapid Response Research (RAPID) grant supports research that will result in spatiotemporal mucosalivary droplet transmission range data required to develop covid-19 mitigation network methods with funding from the CARES Act managed by the Condensed Matter Physics Program in the Division of Materials Research of the Mathematical and Physical Sciences Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    200000.0USD
257    Dr Bedford Jonathan    STANFORD UNIVERSITY    2020-05-01    2021-04-30    RAPID: Collaborative Research: Computational Drug Repurposing for COVID-19    Computer and Information Science and Engineering - With the disruptive nature of the COVID-19 pandemic, effective treatments could save the lives of severely ill patients, protect individuals with a high risk of infection, and reduce the time patients spend in hospital beds. However, there are currently no effective treatments for COVID-19. Traditional methodologies take years to develop and test compounds from scratch. Machine learning provides promising new approaches to repurpose drugs that are safe and already approved for other diseases. This project will develop a machine learning toolset to expedite the development of safe and effective medicines for COVID-19. The toolset will rapidly identify safe repurposing opportunities for approved and experimental drugs. It will predict whether treatments may have therapeutic effects in COVID-19 patients, allowing the identification of drugs and drug cocktails that are safe and plentiful enough to treat a substantial number of patients. By putting tools in the hand of practitioners, the activities in this project will have an immediate impact. They will result in actionable predictions that are accurate and interpretable. <br/><br/>Recently, the principal investigators have developed a series of machine learning tools to identify drug repurposing opportunities. Building on foundational previous work, in this project, the principal investigators will first build a large COVID-19 focused knowledge graph that will capture fundamental and COVID-19-specific biological knowledge. The graph learning methods will be adapted to identify safe drugs and drug cocktails for COVID-19. To predict the safety of cocktails with two or more drugs, the methods will generalize to an exponentially large space of high-order drug combinations. In addition to drug safety, efficacy is a crucial endpoint for drug development. The project will develop a novel graph neural network (GNN) method to identify efficacious drug repurposing opportunities, even for diseases, such as COVID-19, that do not yet have any drug treatments and thereby, no label, supervised information. The method will predict what drugs and drug combinations may have a therapeutic effect on COVID-19. Finally, the principal investigators will integrate the developed tools into a complete, explainable framework that will generate predictions, provide explanations, and incorporate human feedback into the machine learning loop. This project will provide new, open tools for rapid drug repurposing that will be relevant for COVID-19 and other emerging pathogens. Additionally, the project will provide unique opportunities for multi-disciplinary curriculum development, training and advising, and professional activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    100000.0USD
258    Dr Bedford Jonathan    University of California-Davis    2020-05-01    2021-04-30    RAPID: Using Data Science and Biophysical Models to Address the COVID-19 Pandemic    Mathematical and Physical Sciences - COVID-19, the disease caused by the SARS-CoV-2 coronavirus, is at the center of one of the most dangerous pandemics the world has ever known. As it spreads through the Human population the virus mutates producing proteins that can lead to higher infection rates (infectivity), and an increased ability to cause severe disease (virulence). This project will predict the most likely mutations of the virus by combining methods from machine learning, mathematics and biophysics. Specifically, the proteins resulting from viral mutations will be experimentally synthesized, and their infectivity and virulence will be tested by the project team through a collaboration with researchers in industry. This project benefits from unprecedented access to genomic data compiled on SARS-CoV-2, combined with a rich set of novel tools developed through interdisciplinary advances in data science, mathematics, and biophysics. The results of this project will build a pipeline capable of assisting the development of vaccines and drugs against COVID-19 while simultaneously advancing the fields of machine learning and mathematical virology. The project team is led by mathematicians, molecular biologists and biotechnology experts working in an interdisciplinary and collaborative setting. Students and postdoctoral researchers will be trained and will participate in publicly disseminating the findings and results of the project.<br/><br/> <br/>The SARS-CoV-2 coronavirus is believed to have originated as a bat virus and to have evolved through a combination of sequence mutations, recombination, and natural selection to be infectious in human hosts. Some of the most relevant sequence variations occurred in the S gene encoding the Spike (S) protein. As SARS-CoV-2 spreads through the Human population, mutations of the S gene can potentially increase viral infectivity and virulence. Within the framework of an evolutionary algorithm, the PIs will combine graph theory, topological data analysis, and computational biophysics to characterize the most likely mutations of the S protein. This powerful interdisciplinary approach will draw upon existing experimental data from SARS-CoV-2. The PIs will collaborate with an industrial partner to experimentally design the peptides corresponding to those predicted sequences, and use binding affinity assays and cryo-electron microscopy to test binding of the peptides to the human receptor (ACE2). The resulting pipeline will help us better understand the evolutionary landscape of viral proteins and will assist researchers in the development of anti-viral drugs and vaccines. Future extensions of this work will increase our understanding of how viruses are transmitted across species and propagate in humans. The project will provide multi-disciplinary student and postdoctoral training. The PIs will broadly disseminate their results, as well as the data they collect and software they design.<br/><br/>With this award, the Mathematical Biology Program in the Division of Mathematical Sciences and the Chemistry of Life Processes Program in the Division of Chemistry are supporting Drs. Arsuaga, Rodriguez, and Vazquez from University of California-Davis to study genomic variations of the SARS-CoV-2 viral spike (S) protein and predict the expansion range of transmission in Human populations.<br/><br/>This grant is being awarded using funds made available by the Coronavirus Aid, Relief, and Economic Security (CARES) Act supplemental funds allocated to MPS.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    199998.0USD
259    Dr Bedford Jonathan    Princeton University    2020-05-15    2021-04-30    RAPID: Open Research Infrastructure for COVID-19 Ventilator Data    Mathematical and Physical Sciences - The lungs are a key avenue of attack for the SARS-CoV-2 virus. Respiratory problems are primary symptoms of COVID-19, and early indication is that it does not behave like previous examples of Acute Respiratory Distress Syndrome (ARDS). A severe urgency exists to understand how to provide optimal care for patients requiring artificial ventilation, to minimize both mortality and adverse long-term effects on those patients who survive. The project will illuminate lung function under the stress of COVID-19 and provide open tools to engage the larger community to help understand this very urgent societal problem. The project output will include instrumentation advances, software and data, as well as models of lung function under the stress of COVID-19. The project will also inform the medical community as to how to treat COVID-19 patients, because COVID-19 differs notably from prior experience with ARDS.<br/><br/>Respiration and lung function is fundamentally a dynamical physical system amenable to traditional pressure/volume/flow relationships, with a quantity called "lung compliance." COVID-19 is unique, in that the underlying biology can lead to changes in the parameters of this dynamical system that are surprisingly fast, and different from previous ARDS cases, on the time scale of hours or days. Medical personnel need to navigate the evolving nature of the consequences of the viral infection as well as mechanical ventilation induced lung inflammation and potential injury, with outcomes ranging from recovery with varying impacts on post-illness lung function to death. This project consists of three related activities: (1) Instrumentation: Continued development of a low-cost, open-source ventilator monitor, including additional options for readily sourceable parts, and related documentation on calibrations. (2) Data: Development, with the broader community, of open datasets of breathing and ventilator data, including flow, pressure, O2 levels, and derived quantities of interest to enable innovation and machine learning in a space that otherwise lacks open data. (3) Models: Development of open simulations, visualizations, and models for mechanical ventilation and the breathing process, under stresses like COVID-19, that enable a physicist's understanding of the system, enable innovation, and can potentially aid the medical community.<br/><br/>This grant is being awarded using funds made available by the Coronavirus Aid, Relief, and Economic Security (CARES) Act supplement allocated to MPS.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    200000.0USD
260    Dr Bedford Jonathan    Unity Health Toronto    2020-05-15    2021-04-30    Acute Respiratory Mortality Surveillance (ARMS) for Coronavirus Infection (COVID-19): A globally relevant technology to strengthen mortality surveillance for acute respiratory deaths in many countries lacking complete medical certification of death    The current global infectious threat, COVID-19, has not yet been widely detected in sub-Saharan Africa or other low income countries in Asia. It is almost inevitable that it will reach those places. While unusual spikes in infection-related deaths can register quickly in higher income countries and in China, they can go unrecognized for weeks or months in low-income settings where even very ill people do not go to a hospital, infecting others. Detecting a mortality signal is important and may be the first step in recognizing a serious outbreak. We propose to build on our extensive experience using verbal autopsy (VA) in the long-running Indian Million Death Study, and ongoing studies in China, Hong Kong, Ethiopia and Sierra Leone to develop an enhanced verbal autopsy module to identify deaths from COVID-19. This will serve as a model for the next novel pathogen-as near as possible to real time in settings without routine medical certification of death. We will test three hypotheses: #1 An "Acute Respiratory Mortality Surveillance" (ARMS) module can be added quickly to the WHO VA instrument and validated against hospitalized cases and deaths (paired with epidemiological information and machine learning) to distinguish COVID-19 from other causes of respiratory deaths. #2 Early deployment of ARMS in China, Hong Kong, India, Sierra Leone, and Ethiopia will help establish baseline distributions of usual acute respiratory deaths, as a comparator for COVID-19 deaths, and to inform modelling. #3 Effective knowledge translation of an open-source, widely-available ARMS module will improve the global response to COVID-19, particularly in the lowest income countries and help to improve mortality assessments for any subsequent COVID-19 waves. A successful ARMS will contribute to stopping the current outbreak and add novel surveillance tools. All materials and results will be made available globally to ensure the broadest use.    Canadian Institutes of Health Research    Research Grant    956320.0CAD
261    Professor Kaiser Marcus    Newcastle University    2019-04-01    2022-03-31    Modelling dementia progression based on machine learning and simulations    The current global infectious threat, COVID-19, has not yet been widely detected in sub-Saharan Africa or other low income countries in Asia. It is almost inevitable that it will reach those places. While unusual spikes in infection-related deaths can register quickly in higher income countries and in China, they can go unrecognized for weeks or months in low-income settings where even very ill people do not go to a hospital, infecting others. Detecting a mortality signal is important and may be the first step in recognizing a serious outbreak. We propose to build on our extensive experience using verbal autopsy (VA) in the long-running Indian Million Death Study, and ongoing studies in China, Hong Kong, Ethiopia and Sierra Leone to develop an enhanced verbal autopsy module to identify deaths from COVID-19. This will serve as a model for the next novel pathogen-as near as possible to real time in settings without routine medical certification of death. We will test three hypotheses: #1 An "Acute Respiratory Mortality Surveillance" (ARMS) module can be added quickly to the WHO VA instrument and validated against hospitalized cases and deaths (paired with epidemiological information and machine learning) to distinguish COVID-19 from other causes of respiratory deaths. #2 Early deployment of ARMS in China, Hong Kong, India, Sierra Leone, and Ethiopia will help establish baseline distributions of usual acute respiratory deaths, as a comparator for COVID-19 deaths, and to inform modelling. #3 Effective knowledge translation of an open-source, widely-available ARMS module will improve the global response to COVID-19, particularly in the lowest income countries and help to improve mortality assessments for any subsequent COVID-19 waves. A successful ARMS will contribute to stopping the current outbreak and add novel surveillance tools. All materials and results will be made available globally to ensure the broadest use.    Medical Research Council    Research Grant    304844.0GBP
262    Dr. LODE Axel Ulrich    University of Vienna    2019-01-01    2021-12-31    Numerics for many-body physics and single-shot images    Numerical models for many-body physics and single-shot images Scientific Abstract: We develop and apply improved simulation tools and numerical techniques for the time-dependent many-body Schrödinger equation for describing dynamical quantum systems used in state-of-the art experiments with ultracold atoms like the ones in the groups at the AtomInstitut, TU Wien. Our key goal is to investigate what one can really know about quantum many-body states of ultracold atoms. We go beyond models with a single Gross-Pitaevskii equation which cannot capture the phenomena in our focus like correlations and squeezing, that are observed in current experiments. A fundamental obstacle is to extract the information about correlations and squeezing from the experiment. In many experimental setups, the observations consist in so-called "single-shot images" of the atomic clouds. Theoretically, single-shot images represent a projective measurement of the many-body wavefunction. The current strategy necessitates a very large number of single-shot images to be produced and analyzed. We plan to use and improve a sophisticated numerical tool, the MCTDH-(B/F), that allows to describe correlations and squeezing. We aim to enable a direct and predictive modeling. We will implement an efficient MCTDH-(B/F) software and apply it for the simulation of also two- and three-dimensional many-body dynamics. To directly compare model predictions with experimental images, the single-shot-imaging process is also simulated. To face the key challenge of what one can really know about quantum many-body states of ultracold atoms, we develop statistical analysis and machine learning algorithms that optimally harness the information about squeezing and correlations in the single-shot images. The project core team consists of the PI, Axel Lode, a specialist on the Schrödinger equation and the MCTDH-(B/F) method, one PostDoc, that will work on the generation and analysis of single-shot images with a focus on correlations and on squeezing, and one PhD student that will apply the developed numerical tools in direct collaboration with the experimental physics project partners at the AtomInstitut, TU Wien. The core team is embedded and supported by the WPI applied math and experimental quantum physics experts N. J. Mauser, J. Schmiedmayer, T. Schumm, all WPI full members via START, Wittgenstein, ERC awards, etc..    Austrian Science Fund FWF    01 Stand-Alone Projects    389174.63EUR
263    Dr Brierley Liam    University of Liverpool    2019-10-01    2022-09-30    Ecology or genetics? Adapting machine learning approaches to understand determinants of cross-species transmission and virulence in RNA viruses    Numerical models for many-body physics and single-shot images Scientific Abstract: We develop and apply improved simulation tools and numerical techniques for the time-dependent many-body Schrödinger equation for describing dynamical quantum systems used in state-of-the art experiments with ultracold atoms like the ones in the groups at the AtomInstitut, TU Wien. Our key goal is to investigate what one can really know about quantum many-body states of ultracold atoms. We go beyond models with a single Gross-Pitaevskii equation which cannot capture the phenomena in our focus like correlations and squeezing, that are observed in current experiments. A fundamental obstacle is to extract the information about correlations and squeezing from the experiment. In many experimental setups, the observations consist in so-called "single-shot images" of the atomic clouds. Theoretically, single-shot images represent a projective measurement of the many-body wavefunction. The current strategy necessitates a very large number of single-shot images to be produced and analyzed. We plan to use and improve a sophisticated numerical tool, the MCTDH-(B/F), that allows to describe correlations and squeezing. We aim to enable a direct and predictive modeling. We will implement an efficient MCTDH-(B/F) software and apply it for the simulation of also two- and three-dimensional many-body dynamics. To directly compare model predictions with experimental images, the single-shot-imaging process is also simulated. To face the key challenge of what one can really know about quantum many-body states of ultracold atoms, we develop statistical analysis and machine learning algorithms that optimally harness the information about squeezing and correlations in the single-shot images. The project core team consists of the PI, Axel Lode, a specialist on the Schrödinger equation and the MCTDH-(B/F) method, one PostDoc, that will work on the generation and analysis of single-shot images with a focus on correlations and on squeezing, and one PhD student that will apply the developed numerical tools in direct collaboration with the experimental physics project partners at the AtomInstitut, TU Wien. The core team is embedded and supported by the WPI applied math and experimental quantum physics experts N. J. Mauser, J. Schmiedmayer, T. Schumm, all WPI full members via START, Wittgenstein, ERC awards, etc..    Medical Research Council    Fellowship    235657.0GBP
264    Dr Brierley Liam    University of Liverpool    2019-10-01    2022-09-30    Ecology or genetics? Adapting machine learning approaches to understand determinants of cross-species transmission and virulence in RNA viruses    Emerging infectious diseases remain a prominent threat to global health, e.g., Ebola virus, Zika virus. In 2015, the WHO designated 'Disease X' to indicate the serious potential of previously unknown emerging pathogens to cause public health crises. Though zoonotic RNA viruses are known to present higher risks of emergence, detailed determinants of cross-species transmission remain unclear. Zoonotic viruses also vary widely in their capability to cause severe disease. To predict public health impacts of 'Disease X', a better understanding of which traits drive this variation in infectivity and virulence is urgently needed. Whilst previous approaches have focused on ecological predictors, these traditional frameworks have been unable to capture the information within increasingly available RNA virus sequences. This research aims to capitalise upon the potential power within large genetic data resources and quantify comparative influences of genetic versus ecological traits of RNA viruses and hosts upon cross-species transmission dynamics. To fully integrate novel, high-dimensional genetic data, new analytical approaches are needed. I will apply machine learning as a state-of-the-art statistical methodology, comparing several advanced approaches, e.g. gradient boosting, a method of gradual model learning which outperforms traditional methods. Models will span all known mammal and avian RNA viruses (22 families) using the exceptional breadth of EID2, a large, host-virus infectivity dataset. This project will additionally develop further text-mining tools to capture and integrate virulence data within EID2. The proposed models will allow tests of evolutionary theory across a range of RNA viruses. Quantified model outputs will contribute to public health risk assessments by informing prioritisation for novel viruses and advancing frameworks for emergence predictions, moving towards a 'smarter', empirically-driven strategy to prevent future disease burden.    UK Research and Innovation    Fellowship    235657.0GBP
265    Dr Brierley Liam    London School of Economics    2019-10-01    2022-09-30    COVID-19: Outreach to Domestic Abuse Victims in Times of Quarantine    Emerging infectious diseases remain a prominent threat to global health, e.g., Ebola virus, Zika virus. In 2015, the WHO designated 'Disease X' to indicate the serious potential of previously unknown emerging pathogens to cause public health crises. Though zoonotic RNA viruses are known to present higher risks of emergence, detailed determinants of cross-species transmission remain unclear. Zoonotic viruses also vary widely in their capability to cause severe disease. To predict public health impacts of 'Disease X', a better understanding of which traits drive this variation in infectivity and virulence is urgently needed. Whilst previous approaches have focused on ecological predictors, these traditional frameworks have been unable to capture the information within increasingly available RNA virus sequences. This research aims to capitalise upon the potential power within large genetic data resources and quantify comparative influences of genetic versus ecological traits of RNA viruses and hosts upon cross-species transmission dynamics. To fully integrate novel, high-dimensional genetic data, new analytical approaches are needed. I will apply machine learning as a state-of-the-art statistical methodology, comparing several advanced approaches, e.g. gradient boosting, a method of gradual model learning which outperforms traditional methods. Models will span all known mammal and avian RNA viruses (22 families) using the exceptional breadth of EID2, a large, host-virus infectivity dataset. This project will additionally develop further text-mining tools to capture and integrate virulence data within EID2. The proposed models will allow tests of evolutionary theory across a range of RNA viruses. Quantified model outputs will contribute to public health risk assessments by informing prioritisation for novel viruses and advancing frameworks for emergence predictions, moving towards a 'smarter', empirically-driven strategy to prevent future disease burden.    UK Research and Innovation    Fellowship    235657.0GBP
266    Dr Wang Dennis    University of Sheffield    2019-03-01    2021-08-31    Evidencing subtypes of disorders with comorbidities through a consensus of clinical and multi-omic traits    Emerging infectious diseases remain a prominent threat to global health, e.g., Ebola virus, Zika virus. In 2015, the WHO designated 'Disease X' to indicate the serious potential of previously unknown emerging pathogens to cause public health crises. Though zoonotic RNA viruses are known to present higher risks of emergence, detailed determinants of cross-species transmission remain unclear. Zoonotic viruses also vary widely in their capability to cause severe disease. To predict public health impacts of 'Disease X', a better understanding of which traits drive this variation in infectivity and virulence is urgently needed. Whilst previous approaches have focused on ecological predictors, these traditional frameworks have been unable to capture the information within increasingly available RNA virus sequences. This research aims to capitalise upon the potential power within large genetic data resources and quantify comparative influences of genetic versus ecological traits of RNA viruses and hosts upon cross-species transmission dynamics. To fully integrate novel, high-dimensional genetic data, new analytical approaches are needed. I will apply machine learning as a state-of-the-art statistical methodology, comparing several advanced approaches, e.g. gradient boosting, a method of gradual model learning which outperforms traditional methods. Models will span all known mammal and avian RNA viruses (22 families) using the exceptional breadth of EID2, a large, host-virus infectivity dataset. This project will additionally develop further text-mining tools to capture and integrate virulence data within EID2. The proposed models will allow tests of evolutionary theory across a range of RNA viruses. Quantified model outputs will contribute to public health risk assessments by informing prioritisation for novel viruses and advancing frameworks for emergence predictions, moving towards a 'smarter', empirically-driven strategy to prevent future disease burden.    The Academy of Medical Sciences    Springboard Round 4    99411.0GBP
267    Dr Charlton Peter    Cambridge, University of    2019-03-01    2021-08-31    Using clinical and consumer devices to enhance screening for atrial fibrillation    Emerging infectious diseases remain a prominent threat to global health, e.g., Ebola virus, Zika virus. In 2015, the WHO designated 'Disease X' to indicate the serious potential of previously unknown emerging pathogens to cause public health crises. Though zoonotic RNA viruses are known to present higher risks of emergence, detailed determinants of cross-species transmission remain unclear. Zoonotic viruses also vary widely in their capability to cause severe disease. To predict public health impacts of 'Disease X', a better understanding of which traits drive this variation in infectivity and virulence is urgently needed. Whilst previous approaches have focused on ecological predictors, these traditional frameworks have been unable to capture the information within increasingly available RNA virus sequences. This research aims to capitalise upon the potential power within large genetic data resources and quantify comparative influences of genetic versus ecological traits of RNA viruses and hosts upon cross-species transmission dynamics. To fully integrate novel, high-dimensional genetic data, new analytical approaches are needed. I will apply machine learning as a state-of-the-art statistical methodology, comparing several advanced approaches, e.g. gradient boosting, a method of gradual model learning which outperforms traditional methods. Models will span all known mammal and avian RNA viruses (22 families) using the exceptional breadth of EID2, a large, host-virus infectivity dataset. This project will additionally develop further text-mining tools to capture and integrate virulence data within EID2. The proposed models will allow tests of evolutionary theory across a range of RNA viruses. Quantified model outputs will contribute to public health risk assessments by informing prioritisation for novel viruses and advancing frameworks for emergence predictions, moving towards a 'smarter', empirically-driven strategy to prevent future disease burden.    British Heart Foundation    Springboard Round 4    251475.0GBP
268    Dr Charlton Peter    U-FLOOR TECHNOLOGIES LTD    2019-03-01    2021-08-31    OptimisAir - Air quality control combined with behaviourial science    Occupants' daily activities make up 35% of the factors leading to indoor air-pollution \[UCL,2018\]. In the face of COVID-19 we are expecting people to spend much more time in their homes which has two serious knock-on effects: 1) occupants experience longer exposure to indoor pollutants, increasing the risk of respiratory illnesses; 2) Poor IAQ is linked to COPD, asthma, stroke & heart diseases -- all of which are underlying health conditions, increasing the risk of severe illness from covid-19, putting further pressure on the NHS. To address this challenge we are now developing OptimisAir: an integrated 'Indoor air quality management system' that helps Registered Social Landlords reducing their maintenance costs through automated airflow control combined with AI-based activity-recognition and nudge-techniques to reduce the root cause of poor indoor air quality. It uses IoT-enabled sensors to monitor indoor pollutants and utilises game-changing technologies (machine-learning, activity-recognition, 'nudge'/behavioural sciences) to tackle an age-old problem: poor indoor air quality, dampness and draughts in homes. The outcome of the project creates a novel, patented system at an extremely competitive cost.    UK Research and Innovation    Springboard Round 4    49703.0GBP
269    Dr Charlton Peter    ELECTRONIC MEDIA SERVICES LIMITED    2019-03-01    2021-08-31    Conquering COVID in Construction – a safe, managed return to site for construction workers    The IHS Markit purchasing managers' index for UK construction dropped to 39.3 last month from 52.6 the previous month, the lowest reading in more than 10 years. The UK construction sector employs 2.4m workers many of whom are self-employed and are unable to benefit from the Treasury's Furlough Scheme. The sector is a significant contributor to the UK's economic activity producing about 6 per cent of the country's total economic output. The vision for the project is an easy to use health check and tracking app that will give the worker and their employer a simple red/green check of their ability to work. This would enable the industry to end the lockdown, re-engage the predominately self-employed workforce and restart economic activity in a significant sector. The red/green advice from the app would be based on: 1\. Daily self-declared monitoring of general health and especially of any symptoms specific to COVID-19\. These declarations will be performed even when the user is self-isolating. 2\. A tracking feature that would record the user's location while at work and who else they have been in contact with (e.g. < 5 metres separation). 3\. Alerting when there are too many people too close together. 4\. Occupancy of any welfare units, so they can self-time their breaks to minimise unnecessary contacts. 5\. Alerting of the user when a co-worker who they have been in contact with is developing symptoms or who has tested positive for COVID-19\. 6\. Machine Learning-based algorithm to track the development of symptoms in the workforce. The app would have a dashboard for the employer showing who is currently on-site, real-time hot spots showing where workers are congregating so they can be instructed to disperse, a list of staff who have reported symptoms or have tested positive, a list of staff who should be self-isolating because they may have come into contact with another infected member of staff, potential return dates for self-isolating staff plus a list of staff who have tested positive for antibodies and may be immune to re-infection. Both China and South Korea have demonstrated the benefit of using tracking and contact tracing to reduce the spread of COVID-19\. Therefore, the proposed system can be part of an industry-led approach that will help the global construction sector to safely return to work.    UK Research and Innovation    Springboard Round 4    49882.0GBP
270    Dr Charlton Peter    DEEP RENDER LTD    2019-03-01    2021-08-31    AI-based Image-As-Video Streaming    Deep Render Ltd is a London based deep-tech start-up developing the next generation of AI-based media compression algorithms. Our proprietary and patented technology is at the forefront of machine learning research. Deep Render is combining the fields of artificial intelligence, statistics and information theory to unlock the fundamental limits of image and video compression: The human eye is the best data compressor known to humanity - with compression ratios at least 2,000 times better than everything developed to date. Our Biological Compression approach approximates the neurological processes of the human eye through a non-linear, learning-based approach, thereby creating a novel class of highly efficient compression algorithms. We are world leaders in this domain, and our unique, AI-based image compression technology is already providing a 75% efficiency gain over the best previous compression standards. As global data consumption is growing exponentially with more than 80% of traffic being Image/Video, Deep Render's AI-based compression technology is vital to bypass broadband constraints. The outbreak of COVID-19 has now accelerated this growth, as a result of the crisis, internet usage has increased significantly. In particular, the demand for live-streaming and video-chat services. Therefore we want to apply our already working image compression codec to live-video streaming. The outcome of the project is to extend our image compression codec to an image-as-video live-streaming codec, at least 75% more efficient than the current state-of-the-art. Our target customers are the live video chat services (Zoom, Skype, Microsoft Teams), as well as the entertainment industry, including live streaming platforms (Twitch, Facebook, Instagram, YouTube). Our value proposition is easy to understand. By making file sizes 75% smaller, we directly increase the bandwidth supply of the internet for live-streaming by a factor of 4\. Increasing the bandwidth supply by making file sizes smaller, is magnitudes more value- and time-efficient than increasing the bandwidth supply through rewiring the globe with progressively more fibre-cables. Deep Render is going to help create a new age in which bandwidth constraints are a problem of the past. As a result of COVID 19 solving this problem has gained more importance and Deep Render is determined to create a fast solution.    UK Research and Innovation    Springboard Round 4    50000.0GBP
271    Dr Charlton Peter    V2G EVSE LIMITED    2019-03-01    2021-08-31    Covid-19 eHealth Data Acquisition Unit (COVeHealth)    The UK is currently in "lock down" due to the novel coronavirus pandemic. Before putting the currently locked down country back to work we need to: 1) Reduce the incidence of cases to reduce the burden on the NHS to more normal levels 2) Once that is achieved, we need to prevent the virus from catching fire a second time To reduce the risk of a second wave, we need to develop tools to enable us to: a) Preemptively identify early stage Covid-19 sufferers b) Use these tools to guard the borders of spaces (hospitals, homes, shops, workplaces, shopping malls, schools, universities etc.) The World Health Organization's message is that we must **"Find, isolate, test and treat every case to break the chains of Covid transmission."** Covid-19 screening tests are currently performed at a modest distance by a health worker using a "no touch" infrared thermometer to detect the tell tale fever. The other common symptom indicative of Covid-19 is the persistent dry cough, detected by the characteristic sound. What is desperately needed, both in the UK and around the World, is a way of performing screening for early stage Covid-19 symptoms remotely. With DfT seed funding V2G EVSE have developed a low cost "smart" 0G to 5G enabled communications controller with a wide range of input/output, currently configured to monitor and control an electric vehicle charging station and securely communicate with a cloud based management system. We will repurpose our existing technology by connecting the controller to a microphone and infra-red camera, then use novel machine learning algorithms to detect the characteristic cough and fever of Covid-19 sufferers. This will allow us to create an installed or hand held device that can be used to identify those with potential Covid-19 symptoms. Since our existing hardware is effectively the internals of a smartphone with no touch screen but more versatile communications, including Bluetooth, we are also ideally placed to participate in the recently announced Apple/Google track/trace initiative. The COVeHealth project will develop proof of concept prototypes of a "commercial" version suitable for use at the entrance to public spaces and an alternative low-cost "domestic" version for use hand held or in confined spaces.    UK Research and Innovation    Springboard Round 4    50000.0GBP
272    Univ.Prof. Dr. MAASS Wolfgang    Graz University of Technology    2003-10-07    2006-11-06    Computer Models for Biological Vision Systems    The UK is currently in "lock down" due to the novel coronavirus pandemic. Before putting the currently locked down country back to work we need to: 1) Reduce the incidence of cases to reduce the burden on the NHS to more normal levels 2) Once that is achieved, we need to prevent the virus from catching fire a second time To reduce the risk of a second wave, we need to develop tools to enable us to: a) Preemptively identify early stage Covid-19 sufferers b) Use these tools to guard the borders of spaces (hospitals, homes, shops, workplaces, shopping malls, schools, universities etc.) The World Health Organization's message is that we must **"Find, isolate, test and treat every case to break the chains of Covid transmission."** Covid-19 screening tests are currently performed at a modest distance by a health worker using a "no touch" infrared thermometer to detect the tell tale fever. The other common symptom indicative of Covid-19 is the persistent dry cough, detected by the characteristic sound. What is desperately needed, both in the UK and around the World, is a way of performing screening for early stage Covid-19 symptoms remotely. With DfT seed funding V2G EVSE have developed a low cost "smart" 0G to 5G enabled communications controller with a wide range of input/output, currently configured to monitor and control an electric vehicle charging station and securely communicate with a cloud based management system. We will repurpose our existing technology by connecting the controller to a microphone and infra-red camera, then use novel machine learning algorithms to detect the characteristic cough and fever of Covid-19 sufferers. This will allow us to create an installed or hand held device that can be used to identify those with potential Covid-19 symptoms. Since our existing hardware is effectively the internals of a smartphone with no touch screen but more versatile communications, including Bluetooth, we are also ideally placed to participate in the recently announced Apple/Google track/trace initiative. The COVeHealth project will develop proof of concept prototypes of a "commercial" version suitable for use at the entrance to public spaces and an alternative low-cost "domestic" version for use hand held or in confined spaces.    Austrian Science Fund FWF    National Research Networks NFN    50000.0GBP
273    Univ.Prof. Dr. SCHMUTH Matthias    Medical University of Innsbruck    2019-05-01    2022-04-30    Biomolecular Analyses for Tailored Medicine in AcneiNversa (BATMAN)    Acne Inversa (AI) is a chronic inflammatory disease involving hair follicles that imposes a major physical and psychological burden on patients with significant costs for health systems. Genetic variants affecting different pathways result in wide spectrum of AI phenotypes and tracking gene variants is essential to design personalized treatments. The proposal aims to bring together medical, genetic, experimental and lifestyle data to create holistic health records (HHR), which will allow us to build a personalized model of each patient and to tailor specific treatments based on their personal characteristics. Research on animal or cellular models will be harnessed to validate hypotheses on genetic variants, generating useful information with immediate translational impact on patient stratification and therapeutic options, and also providing a wide-scale overview of previously identified and novel risk markers. DNA will be obtained from AI cases from 3 different locations in Europe. Data will be compiled from whole exome sequencing, whole genome genotyping SNPs arrays and transcriptomic signatures of hair follicle cells and novel mouse models. Genomic information will be merged with clinical evaluations and lifestyle data by advanced machine-learning and data mining algorithms. By the end of the project, our consortium intends to: • identify genetic variants associated with AI susceptibility, severity and response to treatment • design in vivo and in vitro models for investigations on the main biological pathways affected by AI and testing the impact of genetic variants on immune and cutaneous cell biology • produce an HHR to complement medical record by developing a smartphone application to remotely monitor the physical and psychological wellbeing of patients and advise them on physical activity and dietary and smoking habits • propose novel stratification methods that clinicians can use to assess severity, choose the therapy and follow the outcome    Austrian Science Fund FWF    02 International programmes    91230.13EUR
274    Dr Clarke Emily    University of Leeds    2019-11-01    2023-07-31    Enhanced phenotyping of melanoma whole slide images with artificial intelligence    Acne Inversa (AI) is a chronic inflammatory disease involving hair follicles that imposes a major physical and psychological burden on patients with significant costs for health systems. Genetic variants affecting different pathways result in wide spectrum of AI phenotypes and tracking gene variants is essential to design personalized treatments. The proposal aims to bring together medical, genetic, experimental and lifestyle data to create holistic health records (HHR), which will allow us to build a personalized model of each patient and to tailor specific treatments based on their personal characteristics. Research on animal or cellular models will be harnessed to validate hypotheses on genetic variants, generating useful information with immediate translational impact on patient stratification and therapeutic options, and also providing a wide-scale overview of previously identified and novel risk markers. DNA will be obtained from AI cases from 3 different locations in Europe. Data will be compiled from whole exome sequencing, whole genome genotyping SNPs arrays and transcriptomic signatures of hair follicle cells and novel mouse models. Genomic information will be merged with clinical evaluations and lifestyle data by advanced machine-learning and data mining algorithms. By the end of the project, our consortium intends to: • identify genetic variants associated with AI susceptibility, severity and response to treatment • design in vivo and in vitro models for investigations on the main biological pathways affected by AI and testing the impact of genetic variants on immune and cutaneous cell biology • produce an HHR to complement medical record by developing a smartphone application to remotely monitor the physical and psychological wellbeing of patients and advise them on physical activity and dietary and smoking habits • propose novel stratification methods that clinicians can use to assess severity, choose the therapy and follow the outcome    Medical Research Council    Fellowship    245899.0GBP
275    Professor Counsell Serena    King's College London    2014-09-01    2019-12-31    In vivo microstructural neuroimaging in infants at risk of developing neurocognitive delay or neurobehavioural disorders    None    Medical Research Council    Research Grant    814464.0GBP
276    Dr Brown Benjamin    University of Manchester    2019-01-01    2023-01-01    Actionable Analytics Linking Patient, Practitioner and Population Primary Care    None    Wellcome Trust    Clinical Research Career Development Fellowship    858035.0GBP
277    Prof de Lusignan Simon    University of Surrey    2018-10-01    2021-03-31    Royal College of General Practitioners (RCGP), Research and Surveillance Centre (RSC) quinquagenarian (QQG) practice network. Creating a longitudinal linked sentinel database of 50 years clinical and virology data and prospective research platform.    None    Wellcome Trust    Biomedical Resources Grant    786564.0GBP
278    Dr Nellaker Christoffer    University of Oxford    2016-06-01    2019-07-30    Automated phenotyping to accurately infer functional variants in clinical genetics    None    Medical Research Council    Research Grant    319522.0GBP
279    Dr Chikowore Tinashe    University of the Witwatersrand    2019-02-01    2022-02-01    Characterisation of gene-lifestyle interactions associated with obesity-related traits in African populations    None    Wellcome Trust    International Training Fellowship - Full    224233.0GBP
280    Dr Chikowore Tinashe    University of Exeter    2019-02-01    2022-02-01    COVID-19 (Mis)Information Exposure and Messaging Effects in the United Kingdom    Fighting the COVID-19 pandemic requires understanding what information people have about the disease, which misperceptions might be prevalent, and how officials can improve public knowledge and encourage behaviours that will protect public health. First, this study will measure beliefs and attitudes about COVID-19, providing a thorough map of accurate knowledge, but also of misperceptions, particularly those driven by conspiratorial thinking. Second, the study will catalogue the online sources from which our respondents get COVID-19-related information. This will allow us to gauge which accurate information sources reach a wide audience and which sources of online misinformation are systematically distorting people's views. Third, the study will test whether public health messages that seek to correct misinformation are actually effective in changing people's beliefs. Behavioural and survey data will be collected in a multi-wave nationally representative survey that measures both prevalence of false and accurate beliefs about COVID-19 (including beliefs in misperceptions and conspiracy theories) and support for recommendations from public health authorities. To evaluate responses to information from public health officials, the second survey wave will include a randomized experiment evaluating the effects of messaging from health and medical authorities. Finally, the study will measure the quality of the information people consume online about the pandemic by analysing the behavioural data provided by respondents using a combination of human-coded and machine learning approaches. These data will make it possible to identify which groups are most frequently exposed to inaccurate or untrustworthy information about COVID-19, which should aid in the design of effective interventions.    UK Research and Innovation    International Training Fellowship - Full    308109.0GBP
281    Professor Loram Ian    Manchester Metropolitan University    2020-01-01    2022-12-31    Quantification of head and trunk control for children with neuromotor and neuromuscular disorders    Fighting the COVID-19 pandemic requires understanding what information people have about the disease, which misperceptions might be prevalent, and how officials can improve public knowledge and encourage behaviours that will protect public health. First, this study will measure beliefs and attitudes about COVID-19, providing a thorough map of accurate knowledge, but also of misperceptions, particularly those driven by conspiratorial thinking. Second, the study will catalogue the online sources from which our respondents get COVID-19-related information. This will allow us to gauge which accurate information sources reach a wide audience and which sources of online misinformation are systematically distorting people's views. Third, the study will test whether public health messages that seek to correct misinformation are actually effective in changing people's beliefs. Behavioural and survey data will be collected in a multi-wave nationally representative survey that measures both prevalence of false and accurate beliefs about COVID-19 (including beliefs in misperceptions and conspiracy theories) and support for recommendations from public health authorities. To evaluate responses to information from public health officials, the second survey wave will include a randomized experiment evaluating the effects of messaging from health and medical authorities. Finally, the study will measure the quality of the information people consume online about the pandemic by analysing the behavioural data provided by respondents using a combination of human-coded and machine learning approaches. These data will make it possible to identify which groups are most frequently exposed to inaccurate or untrustworthy information about COVID-19, which should aid in the design of effective interventions.    Medical Research Council    Research Grant    774510.0GBP
282    Professor Loram Ian    QUEEN MARY UNIVERSITY OF LONDON    2019-07-01    2020-06-30    Implementation of a 3D computational mouse atlas for detection of pancreatic tumours in transgenic mice    High impact cancer research demands the use of clinically relevant disease models. One approach is to use genetically modified mouse models (GEMM's) and this has seen an increase in the number of animals bred for scientific research, a high proportion of which are wasted as they don't have the correct phenotype. Tumour development in GEMMs is difficult to evaluate as their disease develops over long periods of time, usually deep within the body and so tumour detection and measurement of response is challenging unless the animal is sacrificed. The mouse pancreas is an ill-defined organ and in GEMMs of pancreatic cancer, the development of tumour is extremely difficult to assess non-invasively. Magnetic resonance imaging (MRI) and ultrasound (US) are both useful but their potential to reduce animal numbers by providing early detection of tumour and accurate longitudinal imaging data is limited for a number of reasons. MRI is preferred but can be expensive, less widely available and accurate determination of tumour volume via image analysis of the abdominal area is challenging. However, we have developed a 3D computational mouse atlas (3D-CAMMP) that can automatically detect pancreas and pancreatic tumour (with an accuracy of 95%) in MR images of a commonly-used pancreatic GEMM (the KPC mouse). This machine learning model uses imaging data collected on a relatively inexpensive low field MRI instrument. Automatic detection allows a non-expert user to perform MRI imaging and analysis with minimal training. In this project we will transfer use of the 3D-CAMMP mouse atlas to three cancer institutes in the UK, all with high numbers of KPC mice. We are confident that early and accurate identification of tumours will allow optimised evaluation of potential treatments to reduce variability, experiment duration and ultimately have the effect of reducing numbers of required animals used in pancreatic cancer research.    National Centre for the Replacement, Refinement and Reduction of Animals in Research    Skills and Knowledge Transfer    75593.0GBP
283    Dr Cangiani Andrea    University of Nottingham    2019-12-31    2021-12-30    LOng-Term anatomical fluid dynamics for new Univentricular heartS palliation (LOTUS)    High impact cancer research demands the use of clinically relevant disease models. One approach is to use genetically modified mouse models (GEMM's) and this has seen an increase in the number of animals bred for scientific research, a high proportion of which are wasted as they don't have the correct phenotype. Tumour development in GEMMs is difficult to evaluate as their disease develops over long periods of time, usually deep within the body and so tumour detection and measurement of response is challenging unless the animal is sacrificed. The mouse pancreas is an ill-defined organ and in GEMMs of pancreatic cancer, the development of tumour is extremely difficult to assess non-invasively. Magnetic resonance imaging (MRI) and ultrasound (US) are both useful but their potential to reduce animal numbers by providing early detection of tumour and accurate longitudinal imaging data is limited for a number of reasons. MRI is preferred but can be expensive, less widely available and accurate determination of tumour volume via image analysis of the abdominal area is challenging. However, we have developed a 3D computational mouse atlas (3D-CAMMP) that can automatically detect pancreas and pancreatic tumour (with an accuracy of 95%) in MR images of a commonly-used pancreatic GEMM (the KPC mouse). This machine learning model uses imaging data collected on a relatively inexpensive low field MRI instrument. Automatic detection allows a non-expert user to perform MRI imaging and analysis with minimal training. In this project we will transfer use of the 3D-CAMMP mouse atlas to three cancer institutes in the UK, all with high numbers of KPC mice. We are confident that early and accurate identification of tumours will allow optimised evaluation of potential treatments to reduce variability, experiment duration and ultimately have the effect of reducing numbers of required animals used in pancreatic cancer research.    Medical Research Council    Research Grant    322383.0GBP
284    Dr Laine Romain    University College London    2019-12-28    2022-12-27    How does virus shape relate to infectivity?    High impact cancer research demands the use of clinically relevant disease models. One approach is to use genetically modified mouse models (GEMM's) and this has seen an increase in the number of animals bred for scientific research, a high proportion of which are wasted as they don't have the correct phenotype. Tumour development in GEMMs is difficult to evaluate as their disease develops over long periods of time, usually deep within the body and so tumour detection and measurement of response is challenging unless the animal is sacrificed. The mouse pancreas is an ill-defined organ and in GEMMs of pancreatic cancer, the development of tumour is extremely difficult to assess non-invasively. Magnetic resonance imaging (MRI) and ultrasound (US) are both useful but their potential to reduce animal numbers by providing early detection of tumour and accurate longitudinal imaging data is limited for a number of reasons. MRI is preferred but can be expensive, less widely available and accurate determination of tumour volume via image analysis of the abdominal area is challenging. However, we have developed a 3D computational mouse atlas (3D-CAMMP) that can automatically detect pancreas and pancreatic tumour (with an accuracy of 95%) in MR images of a commonly-used pancreatic GEMM (the KPC mouse). This machine learning model uses imaging data collected on a relatively inexpensive low field MRI instrument. Automatic detection allows a non-expert user to perform MRI imaging and analysis with minimal training. In this project we will transfer use of the 3D-CAMMP mouse atlas to three cancer institutes in the UK, all with high numbers of KPC mice. We are confident that early and accurate identification of tumours will allow optimised evaluation of potential treatments to reduce variability, experiment duration and ultimately have the effect of reducing numbers of required animals used in pancreatic cancer research.    Medical Research Council    Fellowship    301830.0GBP
285    Mr Harrison Conrad    University of Oxford    2019-12-28    2022-12-27    Transforming outcome measures in plastic surgery through computer science    High impact cancer research demands the use of clinically relevant disease models. One approach is to use genetically modified mouse models (GEMM's) and this has seen an increase in the number of animals bred for scientific research, a high proportion of which are wasted as they don't have the correct phenotype. Tumour development in GEMMs is difficult to evaluate as their disease develops over long periods of time, usually deep within the body and so tumour detection and measurement of response is challenging unless the animal is sacrificed. The mouse pancreas is an ill-defined organ and in GEMMs of pancreatic cancer, the development of tumour is extremely difficult to assess non-invasively. Magnetic resonance imaging (MRI) and ultrasound (US) are both useful but their potential to reduce animal numbers by providing early detection of tumour and accurate longitudinal imaging data is limited for a number of reasons. MRI is preferred but can be expensive, less widely available and accurate determination of tumour volume via image analysis of the abdominal area is challenging. However, we have developed a 3D computational mouse atlas (3D-CAMMP) that can automatically detect pancreas and pancreatic tumour (with an accuracy of 95%) in MR images of a commonly-used pancreatic GEMM (the KPC mouse). This machine learning model uses imaging data collected on a relatively inexpensive low field MRI instrument. Automatic detection allows a non-expert user to perform MRI imaging and analysis with minimal training. In this project we will transfer use of the 3D-CAMMP mouse atlas to three cancer institutes in the UK, all with high numbers of KPC mice. We are confident that early and accurate identification of tumours will allow optimised evaluation of potential treatments to reduce variability, experiment duration and ultimately have the effect of reducing numbers of required animals used in pancreatic cancer research.    National Institute for Health Research (Department of Health)    Fellowship    384813.0GBP
286    Dr. LUGHOFER Edwin    University of Linz    2020-03-01    2023-02-28    Interactive Machine Learning with Evolving Fuzzy Systems    High impact cancer research demands the use of clinically relevant disease models. One approach is to use genetically modified mouse models (GEMM's) and this has seen an increase in the number of animals bred for scientific research, a high proportion of which are wasted as they don't have the correct phenotype. Tumour development in GEMMs is difficult to evaluate as their disease develops over long periods of time, usually deep within the body and so tumour detection and measurement of response is challenging unless the animal is sacrificed. The mouse pancreas is an ill-defined organ and in GEMMs of pancreatic cancer, the development of tumour is extremely difficult to assess non-invasively. Magnetic resonance imaging (MRI) and ultrasound (US) are both useful but their potential to reduce animal numbers by providing early detection of tumour and accurate longitudinal imaging data is limited for a number of reasons. MRI is preferred but can be expensive, less widely available and accurate determination of tumour volume via image analysis of the abdominal area is challenging. However, we have developed a 3D computational mouse atlas (3D-CAMMP) that can automatically detect pancreas and pancreatic tumour (with an accuracy of 95%) in MR images of a commonly-used pancreatic GEMM (the KPC mouse). This machine learning model uses imaging data collected on a relatively inexpensive low field MRI instrument. Automatic detection allows a non-expert user to perform MRI imaging and analysis with minimal training. In this project we will transfer use of the 3D-CAMMP mouse atlas to three cancer institutes in the UK, all with high numbers of KPC mice. We are confident that early and accurate identification of tumours will allow optimised evaluation of potential treatments to reduce variability, experiment duration and ultimately have the effect of reducing numbers of required animals used in pancreatic cancer research.    Austrian Science Fund FWF    Stand-Alone Projects    409109.4EUR
287    Dr. LUGHOFER Edwin    Arizona State University    2020-04-15    2021-03-31    Collaborative Research: RAPID: RTEM: Rapid Testing as Multi-fidelity Data Collection for Epidemic Modeling    Biological Sciences - The novel coronavirus (COVID-19) epidemic is generating significant social, economic, and health impacts and has highlighted the importance of real-time analysis of the spatio-temporal dynamics of emerging infectious diseases. COVID-19, which emerged out of the city of Wuhan in China in December 2019 is now spreading in multiple countries. It is particularly concerning that the case fatality rate appears to be higher for the novel coronavirus than for seasonal influenza, and especially so for older populations and those with prior health conditions such as cardiovascular disease and diabetes. Any plan for stopping the epidemic must be based on a quantitative understanding of the proportion of the at-risk population that needs to be protected by effective control measures in order for transmission to decline sufficiently and quickly enough for the epidemic to end. Different data collection and testing modalities and strategies available to help calibrate transmission models and predict the spread/severity of a disease, have variable costs, response times, and accuracies. In this Rapid Response Research (RAPID) project, the team will examine the problem of establishing optimal practices for rapid testing for the novel coronavirus. The result will be the Rapid Testing for Epidemic Modeling (RTEM), which will translate into science-based predictions of the COVID-19 epidemic's characteristics, including the duration and overall size, and help the global efforts to combat the disease. The RTEM will fill an important gap in data-driven decision making during the COVID-19 epidemic and, thus, will enable services with significant national economic and health impact. The educational impact of the project will be on mentoring of post-doctoral and PhD researchers and on curricula by incorporating research challenges and outcomes into existing undergraduate and graduate classes. <br/><br/>Computational models for the spatio-temporal dynamics of emerging infectious diseases and data- and model-driven computer simulations for disease spreading are increasingly critical in predicting geo-temporal evolution of epidemics as well as designing, activating, and adapting practices for controlling epidemics. In this project, the researchers tackle a Rapid Testing for Epidemic Modeling (RTEM) problem: Given a partially known target disease model and a set of testing modalities (from surveys to surveillance testing at known disease hotspots), with varying costs, accuracies, and observational delays, what is the best rapid testing strategy that would help recover the underlying disease model? Several scientific questions arise: What is the value of testing? Should only sick people be tested for virus detection? What level of resources should be devoted to the development of highly accurate tests (low false positives, low false negatives)? Is it better to use only one type of test aiming at the best cost/effectiveness trade off, or a non-homogeneous testing policy? Naturally these questions need to be investigated at the interface of epidemiology, computer science, machine learning, mathematical modeling and statistics. As part of the work, the team will develop a model of transmission dynamics and control, tailored to COVID-19 in a way that accommodates diagnostic testing with varying fidelities and delays underlying a rapid testing regimen. The investigators will further integrate the resulting RTEM-SEIR model with EpiDMS and DataStorm for executing continuous coupled simulations.<br/><br/>This project is jointly funded through the Ecology and Evolution of Infectious Diseases program (Division of Environmental Biology) and the Civil, Mechanical and Manufacturing innovation program (Engineering).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    122998.0USD
288    Dr. LUGHOFER Edwin    Florida International University    2020-05-01    2021-04-30    RAPID: #COVID-19: Understanding Community Response in the Emergence and Spread of Novel Coronavirus through Health Risk Communications in Socio-Technical Systems    Computer and Information Science and Engineering - Risk perception and risk averting behaviors of vulnerable communities in the emergence and spread of COVID-19 are spatio-temporal functions of individual or group interactions with their online social neighbors within or outside their communities and such interactions need to be captured through diverse information channels (e.g. traditional outlets such as radio, television, internet and/or non-traditional outlets such as social media). The primary goal of this Rapid Response Research (RAPID) project is to collect time-sensitive online social media and crowd-sourced data and analyze patterns of health-risk communication and community response in the emergence and spread of novel Coronavirus using data-driven methods and network science theories. The major focus will be towards understanding how individuals are socially influenced online, while communicating risk and interacting in their respective communities as the disease continues to spread. The notion of influence will be captured by quantifying the network effects on such communication behavior and characterizing how information is exchanged among people who are socially connected online and exposed to health risk in such outbreaks of disease. Given that communities responded to COVID-19 with limited or no preparation and there is uncertainty in the length of recovery for the communities already affected while new communities being threatened, the data collection effort requires rapid response for better coverage and careful monitoring. The data will include large-scale ephemeral online interactions of people in the affected communities and public officials who are involved in COVID-19 response, recovery, and mitigation efforts, followed by a data-driven network analytics and infographics of COVID-19 risk communication strategies and risk averting behaviors adopted. The proposed research will not only expand the knowledge base of spatio-temporal dynamics of risk perception and dissemination strategies in the emergence and aftermath of a major disease outbreak, but will also result in data-driven inference techniques to improve our understanding of how people express diverse concerns and how to harness and embed such information for designing intervention measures. The methodologies and findings of this rapid response research will benefit emergency management and public health agencies to define targeted information dissemination policies for public with diverse needs based on how people reacted to COVID-19 and their social network characteristics, activities, and interactions in response to similar public health hazards.<br/><br/>Public engagement in risk communication can lead to more effective decision-making and enhanced public feedback to the regulatory process. The primary goal of this RAPID project is to mine and analyze large-scale time-sensitive perishable crowd-sourced and social media data (rich spatio-temporal data) and reveal patterns of health-risk communication and community response in the emergence and spread of novel Coronavirus using data-driven methods and network science theories. The specific aims are threefold: (1) to document how public interact and communicate health risk information through their online social networks during a major disease outbreak; (2) to authenticate data from multiple sources and detect anomalies to avoid information overload and spread of misinformation; and (3) to examine how online social networks influence protective actions (e.g., social distancing, self-quarantine decisions) i.e. information cascades in health risk communication. To achieve the goal and aims, the project will utilize ephemeral time and geo-tagged social media interactions of users, agencies, news sources supplemented with crowd-sourced information on COVID-19. This study will have five theoretical and methodological contributions to the literature. It will: (1) advance our understanding of how individuals are socially influenced online, while communicating health risks and interacting in their respective communities as the disease continues to spread; (2) inform the literature on how information is exchanged among people who are socially connected online and exposed to health risk in such outbreaks of disease; (3) use novel machine-learning and network science models to quantify influence and network effects on such communication behavior; (4) capture the variability in network composition, risk communication strategies and risk averting behaviors adopted based on spatio-temporal correlations of risk and disease contagion; (5) ensure authenticity of the collected data from multiple sources and develop more accurate fully-distributed computational algorithms tailored to health risk anomaly detection in socio-technical systems. The findings from this research will be useful to public health and emergency management agencies for tailoring effective information dissemination policies for diverse user groups based on their social network characteristics, activities, and interactions in response to similar public health hazards. The methodologies, and implications of this research can be transferred in designing effective intervention policies to other natural and man-made disaster contexts in which public health risks become major concerns. The project will engage, mentor, and offer an innovative active learning environment for K-12, undergraduate, and graduate students by giving priority to disadvantaged and underrepresented communities in USA. The project will train students on computational skills required for collecting, storing, processing, analyzing and modeling large-scale data using high performance computational resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    79380.0USD
289    Dr. LUGHOFER Edwin    University of Pittsburgh    2020-05-01    2021-04-30    RAPID: Countering COVID-19 Misinformation via Situation-Aware Visually Informed Treatment    Computer and Information Science and Engineering - As the COVID-19 pandemic spreads, countries and cities around the globe have taken stringent measures including quarantine and regional lockdown. The increasing isolation, along with the panic and anxiety, creates challenges for countering misinformation--people are increasingly tapping into online information sources already familiar to them with declining chances of accessing alternative stories. This project will develop mechanisms based on text and image analysis, social psychology, and crowd-sourcing that can be used in a timely manner to counter misinformation during the ongoing COVID-19 crisis and beyond. One of the novel features of the approach is to deal with a specific instance of misinformation by crowd-sourcing authentic images that counter this misinformation. This research will contribute to the scientific understanding of misinformation and of persuasive narrative construction, to the assessment of risk for the spread of misinformation, and to the development of mechanisms to counter misinformation. <br/><br/>The technical aims of this project are divided into three thrusts. The first thrust will investigate what information content and which specific part of a multimodal social media post (e.g, a piece of text, text with an image, image with an embedded slogan) will receive stronger responses and hence increase the likelihood of the post being shared. The second thrust will create metrics to assess the likelihood of the spread of misinformation based on predictors learned from the content to which users are exposed. The third thrust will focus on the development of a system to counter misinformation based on citizen journalists? inputs of field investigations and on machine learning techniques. Finally, the system will be evaluated by survey studies and interviews to examine the system?s usability, usefulness, and effectiveness in reducing the spreading and impact of misinformation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    104491.0USD
290    Dr. LUGHOFER Edwin    George Washington University    2020-05-01    2021-04-30    RAPID: A novel platform for data integration and deep learning on COVID-19    Biological Sciences - The COVID-19 pandemic, caused by the SARS-CoV-2 virus, has fundamentally changed the world, and yet its ultimate impact is unknown. While China has experienced a slowdown in new cases, infections in the US continue to rise and are threatening to exceed our health care system?s capacity. Tests capacities are limited compared to the need, hospital services are becoming overwhelmed, and critical supplies are in shortage. There is a diversity of efforts currently ongoing to develop both new treatments as well as vaccine strategies to combat COVID-19. Yet, we know from experience, the virus will evolve solutions to both host immune systems and intervention strategies. In order to diminish both the short-term and long-term impacts of COVID-19, it is essential to develop robust, repeatable, and accessible tools to integrate and analyze the diversity of data becoming available in the face of the COVID-19 pandemic. The development of a platform to characterize the dynamic nature of mutations in the virus and testing for associations with clinical variables and biomarkers is an essential broader impact and will help in making informed predictions of health outcomes such as the stage of the severity of the disease and efficacy of treatment. Additionally, this project provides professional development opportunities for early career researchers.<br/><br/>Advances in omics technologies provide a broad and deep range of genotypic and phenotypic data to integrate with clinical phenotypes. Machine learning techniques such as clustering using phylogenetic distance and Deep Neural Networks (DNNs) are suitable techniques to link these DNA level changes to clinical metadata for human disease prediction, diagnosis, and therapeutics. This project develops tools within an open-source platform for documented, repeatable analyses that can be conducted in real-time allowing integration of data from patients with new treatments/vaccines strategies. This deep learning bioinformatics platform will allow the prioritization of genes associated with outcome predictors, including health, therapeutic, and vaccine outcomes, as well as inform improved DNA tests for predicting disease status and severity. The computational tools developed in this study will provide the research community and health professionals with comprehensive and generic approaches for characterizing the dynamics of genotype/phenotype associations in viruses. Such tools allow healthcare professionals and researchers to address specific properties of viruses such as frequency and location of mutations across the viral genome. When added to other clinical and epidemiological data, such information could help pave the way to better treatments or a vaccine. The developed platform will provide a venue for robust, open, repeatable analyses of COVID-19 as more and more data become available.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    200000.0USD
291    Dr. LUGHOFER Edwin    George Mason University    2020-05-15    2021-04-30    RAPID/Collaborative Research: Human-AI Teaming for Big Data Analytics to Enhance Response to the COVID-19 Pandemic    Engineering - Social media data can provide important clues and local knowledge that can help emergency managers and responders better comprehend and capture the evolving nature of many disasters. Yet humans alone cannot grasp the vast data generated by social media, so computers are used to assist. Very little is currently known about how to leverage the skills of humans and machines when they work together (human-machine teaming) to identify meaningful patterns in social media data. Therefore, the fundamental issues this Rapid Response Research (RAPID) project seeks to address are 1) understanding the process of real-time decisions that human digital volunteers make when they rapidly convert social media data into structured codes the machine (Artificial Intelligence algorithms) can understand, and 2) using this knowledge to improve human-machine teaming. This project advances the field by revealing the unique abilities that both humans and machines bring when working together to comprehend social media patterns during an evolving disaster. It supports education and diversity by providing research experiences to diverse students, as well as generating data useful for interdisciplinary courses teaching teamwork, social media analysis, and human-machine teaming. Finally, the findings can help emergency managers better train their volunteers who comb through social media using their understanding of the local knowledge and built environment to help machines see new patterns in data. Hence, this project supports NSF's mission to promote the progress of science and to advance the nation's health, prosperity, and welfare by articulating the unique value that both humans and computers bring that can lead to better decisions during disasters. The goal of this research is to better understand the real-time decisions that human annotators make under different environmental constraints, and how those contribute to the learning of Artificial Intelligence (AI) models. Under time constraints and information overload, human decision-making capabilities are limited; yet, humans still have a unique ability to understand the contextual references to the structures in the built environment that machines cannot recognize. For example, the meaning of the tweet, ?Memorial is overloaded,? -- which means the hospital, called Memorial, is out of beds for patients ?- can be lost on AI systems that lack the knowledge of the built environment. This example demonstrates the value that humans in the loop offer in a human-AI teaming context. <br/><br/>This research focuses on capturing the ephemeral data from a variety of social media sources and our two research thrusts include: 1) online observations of Community Emergency Response Team (CERT) volunteers and a manager (a collaborator on this project) using think-aloud and cognitive interviewing strategies to reveal the real-time mental models used to make coding decisions for annotation tasks; and 2) an empirical analysis of different sampling algorithms for active (machine) learning paradigms to develop a typology of machine errors under diverse contexts that affect the quality of human decision making for annotation. This research will generate design guidelines that bridge the gap between the mechanisms used for real-time data processing with AI models and the understanding of context contributed by a human user teaming with the AI models. Using theories of human decision-making combined with knowledge of how AI functions, this project provides a real-time, mid-disaster examination of 1) how humans understand, process, and interpret social media messages, and 2) how to refine AI algorithms to optimize active learning paradigm. This understanding will provide a theoretical framework enabling future research to develop protocols to optimize human-AI teaming by using concepts such as motivation and information theory. This work can help emergency managers conduct better training of their CERT volunteers and other annotators and provide clearer guidelines for how to communicate the unique value that humans bring to the annotation process for AI systems. Both our protocols and developed understanding of how humans interact with AI systems will be helpful for global health organizations, local and state-level disaster decision-makers, as well as provide direction for the vast CERT network in the United States.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    24266.0USD
292    Dr. LUGHOFER Edwin    Auburn University    2020-05-15    2021-04-30    RAPID/Collaborative Research: Quantifying Social Media Data for Improved Modeling of Mitigation Strategies for the COVID-19 Pandemic    Engineering - This Rapid Response Research (RAPID) grant will support research that will contribute new knowledge related to modeling social behavior and community activity during the COVID-19 pandemic, as well as future pandemics with COVID-19 characteristics. The model focuses on compliance with mitigation strategies and public health guidelines, thus enabling the selection of policies that are most effective in promoting both the progress of science and advancing national health and prosperity. Various pandemic models are currently being used to predict the spread of a virus and establish which mitigation strategies are the most effective. These models are heavily based on assumptions and may include an oversimplified reality of how populations react and behave. This research will provide needed knowledge and methods for the development of a model of how individuals in the U.S. react to certain mitigation strategies, such as social-distancing, stay-at-home orders, quarantines, and travel advisories, by mining and analyzing social media data during the COVID-19 crisis. This enhanced modeling approach and its resultant model will be of great value to disaster response managers and policy/decision makers to understand human social behavior. This work allows assessment of the effectiveness of mitigation strategies and public health guidelines during pandemics (and other crises). This project will also form the basis of a publicly available case study suitable for university level students that can be widely incorporated in courses. <br/><br/>Although individual-based and homogeneous mixing pandemic models provide useful insights and predictive capabilities within a range of possibilities, they are highly sensitive to people?s actions. This research aims to provide an enhanced approach to model social behavior and community activity during a pandemic in terms of compliance with mitigation strategies and public health guidelines. Social media data present a brief window of opportunity for research on how, and to what extent, the public does or does not comply with the recommended mitigation strategies and public health guidelines. The research team will collect real-time data from social media related to COVID19-exposed regional populations in the U.S. The data will be analyzed using machine learning techniques to identify non-mutually exclusive clusters of people based on similarity of their demographic, geographic, and time information, and establish relationships among clusters. The analyzed data will form the basis of a data-driven multi-paradigm simulation model that captures changes in public sentiment over time, quantifies the resistance/compliance with mitigation strategies and health guidelines, and gauges overall effectiveness of various mitigation strategies and advice over time.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    141527.0USD
293    Dr. LUGHOFER Edwin    University of Michigan Ann Arbor    2020-05-15    2021-04-30    RAPID: Subtyping and Identifying Shared Genomic Sequences of SARS-CoV-2 (COVID-19)    Biological Sciences - People are threatened by an unprecedented pandemic: COVID-19 (caused by SARS-CoV-2). This virus is now threatening not only physical health, but also psychology, education, economy, and every corner of the infrastructure of society. So far, there is no treatment for this disease, while vaccines and neutralizing antibodies are perceived as one of the eventual solutions to this crisis. A critical piece of knowledge supporting vaccine and antibody development is understanding the genome of this virus. What is the common sequence shared among the SARS-CoV-2 strains across the globe? What are the subtypes? Are the genome variances overlapping with important genomic regions for vaccine design? Using state of the art machine learning approaches, this research will identify the shared, representative sequence across SARS-CoV-2 strains and group them by major types. This project will connect this information to the important genomic regions identified in the literature that can be used for vaccines, and thereby continuously inform the ongoing effort of vaccine development, antibody selection, and therapeutic development. The research from this study would provide society benefits through monthly updates onto web interfaces that allow the vaccine developers, the biomedical research community as well as the general public to conveniently get access to the above information. This project will support training of a graduate student in bioinformatics and provide outreach opportunities to K-12 students and the public.<br/><br/>This work will be a continuous effort to monthly subtype SARS-CoV-2 strains and update the shared sequences of SARS-CoV-2, in order to facilitate vaccine development and antibody design. Specifically, this research will be focusing on three objectives: 1) identifying and updating the common sequences of SARS-CoV-2 by forcing the common sequence to have the minimal evolutionary distance with all strains, or covering as many sequences as possible 2) subtyping the SARS-CoV-2 strains into major groups, which will be important to inform treatment, management and prevention measures; 3) connecting the subtyped and common genomic sequences of SARS-CoV-2 to epitopes identified in the literature. To develop vaccines or neutralizing antibody treatment, it is critical that the major variations are covered and considered. The algorithms and visualization tools will overlay the curated list of potential epitopes on top of the subtypes and the shared sequence of the virus genomes, and directly support the effort of vaccine and antibody development. This RAPID award is made by the Systematics and Biodiversity Science Cluster in the Division of Environmental Biology, using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    199705.0USD
294    Dr. LUGHOFER Edwin    University of California-Los Angeles    2020-06-01    2021-05-31    RAPID: Dynamic Graph Neural Networks for Modeling and Monitoring COVID-19 Pandemic    Computer and Information Science and Engineering - The novel coronavirus, COVID-19, has become one of the biggest pandemics in human history and has generated lasting impacts on public health, society, and economy. The number of cases in the United States has passed 1 million with a total number of deaths over 50 thousand. There is an urgent need for research and development that can bring a predictive understanding of the spread of the virus, thereby enabling mitigation methods to alleviate the negative effects of COVID-19. Traditional epidemiological models usually take into consideration only a small number of features in building a prediction model, which may not be able to capture potential risk factors and effects of various intervention mechanisms of this new pandemic. In this project the investigators develop novel machine learning methods that can simultaneously model and predict the COVID-19 spread, detect and monitor risk factors, and evaluate effectiveness of interventions over time and space. <br/><br/>The new model ingests and integrates heterogeneous and rapidly accumulating data across diverse sources, such as publications, news, census, social media, and outbreak observation trackers. It employs a new contextualized language model to accurately recognize named entities and relations from vast text data and build knowledge graphs to extract potential risk factors. A dynamic graph is constructed. Each location node may have a set of static and time-dependent attributes. Events, individual behaviors, social activities, interventions are mapped to activity nodes with edges connecting to the corresponding location nodes at the time. A novel dynamic graph neural network is trained to perform joint predictions of all locations over time. Activity nodes of significant attention weights represent major risk factors or effective intervention mechanisms. The project will result in public dissemination of the prediction model and all source codes, immediately benefiting the combat against COVID-19.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    90000.0USD
295    Dr. PETUTSCHNIGG Alexander    University of Applied Sciences Salzburg    2018-02-01    2021-01-31    TreeTrace - Biometric fingerprints of trees: log tracing from forest to sawmill and early esti    With the increasing amount of imaging devices installed at sawmills, the importance of using these data for improving workflow and for increasing revenues in the wood processing industries is growing. In this context, challenging questions with respect to imaging and image processing technology arise, several of which will be tackled in this joint project. The project considers two application cases as follows: The first application case is the question of tracing tree logs from the forest harvesting site to the sawmill by using biometrics related tree log recognition techniques based on image processing of cross-section data only. This approach of course assumes the additional availability of imaging sensors in the forest. Since there is a trend for installing CT imaging devices at sawmills, which are of course not available in the forest, the challenging issue of cross modality matching arises. The second application case is the determination of wood quality from cross-section imagery, applicable already in the forest, and/or at the sawmill. Obviously, these two application cases share many aspects. (1) They can be combined at application level, i.e. wood quality may be determined already in the forest due to imaging devices available for the tracing application, and further refined using the sensors available at the sawmill. Conversely, CT data from the sawmill, acquired to analyse the wood quality, can be used for the tracing application; (2) data preprocessing and many features extracted are required for both, matching cross section images as well as automated wood quality analysis; (3) the questions which imaging sensors should be employed and how the resulting data can be combined effectively have to be answered. Thus, synergies arise between these two application cases which will be efficiently exploited in the project. A common data set for experimental validation can be used (which implies also sharing employed sensors), ground truth data established wrt. annotating images can be shared, many software components implementing preprocessing (e.g. pith detection, cross section texture segmentation, contrast optimisation) as well as feature extraction techniques (e.g. annual ring detection, spiral growth detection) can be developed jointly and shared subsequently. The project will break new grounds in the area of wood imaging and processing of corresponding data with advanced algorithms in vision and machine learning with particular focus on cross modality processing. While those techniques are being developed for two specific application cases, the developed algorithms will be applicable to a wide range of applications in wood imagery processing and analysis as well as for other domains where similar settings arise.    Austrian Science Fund FWF    Research Grant    328573.89EUR
296    Dr Deprez Maria    King's College London    2019-06-15    2021-06-14    Artificial intelligence to characterise abnormal brain development and neurocognitive disorders    With the increasing amount of imaging devices installed at sawmills, the importance of using these data for improving workflow and for increasing revenues in the wood processing industries is growing. In this context, challenging questions with respect to imaging and image processing technology arise, several of which will be tackled in this joint project. The project considers two application cases as follows: The first application case is the question of tracing tree logs from the forest harvesting site to the sawmill by using biometrics related tree log recognition techniques based on image processing of cross-section data only. This approach of course assumes the additional availability of imaging sensors in the forest. Since there is a trend for installing CT imaging devices at sawmills, which are of course not available in the forest, the challenging issue of cross modality matching arises. The second application case is the determination of wood quality from cross-section imagery, applicable already in the forest, and/or at the sawmill. Obviously, these two application cases share many aspects. (1) They can be combined at application level, i.e. wood quality may be determined already in the forest due to imaging devices available for the tracing application, and further refined using the sensors available at the sawmill. Conversely, CT data from the sawmill, acquired to analyse the wood quality, can be used for the tracing application; (2) data preprocessing and many features extracted are required for both, matching cross section images as well as automated wood quality analysis; (3) the questions which imaging sensors should be employed and how the resulting data can be combined effectively have to be answered. Thus, synergies arise between these two application cases which will be efficiently exploited in the project. A common data set for experimental validation can be used (which implies also sharing employed sensors), ground truth data established wrt. annotating images can be shared, many software components implementing preprocessing (e.g. pith detection, cross section texture segmentation, contrast optimisation) as well as feature extraction techniques (e.g. annual ring detection, spiral growth detection) can be developed jointly and shared subsequently. The project will break new grounds in the area of wood imaging and processing of corresponding data with advanced algorithms in vision and machine learning with particular focus on cross modality processing. While those techniques are being developed for two specific application cases, the developed algorithms will be applicable to a wide range of applications in wood imagery processing and analysis as well as for other domains where similar settings arise.    The Academy of Medical Sciences    Springboard Round 4    99895.85GBP
297    Dr Schweikert Gabriele    University of Dundee    2019-06-01    2021-05-31    Novel Machine Learning Techniques to Elucidate Function and Dynamics of Epigenomic Mechanisms    With the increasing amount of imaging devices installed at sawmills, the importance of using these data for improving workflow and for increasing revenues in the wood processing industries is growing. In this context, challenging questions with respect to imaging and image processing technology arise, several of which will be tackled in this joint project. The project considers two application cases as follows: The first application case is the question of tracing tree logs from the forest harvesting site to the sawmill by using biometrics related tree log recognition techniques based on image processing of cross-section data only. This approach of course assumes the additional availability of imaging sensors in the forest. Since there is a trend for installing CT imaging devices at sawmills, which are of course not available in the forest, the challenging issue of cross modality matching arises. The second application case is the determination of wood quality from cross-section imagery, applicable already in the forest, and/or at the sawmill. Obviously, these two application cases share many aspects. (1) They can be combined at application level, i.e. wood quality may be determined already in the forest due to imaging devices available for the tracing application, and further refined using the sensors available at the sawmill. Conversely, CT data from the sawmill, acquired to analyse the wood quality, can be used for the tracing application; (2) data preprocessing and many features extracted are required for both, matching cross section images as well as automated wood quality analysis; (3) the questions which imaging sensors should be employed and how the resulting data can be combined effectively have to be answered. Thus, synergies arise between these two application cases which will be efficiently exploited in the project. A common data set for experimental validation can be used (which implies also sharing employed sensors), ground truth data established wrt. annotating images can be shared, many software components implementing preprocessing (e.g. pith detection, cross section texture segmentation, contrast optimisation) as well as feature extraction techniques (e.g. annual ring detection, spiral growth detection) can be developed jointly and shared subsequently. The project will break new grounds in the area of wood imaging and processing of corresponding data with advanced algorithms in vision and machine learning with particular focus on cross modality processing. While those techniques are being developed for two specific application cases, the developed algorithms will be applicable to a wide range of applications in wood imagery processing and analysis as well as for other domains where similar settings arise.    The Academy of Medical Sciences    Springboard Round 4    99705.0GBP
298    Dr Cowley Lauren    University of Bath    2020-05-01    2022-04-30    Novel machine learning models for real time pathogen management    With the increasing amount of imaging devices installed at sawmills, the importance of using these data for improving workflow and for increasing revenues in the wood processing industries is growing. In this context, challenging questions with respect to imaging and image processing technology arise, several of which will be tackled in this joint project. The project considers two application cases as follows: The first application case is the question of tracing tree logs from the forest harvesting site to the sawmill by using biometrics related tree log recognition techniques based on image processing of cross-section data only. This approach of course assumes the additional availability of imaging sensors in the forest. Since there is a trend for installing CT imaging devices at sawmills, which are of course not available in the forest, the challenging issue of cross modality matching arises. The second application case is the determination of wood quality from cross-section imagery, applicable already in the forest, and/or at the sawmill. Obviously, these two application cases share many aspects. (1) They can be combined at application level, i.e. wood quality may be determined already in the forest due to imaging devices available for the tracing application, and further refined using the sensors available at the sawmill. Conversely, CT data from the sawmill, acquired to analyse the wood quality, can be used for the tracing application; (2) data preprocessing and many features extracted are required for both, matching cross section images as well as automated wood quality analysis; (3) the questions which imaging sensors should be employed and how the resulting data can be combined effectively have to be answered. Thus, synergies arise between these two application cases which will be efficiently exploited in the project. A common data set for experimental validation can be used (which implies also sharing employed sensors), ground truth data established wrt. annotating images can be shared, many software components implementing preprocessing (e.g. pith detection, cross section texture segmentation, contrast optimisation) as well as feature extraction techniques (e.g. annual ring detection, spiral growth detection) can be developed jointly and shared subsequently. The project will break new grounds in the area of wood imaging and processing of corresponding data with advanced algorithms in vision and machine learning with particular focus on cross modality processing. While those techniques are being developed for two specific application cases, the developed algorithms will be applicable to a wide range of applications in wood imagery processing and analysis as well as for other domains where similar settings arise.    The Academy of Medical Sciences    Springboard Round 5    84905.59GBP
299    Dr Namburete Ana    University of Oxford    2021-05-01    2023-04-30    Ultrasound-Based Assessment of Brain Folding Patterns in Early Pregnancy    With the increasing amount of imaging devices installed at sawmills, the importance of using these data for improving workflow and for increasing revenues in the wood processing industries is growing. In this context, challenging questions with respect to imaging and image processing technology arise, several of which will be tackled in this joint project. The project considers two application cases as follows: The first application case is the question of tracing tree logs from the forest harvesting site to the sawmill by using biometrics related tree log recognition techniques based on image processing of cross-section data only. This approach of course assumes the additional availability of imaging sensors in the forest. Since there is a trend for installing CT imaging devices at sawmills, which are of course not available in the forest, the challenging issue of cross modality matching arises. The second application case is the determination of wood quality from cross-section imagery, applicable already in the forest, and/or at the sawmill. Obviously, these two application cases share many aspects. (1) They can be combined at application level, i.e. wood quality may be determined already in the forest due to imaging devices available for the tracing application, and further refined using the sensors available at the sawmill. Conversely, CT data from the sawmill, acquired to analyse the wood quality, can be used for the tracing application; (2) data preprocessing and many features extracted are required for both, matching cross section images as well as automated wood quality analysis; (3) the questions which imaging sensors should be employed and how the resulting data can be combined effectively have to be answered. Thus, synergies arise between these two application cases which will be efficiently exploited in the project. A common data set for experimental validation can be used (which implies also sharing employed sensors), ground truth data established wrt. annotating images can be shared, many software components implementing preprocessing (e.g. pith detection, cross section texture segmentation, contrast optimisation) as well as feature extraction techniques (e.g. annual ring detection, spiral growth detection) can be developed jointly and shared subsequently. The project will break new grounds in the area of wood imaging and processing of corresponding data with advanced algorithms in vision and machine learning with particular focus on cross modality processing. While those techniques are being developed for two specific application cases, the developed algorithms will be applicable to a wide range of applications in wood imagery processing and analysis as well as for other domains where similar settings arise.    The Academy of Medical Sciences    Springboard Round 5    99963.0GBP
300    Professor Benedetto Umberto    Bristol, University of    None    None    The BHF-Turing Cardiovascular Data Science Awards (Second Call): Machine learning for risk prediction in adult cardiac surgery in United Kingdom (joint funding with The Alan Turing Institute)    None    British Heart Foundation    None    62473.0GBP
301    Dr Thanh Bui Quang    VNU University of Science    2019-10-28    2020-10-27    A Knowledge Network of Modelling and ICT for Building Resilience towards flooding in mountainous areas of Vietnam    Coastal countries are among the most vulnerable region to the impact of climate change induced disasters that happen in unprecedented magnitude and frequency. The prevention of these events is impossible in short-term solutions, but the negative impacts can be mitigated through risk preparedness plans. Among devastated disasters, flash floods are one of the most dangerous natural hazards in mountainous areas, especially in Vietnam where mountain encounters three-quarter of the country area. Flood causes severe damages to people and makes adverse impacts on social, economic development across the country, particularly to the ethnic communities. The management of flood, including flood risk prediction and information sharing, requires accurate spatial data and temporal information and effective preparedness plans to minimize loses. The identification of vulnerable areas to the flood significantly contributes to reducing the damages to human settlements, agriculture, and livelihood by avoiding more construction and developments in the prone areas. This study investigates the potential integration of remote sensing data, geospatial information system, machine learning and ICT to model flashflood and to establish a web-based system for dissemination of flashflood information. The expected impact of the proposal is to support decision-making processes and to build resilience to flooding in the mountainous area of Vietnam. The project team will work with different stakeholders to identify their requirements and to promote the modelling framework. This project will extend the knowledge of the two applicants through sharing flood data, modelling algorithms and successful lessons.    The Academy of Medical Sciences    None    24250.0GBP
302    Dr Thanh Bui Quang    Lancaster University    2019-10-28    2020-10-27    New Approaches to Bayesian Data Science: Tackling Challenges from the Health Sciences    The health sciences have seen an explosion in the amount of data collected at both individual and population levels. This data can be varied, including genetic information, health records, data on activity levels obtained from wearable devices, and image data from scans. There is huge potential for improved diagnoses, timely interventions and more effective treatments if we can fully extract understanding from this data. Example applications included real-time monitoring of patients, developing personalised treatment, or real-time monitoring and decision-making for epidemics. However the data science challenges in extracting these insights are vast. Features of these challenges include the need to make inferences about and decisions for individuals from within a population, and the need to synthesise information from disparate data sources and data types. Whilst we have substantial data collected at a population level, the amount of information on any given individual may be still be limited. Appropriately quantifying uncertainty is crucial for making decisions, with the optimal decision often being driven by the probability of relatively rare events (e.g. extreme reaction to a drug). We need model-based approaches to data science that can leverage scientific understanding, but we need the statistical analyses to be robust to unavoidable inadequacies of these models. Underpinning many of these applications is the requirement to develop new understanding, and this differs from a focus on making predictions that it is most common among current statistical or machine learning methods. Bayesian data science provides a natural framework for tackling these challenges. Bayesian methods are model-based, can appropriately quantify and propagate uncertainty, and through hierarchical models are able to use population-level information when making inferences about individuals. Repeated application of Bayes theorem gives a natural paradigm for synthesizing information across multiple data sources. However, current Bayesian data science methods are not feasible for many modern, big-data, applications in the health sciences. Bayesian methods require integrating over uncertainty. Such high-dimensional integration carries a substantial computational overhead when compared to alternative, often optimization-based, data science methods. So while the motivation for Bayesian analysis is clear, this computational overhead means that, currently, implementing Bayesian approaches is often not feasible. This programme of research will develop the new approaches to Bayesian data science that are needed both within the health sciences and more widely. It builds on recent breakthroughs in Monte Carlo integration methods that show great promise for being efficient for large data; and on new paradigms for Bayesian-like updates that are suitable for complex models and which focus modelling effort just on the aspects of these models that are most important. It will address key research challenges in the health sciences -- directly developing new insights and understanding for these.    UK Research and Innovation    None    2952639.0GBP
303    Dr Thanh Bui Quang    CROSS DIGITAL LTD    2019-10-28    2020-10-27    AI-powered Gateway for UK Care Industry    The care sector (residential/domiciliary) is in the frontline due to COVID-19\. There is unprecedented pressure to maintain safety for staff and residents. Those accessing care have been stripped of their ability to see family and enjoy leisure activities. The need to access alternative care solutions either for yourself or for a loved one is more prevalent than ever before. In the short term, people are not able to visit help centres and care homes due to COVID-19\. Cross Digital Ltd will develop a web gateway which harnesses AI (Machine learning) to assess and predict/detect the health needs of care users. Our AI-powered solutions will tackle the emerging societal healthcare and social care needs as well as improving people's wellbeing and giving improved personalised access to support services.    UK Research and Innovation    None    46331.0GBP
304    Dr Thanh Bui Quang    VAMSTAR LIMITED    2019-10-28    2020-10-27    AI-powered COVID-19 supplier risk index and demand planning toolkit    For health systems to better prepare and plan for the months ahead, Vamstar will create a risk-based framework to understand supply chain gaps and the evolving demand at a hospital-level in the UK and EU. The objective of the risk scoring matrix and the Demand-Planning-Toolkit is to focus on the most vulnerable parts of the health systems in Europe and facilitate decision making as quickly as possible. With the supply risk framework, we will be able to predict changes in the overall demand for various essential products and services needed to manage a crisis like COVID-19 and direct the focus of suppliers towards the most needed parts of the care delivery. Combining this with the Demand-Planning-Toolkit, Vamstar will be able to assess which suppliers have fulfilled a current order in the market and make predictions about when they will become fully production-ready to sell again. Additionally, the risk scoring framework and Demand-Planning-Toolkit will help stakeholders in the care delivery chain quickly assess countries that have developed or are developing preventative infrastructure and share the findings. This framework and connected real-time supply chain analytics will ensure that in the future such crises are managed with a more proactive strategy vs a reactive supply chain approach currently prevalent in our health systems. By analysing this dataset through artificial intelligence, we want to understand the Pandemic-Supply-Risk both from a macro (ability) and micro (willingness) levels needed to manage a pandemic like COVID-19\. This toolkit includes necessary items such as face masks but also digital platforms that will aid in patient and population health management across these countries as it undergoes a rapid transformation. Vamstar offers a data science powered platform for predicting and matching public contracts in healthcare and will create an automated "pandemic preparedness toolkit" specifically by using supply chain risk scoring matrix so as to focus on the most vulnerable parts of the health systems affected by the COVID-19 pandemic. It will leverage EU and UK data on public and private tendering, pricing sources and economic annual datasets to do this. Machine learning (ML) and deep learning will be used for tasks such as predicting ongoing list of suppliers with spare capacity, the date of shortages, and prices of key supplies. These predictions and analysis will form part of reports and an autonomous dashboard that will benefit the NHS hospitals and healthcare suppliers.    UK Research and Innovation    None    49794.0GBP
305    Dr Thanh Bui Quang    OKEO LIMITED    2019-10-28    2020-10-27    OKEO - COVID-19 financial modelling    OKEO is a London-based challenger credit card company that uses Open Banking data and machine learning to provide young adults with access to affordable credit. OKEO proposes to develop innovative affordability and credit risk models using current "crisis data" to help OKEO lend money to financially excluded young adults (aged between 20 to 30 years old), self-employed contractors and gig economy workers at low rates during the pandemic. By utilising innovative data modelling and lending to critical sectors of the economy, OKEO's vision is to help meet the needs of financially excluded individuals and help stimulate the UK economy. OKEO is innovating now to respond to the financial needs of individuals brought on by COVID-19 and a weakened economy.    UK Research and Innovation    None    49815.0GBP
306    Dr Thomas T Hannah Mary    Christian Medical College, Vellore    2020-01-01    2024-12-31    Radiomics based tumor phenotypes to predict individual risk and treatment response in head and neck cancer    OKEO is a London-based challenger credit card company that uses Open Banking data and machine learning to provide young adults with access to affordable credit. OKEO proposes to develop innovative affordability and credit risk models using current "crisis data" to help OKEO lend money to financially excluded young adults (aged between 20 to 30 years old), self-employed contractors and gig economy workers at low rates during the pandemic. By utilising innovative data modelling and lending to critical sectors of the economy, OKEO's vision is to help meet the needs of financially excluded individuals and help stimulate the UK economy. OKEO is innovating now to respond to the financial needs of individuals brought on by COVID-19 and a weakened economy.    DBT/Wellcome Trust India Alliance    Early Career Fellowship    13521386INR
307    Professor Chakrabarti Bhismadev    University of Reading    2019-08-01    2024-07-31    Scalable TRansdiagnostic Early Assessment of Mental Health (STREAM)    OKEO is a London-based challenger credit card company that uses Open Banking data and machine learning to provide young adults with access to affordable credit. OKEO proposes to develop innovative affordability and credit risk models using current "crisis data" to help OKEO lend money to financially excluded young adults (aged between 20 to 30 years old), self-employed contractors and gig economy workers at low rates during the pandemic. By utilising innovative data modelling and lending to critical sectors of the economy, OKEO's vision is to help meet the needs of financially excluded individuals and help stimulate the UK economy. OKEO is innovating now to respond to the financial needs of individuals brought on by COVID-19 and a weakened economy.    Medical Research Council    Research Grant    3743775.0GBP
308    Dr Patel Rashmi    King's College London    2016-10-03    2018-10-02    Symptom dimensions in first episode psychosis: predicting|clinical outcomes using natural language processing    Psychotic disorders, including schizophrenia and bipolar disorder, can cause considerable distress to affected individuals and their families. One of the key challenges faced by clinicians is that it is not currently possible to predict clinical outcomes following a first episode of psychosis at an individual patient level. This may reflect the fact that existing diagnostic classification systems do not account for the variation in the symptoms experienced by different individuals and also because of variations in genetic and environmental risk factors for psychosis between different people. Many healthcare providers now record clinical information in the form of electronic health records (EHRs). Natural language processing (NLP) is an automated information extraction technique which allows clinical data to be quickly extracted from the EHRs of large numbers of patients. In this study, I will use NLP techniques to identify the presence of psychotic symptoms in over 2,000 people with FEP and use sophisticated machine learning techniques to develop clinical prediction algorithms to determine risk of future hospitalisation and likelihood of antipsychotic treatment failure. I will evaluate the accuracy of these algorithms to make predictions with the aim of subsequently translating them into clinical practice to support clinical decision making at an individual patient level. At present, there is no way to predict which treatments will work best for individual patients with psychotic disorders. I hope that this research study may make it possible to better tailor individual treatment strategies in order to improve clinical outcomes of people with psychotic disorders.    The Academy of Medical Sciences    Starter Grant for Clinical Lecturers    30000.0GBP
309    Dr Patel Rashmi    King's College London    2018-02-14    2021-02-13    Linking electronic health records with passive smartphone activity data to predict outcomes in psychotic disorders    Psychotic disorders, including schizophrenia and bipolar disorder, can cause considerable distress to affected individuals and their families. One of the key challenges faced by clinicians is that it is not currently possible to predict clinical outcomes following a first episode of psychosis at an individual patient level. This may reflect the fact that existing diagnostic classification systems do not account for the variation in the symptoms experienced by different individuals and also because of variations in genetic and environmental risk factors for psychosis between different people. Many healthcare providers now record clinical information in the form of electronic health records (EHRs). Natural language processing (NLP) is an automated information extraction technique which allows clinical data to be quickly extracted from the EHRs of large numbers of patients. In this study, I will use NLP techniques to identify the presence of psychotic symptoms in over 2,000 people with FEP and use sophisticated machine learning techniques to develop clinical prediction algorithms to determine risk of future hospitalisation and likelihood of antipsychotic treatment failure. I will evaluate the accuracy of these algorithms to make predictions with the aim of subsequently translating them into clinical practice to support clinical decision making at an individual patient level. At present, there is no way to predict which treatments will work best for individual patients with psychotic disorders. I hope that this research study may make it possible to better tailor individual treatment strategies in order to improve clinical outcomes of people with psychotic disorders.    Medical Research Council    Fellowship    326857.0GBP
310    Dr Patel Rashmi    QUALIS FLOW LIMITED    2018-02-14    2021-02-13    Automating compliance for remote construction working    Construction logistics monitoring and compliance is typically paper-based and requires multiple individuals and teams to process, in order to ensure that material movements are handled in a safe and environmentally responsible manner. Qflow is a software tool, using a combination of Internet of Things and machine learning techniques, to digitise the information recovered from paper tickets at a construction site entrance. This ensures that this information is fed directly to the managers who need it most, and without the need for any on-site presence aside from the existing traffic marshals. Information is provided on the material types and waste removals, descriptions, relevant certifications of the supply chain, and listed quantities. This supports several teams to work remotely, including logistics, environmental, QS and data administration. The innovation that Qflow is exploring now is automated compliance screening and anomaly detection, to notify users when there are discrepancies in their data that require investigation, or where there is an illegal waste transfer. The software provides the eyes and ears on site in real-time, and provides access to that information on a cloud-based platform, meaning that those who need the data can access it wherever they are, without having to double handle paperwork and ensuring that the site operations continues smoothly without the need for intervention.    UK Research and Innovation    Fellowship    27374.0GBP
311    Dr. Clemens Jan    UNIVERSITAETSMEDIZIN GOETTINGEN - GEORG-AUGUST-UNIVERSITAET GOETTINGEN - STIFTUNG OEFFENTLICHEN RECHTS    2020-02-01    2025-01-31    Neural Computations Underlying Social Behavior in Complex Sensory Environments    Animals often interact in groups. Animal groups constitute complex sensory environments which challenge the brain and engage complex neural computations. This behavioral context is therefore fruitful for understanding how sophisticated neural computations give rise to behavior. However, it is also technically difficult since many of the relevant sensory cues arise from the members of the group and are therefore hard to quantify or control. Consequently, we only incompletely understand how the brain drives complex social behaviors in naturalistic contexts. To uncover the neural computations underlying social behavior in groups, we are using Drosophila, which provides unprecedented experimental access to the nervous system via genetic tools. Drosophila gathers on rotten fruit to feed and mate. Courtship and aggression dominate social interactions and rely on the recognition of sex-specific chemical cues and the production of context-specific acoustic signals. How are these multi-modal cues integrated to control and switch between courtship and aggression? How is unstable and conflicting sensory information resolved to promote stable behavioral strategies? How does sensory processing adapt to socially crowded environments in order to efficiently target behavior at individual members of the group? These issues will be addressed by combining computational modeling and genetic tools. Using machine learning, we will quantify and model the fine structure of social interactions to identify the social cues that drive behavior. Closed-loop optogenetics and calcium imaging in behaving animals will allow us to test the models and to ultimately reveal how the brain integrates, selects and combines social cues to drive social interactions. This multi-disciplinary approach will uncover the computational principles and mechanisms by which sensory information is processed to drive behavior in the complex sensory environment of animal groups.    European Research Council    Starting Grant    1476920.0EUR
312    Dr. Ruigrok Ynte    UNIVERSITAIR MEDISCH CENTRUM UTRECHT    2020-02-01    2025-01-31    Early recognition of intracranial aneurysms to PRevent aneurYSMal subarachnoid hemorrhage    Intracranial aneurysms usually go undetected until rupture occurs leading to aneurysmal subarachnoid hemorrhage (ASAH), a type of stroke with devastating effects. Early detection and preventive treatment of aneurysms fall short as we do not know who is it at risk and why, as we have insufficient insight in the contribution and interplay of genetic, environmental and intermediate phenotypic risk factors. Given the rarity of the disease, there is a paucity of large and rich cohorts to study risk factors separately with sufficient power. To add to the problem, my preliminary findings suggest disease heterogeneity with subgroup specific risk factors for aneurysms. The sex-related heterogeneity is most eminent in the disease with 2/3 of patients being women. I aim to advance disease understanding to allow early recognition of intracranial aneurysms to prevent ASAH. I have established a new conceptual approach that integrates genetic and environmental risk factors with imaging markers as intermediate phenotypes for genetic factors. With data reduction and machine-learning approaches I will for the first time address disease heterogeneity and aneurysm risk with adequate power. I will develop and validate a tool to automatically detect new imaging markers predicting aneurysm development applying feature-learning models. Next I will elucidate the genetic basis underlying differential imaging risk patterns (imaging genetic factors). I will apply a new hypothesis-free strategy to detect and validate yet unknown environmental risk factors predicting aneurysm presence. I will assess the contribution to disease of all factors detected according to sex. All risk factors will be combined in an aneurysm prediction risk model to understand relative contribution of different risk factors in different subgroups. It will advance disease understanding and individualized risk prediction of aneurysms leading to precision medicine in early aneurysm detection to reduce the burden of ASAH.    European Research Council    Starting Grant    1499108.0EUR
313    Dr. Philiastides Marios    University of Glasgow    2020-09-01    2025-08-31    Dynamic Network Reconstruction of Human Perceptual and Reward Learning via Multimodal Data Fusion    Training and experience can lead to long-lasting improvements in our ability to make decisions based on either ambiguous sensory or probabilistic information (e.g. learning to diagnose a noisy x-ray image or betting on the stock market). These two processes are referred to as perceptual and probabilistic/reward learning, respectively. Despite considerable efforts to uncover the neural systems involved in these processes, perceptual and reward learning have largely been studied in separate lines of research using divergent learning mechanisms. The primary aim of this proposal is to develop a unified framework for integrating these lines of research and understand the extent to which they share a common computational and neurobiological basis. Specifically, we will test the proposition that both the perceptual and reward systems could be understood in a common framework of “reward maximization”, whereby a domain-general reinforcement-guided learning mechanism – based on separate prediction error representations – facilitates future actions and adaptive behavior. To offer a comprehensive spatiotemporal characterization of the relevant networks and their computational principles we will adopt a state-of-the-art multimodal neuroimaging approach to fuse simultaneously-acquired EEG and fMRI data, via machine-learning-inspired multivariate single-trial analysis techniques and computational modelling. The project’s ultimate goal is to empower a level of neuronal and mechanistic understanding that extends beyond what could be inferred with each of these modalities in isolation. We will achieve this goal by exploiting endogenous trial-by-trial electrophysiological variability to build parametric fMRI predictors that can offer additional explanatory power than what can already be achieved by stimulus- or behaviorally-derived predictors, allowing us to go over and beyond what has been reported previously in the literature.    European Research Council    Consolidator Grant    1996043.0EUR
314    Dr. EEFTENS Marloes    SCHWEIZERISCHES TROPEN- UND PUBLIC HEALTH-INSTITUT    2020-05-01    2025-04-30    Beyond seasonal suffering: Effects of Pollen on Cardiorespiratory Health and Allergies    As climate change increases the duration and intensity of the pollen season, allergies to airborne pollen are increasingly common in Europe. Yet, it is not well recognized that high pollen concentrations may increase respiratory and cardiovascular events, leading to mortality and excess hospitalizations. I aim to investigate how short-term exposure to pollen is related to mortality, hospitalization and allergic symptoms, both on its own and synergistically with air pollution and weather. I will develop spatiotemporal exposure models of pollen for the years 2003-2022 based on a network of 14 pollen measurements stations in Switzerland. Taking advantage of large, real-world datasets without selection bias (Swiss National Cohort) and the efficient case-crossover study design, I will investigate the population effects of pollen on daily respiratory and cardiovascular mortality and hospitalization, also accounting for variation in air pollution and weather conditions. To explore individual sensitivity, I will conduct repeated measurements of lung function and airway inflammation in a dedicated panel of 400 allergic patients complemented with opportunistic repeated accounts of self-reported symptoms from the “e-symptoms” app by Swiss Allergy Centre. To provide personalized prevention recommendations and enhance quality of life for the allergic population, I will derive exposure-response relationships based on prevalent pollen, air pollution and weather triggers and individual symptom reports, allowing me to ultimately forecast symptom severity using machine learning techniques. This highly innovative project utilizes available nationwide health datasets and systematic novel data collection methods (in the in-depth panel study), to better understand the role of pollen in respiratory and cardiovascular diseases at both personalized and population levels. The project will prevent and reduce health effects due to pollen, which constitute a large burden on health and economy.    European Research Council    Starting Grant    1381932.0EUR
315    Dr Schmuker Michael    University of Hertfordshire    2020-08-14    2023-08-13    2014217 NeuroNex: From Odor to Action - Discovering Principles of Olfactory-Guided Natural Behavior    As climate change increases the duration and intensity of the pollen season, allergies to airborne pollen are increasingly common in Europe. Yet, it is not well recognized that high pollen concentrations may increase respiratory and cardiovascular events, leading to mortality and excess hospitalizations. I aim to investigate how short-term exposure to pollen is related to mortality, hospitalization and allergic symptoms, both on its own and synergistically with air pollution and weather. I will develop spatiotemporal exposure models of pollen for the years 2003-2022 based on a network of 14 pollen measurements stations in Switzerland. Taking advantage of large, real-world datasets without selection bias (Swiss National Cohort) and the efficient case-crossover study design, I will investigate the population effects of pollen on daily respiratory and cardiovascular mortality and hospitalization, also accounting for variation in air pollution and weather conditions. To explore individual sensitivity, I will conduct repeated measurements of lung function and airway inflammation in a dedicated panel of 400 allergic patients complemented with opportunistic repeated accounts of self-reported symptoms from the “e-symptoms” app by Swiss Allergy Centre. To provide personalized prevention recommendations and enhance quality of life for the allergic population, I will derive exposure-response relationships based on prevalent pollen, air pollution and weather triggers and individual symptom reports, allowing me to ultimately forecast symptom severity using machine learning techniques. This highly innovative project utilizes available nationwide health datasets and systematic novel data collection methods (in the in-depth panel study), to better understand the role of pollen in respiratory and cardiovascular diseases at both personalized and population levels. The project will prevent and reduce health effects due to pollen, which constitute a large burden on health and economy.    Medical Research Council    Research Grant    688656.0GBP
316    Prof Miho Enkelejda    FHNW University of Applied Sciences and Arts Northwestern    2019-10-01    2022-01-01    Discovery of anti-DENV Antibodies Using Artificial Intelligence    As climate change increases the duration and intensity of the pollen season, allergies to airborne pollen are increasingly common in Europe. Yet, it is not well recognized that high pollen concentrations may increase respiratory and cardiovascular events, leading to mortality and excess hospitalizations. I aim to investigate how short-term exposure to pollen is related to mortality, hospitalization and allergic symptoms, both on its own and synergistically with air pollution and weather. I will develop spatiotemporal exposure models of pollen for the years 2003-2022 based on a network of 14 pollen measurements stations in Switzerland. Taking advantage of large, real-world datasets without selection bias (Swiss National Cohort) and the efficient case-crossover study design, I will investigate the population effects of pollen on daily respiratory and cardiovascular mortality and hospitalization, also accounting for variation in air pollution and weather conditions. To explore individual sensitivity, I will conduct repeated measurements of lung function and airway inflammation in a dedicated panel of 400 allergic patients complemented with opportunistic repeated accounts of self-reported symptoms from the “e-symptoms” app by Swiss Allergy Centre. To provide personalized prevention recommendations and enhance quality of life for the allergic population, I will derive exposure-response relationships based on prevalent pollen, air pollution and weather triggers and individual symptom reports, allowing me to ultimately forecast symptom severity using machine learning techniques. This highly innovative project utilizes available nationwide health datasets and systematic novel data collection methods (in the in-depth panel study), to better understand the role of pollen in respiratory and cardiovascular diseases at both personalized and population levels. The project will prevent and reduce health effects due to pollen, which constitute a large burden on health and economy.    Wellcome Trust    Innovator Award: Digital Technologies    658569.8GBP
317    Dr Clarke Jonathan    Imperial College London    2019-10-01    2023-09-30    Care as a Complex System: Understanding the Network Dynamics of Healthcare Delivery    As climate change increases the duration and intensity of the pollen season, allergies to airborne pollen are increasingly common in Europe. Yet, it is not well recognized that high pollen concentrations may increase respiratory and cardiovascular events, leading to mortality and excess hospitalizations. I aim to investigate how short-term exposure to pollen is related to mortality, hospitalization and allergic symptoms, both on its own and synergistically with air pollution and weather. I will develop spatiotemporal exposure models of pollen for the years 2003-2022 based on a network of 14 pollen measurements stations in Switzerland. Taking advantage of large, real-world datasets without selection bias (Swiss National Cohort) and the efficient case-crossover study design, I will investigate the population effects of pollen on daily respiratory and cardiovascular mortality and hospitalization, also accounting for variation in air pollution and weather conditions. To explore individual sensitivity, I will conduct repeated measurements of lung function and airway inflammation in a dedicated panel of 400 allergic patients complemented with opportunistic repeated accounts of self-reported symptoms from the “e-symptoms” app by Swiss Allergy Centre. To provide personalized prevention recommendations and enhance quality of life for the allergic population, I will derive exposure-response relationships based on prevalent pollen, air pollution and weather triggers and individual symptom reports, allowing me to ultimately forecast symptom severity using machine learning techniques. This highly innovative project utilizes available nationwide health datasets and systematic novel data collection methods (in the in-depth panel study), to better understand the role of pollen in respiratory and cardiovascular diseases at both personalized and population levels. The project will prevent and reduce health effects due to pollen, which constitute a large burden on health and economy.    Wellcome Trust    Sir Henry Wellcome Postdoctoral Fellowship    300000.0GBP
318    Ao. Prof. Dr. HOLZINGER Andreas    Medical University of Graz    2019-11-04    2022-11-03    A Reference Model of Explainable AI for the Medical Domain    Towards A Reference Model of Explainable AI for the Medical Domain Andreas Holzinger December, 27, 2018 The progress of statistical machine learning methods has made AI increasingly successful. Deep learning exceeds human performance even in the medical domain. However, their full potential is limited by the difficulty to generate the underlying explanatory structures. The central problem is that they are regarded as "black-boxes" and even if we understand the mathematical principles, they lack an explicit declarative knowledge representation. A motivation for this project are rising legal and privacy issues. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make results re-traceable on demand - understandable by a human domain expert. We learned of a variety of technical solutions which are currently in development, which could help explain AI/ML systems and their decisions. Transparent algorithms could appropriately enhance trust of medical professionals, thereby raising acceptance of machine learning. Recently, the Google AI team emphasized the significance of research in explainable AI, the importance of Human-Computer Interaction (HCI) research for Knowledge Discovery from Data (KDD), and the urgent need for a research framework around the field of interpretability. In our ongoing research collaboration "Machine learning for diagnosis of colorectal cancer" with the Google AI group and the Institute of Pathology (Ethics Vote Medical University Graz 30-184 ex17/18) a training data set for AI/ML in digital pathology has been generated. As agreed with our colleagues at the Google AI group, we have now the chance to use machine learning algorithms developed in this context for our test environment; in return we will research towards making their algorithms explainable for our domain experts, who have the benefits of using the results for their medical teaching. This project will focus on but is not limited to digital pathological data and context. The advantage of this project is working with real-world data (under full ethics/data protection regulations) together with medical experts. This project will provide important contributions to the international research community in the following ways: evidence in various methods of explainability and novel methods and urgently needed tools for benchmarking and evaluation; Moreover, the contributions gained in this project can be used generally for reverse-engineering human learning and cognitive development and specifically to engineer more human like machine learning systems. All outcomes of this project will be made openly available to the international research community.    Austrian Science Fund FWF    Stand-Alone Projects    392773.5EUR
319    Dr Howard James    Imperial College London    2017-10-04    2020-10-03    Applications of machine learning in clinical cardiovascular imaging    Echocardiography (echo) remains the primary tool through which we assess the function of patients’ hearts. It is cheap, quick and safe. However, interpretation of the pictures can be very difficult and requires years of experience for doctors to accurately identify whether a scan is normal or abnormal. Recently, people have used computer programs to try and help in analysing medical images, and they have shown promise in the field of MRI scanning the heart. In echocardiography, however, training a computer to look at scans has been significantly more difficult. I believe I can overcome these issues by (1) developing a system which can identify exactly what heart structures are being scanned at any moment, and (2) building up a new library of echo scans from which to train a computer so it can analyse these structures. This technique of using neural networks to analyse medical images has many other applications and I am working with co-supervisors to employ my skills in different areas. These include creating a smart phone application which can identify a pacemaker’s model, speeding up the treatment of patients in emergency situations.    Wellcome Trust    PhD Training Fellowship for Clinicians    392773.5EUR
320    Dr Howard James    Association Pour La Recherche Et Le Développement Des Méthodes Et Processus Industriels (Armines)    2012-02-01    2017-02-01    Statistical machine learning for complex biological data    This interdisciplinary project aims to develop new statistical and machine learning approaches to analyze high-dimensional, structured and heterogeneous biological data. We focus on the cases where a relatively small number of samples are characterized by huge quantities of quantitative features, a common situation in large-scale genomic projects, but particularly challenging for statistical inference. In order to overcome the curse of dimension we propose to exploit the particular structures of the data, and encode prior biological knowledge in a unified, mathematically sound, and computationally efficient framework. These methodological development, both theoretical and practical, will be guided by and applied to the inference of predictive models and the detection of predictive factors for prognosis and drug response prediction in cancer.    European Research Council    Starting Grant    1496004.0EUR
321    Dr Howard James    ETH Zurich    2012-10-01    2016-03-31    Ribosome synthesis in mammalian cells    Ribosomes are large molecular machines that drive biosynthesis of proteins in all kingdoms of life. The biogenesis of ribosomes is an extremely complex pathway that involves many accessory non-ribosomal proteins and small RNAs. These factors collectively promote rRNA maturation and ribosomal protein deposition. Ribosome biogenesis (RB) is a highly energy demanding process that is stimulated in response to nutrients and growth factors, and diminished by cellular stress. Deregulation of ribosome synthesis is of medical importance, as it leads to various syndromes and is linked to tumorigenesis. Most knowledge on eukaryotic RB comes from studies in yeast, while comparatively little is known in mammalian cells. Here, we propose a comprehensive, interdisciplinary approach at the interfaces of biochemistry, cell biology, systems biology and bioinformatics to fill these gaps. To identify the repertoire of factors involved in human ribosome biogenesis, we will perform genome-wide RNAi-screens on 40S and 60S biogenesis. Our RNAi screening assay is imaging-based and relies on a panel of cell lines carrying fluorescent reporter proteins. Evaluation of the high content screening data will involve assisted machine learning techniques for image analysis as well as bioinformatics tools for prediction of off-target effects and identification of protein networks. In parallel, we will characterize assembly intermediates of human ribosomal subunits on the proteomic and RNA level. We will exploit the screening and proteomics data to study the molecular function of selected trans-acting factors in ribosome assembly, export and maturation. Further, we will investigate how signaling pathways and transcription factors cooperate in the control the synthesis of ribosomal proteins and rRNA. Our goal is to expand the understanding of the mechanisms that govern the formation of a fundamental macromolecular complex in human cells.    Swiss National Science Foundation    Project funding (Div. I-III)    772500.0CHF
322    Dr Edmondson Richard    University of Manchester    2016-04-01    2018-03-31    Artificial Intelligence and surgical decision making: an oxymoron or the way ahead    Some patients with ovarian cancer have surgery before chemotherapy whilst others have surgery after chemotherapy. For some, it may be best not to have an operation at all. Deciding the role and timing of surgery for individual patients remains a key clinical question with enormous ramifications for patients and also for the health service. The situation is almost exactly the same for oligometastatic colorectal cancer where the key question remains whether patients should have simultaneous resection of visceral metastases at the time of their initial surgery. It is no surprise that MDT decision making and subsequent practice is non uniform across the UK. Whilst predictive tools are clearly needed, previous attempts to generate these have failed, principally because they have used limited numbers of factors. A successful predictive algorithm is likely to need a combination of patient factors, tumour biology factors and importantly surgeon factors. We believe that the answer lies in using a big data and machine learning approach. Machine learning algorithms underpin many areas of commerce, banking and other areas of daily life but have not yet been utilised in clinical decision making. We propose to extract data that already exist, but are ?hidden? in NHS data silos, into a usable format. We will then use them to develop, train and validate machine learning algorithms, including neural networks that are ideally suited for this purpose, to predict surgical outcome. This project will provide important tools to answer highly relevant clinical questions but perhaps even more importantly will serve as an exemplar and develop the infrastructure to use big data to improve outcomes for patients with cancer.    Cancer Research UK    Pioneer Award Committee - Pioneer Award    772500.0CHF
323    Dr. LEX Alexander    Graz University of Technology    2013-05-01    2015-08-31    Visual Analysis of Heterogeneous Data using Semantic Subsets    Analyzing and understanding very large and heterogeneous datasets is a fundamental challenge researchers face in many scientific domains. Disciplines such as astronomy, physics and biology have to deal with datasets of an unprecedented scale and complexity. While analyzing these datasets is challenging, they also have the potential to revolutionize our understanding of the underlying processes. To realize this potential, novel analysis approaches have to be developed in all fields of the data sciences. In this proposal for an Erwin Schrödinger fellowship I introduce semantic subsets as a novel method for the visual analysis of large, heterogeneous, and multiple datasets. I propose to leverage machine learning, statistical and other methods to first partition datasets into meaningful subsets, and then use a tight integration of computational and visualization methods to support experts in choosing subsets relevant to a task. These subsets and their relationships are then visualized, facilitating an open, exploratory analysis of the data. The core research challenges addressed in this proposal are how to efficiently and effectively find suitable subsets, manage multiple subsets, and visualize the relationships between them. I argue that this approach is suitable to address the problems posed by the analysis of multiple large and heterogeneous datasets, as it scales well, is highly flexible, and naturally integrates multiple datasets. I intend to develop prototypes realizing the semantic subsets concept for the analysis of biomolecular data in design studies. These applications will be the product of a user-centered design process involving close collaboration with domain experts. The applications will address the domain expert's data analysis problems and aid them in their scientific discovery process. The formal evaluation of the utility of the approach will be conducted using case studies based on longitudinal observations of the deployed applications in addition to controlled user studies. I plan to conduct this research at the Visual Computing Group at Harvard University, lead by Professor Hanspeter Pfister. Professor Pfister and his group have considerable expertise in developing visualization methods for molecular biology. In addition, the greater Boston area is home to many top-tier molecular biology research labs, including the Harvard Medical School and the Broad Institute of MIT and Harvard, to which Professor Pfister and myself have established ties. This environment is therefore uniquely suited to the proposed kind of research. During the planned return phase at the Institute for Computer Graphics and Vision at Graz University of Technology I will not only be able to pass on my gained knowledge to my peers and to students, but will also be able to support Professor Schmalstieg in his agenda of building a strong data visualization group in Graz and thereby strengthen the already sizable Austrian visualization research community.    Austrian Science Fund FWF    Erwin Schrödinger Programme    145130.0EUR
324    Dr Zeldin Oliver    Diamond Light Source Ltd    2015-02-01    2019-01-31    Serial crystallographic studies of radiation sensitive macromolecules.    This proposal outlines a graph-based approach to dealing with the heterogeneous datasets present in serial femtosecond crystallography (SFX). By describing the hundreds to tens of thousands of images in an SFX dataset as the vertices of a graph, and the agreement between pairs of images that share reflections as the edge weights (with no edge between images that have few or zero Miller indices in common), it is possible to refine partialities and merge observations without first averaging all th e images. This opens up exciting new opportunities for sub-population clustering, outlier rejection, and time-resolved methods. Preliminary work where a graph has been created, and the individual image parameters optimized through a graph synchronization procedure has demonstrated the promise of this approach. Many powerful algorithms and machine learning techniques exist for analyzing graphs, and these will be leveraged through close collaboration with members of the Stanford Computer Science D epartment. Biological relevance of the methodology will be kept as a priority throughout by being initially hosted by Dr. Axel Brunger (Molecular and Cellular Physiology), and through collaboration with biology groups and my sponsor, Dr. Dave Stuart.    Wellcome Trust    Sir Henry Wellcome Postdoctoral Fellowship    250000.0GBP
325    Dr Cowley Lauren    University of Bath    2020-05-01    2022-04-30    Novel machine learning models for real time pathogen management    None    The Academy of Medical Sciences    Springboard Round 5    84905.59GBP
326    Dr Namburete Ana    University of Oxford    2021-05-01    2023-04-30    Ultrasound-Based Assessment of Brain Folding Patterns in Early Pregnancy    None    The Academy of Medical Sciences    Springboard Round 5    99963.0GBP
327    Professor Benedetto Umberto    Bristol, University of    2021-05-01    2023-04-30    The BHF-Turing Cardiovascular Data Science Awards (Second Call): Machine learning for risk prediction in adult cardiac surgery in United Kingdom (joint funding with The Alan Turing Institute)    None    British Heart Foundation    Springboard Round 5    62473.0GBP
328    Dr Thanh Bui Quang    VNU University of Science    2019-10-28    2020-10-27    A Knowledge Network of Modelling and ICT for Building Resilience towards flooding in mountainous areas of Vietnam    Coastal countries are among the most vulnerable region to the impact of climate change induced disasters that happen in unprecedented magnitude and frequency. The prevention of these events is impossible in short-term solutions, but the negative impacts can be mitigated through risk preparedness plans. Among devastated disasters, flash floods are one of the most dangerous natural hazards in mountainous areas, especially in Vietnam where mountain encounters three-quarter of the country area. Flood causes severe damages to people and makes adverse impacts on social, economic development across the country, particularly to the ethnic communities. The management of flood, including flood risk prediction and information sharing, requires accurate spatial data and temporal information and effective preparedness plans to minimize loses. The identification of vulnerable areas to the flood significantly contributes to reducing the damages to human settlements, agriculture, and livelihood by avoiding more construction and developments in the prone areas. This study investigates the potential integration of remote sensing data, geospatial information system, machine learning and ICT to model flashflood and to establish a web-based system for dissemination of flashflood information. The expected impact of the proposal is to support decision-making processes and to build resilience to flooding in the mountainous area of Vietnam. The project team will work with different stakeholders to identify their requirements and to promote the modelling framework. This project will extend the knowledge of the two applicants through sharing flood data, modelling algorithms and successful lessons.    The Academy of Medical Sciences    Springboard Round 5    24250.0GBP
329    Dr Thanh Bui Quang    Lancaster University    2019-10-28    2020-10-27    New Approaches to Bayesian Data Science: Tackling Challenges from the Health Sciences    The health sciences have seen an explosion in the amount of data collected at both individual and population levels. This data can be varied, including genetic information, health records, data on activity levels obtained from wearable devices, and image data from scans. There is huge potential for improved diagnoses, timely interventions and more effective treatments if we can fully extract understanding from this data. Example applications included real-time monitoring of patients, developing personalised treatment, or real-time monitoring and decision-making for epidemics. However the data science challenges in extracting these insights are vast. Features of these challenges include the need to make inferences about and decisions for individuals from within a population, and the need to synthesise information from disparate data sources and data types. Whilst we have substantial data collected at a population level, the amount of information on any given individual may be still be limited. Appropriately quantifying uncertainty is crucial for making decisions, with the optimal decision often being driven by the probability of relatively rare events (e.g. extreme reaction to a drug). We need model-based approaches to data science that can leverage scientific understanding, but we need the statistical analyses to be robust to unavoidable inadequacies of these models. Underpinning many of these applications is the requirement to develop new understanding, and this differs from a focus on making predictions that it is most common among current statistical or machine learning methods. Bayesian data science provides a natural framework for tackling these challenges. Bayesian methods are model-based, can appropriately quantify and propagate uncertainty, and through hierarchical models are able to use population-level information when making inferences about individuals. Repeated application of Bayes theorem gives a natural paradigm for synthesizing information across multiple data sources. However, current Bayesian data science methods are not feasible for many modern, big-data, applications in the health sciences. Bayesian methods require integrating over uncertainty. Such high-dimensional integration carries a substantial computational overhead when compared to alternative, often optimization-based, data science methods. So while the motivation for Bayesian analysis is clear, this computational overhead means that, currently, implementing Bayesian approaches is often not feasible. This programme of research will develop the new approaches to Bayesian data science that are needed both within the health sciences and more widely. It builds on recent breakthroughs in Monte Carlo integration methods that show great promise for being efficient for large data; and on new paradigms for Bayesian-like updates that are suitable for complex models and which focus modelling effort just on the aspects of these models that are most important. It will address key research challenges in the health sciences -- directly developing new insights and understanding for these.    UK Research and Innovation    Springboard Round 5    2952639.0GBP
330    Dr Thanh Bui Quang    CROSS DIGITAL LTD    2019-10-28    2020-10-27    AI-powered Gateway for UK Care Industry    The care sector (residential/domiciliary) is in the frontline due to COVID-19\. There is unprecedented pressure to maintain safety for staff and residents. Those accessing care have been stripped of their ability to see family and enjoy leisure activities. The need to access alternative care solutions either for yourself or for a loved one is more prevalent than ever before. In the short term, people are not able to visit help centres and care homes due to COVID-19\. Cross Digital Ltd will develop a web gateway which harnesses AI (Machine learning) to assess and predict/detect the health needs of care users. Our AI-powered solutions will tackle the emerging societal healthcare and social care needs as well as improving people's wellbeing and giving improved personalised access to support services.    UK Research and Innovation    Springboard Round 5    46331.0GBP
331    Dr Thanh Bui Quang    VAMSTAR LIMITED    2019-10-28    2020-10-27    AI-powered COVID-19 supplier risk index and demand planning toolkit    For health systems to better prepare and plan for the months ahead, Vamstar will create a risk-based framework to understand supply chain gaps and the evolving demand at a hospital-level in the UK and EU. The objective of the risk scoring matrix and the Demand-Planning-Toolkit is to focus on the most vulnerable parts of the health systems in Europe and facilitate decision making as quickly as possible. With the supply risk framework, we will be able to predict changes in the overall demand for various essential products and services needed to manage a crisis like COVID-19 and direct the focus of suppliers towards the most needed parts of the care delivery. Combining this with the Demand-Planning-Toolkit, Vamstar will be able to assess which suppliers have fulfilled a current order in the market and make predictions about when they will become fully production-ready to sell again. Additionally, the risk scoring framework and Demand-Planning-Toolkit will help stakeholders in the care delivery chain quickly assess countries that have developed or are developing preventative infrastructure and share the findings. This framework and connected real-time supply chain analytics will ensure that in the future such crises are managed with a more proactive strategy vs a reactive supply chain approach currently prevalent in our health systems. By analysing this dataset through artificial intelligence, we want to understand the Pandemic-Supply-Risk both from a macro (ability) and micro (willingness) levels needed to manage a pandemic like COVID-19\. This toolkit includes necessary items such as face masks but also digital platforms that will aid in patient and population health management across these countries as it undergoes a rapid transformation. Vamstar offers a data science powered platform for predicting and matching public contracts in healthcare and will create an automated "pandemic preparedness toolkit" specifically by using supply chain risk scoring matrix so as to focus on the most vulnerable parts of the health systems affected by the COVID-19 pandemic. It will leverage EU and UK data on public and private tendering, pricing sources and economic annual datasets to do this. Machine learning (ML) and deep learning will be used for tasks such as predicting ongoing list of suppliers with spare capacity, the date of shortages, and prices of key supplies. These predictions and analysis will form part of reports and an autonomous dashboard that will benefit the NHS hospitals and healthcare suppliers.    UK Research and Innovation    Springboard Round 5    49794.0GBP
332    Dr Batada Nizar    University of Edinburgh    2017-05-01    2019-05-01    QMAT-seq: A Novel CRISPR/Cas9 based assay for studying DNA repair associated mutations    Mutations are a root cause of cancer. In skin and lung cancers, they often result from exposure to environmental mutagens such as cigarette smoke or UV light; however, in other cancers, their causes remain unexplained. Restoration of DNA-repair in mice with BRCA1-deficiency prevents tumour formation revealing that defective DNA-repair contributes to cancer. The Alternative-Non-Homologous-End-Joining (A-EJ) pathway has recently been discovered and shown to promote genome instability and therapy resistance in BRCA1-deficient cancers but only few components of A-EJ pathway are known. In addition the causes of their activation remain elusive. Inhibition of A-EJ could be an effective therapy for cancers associated with DNA repair deficiency. In pump-priming experiments that will establish proof of concept for my new laboratory in the UK, I will develop novel quantitative laboratory assay for A-EJ employing cutting-edge CRISPR/Cas9 genome editing, high-throughput sequencing and machine learning algorithm. My long-term goal is to elucidate A-EJ regulation, to identify new members of the A-EJ pathway and to define its mutational footprints that will serve as a biomarker of DNA repair deficiency. The outcomes of this seed funding will enable me to secure long-term funding to study A-EJ in cancer and its potential in cancer stratification and personalized therapy.    Wellcome Trust    Seed Award in Science    100000.0GBP
333    Dr Holt Tim    NHS Oxfordshire CCG    2019-04-01    2020-06-30    Early detection of bowel cancer in UK primary care    Mutations are a root cause of cancer. In skin and lung cancers, they often result from exposure to environmental mutagens such as cigarette smoke or UV light; however, in other cancers, their causes remain unexplained. Restoration of DNA-repair in mice with BRCA1-deficiency prevents tumour formation revealing that defective DNA-repair contributes to cancer. The Alternative-Non-Homologous-End-Joining (A-EJ) pathway has recently been discovered and shown to promote genome instability and therapy resistance in BRCA1-deficient cancers but only few components of A-EJ pathway are known. In addition the causes of their activation remain elusive. Inhibition of A-EJ could be an effective therapy for cancers associated with DNA repair deficiency. In pump-priming experiments that will establish proof of concept for my new laboratory in the UK, I will develop novel quantitative laboratory assay for A-EJ employing cutting-edge CRISPR/Cas9 genome editing, high-throughput sequencing and machine learning algorithm. My long-term goal is to elucidate A-EJ regulation, to identify new members of the A-EJ pathway and to define its mutational footprints that will serve as a biomarker of DNA repair deficiency. The outcomes of this seed funding will enable me to secure long-term funding to study A-EJ in cancer and its potential in cancer stratification and personalized therapy.    National Institute for Health Research (Department of Health)    Full Grant    78267.0GBP
334    Prof. BAAYEN Rolf Harald    University Of Tuebingen    2017-09-01    2022-08-31    Wide Incremental learning with Discrimination nEtworks    Although homo sapiens has been endowed with language for over 50,000 years, the invention of alphabet-like scripts 3,000 years ago dominates Western linguistic thinking. Training in literacy starts in early childhood, and because of this, words and letter-like sound units can naturally seem to be the building blocks of language. The Chinese writing system highlights the cultural-specificity of this approach: characters are juxtaposed without intervening spaces, and their interpretation is highly context-dependent. Words are not singled out. And although more frequent characters contain parts indicating pronunciation, it is syllables that are referred to, not letter-like sound units. The research proposed here seeks to break the hold that the alphabet-centric approach has on our understanding of language by exploring the idea that instead of being phone and word-based, languages use low-level properties of the acoustic signal to directly reduce uncertainty about the messages encoded in the speech signal. My work with wide learning networks (two-layer networks with many thousands of units, using the simplest possible error-driven learning rule) provides remarkable support for this suggestion: For reading and speech comprehension, their performance closely matches both the strengths and the weaknesses of human processing. Especially at a time when machine learning and artificial intelligence are moving beyond human capacity, it is a methodological imperative to study and work with algorithms reflecting both the advantages and disadvantages of human learning. I am requesting funding to take this radically novel research program to the next level by further developing our account of auditory comprehension, by modeling more typologically diverse languages, by extending this approach to speech production, and by developing a discrimination-based language theory.    European Research Council    Advanced Grant    2496875.0EUR
335    Dr Taylor Peter    Newcastle University    2018-08-13    2020-08-13    EpiChange: Quantifying longitudinal changes after epilepsy surgery    Resective surgery for epilepsy, where the part of the brain thought to cause seizures is removed, leads to seizure freedom in around 70% of patients 1 year post-surgery. This falls to around 50% at 5 years post-surgery. It is not fully understood why surgery only works initially for some patients, and why this falls over time post-operatively. Surgery has a substantial immediate impact on brain structure, however, the long-term impact of surgery on brain dynamics is poorly understood. In order to make progress in this area we will perform a retrospective analysis of longitudinally acquired electroencephalographic (EEG) data. EEG recordings were made pre-operatively, and post-operatively in 76 patients for up to 5 years. Using univariate and multivariate data analysis, in conjunction with machine learning, we will learn how brain dynamics change after surgery, and if this change relates to outcome. Crucially, we will attempt to identify which factors in brain dynamics correlate with seizure relapse, even years after surgery. If successful, this will pave the way to a larger project where changes can be reverse engineered to give predictions of post-operative decline using pre-operative data. Long-term, this research has implications for other disorders involving longitudinal decline following structural brain damage.    Wellcome Trust    Seed Award in Science    99978.0GBP
336    Dr Papoutsi Chrysanthi    University of Oxford    2017-09-01    2018-12-01    Digital health for patient safety: a case study in epilepsy care    This project will focus on the potential for machine learning to improve patient safety and will interrogate whether current ethical and regulatory frameworks adequately account for the role of artificial intelligence in healthcare. In contrast to prevailing conceptions of patient safety driven by biomedical and technocratic priorities, I will pursue an understanding of patient safety as an emergent, practical accomplishment between patients, lay carers and service providers. Methods will include a theory-driven review of the literature on patient safety and machine learning, followed by an empirical case study on assisted living technologies for people with epilepsy and their carers. Qualitative methods will be used, such as interviews, ethnographic field notes, multi-modal qualitative data and document analysis. Rich, methodologically-robust empirical data will be analysed by drawing on theoretical frameworks from medical sociology and science and technology studies, to extend previous theorisations of patient safety practices.This award will also contribute to transdisciplinary learning and knowledge exchange. Drawing on the research on assisted living technologies for epilepsy, a number of workshops will be organised between clinicians, computer and data scientists, technology developers, patient and carers, social scientists and health services researchers, charities and commercial organisations. This work will advance the field theoretically and will contribute to tangible changes in digital health technology development and co-production.    The Academy of Medical Sciences    Springboard - Health of the Public 2040 Round 1    49939.53GBP
337    Dr Keating Peter    University College London    2017-05-01    2019-05-01    Developmental impact of intermittent hearing loss on spatial hearing in noisy environments    I aim to understand how intermittent unilateral hearing loss affects complex aspects of auditory development, using spatial hearing in noisy environments as a model system. Problem: Otitis media is one of the most common diseases in young children, and can often produce an intermittent unilateral hearing loss. This can disrupt normal development and produce impairments that persist even after normal hearing is restored. Spatial hearing contributes to listening in noisy environments, and this important skill is particularly impaired by developmental hearing loss. However, the neurophysiological changes that produce these impairments are unknown.Research: To address this, I will induce an intermittent unilateral hearing loss in ferrets during development. I will then perform bilateral extracellular recordings in A1 as these animals perform a spatial sound segregation task, and use machine learning to link neural activity with behaviour. I will perform these experiments during periods of both hearing loss and normal hearing, and investigate whether these animals: (i) have adapted to the hearing loss, and (ii) are impaired even when the hearing loss is absent.Impact: By understanding how the brain adapts to hearing loss, we will be able to design better cochlear implants and hearing aids, and better train people to use these devices. By linking impairments with physiological changes in neuronal populations, we will gain insight into pathophysiological markers that could help identify at-risk children. This would facilitate earlier, and better targeted, clinical intervention, and help us prevent, detect, and reverse the underlying pathophysiology.    The Academy of Medical Sciences    Springboard Round 2    99612.0GBP
338    Prof. INDIVERI Giacomo    University Of Zurich    2017-09-01    2022-08-31    Neuromorphic Electronic Agents: from sensory processing to autonomous cognitive behavior    Neural networks and deep learning algorithms are currently achieving impressive state-of-the-art results. In parallel computational neuroscience has made tremendous progress with both theories of neural computation and with hardware implementations of dedicated brain-inspired computing platforms. However, despite this remarkable progress, today’s artificial systems are still not able to compete with biological ones in tasks that involve processing of sensory data acquired in real-time, in complex and uncertain settings. One of the reasons is that neural computation in biological systems is very different from the way today's computers operate: it is tightly linked to the properties of their computational embodiment, to the physics of their computing elements and to their temporal dynamics. Conventional computers on the other hand operate with mainly serial and synchronous logic gates, with functions that are decoupled from their hardware implementation, and with discretized and virtual time. In this project we will combine the recent advancements in machine learning and neural computation with the latest developments in neuromorphic computing technology to design autonomous systems that can express robust cognitive behavior while interacting with the environment, through the physics of their computing substrate. To achieve this we will embed in robotic platforms microelectronic neuromorphic processors and sensors that implement biophysically realistic neural computational primitives and dynamics. We will adopt active-sensing and on-line spike-based learning strategies, context and state-dependent computation, and probabilistic inference methods for "programming" these neuromorphic cognitive agents to solve challenging tasks in real-time. Our results will lead to compact low-power intelligent sensory-motor systems that will have a large impact on service and consumer robotics, Internet of Things, as well as prosthetics and personalized medicine.    European Research Council    Consolidator Grant    1999090.0EUR
339    Mr Dang Giang    University of Oxford    2017-09-01    2020-03-01    Forecasting dengue cases: Vietnam as a case study    Vector-borne infectious diseases continue to be a burden to Vietnam’s economy and population health. Understanding the spatiotemporal patterns of the disease transmission is thus vital for planning resources and targeting control measures. In this work, we propose a large-scale study on how different factors interact and contribute to the dynamics of dengue in Vietnam. We will employ multiple modelling techniques to analyse the dynamics of the disease and how to best predict future cases. We hypothesise that urbanisation has played a significant role in deriving the distribution of the Aedes mosquito and on dengue transmission, and will therefore incorporate factors about urbanisation gained from satellite images into predictive models. These models will use three main methods: statistical, mechanistic and machine learning. The key goal is to predict the number of new dengue cases as far as possible, and we will work in collaboration with those who use the forecasts to assess the most useful timeframe and accuracy. A particular aim will be the prediction of upcoming hotspots of dengue transmission so that hospitals can plan their resources. We will work to present the results clearly so they can best be used to help to reduce dengue burden in Vietnam.    Wellcome Trust    International Masters Fellowship    82952.0GBP
340    Dr Flasche Stefan    London School of Hygiene & Tropical Medicine    2018-01-01    2023-01-01    Using mathematical modelling to re-think global pneumococcal immunisation strategies.    Vector-borne infectious diseases continue to be a burden to Vietnam’s economy and population health. Understanding the spatiotemporal patterns of the disease transmission is thus vital for planning resources and targeting control measures. In this work, we propose a large-scale study on how different factors interact and contribute to the dynamics of dengue in Vietnam. We will employ multiple modelling techniques to analyse the dynamics of the disease and how to best predict future cases. We hypothesise that urbanisation has played a significant role in deriving the distribution of the Aedes mosquito and on dengue transmission, and will therefore incorporate factors about urbanisation gained from satellite images into predictive models. These models will use three main methods: statistical, mechanistic and machine learning. The key goal is to predict the number of new dengue cases as far as possible, and we will work in collaboration with those who use the forecasts to assess the most useful timeframe and accuracy. A particular aim will be the prediction of upcoming hotspots of dengue transmission so that hospitals can plan their resources. We will work to present the results clearly so they can best be used to help to reduce dengue burden in Vietnam.    Wellcome Trust    Sir Henry Dale Fellowship    1097366.0GBP
341    Dr Flasche Stefan    Department of Computing Imperial College London    2015-09-01    2016-02-29    Multimodal patient-specific eye model for delineation and treatment planning of ocular tumors    In the past few years, retinoblastoma and uveal melanoma, the most common cancers of the eye, have witnessed an important advance in the field of medical imaging and its applications in ophthalmology. MRI, CT, Fundus Photography and Ultrasound are among the image modalities of reference for treatment planning and diagnosis confirmation of the disease. However, existing methods for modeling the eye are still imprecise and they do not benefit from state of the art techniques developed in other medical image processing applications. In addition, there is a disconnection between the different image modalities due to the lack of common anatomical landmarks and to the difficult method standardization in ophthalmic radiation oncology. This situation mainly affects the treatment planning (radiation dose optimization) step involved in External Beam Radiotherapy (EBRT), Cryotherapy and Brachytherapy, thus precluding optimal preservation of healthy tissue in patients. Furthermore, some patients need to undergo eye surgery prior to treatment planning, with the objective of offering a better tracking of the tumor location during therapy, therefore they require the implant of tantalum fiducial markers in the posterior part of the eye. Hence, techniques for preventing eye surgery while providing similar tracking results would offer an unprecedented opportunity for improving the patient’s quality of life during and after treatment. During the last two years we have developed a method to fully automatically segment the eye in the 3D MRI, and collaborated in the preliminary fusion of MRI with Fundus photography. These contributions follow the motivations of the presented project: i) create a method for the automatic delineation of pathological areas inside the eye in the MRI by combining multiple MRI sequences and machine learning techniques, ii) propose a framework for the fusion of MRI and Fundus photography for robust and accurate eye tumor delineation towards preventing eye surgery and iii) to set the basis for techniques to predict the tumor growth based on the shape and location of the tumor, and the technique applied for treatment. These ideas and the extended motivation can be read in detail in the research plan.    Swiss National Science Foundation    Doc.Mobility    1097366.0GBP
342    Prof Boylan Geraldine    University College Cork    2018-01-08    2020-01-08    Development of a Neonatal Brain Health Index (DELPHI)    In the past few years, retinoblastoma and uveal melanoma, the most common cancers of the eye, have witnessed an important advance in the field of medical imaging and its applications in ophthalmology. MRI, CT, Fundus Photography and Ultrasound are among the image modalities of reference for treatment planning and diagnosis confirmation of the disease. However, existing methods for modeling the eye are still imprecise and they do not benefit from state of the art techniques developed in other medical image processing applications. In addition, there is a disconnection between the different image modalities due to the lack of common anatomical landmarks and to the difficult method standardization in ophthalmic radiation oncology. This situation mainly affects the treatment planning (radiation dose optimization) step involved in External Beam Radiotherapy (EBRT), Cryotherapy and Brachytherapy, thus precluding optimal preservation of healthy tissue in patients. Furthermore, some patients need to undergo eye surgery prior to treatment planning, with the objective of offering a better tracking of the tumor location during therapy, therefore they require the implant of tantalum fiducial markers in the posterior part of the eye. Hence, techniques for preventing eye surgery while providing similar tracking results would offer an unprecedented opportunity for improving the patient’s quality of life during and after treatment. During the last two years we have developed a method to fully automatically segment the eye in the 3D MRI, and collaborated in the preliminary fusion of MRI with Fundus photography. These contributions follow the motivations of the presented project: i) create a method for the automatic delineation of pathological areas inside the eye in the MRI by combining multiple MRI sequences and machine learning techniques, ii) propose a framework for the fusion of MRI and Fundus photography for robust and accurate eye tumor delineation towards preventing eye surgery and iii) to set the basis for techniques to predict the tumor growth based on the shape and location of the tumor, and the technique applied for treatment. These ideas and the extended motivation can be read in detail in the research plan.    Wellcome Trust    Innovator Award    512574.0EUR
343    Dr Wallace Edward    University of Edinburgh    2018-01-19    2023-01-19    Dynamic regulation of mRNA processing in adapting fungi    In the past few years, retinoblastoma and uveal melanoma, the most common cancers of the eye, have witnessed an important advance in the field of medical imaging and its applications in ophthalmology. MRI, CT, Fundus Photography and Ultrasound are among the image modalities of reference for treatment planning and diagnosis confirmation of the disease. However, existing methods for modeling the eye are still imprecise and they do not benefit from state of the art techniques developed in other medical image processing applications. In addition, there is a disconnection between the different image modalities due to the lack of common anatomical landmarks and to the difficult method standardization in ophthalmic radiation oncology. This situation mainly affects the treatment planning (radiation dose optimization) step involved in External Beam Radiotherapy (EBRT), Cryotherapy and Brachytherapy, thus precluding optimal preservation of healthy tissue in patients. Furthermore, some patients need to undergo eye surgery prior to treatment planning, with the objective of offering a better tracking of the tumor location during therapy, therefore they require the implant of tantalum fiducial markers in the posterior part of the eye. Hence, techniques for preventing eye surgery while providing similar tracking results would offer an unprecedented opportunity for improving the patient’s quality of life during and after treatment. During the last two years we have developed a method to fully automatically segment the eye in the 3D MRI, and collaborated in the preliminary fusion of MRI with Fundus photography. These contributions follow the motivations of the presented project: i) create a method for the automatic delineation of pathological areas inside the eye in the MRI by combining multiple MRI sequences and machine learning techniques, ii) propose a framework for the fusion of MRI and Fundus photography for robust and accurate eye tumor delineation towards preventing eye surgery and iii) to set the basis for techniques to predict the tumor growth based on the shape and location of the tumor, and the technique applied for treatment. These ideas and the extended motivation can be read in detail in the research plan.    Wellcome Trust    Sir Henry Dale Fellowship    1337768.0GBP
344    Dr Wallace Edward    University of Berne    2015-02-01    2018-01-31    INTACT - INTerstitial pneumonia pattern Analysis for CompuTer-aided diagnosis    Interstitial Lung Diseases (ILDs) is a group of more than 200 chronic lung disorders characterized by scarring and/or inflammation of the lung tissue that leads to respiratory failure. In most cases, ILDs have unknown causes so they are described as Idiopathic Interstitial Pneumonia (IIP). Clinical examination, chest radiography and Computed Tomography (CT) constitute the first steps of the ILD diagnosis procedure, however most forms of ILD require additional invasive histological confirmation which introduces potential risks for the patient and increases common health care costs. Although there is currently no permanent cure, the precise diagnosis of ILDs is crucial for achieving the optimal treatment as well as investigating novel therapies. Idiopathic Pulmonary Fibrosis (IPF) constitutes the most prevalent IIP and carries a rather poor prognosis. Non-Specific Interstitial Pneumonia (NSIP) is a pathological subtype of IIP that mimics IPF in its clinical presentation, having though a more favorable prognosis. The high prevalence of IPF and NSIP among the ILDs combined with their similar clinical/radiological manifestations and their rather different prognosis and treatment make the corresponding differential diagnosis both crucial and challenging.The aim of the proposed project is to develop a computational system that will assist clinicians with the diagnosis of ILDs while avoiding the dangerous, expensive and time-consuming invasive biopsies. The system will provide a computerized differential diagnosis, based on radiological data and clinical/biochemical markers and will focus on the discrimination between the IPF and the NSIP while keeping a generic architecture that could be expanded to most types of ILDs. The appropriate interpretation of the available radiological data combined with clinical/biochemical information can provide reliable diagnosis able to improve the diagnostic accuracy of the clinicians.The proposed system will consist of two major modules: the image analysis module and the diagnosis support module. The former will segment the lung parenchyma from the CT images and perform detection/classification of the existing ILD patterns. Then, the extent and distribution of the lung abnormalities will be quantified. The second module will take as input the image analysis results together with the available clinical/biochemical data and based on the existing medical knowledge and machine learning techniques will provide as output a ranked list indicating the most likely disease. New methods for describing the 2D and 3D texture of CT patterns will be proposed and evaluated comparatively to the well-established literature. Moreover, the most appropriate artificial intelligence tools will be determined for classifying the CT patterns and producing the final diagnosis. For the purposes of the project two databases will be used: the publicly available Talisman database by the University Hospital of Geneva and the Inselspital database containing complementary 3D data from Multiple Detector CT (MDCT) scanners which will be created within the framework of the project.The project will strengthen ILD lung CT-image analysis by introducing novel computer vision and machine learning techniques and evaluating comparatively the existing literature. The improvement of the clinicians’ diagnostic accuracy on ILDs will result in optimizing the corresponding treatment so the patient life will be prolonged and its quality will be improved. Furthermore, the research for novel treatments will be promoted. By achieving a reliable diagnosis based on CT, the patients will avoid the potential risks of bleeding and general anaesthesia as well as the high costs that come with a surgical biopsy.    Swiss National Science Foundation    Project funding (Div. I-III)    263604.0CHF
345    Dr Wallace Edward    University of Lausanne    2017-04-01    2020-03-31    Efficient and accurate comparative genomics to make sense of high volume low quality data in biology    The amount of biological data that is used to study biological and medical questions is increasing drastically. The advances of genomic technologies enable now many research groups to assemble large scale genomic data for a large spectrum of organisms, and the challenge has now shifted from producing to analysing these large amounts of genomic data.Making sense of all that data relies on comparative genomics to identify the conserved or divergent elements, and elucidate the ones that are associated with essential housekeeping functions and those associated with innovation or adaptation. For instance, an important question is whether a gene has the same function in a model organism such as fly or mouse and in humans. However, this simple question leads to complex methodological issues. Finding the corresponding (“orthologous”) genes in different species is not trivial computationally and is dependent on the quality of the data. Characterizing differences between orthologous genes as functionally relevant or inconsequential is also computationally intensive and dependent on data quality.Because of this complexity, typical comparative genomics approaches tend to focus on few high-quality genomes and to analyse each gene family independently. These analyses, however, fail to capitalize on the increase in available data and ignore interaction (co-evolution) among genes.By contrast, in this project, we aim to develop a comparative genomics approach that leverages the abundant but noisy and heterogeneous data generated, and models coevolution of multiple genes in functional modules such as metabolic pathways. To achieve this, we will: 1) combine high- and low-quality genomic data available, with an emphasis on robustness to data incompleteness and inaccuracies, and scalability to tens of thousands of genomes.2) implement stringent quality controls-via statistical tests, empirical benchmarks, and filters;3) develop efficient machine learning algorithms that can cope with orders of magnitude more data.This approach tackles head-on the “variety”, “veracity”, and “volume” aspects of IBM’s framework of Big Data. Our project also has implications outside biological research. Data curation and homology assessment (WP1) are essential in free text and language analyses, while machine learning approaches for hypothesis prioritization (WP2) is a key element in computer science.    Swiss National Science Foundation    NRP 75 Big Data    573911.0CHF
346    Dr Wallace Edward    University of Basel    2015-08-01    2018-07-31    Multiscale dynamics of dog rabies elimination    Multiscale dynamics of dog rabies elimination The aim of this project is to elucidate the contributions of population vaccination coverage and the vaccine immunity of individual dogs on the interruption of dog rabies transmission; determine the role of population density of dogs in the transmission of rabies; and identify the optimal frequency and coverage of vaccination campaigns. The proposed project will help define the most cost-effective dog mass vaccination strategies for rabies elimination in Africa and Asia. Rabies is a zoonotic disease that is responsible for substantial human mortality in Asia and Africa, but recent studies have suggested that elimination is possible. We hypothesize that the population level aspects of vaccination coverage contribute more to the dynamics of dog rabies elimination than the kinetics of protective antibodies within individual dogs; and that the transmission of dog rabies is density dependent below a threshold of 100 dogs per km2.Approach: In 2012 and 2013 we vaccinated 18,200 and 20,000 dogs in N’Djaména, Chad, reaching a population coverage of more than 70%. Dog rabies incidence dropped from one rabid dog per week prior to the mass vaccination to less than one rabid dog in eight months afterwards. The last rabid dog was recorded in January 2014. Because of the multiple scales (between dogs and within dogs) in rabies transmission and immune dynamics, this unique data set will be used for comparative mathematical modeling approaches with individual based (contact networks and machine learning) and population-based models. First, we use dog to dog and dog to human contact network data, collected by observational studies, to extrapolate individual dog contact networks to a citywide contact network as a basis for rabies transmission models. Next we will develop metapopulation models that extend an existing compartmental model into 4 or 5 interconnected sub-models, based on the spatial heterogeneity of dog populations in N’Djaména, with assumptions of both frequency-dependent and density-dependent contact rates. Finally we will develop an individual dog based machine learning model and simulate the kinetics of protective antibodies in new populations. The three sets of models will be calibrated to existing data from the previous vaccination campaigns and compared by goodness of fit measures that evaluate parsimony and determine best fitting models (and model assumptions). The models will be used to predict the effectiveness of vaccination campaigns at various frequencies and coverage levels in preventing rabies epidemics due to imported cases. These predictions can then be validated against numbers of any new dog and human rabies cases through the project duration. Expected results and societal impact: This project will generate new knowledge on dog rabies transmission dynamics and potential for elimination; provide advice on optimal vaccination strategies; and identify the most realistic and parsimonious models for the follow-up of forthcoming dog mass vaccination campaigns in Africa and Asia in the framework of the Global Alliance for Rabies Control (GARC).    Swiss National Science Foundation    Project funding (Div. I-III)    491517.0CHF
347    Dr Wallace Edward    Department of Genetics Stanford University School of Medicine    2017-10-01    2019-03-31    Dissecting the regulatory landscape of the human genome    A large number of genetic variants linked to phenotypic changes are located in the non-coding regions of the genome, suggesting that their effects are most likely the result of changes in gene expression regulation. Expression quantitative trait locus (eQTL) analyses allow finding of regulatory genetic variants, which are linked to changes in gene expression. However, the regulatory elements perturbed by the genetic variants must be identified to fully understand these mechanisms. I propose to develop a computational framework to predict enhancers and their target genes based on genome-wide chromatin and gene expression data in an eQTL study setting. First, a set of high-confidence enhancers in human lympoblastoid cell lines (LCL) will be created from published literature and available databases. A machine-learning algorithm will then be trained based on posttranslational histone modifications, transcription factor binding and DNA sequence conservation at these enhancers. This model will then be used to predict genome-wide enhancers in additional 47 LCLs. Furthermore, data on detected eQTLs and dynamics in gene expression and chromatin variability will be used to associate the predicted enhancers to target genes, thus deciphering the cis-regulatory network around eQTL genes and pinpointing candidate regulatory elements that are perturbed by genetic variants.    Swiss National Science Foundation    Early Postdoc.Mobility    491517.0CHF
348    Dr Wallace Edward    University of Zurich    2015-05-01    2018-04-30    HIV-1 Transmission in Switzerland: viral transmission traits, superinfection and drug resistance    This translational research project is a continuation of SNF 130865. We aim to unravel traits of HIV transmission in three conceptually different situations of high clinical relevance: transmission leading to de novo infection (Aim 1), spread of HIV between infected individuals (conferred to as superinfection (SI); Aim 2) and transmission of drug resistant virus (Aim 3). The proposed studies are based on two renowned longitudinal studies with extensive biobanks: The Swiss HIV Cohort and the Zurich Primary HIV-1 infection study (ZPHI). AIM 1. Here we will determine whether specific genotypic and phenotypic viral traits of transmitted/founder viruses (T/F) exist. Characterization of T/F viruses, the earliest virus population that emerges in a newly infected individual, is of high interest as understanding their transmis-sion pathways may open avenues for prophylaxis and treatment. Current data on T/F virus features led to intriguing in-sights, but no definitive picture of genetic and phenotypic traits of T/F viruses has emerged. A limitation of previous studies was that features of T/F viruses had to be compared to unrelated viruses from chronically infected patients as transmitting individuals were in most cases not identified. Here using phylogenetically linked T/F viruses and their transmitters we will be able to study phylogenetically linked transmitter-recipient pairs. Thus far we identified 79 acutely HIV infected (recipients) and 107 chronically infected individuals with genetically related HIV strains which are their likely transmitters. Plasma samples close to estimated time of infection will be chosen for virus genomic analysis from transmitters and recipients to retrieve full length HIV-genomes employing Next Generation Sequencing (NGS) followed by haplotype reconstruction. T/F virus will be compared to variants present in the transmitter regarding, length, specific mutations, deletions/insertions, glycosylation sites, charge and frequency to define whether stochastic or non-stochastic transmission occurs. If genotypic traits are identified, gene transfer and mutagenesis experiments will be performed followed by phenotypic analysis to discern if indeed signifying characteristics of T/F viruses exist. To this end we will compare transmitter and T/F viruses for features implicated in shaping transmission including replication capacity, sensitivity to neutralizing antibodies and entry inhibitors, entry kinetics and interferon sensitivity. AIM 2: Here we will systematically explore frequency and risk factors for HIV-1 superinfection in the SHCS and ZPHI. A first phylogenetic screen based on pol sequences has revealed 150-312 potential SI cases. By retrieving longitudinal plasma samples before patients went on antiretroviral therapy we will seek to confirm SI by full length NGS and define approximate time points of SI events. In a detailed virus genomic analysis we aim to reconstruct haplotypes, study recombination events. A specific emphasis of our studies is on determining risk factors for acquiring SI using detailed patient, disease and host genomic data available. Viral fitness of initial circulating and superinfecting strains will be computed using machine learning techniques. To test whether SI is prevented by neutralizing antibody responses, those will be compared among superinfected and non-superinfected individuals. Finally, viral setpoint of cases and controls will be compared to determine the impact of SI on disease progression. AIM 3: Here we will investigate transmission of HIV-1 drug resistance mutations (TDR) within the SHCS, specifically focussing on the non-B compared to the subtype B epidemic, newer drugs, and reversion rates of TDR. We will study TDR prevalence over time for B and non- B subtypes in all drug naives and, specifically, in recently infected individuals. We will particularly focus on newer drugs. In patients with TDR we will determine reversion rates for specific TDR and fitness cost by taking into account the pol genotypic backbone indepen-dently of TDR. Furthermore, by performing phylogenetic analysis we will study whether there are differences in transmission dynamics between B- and non-B subtype TDR. This analysis will reveal whether TDR in the non-B subtypes is increasing as would be expected due to the recent, wide spread roll out of antiretroviral drugs in developing countries.    Swiss National Science Foundation    Project funding (special)    834000.0CHF
349    Dr Wallace Edward    University of Berne    2017-10-01    2020-09-30    Stroke treatment goes personalized: Gaining added diagnostic yield by computer-assisted treatment selection (the STRAY-CATS project)    Stroke is the second most frequent cause of death and a major cause of disability in industrial countries: in patients who survive, stroke is frequently associated with high socioeconomic costs due to persistent disability. In clinical practice, advanced neuroimaging techniques are increasingly employed for a quick, reliable diagnosis and stratification for therapy. Tissue-at-risk estimation is frequently performed by MRI, with the infarct core being identified as an area of restricted diffusion on diffusion-weighted magnetic resonance imaging (DWI-MRI). The surrounding severly hypoperfused and potentially salvageable tissue tissue (i.e. the “penumbra”) is characterized by its delay in arterial transit time using perfusion-weighted MRI. The clinical image interpretation is routinely performed as a visual analysis done by neuroradiologists and/or neurological stroke experts.There is class I evidence that intravenous thrombolysis is a safe and effective therapy within an estimated time frame of 4.5 h after stroke onset. Very recently, four prospective studies demonstrated the superiority of mechanical thrombectomy in proximal vessel occlusions within a time frame of 6 h after stroke onset. Mechanical thrombectomy has thus become the treatment option of choice to achieve an early and sustained revascularization of proximally occluded vessels in specialized stroke centres2. The recent advent of mechanical thrombectomy has now raised an urgent question that needs to be answered: “can we predict advantageous tissue survival if mechanical thrombectomy is successfully applied compared to the natural course of disease in the presence of sufficient vs. insufficient collaterals?” The availability of a safe, reproducible and reliable information about the expected tissue salvage would allow not only to select patients that would benefit from mechanical thrombectomy, it would further allow to select patients for revascularization in a time window that exceeds 6 h if sufficient collateral flow enables sustained tissue survival. It is essential that indicators for further success of endovascular therapy can be calculated as soon after admission as possible, in order to save as much of the brain tissue as possible: '*time is brain*'. Computer-assisted and automated tissue segregation of the infarct core and salvageable penumbra using compound information from multimodal MRI offers a novel and robust standardized solution to this problem: while simple thresholding based on perfusion and diffusion imaging provide only a crude estimate of the tissue at risk, machine-learning approaches based on multimodal MR data overcome the limited accuracy of linear analyses. The proposed machine-learning approach incorporates thus two separate goals, i) to quantify penumbral collateral flow in the acute emergency setting and ii) to identify fingerprints that disentangle salvageable vs. non-salvageable tissue based on machine learning in a “big data” approach based on multiparametric imaging. We will provide means for i) by transforming the interpretation of image features into an interpretation of the underlying stationary flow field. This will allow us to combine information of all available MR imaging data, to quantify the collateral blood flow in the individual patient before and during intervention, and to compare 4D flow patterns at a population level. We will ii) build on our existing predictive models of stroke outcome, incorporating FLAIR and SWI maps and making use of the 'big data' that have been acquired during the last years in more than 1000 patients that underwent intraarterial thrombolysis or thrombectomy. Our overall goal is to investigate if, given sufficient training data, predictive maps of the infarction can improve on the current 'penumbra' concept as a tool for identifying patients who will have a favourable response to reperfusion therapy.    Swiss National Science Foundation    Project funding (Div. I-III)    474000.0CHF
350    Dr Desrivieres Sylvane    King's College London    2017-09-01    2019-08-31    Neurobiological underpinning of eating disorders: integrative biopsychosocial longitudinal analyses in adolescents    None    Medical Research Council    Research Grant    230792.0GBP
351    Professor Kadioglu Aras    University of Liverpool    2017-07-10    2020-12-31    Mechanisms for acquisition and transmission of successful antibiotic resistant pneumococcal clones pre- and post-vaccination    None    Medical Research Council    Research Grant    273163.0GBP
352    Dr Meyer Nicholas    King's College London    2016-10-01    2020-10-31    Detecting early signs of relapse in psychosis using remote monitoring technology    None    Medical Research Council    Fellowship    243384.0GBP
353    Dr Mukherjee Sach    MRC Biostatistics Unit    2014-03-01    2016-11-30    Statistics and machine learning for precision medicine    None    Medical Research Council    Unit    243384.0GBP
354    Dr Waithe Dominic    University of Oxford    2018-04-01    2021-03-31    Quantitative and Real-Time Image Analysis for Advanced Light Microscopy.    None    Medical Research Council    Fellowship    345783.0GBP
355    Dr Hirst Yasemin    University College London    2016-05-01    2017-12-31    CRUK Guardian Angels    Background: Most patients with cancer present with symptoms / via symptomatic routes (including emergency, urgent two week GP referrals or other – non-urgent- referrals). One year survival following the cancer diagnosis relies on early detection.Helping doctors to suspect cancer after patients have presented with symptoms is obviously crucial – but at the same time it is also crucial to support patients to seek help soon after symptoms have developed. Delays in patients' symptom appraisal and help-seeking have usually been studied retrospectively in surveys and interviews. However, there is vast amount of existing data available in the digital and commercial platforms to be used in prospective and complex study designs. To some extent, individual (e.g. google searches, symptom checkers), consumer (e.g. loyalty card) and clinical data (GP visits, referral etc) have previously been a focus of research but have not yet been investigated collectively to understand delays in symptom appraisal and help-seeking in cancer alarm symptoms. Aims: The proposed research aims to investigate delays in symptom appraisal and subsequent help-seeking behaviour using 'Big Data'. The future aim is to link individual, consumer and clinical datasets via a secure mobile phone application, and assess whether it is possible to identify signs of cancer looking at behaviour change and self-assessment using machine learning algorithms. Methods: The project includes three multidisciplinary (Health Psychology, Computing, Advanced Statistics and Cancer Epidemiology) studies that will (a) explore the lay language that is being used to articulate cancer symptoms in order to generate a dictionary of possible online search terms, (b) investigate the associations between symptom perception and behaviour change using consumer data, e.g. continuous purchase of cough medicine etc., and (c) explore public perceptions on data linkage to monitor cancer risk/warning signs. The first two studies will be using data-mining techniques, and the last study will conduct focus groups using inductive thematic analysis to assess the acceptability of using data linkage to monitor indicators of cancer symptoms. How the results of this research will be used; Beyond our future ambition, this project aims to produce a glossary based on lay expressions of cancer symptoms in order to better understand online information-seeking behaviours. We also aim to provide a detailed report on the availability of data from different resources, and evaluate the feasibility of the prospective studies. The results of this project will initiate further cancer research into understanding the ever increasing volumes of data across disciplines.    Cancer Research UK    PRC - Early Diagnosis - Innovation Grant    345783.0GBP
356    Dr Baranowski Elizabeth    University of Birmingham    2018-09-11    2020-12-10    Steroid Metabolomics for Diagnosis and Monitoring of Inborn Steroidogenesis Disorders    Background: Most patients with cancer present with symptoms / via symptomatic routes (including emergency, urgent two week GP referrals or other – non-urgent- referrals). One year survival following the cancer diagnosis relies on early detection.Helping doctors to suspect cancer after patients have presented with symptoms is obviously crucial – but at the same time it is also crucial to support patients to seek help soon after symptoms have developed. Delays in patients' symptom appraisal and help-seeking have usually been studied retrospectively in surveys and interviews. However, there is vast amount of existing data available in the digital and commercial platforms to be used in prospective and complex study designs. To some extent, individual (e.g. google searches, symptom checkers), consumer (e.g. loyalty card) and clinical data (GP visits, referral etc) have previously been a focus of research but have not yet been investigated collectively to understand delays in symptom appraisal and help-seeking in cancer alarm symptoms. Aims: The proposed research aims to investigate delays in symptom appraisal and subsequent help-seeking behaviour using 'Big Data'. The future aim is to link individual, consumer and clinical datasets via a secure mobile phone application, and assess whether it is possible to identify signs of cancer looking at behaviour change and self-assessment using machine learning algorithms. Methods: The project includes three multidisciplinary (Health Psychology, Computing, Advanced Statistics and Cancer Epidemiology) studies that will (a) explore the lay language that is being used to articulate cancer symptoms in order to generate a dictionary of possible online search terms, (b) investigate the associations between symptom perception and behaviour change using consumer data, e.g. continuous purchase of cough medicine etc., and (c) explore public perceptions on data linkage to monitor cancer risk/warning signs. The first two studies will be using data-mining techniques, and the last study will conduct focus groups using inductive thematic analysis to assess the acceptability of using data linkage to monitor indicators of cancer symptoms. How the results of this research will be used; Beyond our future ambition, this project aims to produce a glossary based on lay expressions of cancer symptoms in order to better understand online information-seeking behaviours. We also aim to provide a detailed report on the availability of data from different resources, and evaluate the feasibility of the prospective studies. The results of this project will initiate further cancer research into understanding the ever increasing volumes of data across disciplines.    Medical Research Council    Fellowship    167889.0GBP
357    Dr Fujita Andre    University of Sao Paulo    2018-03-31    2020-03-20    A model-based graph clustering approach for autism stratification    Autism spectrum disorder (ASD) is usually diagnosed by behavioral analyses and currently there is no objective test. Even the latest computational methods (e.g. support vector machine and deep learning) based on analyses of bloodoxygen-level dependent (BOLD) signal yield classification accuracies of about 60 to 70%, which are unsatisfactory for clinical use. However, if we focus on a group of individuals sharing a specific phenotype, we obtain better results, probably because there is more than one “type” of ASD. Thus, our main goal is to stratify the ASD into subgroups, not by the direct analysis of the BOLD signal as it is usually done, but by the functional brain network (FBN) based on the BOLD signal. The rationale is that ASD can be explained by the differences in how neurons interact. However, there are at least three technical drawbacks with this approach: (i) to the best of our knowledge, there is no method to cluster FBNs (note that this problem is different of the clustering of the vertices of the network as done by the spectral clustering algorithm); (ii) even individuals belonging to the same group present different FBNs (intrinsic fluctuation), which makes the analysis using standard computational methods unfruitful; and (iii) confounders such as age, gender, and other clinical parameters may affect the clustering structure. To overcome these problems, we will represent the FBNs as random graphs and assume that probabilistic models (e.g. Watts-Strogatz and Barabási-Albert models) generate the FBNs. Then, we will define that FBNs generated by the same model belong to the same group while FBNs generated by different models belong to different groups (similar to the Gaussian mixture model). Confounders’ effects will be removed by covarying the probabilistic distributions. We hope this stratification contribute for a better understanding of the mechanisms underlying ASD. To develop this method, Dr. Fujita of USP contacted Dr. Mourao-Miranda of UCL to complement his expertise in classification methods. Dr. Fujita works on statistical methods on graphs while Dr. Mourao-Miranda is specialist in machine learning with applications in psychiatric disorders. To train Dr. Fujita’s group in the field of machine learning, the partners will organize two courses, one at UCL and another at USP. Moreover, two Ph.D. candidates from USP will be sent for six months each one to UCL to study cutting-edge classification techniques. This project is essential to obtain novel insights, solidify this cooperation, and improve the quality of this research.    The Academy of Medical Sciences    Newton Advanced Fellowship    61814.0GBP
358    Professor Zaikin Alexey    University College London    2019-01-15    2022-01-14    HSM: Construction of graph-based network longitudinal algorithms to identify screening and prognostic biomarkers and therapeutic targets (GBNLA)    Autism spectrum disorder (ASD) is usually diagnosed by behavioral analyses and currently there is no objective test. Even the latest computational methods (e.g. support vector machine and deep learning) based on analyses of bloodoxygen-level dependent (BOLD) signal yield classification accuracies of about 60 to 70%, which are unsatisfactory for clinical use. However, if we focus on a group of individuals sharing a specific phenotype, we obtain better results, probably because there is more than one “type” of ASD. Thus, our main goal is to stratify the ASD into subgroups, not by the direct analysis of the BOLD signal as it is usually done, but by the functional brain network (FBN) based on the BOLD signal. The rationale is that ASD can be explained by the differences in how neurons interact. However, there are at least three technical drawbacks with this approach: (i) to the best of our knowledge, there is no method to cluster FBNs (note that this problem is different of the clustering of the vertices of the network as done by the spectral clustering algorithm); (ii) even individuals belonging to the same group present different FBNs (intrinsic fluctuation), which makes the analysis using standard computational methods unfruitful; and (iii) confounders such as age, gender, and other clinical parameters may affect the clustering structure. To overcome these problems, we will represent the FBNs as random graphs and assume that probabilistic models (e.g. Watts-Strogatz and Barabási-Albert models) generate the FBNs. Then, we will define that FBNs generated by the same model belong to the same group while FBNs generated by different models belong to different groups (similar to the Gaussian mixture model). Confounders’ effects will be removed by covarying the probabilistic distributions. We hope this stratification contribute for a better understanding of the mechanisms underlying ASD. To develop this method, Dr. Fujita of USP contacted Dr. Mourao-Miranda of UCL to complement his expertise in classification methods. Dr. Fujita works on statistical methods on graphs while Dr. Mourao-Miranda is specialist in machine learning with applications in psychiatric disorders. To train Dr. Fujita’s group in the field of machine learning, the partners will organize two courses, one at UCL and another at USP. Moreover, two Ph.D. candidates from USP will be sent for six months each one to UCL to study cutting-edge classification techniques. This project is essential to obtain novel insights, solidify this cooperation, and improve the quality of this research.    Medical Research Council    Research Grant    469754.0GBP
359    Ao. Univ. Prof.Dr. KAUTZKY-WILLER Alexandra    Medical University of Vienna    2019-01-21    2022-01-20    Gender Outcomes and Well-being Development Group (GOING-FWD GNP78)    Gender Outcomes INternational Group: to Further Well-being Development (GOING-FWD) Background: Beyond biological sex, gender is increasingly recognized as a pivotal determinant of health. However, there are no standardized gender measurements. We hypothesize that gender-related factors and their effect will vary substantially between countries and diseases. Aims: The overarching aims of this large Consortium are to integrate sex and gender dimensions in applied health research, to evaluate their impact on clinical cost-sensitive outcomes and patients reported outcomes related to quality of life in noncommunicable diseases including cardiovascular disease, metabolic disease, chronic kidney disease and neurological disease. We also aim to construct innovative ways to disseminate the application of gender measurement towards personalized approaches to chronic disease prevention, diagnosis and treatment. Methods: With a five-country transatlantic network comprised of 30 investigators, we will benchmark innovative solutions to measure gender in retrospective cohorts. Based on consensus, we will develop a framework to identify gender-related factors, as well as cost-sensitive and patients reported outcomes and measure their associations in 32 accessible cohorts of patients affected by cardiovascular, chronic kidney and neurological diseases and metabolic syndrome. Large database analysis and when appropriate machine learning approaches will allow the derivation of pan and within country disease specific gender scores which will be validated through e-Health and m-Health applications in prospective disease groups. Educational modules will be developed to promote awareness, implementation and dissemination. Innovation: As a five-country multidisciplinary Consortium with access to granular large databases, we are uniquely positioned to harness an innovative methodology that will provide a framework to close gender gaps in chronic disease management and promote knowledge transfer in the scientific community and clinical practice.    Austrian Science Fund FWF    02 International programmes    298552.8EUR
360    Dr Li Ke    University of Exeter    2019-05-01    2023-04-30    Transfer Optimisation System for Adaptive Automated Nature-Inspired Optimisation    Hard optimisation problems are ubiquitous across the breadth of science, engineering and economics. For example, in water system planning and management, water companies are often interested in optimising several system performance measures of their infrastructures in order to provide sustainable and resilient water/wastewater services that are able to cope with and recover from disruption, as well as wider challenges brought by climate change and population increase. As a classic discipline, significant advances in both theory and algorithms have been achieved in optimisation. However, almost all traditional optimisation solvers, ranging from classic methods to nature-inspired computational intelligence techniques, ignore some important facts: (i) real-world optimisation problems seldom exist in isolation; and (ii) artificial systems are designed to tackle a large number of problems over their lifetime, many of which are repetitive or inherently related. Instead, optimisation is run as a 'one-off' process, i.e. it is started from scratch by assuming zero prior knowledge each time. Therefore, knowledge/experience from solving different (but possibly related) optimisation exercises (either previously completed or currently underway), which can be useful for enhancing the target optimisation task at hand, will be wasted. Although the Bayesian optimisation considers incorporating some decision maker's knowledge as a prior, the gathered experience during the optimisation process is discarded afterwards. In this case, we cannot expect any automatic growth of their capability with experience. This practice is counter-intuitive from the cognitive perspective where humans routinely grow from a novice to domain experts by gradually accumulating problem-solving experience and making use of existing knowledge to tackle new unseen tasks. In machine learning, leveraging knowledge gained from related source tasks to improve the learning of the new task is known as transfer learning, an emerging field that considerable success has been witnessed in a wide range of application domains. There have been some attempts on applying transfer learning in evolutionary computation, but they do not consider the optimisation as a closed-loop system. Moreover, the recurrent patterns within problem-solving exercises have been discarded after optimisation, thus experience cannot be accumulated over time. The proposed research will develop a revolutionary general-purpose optimiser (as known as transfer optimisation system) that will be able to learn knowledge/experience from previous optimisation process and then autonomously and selectively transfer such knowledge to new unseen optimisation tasks. The transfer optimisation system places adaptive automation at the heart of the development process and explores novel synergies at the crossroads of several disciplines including nature-inspired computation, machine learning, human-computer interaction and high-performance parallel computing. The outputs will bring automation in industry, including an optimised/shortened production cycle, reduced resource consumption and more balanced and innovative products, which have great potentials to result in economic savings and an increase of turnover. The proposed methods will be rigorously evaluated by the industrial partners, first in water industry and will be expanded to a boarder range of sectors which put the optimisation at the heart of their regular production/management process (e.g. renewable energy, healthcare, automotive, appliance and medicine manufacturers).    Medical Research Council    Fellowship    1097642.0GBP
361    Professor Hogg Claire    Royal Brompton and Harefield NHS Foundation Trust    2019-03-01    2021-02-28    Improving Primary Ciliary Dyskinesia diagnosis using artificial intelligence.    Hard optimisation problems are ubiquitous across the breadth of science, engineering and economics. For example, in water system planning and management, water companies are often interested in optimising several system performance measures of their infrastructures in order to provide sustainable and resilient water/wastewater services that are able to cope with and recover from disruption, as well as wider challenges brought by climate change and population increase. As a classic discipline, significant advances in both theory and algorithms have been achieved in optimisation. However, almost all traditional optimisation solvers, ranging from classic methods to nature-inspired computational intelligence techniques, ignore some important facts: (i) real-world optimisation problems seldom exist in isolation; and (ii) artificial systems are designed to tackle a large number of problems over their lifetime, many of which are repetitive or inherently related. Instead, optimisation is run as a 'one-off' process, i.e. it is started from scratch by assuming zero prior knowledge each time. Therefore, knowledge/experience from solving different (but possibly related) optimisation exercises (either previously completed or currently underway), which can be useful for enhancing the target optimisation task at hand, will be wasted. Although the Bayesian optimisation considers incorporating some decision maker's knowledge as a prior, the gathered experience during the optimisation process is discarded afterwards. In this case, we cannot expect any automatic growth of their capability with experience. This practice is counter-intuitive from the cognitive perspective where humans routinely grow from a novice to domain experts by gradually accumulating problem-solving experience and making use of existing knowledge to tackle new unseen tasks. In machine learning, leveraging knowledge gained from related source tasks to improve the learning of the new task is known as transfer learning, an emerging field that considerable success has been witnessed in a wide range of application domains. There have been some attempts on applying transfer learning in evolutionary computation, but they do not consider the optimisation as a closed-loop system. Moreover, the recurrent patterns within problem-solving exercises have been discarded after optimisation, thus experience cannot be accumulated over time. The proposed research will develop a revolutionary general-purpose optimiser (as known as transfer optimisation system) that will be able to learn knowledge/experience from previous optimisation process and then autonomously and selectively transfer such knowledge to new unseen optimisation tasks. The transfer optimisation system places adaptive automation at the heart of the development process and explores novel synergies at the crossroads of several disciplines including nature-inspired computation, machine learning, human-computer interaction and high-performance parallel computing. The outputs will bring automation in industry, including an optimised/shortened production cycle, reduced resource consumption and more balanced and innovative products, which have great potentials to result in economic savings and an increase of turnover. The proposed methods will be rigorously evaluated by the industrial partners, first in water industry and will be expanded to a boarder range of sectors which put the optimisation at the heart of their regular production/management process (e.g. renewable energy, healthcare, automotive, appliance and medicine manufacturers).    National Institute for Health Research (Department of Health)    Full Grant    341942.0GBP
362    Professor Jefferson Emily    University of Dundee    2019-08-01    2024-07-31    MICA: InterdisciPlInary Collaboration for efficienT and effective Use of clinical images in big data health care RESearch: PICTURES    Hard optimisation problems are ubiquitous across the breadth of science, engineering and economics. For example, in water system planning and management, water companies are often interested in optimising several system performance measures of their infrastructures in order to provide sustainable and resilient water/wastewater services that are able to cope with and recover from disruption, as well as wider challenges brought by climate change and population increase. As a classic discipline, significant advances in both theory and algorithms have been achieved in optimisation. However, almost all traditional optimisation solvers, ranging from classic methods to nature-inspired computational intelligence techniques, ignore some important facts: (i) real-world optimisation problems seldom exist in isolation; and (ii) artificial systems are designed to tackle a large number of problems over their lifetime, many of which are repetitive or inherently related. Instead, optimisation is run as a 'one-off' process, i.e. it is started from scratch by assuming zero prior knowledge each time. Therefore, knowledge/experience from solving different (but possibly related) optimisation exercises (either previously completed or currently underway), which can be useful for enhancing the target optimisation task at hand, will be wasted. Although the Bayesian optimisation considers incorporating some decision maker's knowledge as a prior, the gathered experience during the optimisation process is discarded afterwards. In this case, we cannot expect any automatic growth of their capability with experience. This practice is counter-intuitive from the cognitive perspective where humans routinely grow from a novice to domain experts by gradually accumulating problem-solving experience and making use of existing knowledge to tackle new unseen tasks. In machine learning, leveraging knowledge gained from related source tasks to improve the learning of the new task is known as transfer learning, an emerging field that considerable success has been witnessed in a wide range of application domains. There have been some attempts on applying transfer learning in evolutionary computation, but they do not consider the optimisation as a closed-loop system. Moreover, the recurrent patterns within problem-solving exercises have been discarded after optimisation, thus experience cannot be accumulated over time. The proposed research will develop a revolutionary general-purpose optimiser (as known as transfer optimisation system) that will be able to learn knowledge/experience from previous optimisation process and then autonomously and selectively transfer such knowledge to new unseen optimisation tasks. The transfer optimisation system places adaptive automation at the heart of the development process and explores novel synergies at the crossroads of several disciplines including nature-inspired computation, machine learning, human-computer interaction and high-performance parallel computing. The outputs will bring automation in industry, including an optimised/shortened production cycle, reduced resource consumption and more balanced and innovative products, which have great potentials to result in economic savings and an increase of turnover. The proposed methods will be rigorously evaluated by the industrial partners, first in water industry and will be expanded to a boarder range of sectors which put the optimisation at the heart of their regular production/management process (e.g. renewable energy, healthcare, automotive, appliance and medicine manufacturers).    Medical Research Council    Research Grant    2851878.0GBP
363    Dr Walsh Simon    Imperial College of Science, Technology and Medicine    2019-02-01    2024-01-31    Fibrotic lung disease on high-resolution computed tomography: predicting disease behaviour using computer algorithms.    Hard optimisation problems are ubiquitous across the breadth of science, engineering and economics. For example, in water system planning and management, water companies are often interested in optimising several system performance measures of their infrastructures in order to provide sustainable and resilient water/wastewater services that are able to cope with and recover from disruption, as well as wider challenges brought by climate change and population increase. As a classic discipline, significant advances in both theory and algorithms have been achieved in optimisation. However, almost all traditional optimisation solvers, ranging from classic methods to nature-inspired computational intelligence techniques, ignore some important facts: (i) real-world optimisation problems seldom exist in isolation; and (ii) artificial systems are designed to tackle a large number of problems over their lifetime, many of which are repetitive or inherently related. Instead, optimisation is run as a 'one-off' process, i.e. it is started from scratch by assuming zero prior knowledge each time. Therefore, knowledge/experience from solving different (but possibly related) optimisation exercises (either previously completed or currently underway), which can be useful for enhancing the target optimisation task at hand, will be wasted. Although the Bayesian optimisation considers incorporating some decision maker's knowledge as a prior, the gathered experience during the optimisation process is discarded afterwards. In this case, we cannot expect any automatic growth of their capability with experience. This practice is counter-intuitive from the cognitive perspective where humans routinely grow from a novice to domain experts by gradually accumulating problem-solving experience and making use of existing knowledge to tackle new unseen tasks. In machine learning, leveraging knowledge gained from related source tasks to improve the learning of the new task is known as transfer learning, an emerging field that considerable success has been witnessed in a wide range of application domains. There have been some attempts on applying transfer learning in evolutionary computation, but they do not consider the optimisation as a closed-loop system. Moreover, the recurrent patterns within problem-solving exercises have been discarded after optimisation, thus experience cannot be accumulated over time. The proposed research will develop a revolutionary general-purpose optimiser (as known as transfer optimisation system) that will be able to learn knowledge/experience from previous optimisation process and then autonomously and selectively transfer such knowledge to new unseen optimisation tasks. The transfer optimisation system places adaptive automation at the heart of the development process and explores novel synergies at the crossroads of several disciplines including nature-inspired computation, machine learning, human-computer interaction and high-performance parallel computing. The outputs will bring automation in industry, including an optimised/shortened production cycle, reduced resource consumption and more balanced and innovative products, which have great potentials to result in economic savings and an increase of turnover. The proposed methods will be rigorously evaluated by the industrial partners, first in water industry and will be expanded to a boarder range of sectors which put the optimisation at the heart of their regular production/management process (e.g. renewable energy, healthcare, automotive, appliance and medicine manufacturers).    National Institute for Health Research (Department of Health)    Full Grant    1053348.0GBP
364    Dr Saxe Andrew    University of Oxford    2019-09-01    2024-08-31    Principles of Learning in Distributed Brain Networks    Hard optimisation problems are ubiquitous across the breadth of science, engineering and economics. For example, in water system planning and management, water companies are often interested in optimising several system performance measures of their infrastructures in order to provide sustainable and resilient water/wastewater services that are able to cope with and recover from disruption, as well as wider challenges brought by climate change and population increase. As a classic discipline, significant advances in both theory and algorithms have been achieved in optimisation. However, almost all traditional optimisation solvers, ranging from classic methods to nature-inspired computational intelligence techniques, ignore some important facts: (i) real-world optimisation problems seldom exist in isolation; and (ii) artificial systems are designed to tackle a large number of problems over their lifetime, many of which are repetitive or inherently related. Instead, optimisation is run as a 'one-off' process, i.e. it is started from scratch by assuming zero prior knowledge each time. Therefore, knowledge/experience from solving different (but possibly related) optimisation exercises (either previously completed or currently underway), which can be useful for enhancing the target optimisation task at hand, will be wasted. Although the Bayesian optimisation considers incorporating some decision maker's knowledge as a prior, the gathered experience during the optimisation process is discarded afterwards. In this case, we cannot expect any automatic growth of their capability with experience. This practice is counter-intuitive from the cognitive perspective where humans routinely grow from a novice to domain experts by gradually accumulating problem-solving experience and making use of existing knowledge to tackle new unseen tasks. In machine learning, leveraging knowledge gained from related source tasks to improve the learning of the new task is known as transfer learning, an emerging field that considerable success has been witnessed in a wide range of application domains. There have been some attempts on applying transfer learning in evolutionary computation, but they do not consider the optimisation as a closed-loop system. Moreover, the recurrent patterns within problem-solving exercises have been discarded after optimisation, thus experience cannot be accumulated over time. The proposed research will develop a revolutionary general-purpose optimiser (as known as transfer optimisation system) that will be able to learn knowledge/experience from previous optimisation process and then autonomously and selectively transfer such knowledge to new unseen optimisation tasks. The transfer optimisation system places adaptive automation at the heart of the development process and explores novel synergies at the crossroads of several disciplines including nature-inspired computation, machine learning, human-computer interaction and high-performance parallel computing. The outputs will bring automation in industry, including an optimised/shortened production cycle, reduced resource consumption and more balanced and innovative products, which have great potentials to result in economic savings and an increase of turnover. The proposed methods will be rigorously evaluated by the industrial partners, first in water industry and will be expanded to a boarder range of sectors which put the optimisation at the heart of their regular production/management process (e.g. renewable energy, healthcare, automotive, appliance and medicine manufacturers).    Wellcome Trust    Sir Henry Dale Fellowship    771226.0GBP
365    Dr Saxe Andrew    University of Lausanne    2017-04-01    2020-03-31    Efficient and accurate comparative genomics to make sense of high volume low quality data in biology    The amount of biological data that is used to study biological and medical questions is increasing drastically. The advances of genomic technologies enable now many research groups to assemble large scale genomic data for a large spectrum of organisms, and the challenge has now shifted from producing to analysing these large amounts of genomic data.Making sense of all that data relies on comparative genomics to identify the conserved or divergent elements, and elucidate the ones that are associated with essential housekeeping functions and those associated with innovation or adaptation. For instance, an important question is whether a gene has the same function in a model organism such as fly or mouse and in humans. However, this simple question leads to complex methodological issues. Finding the corresponding (“orthologous”) genes in different species is not trivial computationally and is dependent on the quality of the data. Characterizing differences between orthologous genes as functionally relevant or inconsequential is also computationally intensive and dependent on data quality.Because of this complexity, typical comparative genomics approaches tend to focus on few high-quality genomes and to analyse each gene family independently. These analyses, however, fail to capitalize on the increase in available data and ignore interaction (co-evolution) among genes.By contrast, in this project, we aim to develop a comparative genomics approach that leverages the abundant but noisy and heterogeneous data generated, and models coevolution of multiple genes in functional modules such as metabolic pathways. To achieve this, we will: 1) combine high- and low-quality genomic data available, with an emphasis on robustness to data incompleteness and inaccuracies, and scalability to tens of thousands of genomes.2) implement stringent quality controls-via statistical tests, empirical benchmarks, and filters;3) develop efficient machine learning algorithms that can cope with orders of magnitude more data.This approach tackles head-on the “variety”, “veracity”, and “volume” aspects of IBM’s framework of Big Data. Our project also has implications outside biological research. Data curation and homology assessment (WP1) are essential in free text and language analyses, while machine learning approaches for hypothesis prioritization (WP2) is a key element in computer science.    Swiss National Science Foundation    NRP 75 Big Data    573911.0CHF
366    Dr Saxe Andrew    EPF Lausanne    2009-04-01    2013-06-30    Chromatin structure-driven computational analysis of gene regulatory regions    The genetic code for gene regulatory information and the molecular mechanisms of transcriptional control are still poorly understood. The advent of high-throughput technologies to map in vivo DNA-protein complexes, in particular chromatin immunoprecipitation combined with ultrahigh-throughput sequencing (ChIP-Seq) raises new hope for a breakthrough. For the first time it is possible to obtain a comprehensive and detailed view of the chromatin structure of an entire genome in a particular cell type. In addition, we are now able to identify the complete repertoire of in vivo binding sites for a given transcription factor under given conditions. In other words, the molecular events that control gene expression in proximity to the DNA suddenly have become visible. The genome-wide ChIP-Seq datasets that have become available over the last year, already led to numerous new insights about gene regulation. We anticipate that very large volumes of similar data will be released during the course of the proposed project. Computational analysis of ChIP-Seq data is in its infancy. For computational biology, the next few years will probably be remembered as a learning phase during which the community has acquired experience how to make best use of this new type of molecular data. Standard protocols for quality-control and low level data processing will arise. The type of biological questions that can be answered will be better defined and stated in terms amenable to computational approaches. Appropriate data structures for higher level analysis will arise and methodological principles and algorithm will consolidate over time. This proposal is meant to be a contribution to this learning process. The focus is on higher level analysis and interpretation, not on primary data processing. Specifically we are interested in questions concerning the relationship between DNA sequence, chromatin structure, transcriptional activation and repression. A key question in gene regulation, especially with regard to large higher eukaryotic genomes, is why a given transcription factor binds only to a subset of potential target sites in the genome. Much of the proposed work is devoted directly or indirectly to this problem. Histone modification maps based on ChIP-Seq provide clues about DNA accessibility, transcriptional status, and other physiological processes acting on a gene regulatory region. Classification and categorization will help to define subsets of genomic regions that offer similar conditions for transcription factor binding. Information on evolutionary conservation may further direct our analyses to functionally relevant chromatin regions. Comprehensive maps of in vivo transcription factor binding sites provide ideal training and test sets for machine learning approaches that may help to answer the key question of what directs gene regulatory proteins to their physiological target sites. Our research will be mostly biological question-driven in this sense. In addition to methodological advances, we hope that the proposed work will lead to a number of specific and experimentally testable hypotheses regarding chromatin-modulated gene regulatory mechanisms.    Swiss National Science Foundation    Project funding (Div. I-III)    199116.1CHF
367    Dr Saxe Andrew    University of Zurich    2010-10-01    2014-09-30    The roles of social context and sleep replay for vocal learning in a songbird    Many complex learning behaviors such as speech learning are strongly influenced by factors including social context and sleep. Although many influences are known today, they have mostly been studied in artificial rather than natural settings and are currently supported only by correlative but not by causal evidence. Our work aims at bridging this gap by studying vocal development in the songbird and its dependence on social interactions with conspecific birds and on neural replay of behavioral sequences during sleep. The songbird is a vocal learner in which the brain mechanisms involved in sleep replay are well known. During sleep, premotor neurons of the vocal apparatus in zebra finches engage in bursting patterns that are reminiscent of their patterns generated during song production. In the past, we have performed extensive studies on the generation of such sleep-burst sequences in large song-control networks. We now want to make use of his knowledge and test for a causal relationship between sleep sequences and vocal development. Such a causal relationship has been widely hypothesized but has never been tested experimentally. We plan to implant stimu-lation electrodes into the brain area that generates sleep sequences and perturb downstream sleep bursts to study their effects on vocal changes. Comparisons will be made with birds that are similarly stimulated, but at daytime rather than during sleep. We hope this research will provide one of the first demonstrations that off-line neural activity during sleep has key roles for procedural learning.The social interactions during vocal learning and their influences have not been studied exten-sively yet. In a different set of experiments we will study the social factors that are beneficial or detrimental to vocal learning. It is known that birds with siblings produce less accurate copies of tutor song than birds without siblings, an effect known as a fraternal inhibition. However, the precise factors of this inhibition and the role of the parents in mediating this inhibition are currently not known. To study such effects of social context and many more, we will design and build a bird monitoring system in which several microphones and a video camera jointly operate to record all songs, the locations of the singers, and the locations of the other birds of a family living inside the same cage. This bird-monitoring system is a difficult instrumentation task because jointly tutored birds sing similar songs and often they sing at the same time. Hence, we will apply sophisticated machine-learning techniques to solve this blind-source separation problem. Our bird monitoring system will be fully automated and minimally rely on human input, thus facilitating behavioral experiments and high-throughput data acquisition. We are convinced that this tool will be useful to a large community of birdsong and neuroethology researchers worldwide, because one of the most successful strategies for understanding brain mechanisms is to study them in natural settings rather than in artificial ones. In future experiments beyond this grant application, we will make use of this bird-monitoring system also in electrophysiological experiments, to study brain mechanisms in a natural social context.The broader relevance of our work extends from songbirds to human social sciences and physi-ology, because effects of sibling number and parental interactions influence speech learning also in children. And, the functions of sleep and dreams remain deeply mysterious and any new insights even in animal models will be highly valuable.    Swiss National Science Foundation    Project funding (Div. I-III)    463572.0CHF
368    Dr Saxe Andrew    Institute for Prediction Technology (UCIPT) Department of Family Medicine University of California    2017-04-01    2017-09-30    Predictive Modeling of cART Medication Adherence and Immuno-virologic outcomes among HIV infected Adults in Lausanne, Switzerland    Advancements in human immunodeficiency tritherapy have allowed patients on treatment to achieve a life expectancy similar to those without the disease. To achieve therapeutic goals, it is important to adhere to treatment regimen. The monitoring of adherence can be done using several methods such as patient reports, pill count and electronic monitoring. Electronic monitoring allows daily monitoring of medication taking through the use of electronic pill boxes (Medication event monitoring systems, MEMS) that store the time and day of opening. This method is used routinely in care at the pharmacy of the Policlinique Medicale Universitaire since 2004. Currently there is medication adherence data for about 500 patients followed at the adherence programme. Few studies have described the erosion of medication adherence over time, but more research is needed to identify the early critical indicators of initial nonadherence signs in HIV. To be able to investigate the effect of medication nonadherence on immune-virologic outcomes, we would like to conduct a predictive modeling analysis of the adherence and clinical data collected retrospectively at our institute in Lausanne. The medication adherence data in Lausanne is very unique as it comprises daily medication intake and interview transcripts with patients about the barriers and facilitators to adherence collected routinely in care of a long period of time (12 years). This predictive modeling will be done using state of the art prediction algorithms combining Big Data Science, statistical modeling, behavioral psychology, clinical and adherence sciences. The the University of California Institute for Prediction Technology (UCIPT) has developed a lot of novel methods for big data science analysis and this joint project will combine unique patient data from the adherence clinic in Lausanne with state of the art predictive machine learning methodology at UCIPT.The analysis of this data can provide unprecedented insights into patient behavior regarding dealing with chronic medication in general, and their struggle to deal with HIV. Consequently, tailored interventions can be developed to prevent nonadherence that can lead to viral failure, and mortality.    Swiss National Science Foundation    Doc.Mobility    463572.0CHF
369    Dr Saxe Andrew    ETH Zurich    2018-02-01    2019-01-31    Spectral analysis of fluorescently labeled amyloids    The self-assembly of prion precursors into oligomers and fibers has been linked to several neurodegenerative and systemic disorders. In particular, the aggregation of Amyloid-ß (Aß), tau and a-synuclein in brain tissue have been associated with the neuropathological process of Alzheimer’s (AD) and Parkinson’s disease (PD). Numerous studies have shown that not only these aggregates can propagate from cell-to-cell in a prion-like manner, but their toxicity and linked pathology strongly depends on their conformations, defining different strains of amyloid. The characterization of the biologically relevant strains remains hard to determine experimentally.To overcome this problem, I propose to implement a computational multicomponent spectral analysis pipeline that will allow to efficiently discriminate amyloid strains. Several fluorescent dyes, such as Congo red and thioflavin, are known to specifically bind to fibrils. Furthermore, their emission spectrum is modulated by the conformation of the fibril. In this project, I will use machine learning to analyze the spectral information of fluorescent microscope image and determine the strains of amyloid. The pipeline developed in this project will allow to characterize the composition of brain aggregates in vitro, in vivo and in brain tissue. Such a method could also greatly aid biophysical studies of amyloids by quickly assessing the quality and composition of samples, as well as give molecular insights linked to the different phenotypes. This understanding should lay the groundwork for the development of effective treatments for AD, PD and other neurodegenerative diseases linked to prions.    Swiss National Science Foundation    Return CH Advanced Postdoc.Mobility    110200.0CHF
370    Dr Saxe Andrew    University of Berne    2016-07-01    2019-06-30    In silico prediction of phage-bacteria infection networks as a tool to implement personalized phage therapy    The emergence and rapid dissemination of antibiotic resistance worldwide threatens medical progress. As a consequence, medicine might face a return to the pre-antibiotic era in a very soon future. The paucity of potential new anti-infectives in the pipeline of pharmaceutical industries urges the need for alternatives to fight this public health problem. Phage therapy might represent such an alternative. This re-emerging therapy uses viruses that specifically infect and kill bacteria during their life cycle to reduce/eliminate bacterial load and cure infections. These viruses, called bacteriophages or phages, have been co-evolving with bacteria for billions years, controlling bacterial populations and epidemics, and contributing to their genetic exchanges. With the advantage of having low impact on the commensal flora, as they are highly strain specific, some phages might, nevertheless, harbor virulence factors and drive horizontal gene transfer mediating dissemination of pathogenic traits including antibiotic resistance, calling for their careful selection before their therapeutic use (see below “Phage lifestyle inside the bacteria”). The success of phage therapy mainly relies on the exact matching between both the target pathogenic bacteria and the therapeutic phage. Therefore, having access to a fully characterized phage library is necessary, although not sufficient, to start with phage therapy. An essential and obligate second step to conceive personalized phage therapy treatments is the capacity to predict the interactions between the target pathogen and its potential phage. The long term goal of the proposed research is, therefore, to develop quantitative and predictive in silico models of phage-bacteria infection networks. These models will describe the interactions between phage and bacteria and will serve to fasten the selection of effective phages to propose phage therapy in a personalized fashion.To efficiently predict successful phage-bacteria interactions suitable for phage therapy, we will develop a novel in silico methodology that will, ultimately, enable the selection of phage candidates from an existing phage library to target a given pathogenic bacteria. To achieve this, we will combine genomic information with state-of-the-art bioinformatic and machine learning techniques, taking advantage of the growing amount of interaction data already available as well as of our own data to keep uncovering new phage families. We will ensure that our methodology brings explanatory power along, thereby shedding light on the relevant genomic features underscoring the interactions. To challenge our approach, we will eventually prospectively validate our methodology using paradigmatic pathogens (Pendleton et al. 2013). For this, we will construct the phage-bacterium infection networks around those pathogens, to identify single phages and/or phage cocktails with extended bactericidal activities, as assessed in different models of infections, including a Galleria melonnella model of infection and a rat endocarditis model. We expect our methodology to drive a paradigm shift in phage therapy, by offering a time-sparing and easy-to-use way to accurately select phages for each individual patient. The methodology will be made available online and several future developments of this project are already envisioned.    Swiss National Science Foundation    Interdisciplinary projects    915766.0CHF
371    Dr. FLEISHMAN Sarel-Jacob    WEIZMANN INSTITUTE OF SCIENCE LTD    2019-01-01    2023-12-31    Automated computational design of site-targeted repertoires of camelid antibodies    We propose to develop the first high-throughput strategy to design, synthesize, and screen repertoires comprising millions of single-domain camelid antibodies (VHH) that target desired protein surfaces. Each VHH will be individually designed for high stability and target-site affinity. We will leverage recent methods developed by our lab for designing stable, specific, and accurate backbones at interfaces, the advent of massive and affordable custom-DNA oligo synthesis, and machine learning methods to accomplish the following aims: Aim 1: Establish a completely automated computational pipeline that uses Rosetta to design millions of VHHs targeting desired protein surfaces. The variable regions in each design will be encoded in DNA oligo pools, which will be assembled to generate the entire site-targeted repertoire. We will then use high-throughput binding screens followed by deep sequencing to characterize the designs’ target-site affinity and isolate high-affinity binders. Aim 2: Develop an epitope-focusing strategy that designs several variants of a target antigen, each of which encodes dozens of radical surface mutations outside the target site to disrupt potential off-target site binding. The designs will be used to isolate site-targeting binders from repertoires of Aim 1. Each high-throughput screen will provide unprecedented experimental data on target-site affinity in millions of individually designed VHHs. Aim 3: Use machine learning methods to infer combinations of molecular features that distinguish high-affinity binders from non binders. These will be encoded in subsequent designed repertoires, leading to a continuous “learning loop” of methods for high-affinity, site-targeted binding. AutoCAb’s interdisciplinary strategy will thus lead to deeper understanding of and new general methods for designing stable, high-affinity, site-targeted antibodies, potentially revolutionizing binder and inhibitor discovery in basic and applied biomedical research.    European Research Council    Consolidator Grant    2337500.0EUR
372    Professor Murray Alison    University of Aberdeen    2018-03-01    2019-02-28    Early-life origins of brain resilience to mental illness and cognitive impairment across the life-course    We propose to develop the first high-throughput strategy to design, synthesize, and screen repertoires comprising millions of single-domain camelid antibodies (VHH) that target desired protein surfaces. Each VHH will be individually designed for high stability and target-site affinity. We will leverage recent methods developed by our lab for designing stable, specific, and accurate backbones at interfaces, the advent of massive and affordable custom-DNA oligo synthesis, and machine learning methods to accomplish the following aims: Aim 1: Establish a completely automated computational pipeline that uses Rosetta to design millions of VHHs targeting desired protein surfaces. The variable regions in each design will be encoded in DNA oligo pools, which will be assembled to generate the entire site-targeted repertoire. We will then use high-throughput binding screens followed by deep sequencing to characterize the designs’ target-site affinity and isolate high-affinity binders. Aim 2: Develop an epitope-focusing strategy that designs several variants of a target antigen, each of which encodes dozens of radical surface mutations outside the target site to disrupt potential off-target site binding. The designs will be used to isolate site-targeting binders from repertoires of Aim 1. Each high-throughput screen will provide unprecedented experimental data on target-site affinity in millions of individually designed VHHs. Aim 3: Use machine learning methods to infer combinations of molecular features that distinguish high-affinity binders from non binders. These will be encoded in subsequent designed repertoires, leading to a continuous “learning loop” of methods for high-affinity, site-targeted binding. AutoCAb’s interdisciplinary strategy will thus lead to deeper understanding of and new general methods for designing stable, high-affinity, site-targeted antibodies, potentially revolutionizing binder and inhibitor discovery in basic and applied biomedical research.    Medical Research Council    P&Cs    192240.0GBP
373    Dr Carragher Raymond    University of Strathclyde    2018-02-14    2021-02-13    Precision Drug Theraputics: Risk Prediction in Pharmacoepidemiology    We propose to develop the first high-throughput strategy to design, synthesize, and screen repertoires comprising millions of single-domain camelid antibodies (VHH) that target desired protein surfaces. Each VHH will be individually designed for high stability and target-site affinity. We will leverage recent methods developed by our lab for designing stable, specific, and accurate backbones at interfaces, the advent of massive and affordable custom-DNA oligo synthesis, and machine learning methods to accomplish the following aims: Aim 1: Establish a completely automated computational pipeline that uses Rosetta to design millions of VHHs targeting desired protein surfaces. The variable regions in each design will be encoded in DNA oligo pools, which will be assembled to generate the entire site-targeted repertoire. We will then use high-throughput binding screens followed by deep sequencing to characterize the designs’ target-site affinity and isolate high-affinity binders. Aim 2: Develop an epitope-focusing strategy that designs several variants of a target antigen, each of which encodes dozens of radical surface mutations outside the target site to disrupt potential off-target site binding. The designs will be used to isolate site-targeting binders from repertoires of Aim 1. Each high-throughput screen will provide unprecedented experimental data on target-site affinity in millions of individually designed VHHs. Aim 3: Use machine learning methods to infer combinations of molecular features that distinguish high-affinity binders from non binders. These will be encoded in subsequent designed repertoires, leading to a continuous “learning loop” of methods for high-affinity, site-targeted binding. AutoCAb’s interdisciplinary strategy will thus lead to deeper understanding of and new general methods for designing stable, high-affinity, site-targeted antibodies, potentially revolutionizing binder and inhibitor discovery in basic and applied biomedical research.    Medical Research Council    Fellowship    300445.0GBP
374    Dr Gurdasani Deepti    Queen Mary University of London    2019-08-05    2021-03-25    Predictive analytics of integrated genomic and clinical data using machine learning and complex statistical approaches    We propose to develop the first high-throughput strategy to design, synthesize, and screen repertoires comprising millions of single-domain camelid antibodies (VHH) that target desired protein surfaces. Each VHH will be individually designed for high stability and target-site affinity. We will leverage recent methods developed by our lab for designing stable, specific, and accurate backbones at interfaces, the advent of massive and affordable custom-DNA oligo synthesis, and machine learning methods to accomplish the following aims: Aim 1: Establish a completely automated computational pipeline that uses Rosetta to design millions of VHHs targeting desired protein surfaces. The variable regions in each design will be encoded in DNA oligo pools, which will be assembled to generate the entire site-targeted repertoire. We will then use high-throughput binding screens followed by deep sequencing to characterize the designs’ target-site affinity and isolate high-affinity binders. Aim 2: Develop an epitope-focusing strategy that designs several variants of a target antigen, each of which encodes dozens of radical surface mutations outside the target site to disrupt potential off-target site binding. The designs will be used to isolate site-targeting binders from repertoires of Aim 1. Each high-throughput screen will provide unprecedented experimental data on target-site affinity in millions of individually designed VHHs. Aim 3: Use machine learning methods to infer combinations of molecular features that distinguish high-affinity binders from non binders. These will be encoded in subsequent designed repertoires, leading to a continuous “learning loop” of methods for high-affinity, site-targeted binding. AutoCAb’s interdisciplinary strategy will thus lead to deeper understanding of and new general methods for designing stable, high-affinity, site-targeted antibodies, potentially revolutionizing binder and inhibitor discovery in basic and applied biomedical research.    Medical Research Council    Fellowship    177596.0GBP
375    Dr Secrier Maria    University College London    2019-06-17    2021-06-17    Quantitative evaluation of gene dosage effects in the Ras/ERK and PI3K/mTOR pathways on metastatic transformation of oesophageal cancer    None    Wellcome Trust    Seed Award in Science    99983.0GBP
376    Dr Secrier Maria    U-FLOOR TECHNOLOGIES LTD    2019-06-17    2021-06-17    OptimisAir - Air quality control combined with behaviourial science    Occupants' daily activities make up 35% of the factors leading to indoor air-pollution \[UCL,2018\]. In the face of COVID-19 we are expecting people to spend much more time in their homes which has two serious knock-on effects: 1) occupants experience longer exposure to indoor pollutants, increasing the risk of respiratory illnesses; 2) Poor IAQ is linked to COPD, asthma, stroke & heart diseases -- all of which are underlying health conditions, increasing the risk of severe illness from covid-19, putting further pressure on the NHS. To address this challenge we are now developing OptimisAir: an integrated 'Indoor air quality management system' that helps Registered Social Landlords reducing their maintenance costs through automated airflow control combined with AI-based activity-recognition and nudge-techniques to reduce the root cause of poor indoor air quality. It uses IoT-enabled sensors to monitor indoor pollutants and utilises game-changing technologies (machine-learning, activity-recognition, 'nudge'/behavioural sciences) to tackle an age-old problem: poor indoor air quality, dampness and draughts in homes. The outcome of the project creates a novel, patented system at an extremely competitive cost.    UK Research and Innovation    Seed Award in Science    49703.0GBP
377    Dr Secrier Maria    ELECTRONIC MEDIA SERVICES LIMITED    2019-06-17    2021-06-17    Conquering COVID in Construction – a safe, managed return to site for construction workers    The IHS Markit purchasing managers' index for UK construction dropped to 39.3 last month from 52.6 the previous month, the lowest reading in more than 10 years. The UK construction sector employs 2.4m workers many of whom are self-employed and are unable to benefit from the Treasury's Furlough Scheme. The sector is a significant contributor to the UK's economic activity producing about 6 per cent of the country's total economic output. The vision for the project is an easy to use health check and tracking app that will give the worker and their employer a simple red/green check of their ability to work. This would enable the industry to end the lockdown, re-engage the predominately self-employed workforce and restart economic activity in a significant sector. The red/green advice from the app would be based on: 1\. Daily self-declared monitoring of general health and especially of any symptoms specific to COVID-19\. These declarations will be performed even when the user is self-isolating. 2\. A tracking feature that would record the user's location while at work and who else they have been in contact with (e.g. < 5 metres separation). 3\. Alerting when there are too many people too close together. 4\. Occupancy of any welfare units, so they can self-time their breaks to minimise unnecessary contacts. 5\. Alerting of the user when a co-worker who they have been in contact with is developing symptoms or who has tested positive for COVID-19\. 6\. Machine Learning-based algorithm to track the development of symptoms in the workforce. The app would have a dashboard for the employer showing who is currently on-site, real-time hot spots showing where workers are congregating so they can be instructed to disperse, a list of staff who have reported symptoms or have tested positive, a list of staff who should be self-isolating because they may have come into contact with another infected member of staff, potential return dates for self-isolating staff plus a list of staff who have tested positive for antibodies and may be immune to re-infection. Both China and South Korea have demonstrated the benefit of using tracking and contact tracing to reduce the spread of COVID-19\. Therefore, the proposed system can be part of an industry-led approach that will help the global construction sector to safely return to work.    UK Research and Innovation    Seed Award in Science    49882.0GBP
378    Dr Secrier Maria    DEEP RENDER LTD    2019-06-17    2021-06-17    AI-based Image-As-Video Streaming    Deep Render Ltd is a London based deep-tech start-up developing the next generation of AI-based media compression algorithms. Our proprietary and patented technology is at the forefront of machine learning research. Deep Render is combining the fields of artificial intelligence, statistics and information theory to unlock the fundamental limits of image and video compression: The human eye is the best data compressor known to humanity - with compression ratios at least 2,000 times better than everything developed to date. Our Biological Compression approach approximates the neurological processes of the human eye through a non-linear, learning-based approach, thereby creating a novel class of highly efficient compression algorithms. We are world leaders in this domain, and our unique, AI-based image compression technology is already providing a 75% efficiency gain over the best previous compression standards. As global data consumption is growing exponentially with more than 80% of traffic being Image/Video, Deep Render's AI-based compression technology is vital to bypass broadband constraints. The outbreak of COVID-19 has now accelerated this growth, as a result of the crisis, internet usage has increased significantly. In particular, the demand for live-streaming and video-chat services. Therefore we want to apply our already working image compression codec to live-video streaming. The outcome of the project is to extend our image compression codec to an image-as-video live-streaming codec, at least 75% more efficient than the current state-of-the-art. Our target customers are the live video chat services (Zoom, Skype, Microsoft Teams), as well as the entertainment industry, including live streaming platforms (Twitch, Facebook, Instagram, YouTube). Our value proposition is easy to understand. By making file sizes 75% smaller, we directly increase the bandwidth supply of the internet for live-streaming by a factor of 4\. Increasing the bandwidth supply by making file sizes smaller, is magnitudes more value- and time-efficient than increasing the bandwidth supply through rewiring the globe with progressively more fibre-cables. Deep Render is going to help create a new age in which bandwidth constraints are a problem of the past. As a result of COVID 19 solving this problem has gained more importance and Deep Render is determined to create a fast solution.    UK Research and Innovation    Seed Award in Science    50000.0GBP
379    Dr Secrier Maria    V2G EVSE LIMITED    2019-06-17    2021-06-17    Covid-19 eHealth Data Acquisition Unit (COVeHealth)    The UK is currently in "lock down" due to the novel coronavirus pandemic. Before putting the currently locked down country back to work we need to: 1) Reduce the incidence of cases to reduce the burden on the NHS to more normal levels 2) Once that is achieved, we need to prevent the virus from catching fire a second time To reduce the risk of a second wave, we need to develop tools to enable us to: a) Preemptively identify early stage Covid-19 sufferers b) Use these tools to guard the borders of spaces (hospitals, homes, shops, workplaces, shopping malls, schools, universities etc.) The World Health Organization's message is that we must **"Find, isolate, test and treat every case to break the chains of Covid transmission."** Covid-19 screening tests are currently performed at a modest distance by a health worker using a "no touch" infrared thermometer to detect the tell tale fever. The other common symptom indicative of Covid-19 is the persistent dry cough, detected by the characteristic sound. What is desperately needed, both in the UK and around the World, is a way of performing screening for early stage Covid-19 symptoms remotely. With DfT seed funding V2G EVSE have developed a low cost "smart" 0G to 5G enabled communications controller with a wide range of input/output, currently configured to monitor and control an electric vehicle charging station and securely communicate with a cloud based management system. We will repurpose our existing technology by connecting the controller to a microphone and infra-red camera, then use novel machine learning algorithms to detect the characteristic cough and fever of Covid-19 sufferers. This will allow us to create an installed or hand held device that can be used to identify those with potential Covid-19 symptoms. Since our existing hardware is effectively the internals of a smartphone with no touch screen but more versatile communications, including Bluetooth, we are also ideally placed to participate in the recently announced Apple/Google track/trace initiative. The COVeHealth project will develop proof of concept prototypes of a "commercial" version suitable for use at the entrance to public spaces and an alternative low-cost "domestic" version for use hand held or in confined spaces.    UK Research and Innovation    Seed Award in Science    50000.0GBP
380    Dr Secrier Maria    OKEO LIMITED    2019-06-17    2021-06-17    OKEO - COVID-19 financial modelling    OKEO is a London-based challenger credit card company that uses Open Banking data and machine learning to provide young adults with access to affordable credit. OKEO proposes to develop innovative affordability and credit risk models using current "crisis data" to help OKEO lend money to financially excluded young adults (aged between 20 to 30 years old), self-employed contractors and gig economy workers at low rates during the pandemic. By utilising innovative data modelling and lending to critical sectors of the economy, OKEO's vision is to help meet the needs of financially excluded individuals and help stimulate the UK economy. OKEO is innovating now to respond to the financial needs of individuals brought on by COVID-19 and a weakened economy.    UK Research and Innovation    Seed Award in Science    49815.0GBP
381    Dr Thomas T Hannah Mary    Christian Medical College, Vellore    2020-01-01    2024-12-31    Radiomics based tumor phenotypes to predict individual risk and treatment response in head and neck cancer    OKEO is a London-based challenger credit card company that uses Open Banking data and machine learning to provide young adults with access to affordable credit. OKEO proposes to develop innovative affordability and credit risk models using current "crisis data" to help OKEO lend money to financially excluded young adults (aged between 20 to 30 years old), self-employed contractors and gig economy workers at low rates during the pandemic. By utilising innovative data modelling and lending to critical sectors of the economy, OKEO's vision is to help meet the needs of financially excluded individuals and help stimulate the UK economy. OKEO is innovating now to respond to the financial needs of individuals brought on by COVID-19 and a weakened economy.    DBT/Wellcome Trust India Alliance    Early Career Fellowship    13521386INR
382    Dr Thomas T Hannah Mary    QUALIS FLOW LIMITED    2020-01-01    2024-12-31    Automating compliance for remote construction working    Construction logistics monitoring and compliance is typically paper-based and requires multiple individuals and teams to process, in order to ensure that material movements are handled in a safe and environmentally responsible manner. Qflow is a software tool, using a combination of Internet of Things and machine learning techniques, to digitise the information recovered from paper tickets at a construction site entrance. This ensures that this information is fed directly to the managers who need it most, and without the need for any on-site presence aside from the existing traffic marshals. Information is provided on the material types and waste removals, descriptions, relevant certifications of the supply chain, and listed quantities. This supports several teams to work remotely, including logistics, environmental, QS and data administration. The innovation that Qflow is exploring now is automated compliance screening and anomaly detection, to notify users when there are discrepancies in their data that require investigation, or where there is an illegal waste transfer. The software provides the eyes and ears on site in real-time, and provides access to that information on a cloud-based platform, meaning that those who need the data can access it wherever they are, without having to double handle paperwork and ensuring that the site operations continues smoothly without the need for intervention.    UK Research and Innovation    Early Career Fellowship    27374.0GBP
383    Dr. Clemens Jan    UNIVERSITAETSMEDIZIN GOETTINGEN - GEORG-AUGUST-UNIVERSITAET GOETTINGEN - STIFTUNG OEFFENTLICHEN RECHTS    2020-02-01    2025-01-31    Neural Computations Underlying Social Behavior in Complex Sensory Environments    Animals often interact in groups. Animal groups constitute complex sensory environments which challenge the brain and engage complex neural computations. This behavioral context is therefore fruitful for understanding how sophisticated neural computations give rise to behavior. However, it is also technically difficult since many of the relevant sensory cues arise from the members of the group and are therefore hard to quantify or control. Consequently, we only incompletely understand how the brain drives complex social behaviors in naturalistic contexts. To uncover the neural computations underlying social behavior in groups, we are using Drosophila, which provides unprecedented experimental access to the nervous system via genetic tools. Drosophila gathers on rotten fruit to feed and mate. Courtship and aggression dominate social interactions and rely on the recognition of sex-specific chemical cues and the production of context-specific acoustic signals. How are these multi-modal cues integrated to control and switch between courtship and aggression? How is unstable and conflicting sensory information resolved to promote stable behavioral strategies? How does sensory processing adapt to socially crowded environments in order to efficiently target behavior at individual members of the group? These issues will be addressed by combining computational modeling and genetic tools. Using machine learning, we will quantify and model the fine structure of social interactions to identify the social cues that drive behavior. Closed-loop optogenetics and calcium imaging in behaving animals will allow us to test the models and to ultimately reveal how the brain integrates, selects and combines social cues to drive social interactions. This multi-disciplinary approach will uncover the computational principles and mechanisms by which sensory information is processed to drive behavior in the complex sensory environment of animal groups.    European Research Council    Starting Grant    1476920.0EUR
384    Dr. Ruigrok Ynte    UNIVERSITAIR MEDISCH CENTRUM UTRECHT    2020-02-01    2025-01-31    Early recognition of intracranial aneurysms to PRevent aneurYSMal subarachnoid hemorrhage    Intracranial aneurysms usually go undetected until rupture occurs leading to aneurysmal subarachnoid hemorrhage (ASAH), a type of stroke with devastating effects. Early detection and preventive treatment of aneurysms fall short as we do not know who is it at risk and why, as we have insufficient insight in the contribution and interplay of genetic, environmental and intermediate phenotypic risk factors. Given the rarity of the disease, there is a paucity of large and rich cohorts to study risk factors separately with sufficient power. To add to the problem, my preliminary findings suggest disease heterogeneity with subgroup specific risk factors for aneurysms. The sex-related heterogeneity is most eminent in the disease with 2/3 of patients being women. I aim to advance disease understanding to allow early recognition of intracranial aneurysms to prevent ASAH. I have established a new conceptual approach that integrates genetic and environmental risk factors with imaging markers as intermediate phenotypes for genetic factors. With data reduction and machine-learning approaches I will for the first time address disease heterogeneity and aneurysm risk with adequate power. I will develop and validate a tool to automatically detect new imaging markers predicting aneurysm development applying feature-learning models. Next I will elucidate the genetic basis underlying differential imaging risk patterns (imaging genetic factors). I will apply a new hypothesis-free strategy to detect and validate yet unknown environmental risk factors predicting aneurysm presence. I will assess the contribution to disease of all factors detected according to sex. All risk factors will be combined in an aneurysm prediction risk model to understand relative contribution of different risk factors in different subgroups. It will advance disease understanding and individualized risk prediction of aneurysms leading to precision medicine in early aneurysm detection to reduce the burden of ASAH.    European Research Council    Starting Grant    1499108.0EUR
385    Dr. Philiastides Marios    University of Glasgow    2020-09-01    2025-08-31    Dynamic Network Reconstruction of Human Perceptual and Reward Learning via Multimodal Data Fusion    Training and experience can lead to long-lasting improvements in our ability to make decisions based on either ambiguous sensory or probabilistic information (e.g. learning to diagnose a noisy x-ray image or betting on the stock market). These two processes are referred to as perceptual and probabilistic/reward learning, respectively. Despite considerable efforts to uncover the neural systems involved in these processes, perceptual and reward learning have largely been studied in separate lines of research using divergent learning mechanisms. The primary aim of this proposal is to develop a unified framework for integrating these lines of research and understand the extent to which they share a common computational and neurobiological basis. Specifically, we will test the proposition that both the perceptual and reward systems could be understood in a common framework of “reward maximization”, whereby a domain-general reinforcement-guided learning mechanism – based on separate prediction error representations – facilitates future actions and adaptive behavior. To offer a comprehensive spatiotemporal characterization of the relevant networks and their computational principles we will adopt a state-of-the-art multimodal neuroimaging approach to fuse simultaneously-acquired EEG and fMRI data, via machine-learning-inspired multivariate single-trial analysis techniques and computational modelling. The project’s ultimate goal is to empower a level of neuronal and mechanistic understanding that extends beyond what could be inferred with each of these modalities in isolation. We will achieve this goal by exploiting endogenous trial-by-trial electrophysiological variability to build parametric fMRI predictors that can offer additional explanatory power than what can already be achieved by stimulus- or behaviorally-derived predictors, allowing us to go over and beyond what has been reported previously in the literature.    European Research Council    Consolidator Grant    1996043.0EUR
386    Dr. EEFTENS Marloes    SCHWEIZERISCHES TROPEN- UND PUBLIC HEALTH-INSTITUT    2020-05-01    2025-04-30    Beyond seasonal suffering: Effects of Pollen on Cardiorespiratory Health and Allergies    As climate change increases the duration and intensity of the pollen season, allergies to airborne pollen are increasingly common in Europe. Yet, it is not well recognized that high pollen concentrations may increase respiratory and cardiovascular events, leading to mortality and excess hospitalizations. I aim to investigate how short-term exposure to pollen is related to mortality, hospitalization and allergic symptoms, both on its own and synergistically with air pollution and weather. I will develop spatiotemporal exposure models of pollen for the years 2003-2022 based on a network of 14 pollen measurements stations in Switzerland. Taking advantage of large, real-world datasets without selection bias (Swiss National Cohort) and the efficient case-crossover study design, I will investigate the population effects of pollen on daily respiratory and cardiovascular mortality and hospitalization, also accounting for variation in air pollution and weather conditions. To explore individual sensitivity, I will conduct repeated measurements of lung function and airway inflammation in a dedicated panel of 400 allergic patients complemented with opportunistic repeated accounts of self-reported symptoms from the “e-symptoms” app by Swiss Allergy Centre. To provide personalized prevention recommendations and enhance quality of life for the allergic population, I will derive exposure-response relationships based on prevalent pollen, air pollution and weather triggers and individual symptom reports, allowing me to ultimately forecast symptom severity using machine learning techniques. This highly innovative project utilizes available nationwide health datasets and systematic novel data collection methods (in the in-depth panel study), to better understand the role of pollen in respiratory and cardiovascular diseases at both personalized and population levels. The project will prevent and reduce health effects due to pollen, which constitute a large burden on health and economy.    European Research Council    Starting Grant    1381932.0EUR
387    Dr. Zlatic Marta    THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE    2019-09-01    2024-08-31    Principles of Learning in a Recurrent Neural Network    Forming memories, generating predictions based on memories, and updating memories when predictions no longer match actual experience are fundamental brain functions. Dopaminergic neurons provide a so-called “teaching signal” that drives the formation and updates of associative memories across the animal kingdom. Many theoretical models propose how neural circuits could compute the teaching signals, but the actual implementation of this computation in real nervous systems is unknown. This project will discover the basic principles by which neural circuits compute the teaching signals that drive memory formation and updates using a tractable insect model system, the Drosophila larva. We will generate, for the first time in any animal, the following essential datasets for a distributed, multilayered, recurrent learning circuit, the mushroom body-related circuitry in the larval brain. First, building on our preliminary work that provides the synaptic-resolution connectome of the circuit, including all feedforward and feedback pathways upstream of all dopaminergic neurons, we will generate a map of functional monosynaptic connections. Second, we will obtain cellular-resolution whole-nervous system activity maps in intact living animals, as they form, extinguish, or consolidate memories to discover the features represented in each layer of the circuit (e.g. predictions, actual reinforcement, and prediction errors), the learning algorithms, and the candidate circuit motifs that implement them. Finally, we will develop a model of the circuit constrained by these datasets and test the predictions about the necessity and sufficiency of uniquely identified circuit elements for implementing learning algorithms by selectively manipulating their activity. Understanding the basic functional principles of an entire multilayered recurrent learning circuit in an animal has the potential to revolutionize, not only neuroscience and medicine, but also machine-learning and robotics.    European Research Council    Consolidator Grant    2350000.0EUR
388    Dr Guo Ya    MRC London Institute of Medical Sciences    2018-07-01    2020-06-01    NPIF Rutherford Fellowship    Gene regulation is of fundamental biological interest and has immediate medical relevance, as it is essential for normal development, and often disrupted in cancer. Gene regulation takes place in the context of chromatin states and 3D chromatin contacts. This field of study has recently been enriched by the application of machine learning/AI approaches to classify chromatin states and chromatin contacts, and by experimental evidence indicating that concepts of self-organised criticality and phase transitions can be productively applied to transitions between chromatin states and the formation of 3D chromatin contacts. This proposal builds on our recent discovery that cohesin and CTCF are selectively required for the regulation of genes that are dynamically expressed during cellular activation and differentiation. This includes genes programmed for up- or downregulated during mouse T cell differentiation, and genes that respond to inflammatory signals in mouse macrophages. Cohesin mutations are recurrent in human acute myeloid leukaemia (AML), and primary AML cells with cohesin mutations show reduced expression of inducible pro-inflammatory genes. As inflammatory signals promote the differentiation of myeloid cells, our data suggest a mechanism for the selection of cohesin mutations whereby AML cells with reduced expression of inflammatory genes evade differentiation in favour of self-renewal.Focus of this project is to understand the mechanisms that underlie the dependence of developmentally regulated and stimulus-responsive genes on genome organiser proteins. We will use molecular biology and genome informatics tools to assemble comprehensive maps of chromatin state (histone modifications, including numerous ChIP-seq data sets from ENCODE, chromatin accessibility, transcription factor binding, transcription) and chromatin contacts (Hi-C). We imagine that the dynamic regulation of inducible and developmental genes may rely on a spectrum of chromatin states that represent intermediates between full activation and maximal repression. These in-between states may be maintained by specific chromatin features (such as bivalency) and by active formation of chromatin contacts by cohesin and CTCF, which counteracts the stable segregation of active and silent chromatin regions. We will apply machine learning/AI approaches to classify these states and to identify features that predict dependency on genome organiser proteins. The results we will uncover new principles of genome organisation and transcriptional regulation and improve our understanding of cohesin mutations in cancer.    Medical Research Council    Fellowship    132676.0GBP
389    Dr Keevil Victoria    University of Cambridge    2019-10-07    2022-10-06    Big data analysis of electronic hospital records: inpatient trajectories and pharmacological exposures associated with mortality in older adults.    Gene regulation is of fundamental biological interest and has immediate medical relevance, as it is essential for normal development, and often disrupted in cancer. Gene regulation takes place in the context of chromatin states and 3D chromatin contacts. This field of study has recently been enriched by the application of machine learning/AI approaches to classify chromatin states and chromatin contacts, and by experimental evidence indicating that concepts of self-organised criticality and phase transitions can be productively applied to transitions between chromatin states and the formation of 3D chromatin contacts. This proposal builds on our recent discovery that cohesin and CTCF are selectively required for the regulation of genes that are dynamically expressed during cellular activation and differentiation. This includes genes programmed for up- or downregulated during mouse T cell differentiation, and genes that respond to inflammatory signals in mouse macrophages. Cohesin mutations are recurrent in human acute myeloid leukaemia (AML), and primary AML cells with cohesin mutations show reduced expression of inducible pro-inflammatory genes. As inflammatory signals promote the differentiation of myeloid cells, our data suggest a mechanism for the selection of cohesin mutations whereby AML cells with reduced expression of inflammatory genes evade differentiation in favour of self-renewal.Focus of this project is to understand the mechanisms that underlie the dependence of developmentally regulated and stimulus-responsive genes on genome organiser proteins. We will use molecular biology and genome informatics tools to assemble comprehensive maps of chromatin state (histone modifications, including numerous ChIP-seq data sets from ENCODE, chromatin accessibility, transcription factor binding, transcription) and chromatin contacts (Hi-C). We imagine that the dynamic regulation of inducible and developmental genes may rely on a spectrum of chromatin states that represent intermediates between full activation and maximal repression. These in-between states may be maintained by specific chromatin features (such as bivalency) and by active formation of chromatin contacts by cohesin and CTCF, which counteracts the stable segregation of active and silent chromatin regions. We will apply machine learning/AI approaches to classify these states and to identify features that predict dependency on genome organiser proteins. The results we will uncover new principles of genome organisation and transcriptional regulation and improve our understanding of cohesin mutations in cancer.    Medical Research Council    Research Grant    302805.0GBP
390    Ms Townson Julia    Cardiff University    2019-12-01    2021-03-01    Developing a predictive tool to promote earlier diagnosis of Type 1 diabetes in childhood for use in primary care    Aims: To develop and validate a predictive tool to help primary care healthcare professionals identify children at risk of Type 1 Diabetes (T1D), aiding early diagnosis and preventing the morbidity and mortality associated with late presentation. Background: Early diagnosis of childhood T1D is critical to avoid diabetic ketoacidosis (DKA). Delayed diagnosis and misdiagnosis are significant risk factors. Identifying children developing T1D amongst many others with similar symptoms who do not have diabetes, is challenging for GPs. Methods: We will use machine learning to develop a predictive model using a contemporary data set of linked primary care records (SAIL databank) and a secondary care diagnostic database (Brecon Cohort) in Wales. The predictive model will be tested in an independent data set, by linking English primary and secondary care datasets. The performance of the model to promote earlier diagnoses, reduce DKA and false positive diagnoses will be assessed. We will then hold a workshop of key stakeholders to determine the utility of the model and how we may test its effectiveness in future. Conclusion: Potentially, the predictive tool could be used within primary care computer systems to prompt GPs to conduct a finger prick test for T1D.    Diabetes UK    Project Grant    105447.0GBP
391    Dr Leyrat Clemence    London Sch of Hygiene and Trop Medicine    2020-04-01    2023-03-31    Enhancing the design and analysis of cluster randomised trials using machine learning    Aims: To develop and validate a predictive tool to help primary care healthcare professionals identify children at risk of Type 1 Diabetes (T1D), aiding early diagnosis and preventing the morbidity and mortality associated with late presentation. Background: Early diagnosis of childhood T1D is critical to avoid diabetic ketoacidosis (DKA). Delayed diagnosis and misdiagnosis are significant risk factors. Identifying children developing T1D amongst many others with similar symptoms who do not have diabetes, is challenging for GPs. Methods: We will use machine learning to develop a predictive model using a contemporary data set of linked primary care records (SAIL databank) and a secondary care diagnostic database (Brecon Cohort) in Wales. The predictive model will be tested in an independent data set, by linking English primary and secondary care datasets. The performance of the model to promote earlier diagnoses, reduce DKA and false positive diagnoses will be assessed. We will then hold a workshop of key stakeholders to determine the utility of the model and how we may test its effectiveness in future. Conclusion: Potentially, the predictive tool could be used within primary care computer systems to prompt GPs to conduct a finger prick test for T1D.    Medical Research Council    Fellowship    301682.0GBP
392    Dr Reynard Charles    The University of Manchester    2020-04-01    2023-03-31    Optimising diagnostic efficiency in the Emergency Department by using advanced machine learning methods to update and personalise a contemporary clinical prediction model for early identification and exclusion of acute coronary syndromes    Aims: To develop and validate a predictive tool to help primary care healthcare professionals identify children at risk of Type 1 Diabetes (T1D), aiding early diagnosis and preventing the morbidity and mortality associated with late presentation. Background: Early diagnosis of childhood T1D is critical to avoid diabetic ketoacidosis (DKA). Delayed diagnosis and misdiagnosis are significant risk factors. Identifying children developing T1D amongst many others with similar symptoms who do not have diabetes, is challenging for GPs. Methods: We will use machine learning to develop a predictive model using a contemporary data set of linked primary care records (SAIL databank) and a secondary care diagnostic database (Brecon Cohort) in Wales. The predictive model will be tested in an independent data set, by linking English primary and secondary care datasets. The performance of the model to promote earlier diagnoses, reduce DKA and false positive diagnoses will be assessed. We will then hold a workshop of key stakeholders to determine the utility of the model and how we may test its effectiveness in future. Conclusion: Potentially, the predictive tool could be used within primary care computer systems to prompt GPs to conduct a finger prick test for T1D.    National Institute for Health Research (Department of Health)    Fellowship    230920.0GBP
393    Dr Reynard Charles    Georgia State University Research Foundation, Inc.    2020-04-15    2021-03-31    Collaborative Research: RAPID: RTEM: Rapid Testing as Multi-fidelity Data Collection for Epidemic Modeling    Biological Sciences - The novel coronavirus (COVID-19) epidemic is generating significant social, economic, and health impacts and has highlighted the importance of real-time analysis of the spatio-temporal dynamics of emerging infectious diseases. COVID-19, which emerged out of the city of Wuhan in China in December 2019 is now spreading in multiple countries. It is particularly concerning that the case fatality rate appears to be higher for the novel coronavirus than for seasonal influenza, and especially so for older populations and those with prior health conditions such as cardiovascular disease and diabetes. Any plan for stopping the epidemic must be based on a quantitative understanding of the proportion of the at-risk population that needs to be protected by effective control measures in order for transmission to decline sufficiently and quickly enough for the epidemic to end. Different data collection and testing modalities and strategies available to help calibrate transmission models and predict the spread/severity of a disease, have variable costs, response times, and accuracies. In this Rapid Response Research (RAPID) project, the team will examine the problem of establishing optimal practices for rapid testing for the novel coronavirus. The result will be the Rapid Testing for Epidemic Modeling (RTEM), which will translate into science-based predictions of the COVID-19 epidemic's characteristics, including the duration and overall size, and help the global efforts to combat the disease. The RTEM will fill an important gap in data-driven decision making during the COVID-19 epidemic and, thus, will enable services with significant national economic and health impact. The educational impact of the project will be on mentoring of post-doctoral and PhD researchers and on curricula by incorporating research challenges and outcomes into existing undergraduate and graduate classes. <br/><br/>Computational models for the spatio-temporal dynamics of emerging infectious diseases and data- and model-driven computer simulations for disease spreading are increasingly critical in predicting geo-temporal evolution of epidemics as well as designing, activating, and adapting practices for controlling epidemics. In this project, the researchers tackle a Rapid Testing for Epidemic Modeling (RTEM) problem: Given a partially known target disease model and a set of testing modalities (from surveys to surveillance testing at known disease hotspots), with varying costs, accuracies, and observational delays, what is the best rapid testing strategy that would help recover the underlying disease model? Several scientific questions arise: What is the value of testing? Should only sick people be tested for virus detection? What level of resources should be devoted to the development of highly accurate tests (low false positives, low false negatives)? Is it better to use only one type of test aiming at the best cost/effectiveness trade off, or a non-homogeneous testing policy? Naturally these questions need to be investigated at the interface of epidemiology, computer science, machine learning, mathematical modeling and statistics. As part of the work, the team will develop a model of transmission dynamics and control, tailored to COVID-19 in a way that accommodates diagnostic testing with varying fidelities and delays underlying a rapid testing regimen. The investigators will further integrate the resulting RTEM-SEIR model with EpiDMS and DataStorm for executing continuous coupled simulations. <br/><br/>This project is jointly funded through the Ecology and Evolution of Infectious Diseases program (Division of Environmental Biology) and the Civil, Mechanical and Manufacturing innovation program (Engineering).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    67000.0USD
394    Dr Reynard Charles    University of California-San Diego    2020-05-01    2021-04-30    RAPID: Explainable Machine Learning for Analysis of COVID-19 Chest CT    Computer and Information Science and Engineering - In December 2019, it was discovered that a widely contagious pneumonia was caused by a new coronavirus infection now named COVID-19. The primary test for detection of the virus is real-time polymerase chain reaction (RT-PCR) with sensitivity of approximately 71% in some studies. However, this test may require several days to provide a result. Perhaps more importantly, imaging with x-ray or computed tomography (CT) are required to confirm pneumonia, which is the principal cause of death, as it leads to acute respiratory distress syndrome (ARDS). Recent studies have shown sensitivity of chest CT for approximately 98% for COVID-19 pneumonia and could provide immediate results but currently require human interpretation. Given the need for rapid, more accurate diagnosis, this project will use, adapt, and evaluate explainable machine learning techniques to diagnosis of COVID-19 pneumonia. This project will improve the understanding of mechanisms of COVID-19 and will help mitigate its impacts.<br/><br/>Viral nucleic acid detection using real-time polymerase chain reaction (RT-PCR) is the primary method for diagnosis of COVID-19 infection, which has rapidly spread worldwide as a global pandemic. Sensitivity of this test for COVID-19 infection has been estimated at approximately 71% in some studies and may require several days for a result. X-ray and CT imaging are complementary technologies that allow diagnosis of COVID-19 pneumonia, which can evolve to acute respiratory distress syndrome (ARDS) -- the principal cause of death in patients with COVID-19 infection. Especially early in the course of the disease, chest CT has multiple advantages over RT-PCR yielding results more quickly and is already widely deployed, but requires expert radiologist interpretation. The number of chest CTs may rapidly exceed the speed and capacity of already strained radiologists. An explainable machine learning algorithm may address this disadvantage to expedite the interpretation of chest CT and assist rapid triage of patients to the ICU, inpatient ward, monitoring unit, or home self-quarantine. Machine learning algorithms, specifically those leveraging deep convolutional neural networks (deep learning), have the potential for facilitating even more rapid diagnosis within minutes. This project seeks to validate the use of explainable deep learning methods to adjust diagnostic operating points for multiple applications, including (a) disease screening, (b) disease staging and prognostication, and (c) evaluation of treatment response.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    101643.0USD
395    Dr. FLEISHMAN Sarel-Jacob    WEIZMANN INSTITUTE OF SCIENCE LTD    2019-01-01    2023-12-31    Automated computational design of site-targeted repertoires of camelid antibodies    We propose to develop the first high-throughput strategy to design, synthesize, and screen repertoires comprising millions of single-domain camelid antibodies (VHH) that target desired protein surfaces. Each VHH will be individually designed for high stability and target-site affinity. We will leverage recent methods developed by our lab for designing stable, specific, and accurate backbones at interfaces, the advent of massive and affordable custom-DNA oligo synthesis, and machine learning methods to accomplish the following aims: Aim 1: Establish a completely automated computational pipeline that uses Rosetta to design millions of VHHs targeting desired protein surfaces. The variable regions in each design will be encoded in DNA oligo pools, which will be assembled to generate the entire site-targeted repertoire. We will then use high-throughput binding screens followed by deep sequencing to characterize the designs’ target-site affinity and isolate high-affinity binders. Aim 2: Develop an epitope-focusing strategy that designs several variants of a target antigen, each of which encodes dozens of radical surface mutations outside the target site to disrupt potential off-target site binding. The designs will be used to isolate site-targeting binders from repertoires of Aim 1. Each high-throughput screen will provide unprecedented experimental data on target-site affinity in millions of individually designed VHHs. Aim 3: Use machine learning methods to infer combinations of molecular features that distinguish high-affinity binders from non binders. These will be encoded in subsequent designed repertoires, leading to a continuous “learning loop” of methods for high-affinity, site-targeted binding. AutoCAb’s interdisciplinary strategy will thus lead to deeper understanding of and new general methods for designing stable, high-affinity, site-targeted antibodies, potentially revolutionizing binder and inhibitor discovery in basic and applied biomedical research.    European Research Council    Consolidator Grant    2337500.0EUR
396    Professor Murray Alison    University of Aberdeen    2018-03-01    2019-02-28    Early-life origins of brain resilience to mental illness and cognitive impairment across the life-course    We propose to develop the first high-throughput strategy to design, synthesize, and screen repertoires comprising millions of single-domain camelid antibodies (VHH) that target desired protein surfaces. Each VHH will be individually designed for high stability and target-site affinity. We will leverage recent methods developed by our lab for designing stable, specific, and accurate backbones at interfaces, the advent of massive and affordable custom-DNA oligo synthesis, and machine learning methods to accomplish the following aims: Aim 1: Establish a completely automated computational pipeline that uses Rosetta to design millions of VHHs targeting desired protein surfaces. The variable regions in each design will be encoded in DNA oligo pools, which will be assembled to generate the entire site-targeted repertoire. We will then use high-throughput binding screens followed by deep sequencing to characterize the designs’ target-site affinity and isolate high-affinity binders. Aim 2: Develop an epitope-focusing strategy that designs several variants of a target antigen, each of which encodes dozens of radical surface mutations outside the target site to disrupt potential off-target site binding. The designs will be used to isolate site-targeting binders from repertoires of Aim 1. Each high-throughput screen will provide unprecedented experimental data on target-site affinity in millions of individually designed VHHs. Aim 3: Use machine learning methods to infer combinations of molecular features that distinguish high-affinity binders from non binders. These will be encoded in subsequent designed repertoires, leading to a continuous “learning loop” of methods for high-affinity, site-targeted binding. AutoCAb’s interdisciplinary strategy will thus lead to deeper understanding of and new general methods for designing stable, high-affinity, site-targeted antibodies, potentially revolutionizing binder and inhibitor discovery in basic and applied biomedical research.    Medical Research Council    P&Cs    192240.0GBP
397    Professor Murray Alison    University of Chicago    2020-05-01    2021-04-30    RAPID: Data-driven Multiscale Integrative Model of the Coronavirus Virion    Mathematical and Physical Sciences - Gregory Voth of the University of Chicago is supported by this RAPID award to develop and deploy multiscale models of the entire SARS-CoV-2 virus, the virus that causes the novel coronavirus infectious disease 2019 (COVID-19). Such multiscale models, at both the atomistic and coarse grain levels, contribute greatly to our understanding of how this virus replicates. Molecular simulations of viral processes in COVID-19 are useful to identify possible weaknesses in the viral life cycle. This research focuses on the dynamics of coronavirus processes, including the conformational transitions that are required for the virus to function. The project has three main foci: 1) all-atom simulations of individual viral proteins that are essential to the viral life cycle; 2) a coarse-grain models to a holistic understanding of entire virion (the virus outside the host cell) and its large scale processes, such as fusion of virions with host cells; and 3) machine-learning-based approaches to link the all-atom and coarse grain models and further refine their accuracy. As part of a larger, international community working on COVID-19, all data, models and analysis code will be made publicly available as soon as they are developed, including through the NSF-funded Molecular Science Software Institute (MolSSI). The complete multiscale picture of virus structure and dynamics will be used to identify potential target sites for drug development and other therapeutic strategies.<br/><br/><br/>The research in this RAPID project is for the development and application of multiscale computer simulation methods to characterize key elements of large-scale viral processes in SARS-CoV-2 replication. To achieve this goal there are three main objectives: (1) to characterize the dynamical behavior of essential viral proteins involved using all-atom molecular dynamics simulations and understand the conformational transitions necessary for their function; (2) to develop and model the complete SARS-CoV-2 virion using coarse-grained simulation methods; and (3) to develop machine learning based approaches that systematically link atomic-level and coarse-grained simulation scales, and facilitate the generation of even more accurate and descriptive coarse-grained models. This research focuses on several biomolecular systems that are urgently needed to understand and characterize the transmission and propagation of the SARS-CoV-2 virus, including the spike protein that mediates entry of the viral particles into host cells, the host cell receptor, angiotensin-converting-enzyme 2, which binds the spike protein, coronavirus protease which catalyzes viral processes, and other viral protein components, especially as structural data and biochemical information are released in the next few months. Coarse-grained simulations will focus on the urgent need to develop a holistic model of the entire SARS-CoV-2 virion as well as its large-scale processes such as the fusion of virions with host cells.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    190595.0USD
398    Professor Murray Alison    Virginia Polytechnic Institute and State University    2020-05-01    2021-04-30    RAPID: MolSSI COVID-19 Biomolecular Simulation Data and Algorithm Consortium    Computer and Information Science and Engineering - In response to the growing COVID-19 pandemic, the Molecular Sciences Software Institute (MolSSI) will leverage its position as a neutral commodity resource to help the global computational molecular sciences community quickly provide their scientific data and expertise to address the COVID-19 crisis. The MolSSI is jointly supported by the Office of Advanced Cyberinfrastructure and the Divisions of Chemistry and Materials Research. The centerpieces of this engagement will be (1) a centralized repository for simulation-related data targeting the virus and host proteins and potential pharmaceuticals, and (2) a select set of MolSSI Software Seed Fellowships for Ph.D. students and postdocs targeting COVID-19 related software tools that operate on the data developed in the repository. These two components will enable the biomolecular simulation community to share and utilize key data and other resources to help identify the structural and dynamic characteristics of the host-virus complex to generate potential leads for therapeutics. Although this project is intended to address the acute COVID-19 crisis, in the near term, it also will impact research communities and the next generation of computational molecular scientists in the confrontation and proactive resolution of future world problems.<br/><br/>The MolSSI will create and curate a large-scale repository containing: simulation input files (structures, configurations, scripts, Jupyter notebooks) in an organized structure; MD trajectories, analysis tools, and ready models for drug discovery; pointers to preprint servers such as arXiv, bioRxiv, and ChemRxiv on biomolecular simulation research in regards SARS-CoV-2; and DOI services that create citable data. In addition, it will engage the molecular sciences community through a set of Software Fellowships for graduate student and postdocs to carry out software development, such as large-scale MD simulations, design of drug discovery tools such as docking, machine learning for small molecule toxicity predictions, and methods for determining whether new drugs are bioavailable or can be synthesized. Collectively, these resources will speed the identification and development of leads for antiviral drugs, analyzing structural effects of genetic variation in the SARS-CoV-2 virus, and inhibitors that can disrupt protein-protein interactions to viral entry into cells and adherence to surfaces that cause disease spread.<br/><br/>This award is being funded by the CARES Act supplemental funds allocated to CISE and MPS.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    200000.0USD
399    Professor Murray Alison    University of Wisconsin-Milwaukee    2020-05-01    2022-04-30    EAGER: Functionally Relevant Structural Heterogeneity in Coronavirus SARS-CoV2 Proteins    Biological Sciences - In order to prevent infections by specific viruses, it is important to understand the molecular details that drive the virus into host cells where they replicate, making more viral particles that spread to other cells in the infected individual. This award will help understand the mechanisms of the SARS-CoV2 infectivity, the virus responsible for the current COVID-19 pandemic, by employing machine learning algorithms to make movies of key proteins involved in driving its infection. There is mounting evidence that viral proteins exist in a range of structures, known as conformations, and that these can play a critical role in their function. In this project, recently developed machine-learning techniques will be used to determine the conformational landscape of key SARS-CoV2 proteins at near-atomic level, with and without antibody involvement. Atomistic insight into the conformational changes in SAR-CoV2 proteins is expected to help clarify the structural basis of virulence in this virus and its successors, ultimately providing a foundation for the development of suitable therapeutic strategies against coronaviruses. <br/><br/>Using experimental cryo-EM snapshots, this project will map the functionally relevant conformational heterogeneities of key SARS-CoV2 proteins to gain a deeper understanding of the role of conformational heterogeneity in this pandemic virus. The specific goals of this project are as follows: (1) Apply advanced machine-learning algorithms to experimental cryo-EM single-particle snapshots in order to determine the energy landscapes of key SARS-CoV2 proteins with and without antibody involvement; (2) Identify the functionally important conformational paths on the relevant energy landscapes; (3) Compare and contrast motions along functional paths with those inferred by discrete clustering methods; (4) Determine the biological implications of conformational motions associated with function; and (5) Make the results widely accessible in order to help facilitate the development of therapeutic strategies.<br/><br/>This RAPID award is made by the Division of Biological Infrastructure (DBI) using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    299723.0USD
400    Dr Wijeratne Peter    University College London    2019-09-01    2022-08-31    Computational models for clinical trial design in Huntington's disease    None    Medical Research Council    Fellowship    273137.0GBP
401    Professor Holmans Peter    Cardiff University    2006-09-01    2009-08-31    MSc in Bioinformatics    The Biostatistics and Bioinformatics Unit (BBU), in close collaboration with five internationally recognised cancer research groups with substantial Cancer Research UK support, requests funding to enable cancer researchers (or future cancer researchers) to study on an MSc course in bioinformatics. We seek this funding to enable highly motivated individuals to train in bioinformatics in order to boost the use of higher level bioinformatics in cancer research. The course will develop the appropriate complementary skills to provide students with the multidisciplinary bioinformatic skill set required to work effectively in the post genomic era in cancer research. The course contains core modules in computer science, statistics and the use of bioinformatics in the postgenomic era and specialist modules in databasing, machine learning and data mining, algorithmic aspects of sequence analysis and molecular modelling. Students complete a mini-research project as well as a full-time 3 month research project and the latter two components are embedded within the collaborating groups. We envisage individuals already in cancer research will obtain these bursaries in order to develop new skills but in some circumstances highly motivated individuals with no previous cancer research experience but with clear aspirations to move into cancer research may be accepted.    Cancer Research UK    Bursary    273137.0GBP
402    Dr Chan Laureen Lui Yan    Cardiff University    2010-09-01    2011-08-31    MSc in Bioinformatics and Genetic Epidemiology & Bioinformatics    The aim of the MSc Bioinformatics programme is to gain skills in computational power to investigate genetic epidemiology and post-genomics biology. The exponential growth of epigenetics and genomic data facilitates innovative approaches to investigate risk of having cancer and underlying mechanisms. As some genes play a dual role in cancer and obesity, association between cancer and obesity can be assessed. I have reported that Bitter melon possesses anti-diabetic and anti-obesity effects. Recent studies revealed its anti-tumour property. It was shown that Bitter melon decreased O6-methylguanine DNA adduct in colonic mucosa. It is hypothesized that localization of CpG island in genes and hence mechanisms could substantiate effects of Bitter melon in obesity-related cancer. Besides, maternal nutrition remodels underlying mechanism of obesity which may exaggerate the growth of tumours and then hinder treatment outcome. Genetics and epigenetics may be clues to delineate mechanisms of obesity-related cancer. The effects of diet on the obesity-cancer association could be investigated in network pathway analysis. Genetic epidemiology and artificial biological models can be established to study time-dependent effects. Having been trained in machine learning, pattern recognition, genetic epidemiology, likelihood and quantitative trait analysis, cutting edge methods could be used to further investigate these polygenic diseases.    Cancer Research UK    Bursary    273137.0GBP
403    Dr Nangalia Vishal    University College London    2013-06-03    2016-08-02    Machine Learning - Early Warning System (ML-EWS)    The aim of the MSc Bioinformatics programme is to gain skills in computational power to investigate genetic epidemiology and post-genomics biology. The exponential growth of epigenetics and genomic data facilitates innovative approaches to investigate risk of having cancer and underlying mechanisms. As some genes play a dual role in cancer and obesity, association between cancer and obesity can be assessed. I have reported that Bitter melon possesses anti-diabetic and anti-obesity effects. Recent studies revealed its anti-tumour property. It was shown that Bitter melon decreased O6-methylguanine DNA adduct in colonic mucosa. It is hypothesized that localization of CpG island in genes and hence mechanisms could substantiate effects of Bitter melon in obesity-related cancer. Besides, maternal nutrition remodels underlying mechanism of obesity which may exaggerate the growth of tumours and then hinder treatment outcome. Genetics and epigenetics may be clues to delineate mechanisms of obesity-related cancer. The effects of diet on the obesity-cancer association could be investigated in network pathway analysis. Genetic epidemiology and artificial biological models can be established to study time-dependent effects. Having been trained in machine learning, pattern recognition, genetic epidemiology, likelihood and quantitative trait analysis, cutting edge methods could be used to further investigate these polygenic diseases.    Medical Research Council    Fellowship    260560.0GBP
404    Dr Hayes Joseph    University College London    2019-01-01    2022-01-01    Personalising the pharmacological treatment of bipolar disorder    The aim of the MSc Bioinformatics programme is to gain skills in computational power to investigate genetic epidemiology and post-genomics biology. The exponential growth of epigenetics and genomic data facilitates innovative approaches to investigate risk of having cancer and underlying mechanisms. As some genes play a dual role in cancer and obesity, association between cancer and obesity can be assessed. I have reported that Bitter melon possesses anti-diabetic and anti-obesity effects. Recent studies revealed its anti-tumour property. It was shown that Bitter melon decreased O6-methylguanine DNA adduct in colonic mucosa. It is hypothesized that localization of CpG island in genes and hence mechanisms could substantiate effects of Bitter melon in obesity-related cancer. Besides, maternal nutrition remodels underlying mechanism of obesity which may exaggerate the growth of tumours and then hinder treatment outcome. Genetics and epigenetics may be clues to delineate mechanisms of obesity-related cancer. The effects of diet on the obesity-cancer association could be investigated in network pathway analysis. Genetic epidemiology and artificial biological models can be established to study time-dependent effects. Having been trained in machine learning, pattern recognition, genetic epidemiology, likelihood and quantitative trait analysis, cutting edge methods could be used to further investigate these polygenic diseases.    Wellcome Trust    Clinical Research Career Development Fellowship    460704.0GBP
405    Dr Hayes Joseph    University of Tennessee Knoxville    2020-05-01    2021-04-30    RAPID: Impacts of Design and Operation Attributes of Mass-Gathering Civil Infrastructure Systems on Pathogen Transmission and Exposure    Engineering - This Rapid Response Research (RAPID) grant will support fundamental research to reveal how the design attributes and operation strategies will influence the transmission of and exposure to infectious pathogens within mass-gathering civil infrastructure systems. During the pandemic of 2019 novel coronavirus, the mass-gathering civil infrastructure systems, such as schools, airports, and public transit systems, can become hot spots for spreading the infectious disease. There remains a striking knowledge gap in understanding the impacts of infrastructure design and operation on the occurrence, distribution, transport, and viability of pathogens. It is imperative to address this knowledge gap. Results from this project will lead to bio-informed guidelines for managing critical civil infrastructure systems to prevent exposure of facility users to pathogenic microorganisms, and reduce risks of spreading infectious diseases, and thus alleviating burdens on healthcare systems and citizens. This project will provide much needed insights for infrastructure design reconfigurations and operation practices during the pandemic, in the recovery, and beyond to prevent further disease outbreaks and support healthy, resilient, and smart communities. As a result, this project will help promote public health, national security, and economic prosperity. In addition, this project will raise public science literacy and awareness of infectious diseases, and improve student education and training, as well as K-12 outreach and engagement activities. <br/><br/>The specific objective of this research is to parameterize relevant design attributes and operation strategies of infrastructure systems, and subsequently evaluate their impacts on pathogen transmission and exposure from spatiotemporal microbiome profiles. Three aims will be pursued: 1) identify and quantify the design attributes and operation strategies that may impact pathogen dynamics; 2) audit the types, abundance, and co-occurrence patterns, as well as spatiotemporal dynamics of microorganisms, particularly pathogens, associated with spatially and functionally distributed system components; and 3) characterize the impacts of design and operation on the transmission and exposure pathways of microorganisms in infrastructure systems. The spatial and functional interdependence of system components will be considered to parameterize design attributes based on building information modeling and syntactic analysis. The operation strategies will be modeled using integrated data sensing and simulation techniques. A model-informed sampling approach will be developed with molecular and metagenomics techniques to characterize spatiotemporal microbiome dynamics. The impacts of design and operation on microbial transmission and exposure pathways will be assessed using integrated source tracking and machine learning methods. At the nexus of infrastructure system engineering and environmental microbiology, this convergence research will provide unique insights into design attributes and operation practices impacting pathogen transmission and exposure in mass-gathering civil infrastructure systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    199809.0USD
406    Dr Hayes Joseph    Massachusetts Institute of Technology    2020-04-01    2021-03-31    RAPID: Immunogenicity of SARS-CoV2 to Human T Cells    Mathematical and Physical Sciences - Pandemics caused by infectious pathogens have plagued humanity since antiquity. The Coronavirus Disease 2019 (COVID-19) caused by the SARS-CoV-2 virus is currently spreading across the world rapidly, including in the United States, with major adverse impact on health and the economy. The SARSCoV-2 outbreak has led to several urgent efforts to develop vaccines that may offer protection against this virus. It is unknown as to whether the current approaches being pursued will elicit protective immune responses in humans. While vaccines have been very effective against many pathogens, the empirical methods for vaccine development pioneered by Pasteur and Jenner over two centuries ago have failed to produce effective vaccines against Human Immune Deficiency Virus, Malaria, Tuberculosis, and many other pathogens. Therefore, rational design of vaccines based on a mechanistic understanding of the pertinent virology and immunology is being pursued, and these efforts include work that is rooted in statistical physics. SARSCoV-2 is phylogenetically most similar to SARS-CoV. This project will use a machine learning approach to understand how the SARS-CoV-2 virus interacts with the immune T cells. This work will directly impact the design of SARS-CoV-2 vaccines and vaccines against future endemic-causing pathogens.<br/><br/>Analyses of patients who have recovered from SARS-CoV shows that antibody responses are not prevalent a few years later, but memory T cell responses are durable and may offer long-term protection. The main questions addressed by this project are 1. Will the SARS-CoV peptides targeted by human T cells that are mutated in SARS-CoV-2 still elicit human T cell responses - i.e. are they immunogenic? 2: Are the 102 peptides identified by host major histocompatibility molecules binding assays alone that are common between SARS-CoV and SARS-CoV-2 immunogenic in humans? If not, they are irrelevant from vaccine design perspective. The goal of the work proposed here is to take a physics-based machine learning approach to determine the immunogenicity of SARS-CoV-2 proteins to human T cell responses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    124472.0USD
407    Dr Hayes Joseph    University of Southern California    2020-05-01    2021-04-30    RAPID: ReCOVER: Accurate Predictions and Resource Allocation for COVID-19 Epidemic Response    Computer and Information Science and Engineering - The recent outbreak of COVID-19 and its world-wide impact calls for urgent measures to contain the epidemic. Predicting the speed and severity of infectious diseases like COVID-19 and allocating medical resources appropriately is central to dealing with epidemics. Epidemics like COVID-19 not only affect world-wide health, but also have profound economic and social impact. Containing the epidemic, providing informed predictions and preventing future epidemics is essential for the global population to resume their day-to-day work and travel without fear. Shortage of resources puts undue stress on healthcare system further risking health of the community. Preparedness and better management of available resources would require specific predictions at the level of cities and counties around the world rather than solely at the level of countries. The project will provide a predictive understanding of the spread of the virus by developing machine learning based computational models to study the transmission of the virus and evaluate the impact of various interventions on disease spread. The project will learn infection prediction models for COVID-19 considering the following. (i) Predicting at state/county/city-level rather than country-level as finer granularity is essential in planning and managing resources. (ii) How infectious a person is changes over time. Learning the model through observed data will help in understanding of the temporal nature of the virality. (iii) At such granularity travel is a significant reason for the spread and needs to be accounted for. (iv) Available data needs to be ?corrected? by finding the number of underlying unreported cases that are not observed and yet influence the epidemic dynamics. The project will also solve the resource allocation problem based on the prediction ? for instance if a certain number of masks will be available next week in a certain state, how should they be distributed across different hospitals in the state (which hospitals and how many in each state)?<br/><br/>Proposed project ReCOVER will use a novel fine-grained, heterogeneous infection rate model to perform predictions at various granularities (hospital/airports, city, state, country) while accounting for human mobility. ReCOVER will integrate data from various sources to build highly accurate models for prediction of the epidemic across the world at various granularity. Due to the ability to capture temporal heterogeneity in infection rate, the approach has the potential to provide insights into infectious nature of COVID-19 which are not fully understood yet. The project will address the issue of unreported cases through temporal analysis of historical infections and correct the data. The right granularities of modeling will be automatically identified, e.g., when to model a state over its cities to trade-off precision for higher reliability in predictions. The proposed project also formulates and solves a resource allocation problem that can guide the response to contain the epidemic and prevent future outbreaks. This is provided by optimal solutions to resource allocation over a network where each node (representing a region) has a function that captures probabilistic response. While the project obtains data with COVID-19 in consideration, the model and algorithms developed under the project are applicable to a wide class of contagious diseases. The project will culminate into an interactive customizable tool that can be used to perform predictions and resource management by a qualified user such as a government entity tasked with managing the epidemic response. The data and code will also be shared with research community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    158592.0USD
408    Dr Hayes Joseph    Colorado State University    2020-05-01    2021-04-30    RAPID: ENSURING INTEGRITY OF COVID-19 DATA AND NEWS ACROSS REGIONS    Computer and Information Science and Engineering - Large amounts of epidemiological data are being generated and collected from a variety of sources to understand the impact and propagation of COVID-19. Similarly, huge amounts of news articles are generated and disseminated about the pandemic to keep the population informed. The appropriateness of the actions taken by individuals, corporations, and governments are often based on the quality of data and news. Thus, ensuring the quality of data and news is important. However, malicious actors can alter the attributes of data records, insert spurious records, or suppress records causing any analysis to be inadequate and misinformation to be propagated. This project addresses the critical problem of defining and identifying spurious data and news concerning COVID-19, and tracking the source of misinformation. The project novelty lies in the development of an approach and associated toolset that adapts and combines Machine Learning technologies to detect spurious data and misinformation and presents the results in a manner that is easy for end users to understand and interpret. The approach detects discrepancies in COVID-19 data and traces the flagged discrepancies back to the data sources. The results obtained from the news sources and those obtained from the medical data analysis are compared to determine correlations between the quality of news and the degree and type of data manipulation performed at any region. The project?s impacts are on significantly enhancing the ability to perform accurate scientific analysis, and detecting and explaining news manipulation with respect to COVID-19. The scientific principles developed in the project are expected to be useful outside the medical domain. The PI and the students identified for this project are minorities. The project will be carried out in the Computer Science Department at Colorado State University which is a BRAID affiliate.<br/><br/>COVID-19 data discrepancies are related to (1) single records, where some field is modified, (2) sequence of records over time forming a temporal dimension, where spurious records have been inserted or records have been suppressed, and (3) sequences of records across regions forming a spatial dimension, where there is a pattern of manipulation or information disclosure across regions. The approach determines the appropriate combination of autoencoders, Long Short-Term Memory (LSTM), Temporal Convolution Network (TCNs), and Convolution Neural Networks (CNNs) that can work with data obtained from medical sources and news containing both spatial and temporal dimensions. The tools help the investigators? collaborators at the University of Colorado Anschutz Medical Center and Center for Disease Control and Prevention to perform data integrity checking of medical records and to provide explanations of integrity violations. The tools also handle different types of data and news alterations pertaining to COVID-19.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    199748.0USD
409    Dr Hayes Joseph    University of California-Irvine    2020-05-01    2021-04-30    RAPID: The Role of Emerging Virtual Cultures in the Prevention of COVID-19 Transmission    Social, Behavioral and Economic Sciences - The COVID-19 pandemic has transformed our relationship to the physical world. Social distancing guidelines have led many people to avoid all forms of public life, from concerts and restaurants to everyday interaction in parks, neighborhoods, and the homes of family and friends. In response there has been a massive increase in online interaction: the internet has suddenly become the primary way that many Americans socialize, labor, and learn. It is crucial to gain a better understanding of how the emergence of these changes is related to the pandemic. Even if a vaccine is discovered, preventing catastrophic levels of COVID-19 transmission into the next few years will depend on social distancing that can be sustained and integrated with work, education, and community. This means going online. The starting point for addressing this global challenge is thus the fact that what we call ?social distancing? is really physical distancing. Successful physical distancing will rely on new forms of social closeness online. Yet there is not just one ?online.? A rapid and effective response requires clarifying the impact of virtual worlds as part of different forms of online interaction that comprise a virtual culture: social network sites, streaming websites, and multiplayer platforms. The project will also train graduate student researchers in methodological approaches for studying online cultures. <br/><br/>This research will be conducted in a densely trafficked virtual world. Virtual worlds are places where individuals interact with avatars in online environments. The investigators have conducted research in a virtual world context for over a decade, and thus have detailed baseline data with which to examine what is happening as a large number of individuals enter that virtual world due to the COVID-19 pandemic. What is the sudden move to virtual worlds doing in terms of social closeness and interaction? How does co-presence in virtual place transform intimacy and collaboration? How might this provide innovative strategies for preventing viral transmission, by forging new forms of social closeness in the context of physical distancing? To investigate these questions, the researchers will conduct participant observation, individual interviews, and group interviews. The study will compare individuals who have spent time in the virtual world for years with individuals who have entered the virtual world after COVID-19. Findings from this research will provide insight into the specific possibilities virtual worlds are providing in the circumstances of societies reshaped by COVID-19. In these new circumstances, virtual worlds will be one element of an online ecosystem linking drones, robots, and autonomous vehicles to mobile devices, social network sites, online games and streaming, augmented reality, artificial intelligence, machine learning, and data analytics. The research will thus provide a better understanding of the place of virtual worlds in this emerging online ecosystem.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    195619.0USD
410    Dr Hayes Joseph    WASHINGTON UNIVERSITY    2020-05-15    2021-04-30    RAPID: A multiscale approach to dissect SARS-CoV-2 attachment to host cells and detect viruses on surfaces    Biological Sciences - The 2019 novel coronavirus, identified as the cause for the pneumonia pathology reported in Wuhan, spread quickly and became a global pandemic. The project will employ experimental methods to develop sensors for the detection of SARSCoV-2 from environmental samples and develop predictive models for virus attachment to cells by applying computational machine learning methods. The outcome of this project will contribute to the development of proactive measures to identify viruses with pandemic potential before they are able to transmit and spread broadly among humans. The graduate students involved in this research will gain experience in protein biochemistry, fluorescence microscopy, and computational simulations and experience utilizing those skills to problems of societal importance.<br/><br/>This NSF Rapid response Research (RAPID) project will support a project that is aimed to characterize receptor interactions mediated by the Spike protein (S) of SARS-CoV-2. Development of fluorescence-based assays to characterize SARSCoV-2 attachment to Angiotensin converting enzyme (ACE2)-functionalized surfaces with controlled density and mobility, identifying peptide mimics of the ACE2 ectodomain for the development of sensors to detect SARSCoV-2 from environmental samples, and develop and validate predictive models of CoV attachment from primary sequence using machine learning constitute the specific goals of this project.<br/><br/>This RAPID award is made by the Molecular Biophysics Program in the Division of Molecular and Cellular Biosciences, using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    200000.0USD
411    Dr Hayes Joseph    Columbus State University    2020-05-15    2021-04-30    RAPID/Collaborative Research: Quantifying Social Media Data for Improved Modeling of Mitigation Strategies for the COVID-19 Pandemic    Engineering - This Rapid Response Research (RAPID) grant will support research that will contribute new knowledge related to modeling social behavior and community activity during the COVID-19 pandemic, as well as future pandemics with COVID-19 characteristics. The model focuses on compliance with mitigation strategies and public health guidelines, thus enabling the selection of policies that are most effective in promoting both the progress of science and advancing national health and prosperity. Various pandemic models are currently being used to predict the spread of a virus and establish which mitigation strategies are the most effective. These models are heavily based on assumptions and may include an oversimplified reality of how populations react and behave. This research will provide needed knowledge and methods for the development of a model of how individuals in the U.S. react to certain mitigation strategies, such as social-distancing, stay-at-home orders, quarantines, and travel advisories, by mining and analyzing social media data during the COVID-19 crisis. This enhanced modeling approach and its resultant model will be of great value to disaster response managers and policy/decision makers to understand human social behavior. This work allows assessment of the effectiveness of mitigation strategies and public health guidelines during pandemics (and other crises). This project will also form the basis of a publicly available case study suitable for university level students that can be widely incorporated in courses. <br/><br/>Although individual-based and homogeneous mixing pandemic models provide useful insights and predictive capabilities within a range of possibilities, they are highly sensitive to people?s actions. This research aims to provide an enhanced approach to model social behavior and community activity during a pandemic in terms of compliance with mitigation strategies and public health guidelines. Social media data present a brief window of opportunity for research on how, and to what extent, the public does or does not comply with the recommended mitigation strategies and public health guidelines. The research team will collect real-time data from social media related to COVID19-exposed regional populations in the U.S. The data will be analyzed using machine learning techniques to identify non-mutually exclusive clusters of people based on similarity of their demographic, geographic, and time information, and establish relationships among clusters. The analyzed data will form the basis of a data-driven multi-paradigm simulation model that captures changes in public sentiment over time, quantifies the resistance/compliance with mitigation strategies and health guidelines, and gauges overall effectiveness of various mitigation strategies and advice over time.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    13406.0USD
412    Dr Hayes Joseph    Northeastern University    2020-05-15    2021-04-30    RAPID: D3SC: Identification of Chemical Probes and Inhibitors Targeting Novel Sites on SARS-CoV-2 Proteins for COVID-19 Intervention    Mathematical and Physical Sciences - The life cycle of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) involves a number of viral proteins and enzymes required for infectivity and replication. Inhibitors that target these enzymes serve as potential therapeutic interventions against coronavirus disease 2019 (COVID-19). With this award, the Chemistry of Life Processes program in the Chemistry Division is supporting the research of Drs. Mary Jo Ondrechen and Penny J. Beuning from Northeastern University to apply computational methods to identify sites in SARS-CoV-2 proteins that would be good targets for binding inhibitors. The project uses artificial intelligence methods developed at Northeastern University to identify pockets and crevices in the structures of viral proteins that may serve as new targets for the development of antiviral agents. Large datasets of natural and synthetic compounds are computationally searched for molecules that fit into these alternative sites, and any compounds that fit will be experimentally tested for their ability to inhibit the functions of these viral enzymes. The project provides training in computational chemistry and biochemical analysis to graduate students and postdoctoral associates.<br/><br/>This project uses the unique Partial Order Optimum Likelihood (POOL) machine learning (ML) method developed by Dr. Ondrechen?s group to predict multiple types of binding sites in SARS-CoV-2 proteins, including catalytic sites, allosteric sites, and other interaction sites. The goals of this project are to apply the POOL-ML method to identify the binding sites on viral pathogen SARS-CoV-2 proteins using the three-dimensional protein structures as input. Molecular dynamics simulations are used to generate conformations for ensemble docking. Compounds from the large molecular databases are computationally docked into the predicted sites to identify potentially strong binding ligands. Candidate ligands to selected SARS-CoV-2 proteins, including the main protease and 2?-O-ribose RNA methyltransferase, are experimentally tested in vitro for binding affinity and the effect of the best predicted inhibitors on catalytic activities determined by direct biochemical assays. All the SARS-CoV-2 protein structures in the Protein Data Bank (PDB) are studied. Compound libraries for the study include: a) selected 2600+ compounds from the ZINC and Enamine databases that are already being manufactured; b) a library of 20,000+ compounds found in foods that the team recently gained access to; these potentially hold some special advantages, including ready availability in the public domain and low cost; and c) the March 2020 open access CAS (American Chemical Society) database of 50,000 compounds with known or potential anti-viral activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    165808.0USD
413    Dr Hayes Joseph    Harvard University    2020-05-01    2021-04-30    RAPID:Collaborative Research: Computational Drug Repurposing for COVID-19    Computer and Information Science and Engineering - With the disruptive nature of the COVID-19 pandemic, effective treatments could save the lives of severely ill patients, protect individuals with a high risk of infection, and reduce the time patients spend in hospital beds. However, there are currently no effective treatments for COVID-19. Traditional methodologies take years to develop and test compounds from scratch. Machine learning provides promising new approaches to repurpose drugs that are safe and already approved for other diseases. This project will develop a machine learning toolset to expedite the development of safe and effective medicines for COVID-19. The toolset will rapidly identify safe repurposing opportunities for approved and experimental drugs. It will predict whether treatments may have therapeutic effects in COVID-19 patients, allowing the identification of drugs and drug cocktails that are safe and plentiful enough to treat a substantial number of patients. By putting tools in the hand of practitioners, the activities in this project will have an immediate impact. They will result in actionable predictions that are accurate and interpretable. <br/><br/>Recently, the principal investigators have developed a series of machine learning tools to identify drug repurposing opportunities. Building on foundational previous work, in this project, the principal investigators will first build a large COVID-19 focused knowledge graph that will capture fundamental and COVID-19-specific biological knowledge. The graph learning methods will be adapted to identify safe drugs and drug cocktails for COVID-19. To predict the safety of cocktails with two or more drugs, the methods will generalize to an exponentially large space of high-order drug combinations. In addition to drug safety, efficacy is a crucial endpoint for drug development. The project will develop a novel graph neural network (GNN) method to identify efficacious drug repurposing opportunities, even for diseases, such as COVID-19, that do not yet have any drug treatments and thereby, no label, supervised information. The method will predict what drugs and drug combinations may have a therapeutic effect on COVID-19. Finally, the principal investigators will integrate the developed tools into a complete, explainable framework that will generate predictions, provide explanations, and incorporate human feedback into the machine learning loop. This project will provide new, open tools for rapid drug repurposing that will be relevant for COVID-19 and other emerging pathogens. Additionally, the project will provide unique opportunities for multi-disciplinary curriculum development, training and advising, and professional activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    99863.0USD
414    Dr Hayes Joseph    University of Minnesota-Twin Cities    2020-05-15    2021-04-30    RAPID: The effect of contact network structure on the spread of COVID-19: balancing disease mitigation and socioeconomic well-being    Biological Sciences - What makes COVID-19 spread rapidly in some places, yet slowly in others? How should society lessen social distancing while limiting an increase in infections? To answer these questions, this Rapid Response Research (RAPID) project seeks to understand how patterns of interpersonal interaction (?structure?) in social contact networks affect disease spread in a population. The researchers will simulate a disease spreading through a variety of social contact networks, and use machine learning to relate each network?s structure to the number and timing of new infections. By limiting structures related to increased disease, societies may be able to reopen other parts of their economies while still curbing overall disease spread. The researchers will produce an interactive web application for the public and decision-makers to visualize trade-offs between reducing disease and maintaining social cohesion. This research will support the professional development of an early career scientist.<br/><br/>This research aims to determine the inherent risk of SARS-CoV-2 spread based on contact network structure. The researchers will use machine learning to 1) identify network structures that influence disease spread and 2) predict disease spread on empirical contact networks. Important network structures will serve as targets for simulated disease mitigation interventions (e.g. reducing structures that increase levels of disease or increasing structures that reduce disease levels). Finally, the researchers will investigate whether future outbreaks of COVID-19 or other diseases could be alleviated through optimizing social contact networks ahead of time. The outcomes of this research will inform and facilitate quick, efficient interventions to reduce the social and economic costs of COVID-19. This research will develop a general framework for relating disease to network structure. Thus, results can be generalized beyond the current pandemic, serving to further our understanding of potential future waves of COVID-19, as well as other directly-transmitted diseases in humans, livestock, and wildlife.<br/><br/>This RAPID award is made by the Ecology and Evolution of Infectious Diseases Program in the Division of Environmental Biology, using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    199136.0USD
415    Dr Hayes Joseph    New Mexico Highlands University    2020-05-15    2021-04-30    RAPID: Machine Learning Methods to Understand, Predict and Reduce the Spread of COVID-19 in Small Communities    Mathematical and Physical Sciences - The ongoing COVID-19 outbreak has recently reached pandemic status spreading all around the world. The severity of the pandemic, along with an enormous impact on world?s economy and society, has forced governments to introduce emergency measures. It is essential to utilize the available statistical data from trusted sources in order to model and evaluate the dynamics of the pandemic spread, to not only better understand such complex systems, but to learn and develop possible solutions to prevent further spread of current and/or similar future outbreaks. Thus, this research, devoted to the development of mathematical models of COVID-19 pandemic spread, addresses an urgent national need. Faculty and students in computer science, anthropology, and computational chemistry at New Mexico Highlands University have formed a diverse group for finding a solution to the complicated problems of the description and prediction of COVID-19 spread. This multidisciplinary project is expected to yield a better understanding of the interconnections among many factors that contribute to the spread of COVID-19. Statistical data will be collected in regions of Northern New Mexico, including San Juan and McKinley Counties in the Navajo Nation and Los Alamos county outside of the Navajo Nation. Analysis of the collected statistical data along with socio-cultural assessment from this project will be presented to New Mexico (NM) tribal and health authorities. The project will aim to provide a scientific basis for the prediction of disease spread and will consider scenarios associated with the possibility of another wave of the pandemic. Students from this minority-serving institution involved in the project will obtain valuable experience in the application of advanced machine learning models and methods in providing fast robust reaction to a national health, economic, and societal crisis.<br/><br/>In this study, machine learning methods will be used to analyze pandemic spread scenarios in different regions and to glean the most important features of the data characterizing the spread. The research team will use both traditional machine learning techniques and advanced methods, such as artificial neural networks, allowing development of virus incidence model capturing dependencies in both linear and nonlinear domains. The work will concentrate on understanding disease spread with regard to multiple socioeconomic factors. The problem can be treated as a sequence modeling one; so, recurrent neural networks and more complex models based on their recurrent cells might be one promising direction. The next step will be to assemble datasets for small isolated communities with different socioeconomic backgrounds and ethnicities ? comparing Navajo Indians living on the Navajo reservation to Los Alamos County (NM) ? and to test the applicability of the developed model to these regions. The spatiotemporal data available on the spread is heterogeneous in character. An important goal of this research is to classify the collected data with respect to the similarity in the epidemic curve behavior and then build separate models for different regions according to this classification. The proposed model will be used for prediction of future incidents and to produce the most effective non-medical recommendations for suppression and prevention of future viral outbreaks.<br/><br/>This research is supported by the Partnerships for Research and Education in Materials (PREM) program and the Condensed Matter and Materials Theory (CMMT) program in the Division of Materials Research in the Directorate for Mathematical and Physical Science using supplemental funds made available by the Coronavirus Aid, Relief, and Economic Security (CARES) Act.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    185747.0USD
416    Dr Hayes Joseph    University of British Columbia    2020-05-15    2021-04-30    Digital Virtual Support of Cases and Contacts to Novel Coronavirus (COVID-19): Readiness and Knowledge Sharing for Global Outbreaks (WelTel PHM)    The global outbreak of COVID-19 is the latest example of a rapidly spreading infectious outbreak with global impact. Infected patients with mild symptoms and asymptomatic contacts need to be isolated, ideally without overwhelming health facilities. WelTel, an integrated virtual care and patient engagement solution, emerged as an innovation initially to support the global HIV pandemic through a Canadian-Kenyan partnership over a decade ago. Co-founded by the lead investigator and registered in British Columbia, WelTel has continued to integrate research into a richly featured virtual care platform that can be used on the frontlines of healthcare delivery. The study aims to: 1-Deploy and co-optimize WelTel to assist in home monitoring and support of COVID-19 cases and contacts; 2- Determine essential linkages and technical demands of the digital health ecosystem for data security purposes and integration into other electronic health records (EHR) & health information management systems (HIMS); 3-Evaluate communication and other metadata captured by the system for public health quality improvement to better understand and reduce barriers (such as stigma); 4-Use novel computing approaches such as natural language processing (NLP) and machine learning to harness artificial intelligence (AI) capabilities to model, predict, and provide insights into future precision public health approaches. Collaborators have necessary expert skills in quantitative and qualitative research methods for rigorous assessment, and come from the countries targeted for the research deployment (Canada, UK, US, Kenya, and Rwanda). A rapid digital landscape analysis will also be done as a part of this research. Virtual care may be an efficient, cost-effective way to provide the necessary public health monitoring and support for patients and contacts of COVID-19 and future emerging communicable pathogens, as well as can inform public health quality improvement and precision care.    Canadian Institutes of Health Research    Research Grant    500000.0CAD
417    Prof. Cotton Matthew    MRC/UVRI & LSHTM Uganda Research Unit    2020-05-15    2021-04-30    African COVID-19 Preparedness (AFRICO19)    Our project, AFRICO19, will enhance capacity to understand SARS-CoV-2/hCoV-19 infection in three regions of Africa and globally. Building on existing infrastructures and collaborations we will create a network to share knowledge on next generation sequencing (NGS), including Oxford Nanopore Technology (MinION), coronavirus biology and COVID-19 disease control. Our consortium links three African sites combined with genomics and informatics support from the University of Glasgow to achieve the following key goals: 1. Support East and West African capacities for rapid diagnosis and sequencing of SARS-CoV-2 to help with contact tracing and quarantine measures. Novel diagnostic tools optimized for this virus will be deployed. An African COVID-19 case definition will be refined using machine learning for identification of SARS-CoV-2 infections. 2. Surveillance of SARS-CoV-2 will be performed in one cohort at each African site. This will use established cohorts to ensure that sampling begins quickly. A sampling plan optimized to detect initial moderate and severe cases followed by household contact tracing will be employed to obtain both mild to severe COVID-19 cases. 3. Provide improved understanding of SARS-CoV-2 biology/evolution using machine learning and novel bioinformatics analyses. Our results will be shared via a real-time analysis platform using the newly developed CoV-GLUE resource.    Wellcome/Department for International Development    Research Grant    2001990.0GBP
418    Prof. Cotton Matthew    Foundation For Research And Technology - Hellas    2012-10-01    2017-10-01    Dissecting the Role of Dendrites in Memory    Understanding the rules and mechanisms underlying memory formation, storage and retrieval is a grand challenge in neuroscience. In light of cumulating evidence regarding non-linear dendritic events (dendritic-spikes, branch strength potentiation, temporal sequence detection etc) together with activity-dependent rewiring of the connection matrix, the classical notion of information storage via Hebbian-like changes in synaptic connections is inadequate. While more recent plasticity theories consider non-linear dendritic properties, a unifying theory of how dendrites are utilized to achieve memory coding, storing and/or retrieval is cruelly missing. Using computational models, we will simulate memory processes in three key brain regions: the hippocampus, the amygdala and the prefrontal cortex. Models will incorporate biologically constrained dendrites and state-of-the-art plasticity rules and will span different levels of abstraction, ranging from detailed biophysical single neurons and circuits to integrate-and-fire networks and abstract theoretical models. Our main goal is to dissect the role of dendrites in information processing and storage across the three different regions by systematically altering their anatomical, biophysical and plasticity properties. Findings will further our understanding of the fundamental computations supported by these structures and how these computations, reinforced by plasticity mechanisms, sub-serve memory formation and associated dysfunctions, thus opening new avenues for hypothesis driven experimentation and development of novel treatments for memory-related diseases. Identification of dendrites as the key processing units across brain regions and complexity levels will lay the foundations for a new era in computational and experimental neuroscience and serve as the basis for groundbreaking advances in the robotics and artificial intelligence fields while also having a large impact on the machine learning community.    European Research Council    Starting Grant    1398000.0EUR
419    Dr Bottle Alex    Imperial College London    2010-09-01    2014-02-28    Can valid and practical risk-prediction or casemix adjustment models, including adjustment for co-morbidity, be generated from English hospital administrative data (Hospital Episode Statistics)?    Understanding the rules and mechanisms underlying memory formation, storage and retrieval is a grand challenge in neuroscience. In light of cumulating evidence regarding non-linear dendritic events (dendritic-spikes, branch strength potentiation, temporal sequence detection etc) together with activity-dependent rewiring of the connection matrix, the classical notion of information storage via Hebbian-like changes in synaptic connections is inadequate. While more recent plasticity theories consider non-linear dendritic properties, a unifying theory of how dendrites are utilized to achieve memory coding, storing and/or retrieval is cruelly missing. Using computational models, we will simulate memory processes in three key brain regions: the hippocampus, the amygdala and the prefrontal cortex. Models will incorporate biologically constrained dendrites and state-of-the-art plasticity rules and will span different levels of abstraction, ranging from detailed biophysical single neurons and circuits to integrate-and-fire networks and abstract theoretical models. Our main goal is to dissect the role of dendrites in information processing and storage across the three different regions by systematically altering their anatomical, biophysical and plasticity properties. Findings will further our understanding of the fundamental computations supported by these structures and how these computations, reinforced by plasticity mechanisms, sub-serve memory formation and associated dysfunctions, thus opening new avenues for hypothesis driven experimentation and development of novel treatments for memory-related diseases. Identification of dendrites as the key processing units across brain regions and complexity levels will lay the foundations for a new era in computational and experimental neuroscience and serve as the basis for groundbreaking advances in the robotics and artificial intelligence fields while also having a large impact on the machine learning community.    National Institute for Health Research (Department of Health)    Full Grant    400921.33GBP
420    Professor Herten Dirk-Peter    University of Birmingham    2020-08-01    2023-08-01    AMS Professorship Award for Professor Dirk-Peter Herten, University of Birmingham    I want to establish a world-leading research group in receptor signalling by providing tools and support for quantitative studies with molecular sensitivity for biomedical research. The prestigious award of the AMS fellowship will enable the purchase of cutting-edge microscopy tools and allow further automation of our microscopes to increase throughput and provide the capacity for super-resolved 3D reconstructions of whole cells. My position in COMPARE aligned to the Institute of Cardiovascular Sciences (ICVS) and the School of Chemistry at the University of Birmingham (UoB) adds expertise in methods development to the COMPARE advanced microscopy facility at UoB and will allow me to work on my quantitative microscopy methods. My appointment creates a unique opportunity to support researchers and clinicians in their search for solutions to urgent biomedical questions, like HIV infection or thrombosis, by use of existing techniques and novel approaches. I will involve Jeremy Pike (UoB, Data Analysis Officer) and Iain Styles (Computer Sciences, Turing Institute Fellow and Deputy Director of COMPARE, UoB) in machine learning approaches for the fast processing of 3D microscopy data. Additionally, I will initiate a planned industry collaboration with IRIS Biotech (Germany) to elaborate on the potential commercialisation of fluorescent probes for super-resolution microscopy, chemical multiplexing and metal cation sensing based on my patent (DE 10 2016 012 162.9). A challenge in methods development is to identify suitable targets and questions for the novel approach. The fellowship will enable collaborations with colleagues on campus and within COMPARE to increase the number of biomedical targets and facilitate translational application of my techniques which focus on T cell receptor signalling and more recently on GPCRs. Examples of new collaborations include the platelet immunoglobulin C-type lectin-like receptors GPVI and CLEC-2 (with Steve Watson, Steve Thomas and Natalie Poulter, ICVS) which are novel targets for a variety of thrombosis and thrombo-inflammatory disorders, and in the chemokine GPCR family (with Dimity Veprintsev, University of Nottingham - UoN) that play an important role in diseases like hypertension, hypoxia, or hypoglycaemia. In this context, I envision a key training centre for advanced microscopy at UoB also addressing problems like fluorescence labelling and probe development. This will involve colleagues in the School of Chemistry as well as COMPARE researchers working on fluorophores (Jon Preece, UoB) and bioactive molecules (Liam Cox, UoB; Barrie Kellam, UoN). COMPARE offered me a generous starting budget COMPARE (£ 450K) and the institute is in the process of refurbishing the labs to my needs, to move 3 co-workers together with my equipment by February 2020. Since I am unable to bring any overseas funding, the award will allow me to get accommodated with the UK grant application system and plan projects to strengthen my translational research (MRC) and the development of novel probes and techniques (BBSRC and EPSCR) without losing any momentum in my ongoing research activities. I have already applied for a translational research grant (BBSCR TRDF) in collaboration with Robert Henderson (University of Edinburgh), which involves quantitative imaging microscopy through use of his 256x256 APD camera.    The Academy of Medical Sciences    AMS Professorship Scheme Round 2    470621.92GBP
421    Professor Huang Xiaolin    Tongji Hospital, Tongji Medical College, Huazhong University of Science and Technology    2018-02-01    2019-06-30    Low cost robotic orthosis for stroke treatment in rural China    China is the worst affected developing country with 2.5M new stroke cases each year and 11.1M stroke survivors at any given time. In the past decades, due to lower socioeconomic status, less stroke awareness, and inequitable distribution of medical resources to rural areas, the incidence rate and burden of stroke in China has increased disproportionately in rural areas of the northeast and central regions. Rehabilitation programmes have been shown to be extremely effective in reducing the disability and restoring walking ability through early training. However, in most rural areas of China no such medical rehabilitation centres or hospitals exist leaving the rural population without the therapies which will allow them to regain mobility functions after stroke. This leads to disability and has broader negative socioeconomic impacts. This project seeks to gather evidence directly from the key stakeholders in the beneficiary ODA countries (China and Kazakhstan) about what their problems and requirements are. The networking activities will allow us to build a full proposal based around actual rather than assumed need, and therefore maximise the impact achieved. The project aims to establish a multi-disciplinary consortium including experts from rehabilitation robotics, sensing, machine learning, physiotherapy and rehabilitation medicine. We aim to provide low-cost robotic solutions to stroke survivors in remote home and community environments. This would reduce therapists’/stroke carers' demand in rural areas of ODA countries whilst maximizing the stroke survivor’s sense of intention, involvement, interaction and achievement of limb movement, with greater likelihood of successful rehabilitation and improved quality of life.    The Academy of Medical Sciences    AMS Professorship Scheme Round 2    22000.0GBP
422    Professor Huang Xiaolin    TWO WORLDS CONSULTING LIMITED    2018-02-01    2019-06-30    udu: AI Platform for Pandemic Intelligence    The UK, amongst others, lacks a coherent infrastructure to support effective direct and timely collection and analysis of pandemic data, about both the progressIon of Covid-19 itself and the population response to public policy aimed at mitigating and profiling its progress. Refining policy and informing the judgement calls required to navigate the balance between lockdown and economic damage requires both accurate data and the ability to rapidly model multiple, 'What if?' scenarios. Current data intelligence systems are partial, fragmented, incomplete, lag reality and, in most cases can only surface what they have specifically been asked to look for. AI systems used to look for patterns are often constrained by the quality and range of data available to them. Existing models tend to look at single factors in isolation, e.g. not taking into account multiple sources of mortality data or failing to take account factors such as population mobility and behaviour, the impact of events such as Cheltenham races, sunny bank holiday weather or other regional and seasonal variations. This can only be addressed through a more holistic approach to data collection and integration. This project therefore uses an advanced data intelligence platform, udu, which is capable of integrating a wide range of data from multiple sources and of multiple types and of actively discovering new data online. It then uses software that can self-organise itself around a task to discover relationships between and patterns in the collected data to provide an inferential view of pandemic impact, policy effectiveness and population behaviour. udu has been established for several years in niche markets. Here, we are building on previous experience by Two Worlds in using udu to create systems for the predictive analysis of environmental change to public health for the first time. The resulting system is intended to be capable of supporting direct exploration by human users, providing an interface (API) to allow other teams to test their own analytic models against the datascape created by udu and supporting local and external machine learning systems with a wider range of high quality data and analysis.    UK Research and Innovation    AMS Professorship Scheme Round 2    49984.0GBP
423    Dr Courtin Emilie    London Sch of Hygiene and Trop Medicine    2020-05-01    2023-04-30    Heterogeneous and long-term effects of social interventions on mortality and psychological health    The UK, amongst others, lacks a coherent infrastructure to support effective direct and timely collection and analysis of pandemic data, about both the progressIon of Covid-19 itself and the population response to public policy aimed at mitigating and profiling its progress. Refining policy and informing the judgement calls required to navigate the balance between lockdown and economic damage requires both accurate data and the ability to rapidly model multiple, 'What if?' scenarios. Current data intelligence systems are partial, fragmented, incomplete, lag reality and, in most cases can only surface what they have specifically been asked to look for. AI systems used to look for patterns are often constrained by the quality and range of data available to them. Existing models tend to look at single factors in isolation, e.g. not taking into account multiple sources of mortality data or failing to take account factors such as population mobility and behaviour, the impact of events such as Cheltenham races, sunny bank holiday weather or other regional and seasonal variations. This can only be addressed through a more holistic approach to data collection and integration. This project therefore uses an advanced data intelligence platform, udu, which is capable of integrating a wide range of data from multiple sources and of multiple types and of actively discovering new data online. It then uses software that can self-organise itself around a task to discover relationships between and patterns in the collected data to provide an inferential view of pandemic impact, policy effectiveness and population behaviour. udu has been established for several years in niche markets. Here, we are building on previous experience by Two Worlds in using udu to create systems for the predictive analysis of environmental change to public health for the first time. The resulting system is intended to be capable of supporting direct exploration by human users, providing an interface (API) to allow other teams to test their own analytic models against the datascape created by udu and supporting local and external machine learning systems with a wider range of high quality data and analysis.    Medical Research Council    Fellowship    306714.0GBP
424    Dr Courtin Emilie    UNIVERSITAIR MEDISCH CENTRUM UTRECHT    2013-08-01    2018-08-01    Intracranial COnnection with Neural Networks for Enabling Communication in Total paralysis    iCONNECT aims to give severely paralyzed people the means to communicate by merely imagining to talk or make hand gestures. Imagining specific movements generates spatiotemporal patterns of neuronal activity in the brain which I intend to record and decode with an intracranial Brain-Computer Interface (BCI) system. Many people suffer from partial or full loss of control over their body due to stroke, disease or trauma, and this will increase with population ageing. With both duration and quality of life beyond 60 increasing in the western world, more and more people will suffer from the consequences of function loss (mostly stroke) with the prospect of living for decades with the handicap, and will stand to benefit from restorative technology that has yet to be developed. I believe that functionality can be restored with brain implants. My goal is to develop a BCI that can interpret activity patterns on the surface of the brain in real-time. For this we need to discover how the brain codes for (imagined) actions, how codes can be captured and decoded and how an intracranial BCI system impacts on a user. I will use state of the art techniques (7 Tesla MRI and electrocorticography, ECoG) to explore brain codes and develop decoding strategies. Interactions between user and implanted device will be studied in paralyzed people. I will directly link decoded movements to animated visual feedback of the same body part, expecting to induce a feeling of ownership of the animation, and thereby a sense of actual movement. This research is only possible because of the latest developments in imaging of human brain activity, machine learning techniques, and micro systems technology. My lab is unique in bringing together all these techniques. Success of the project will lead to deeper understanding of how sensorimotor functions are represented in the human brain. The ability to ?read' the brain will add a new dimension to the field of neural prosthetics.    European Research Council    Advanced Grant    2498829.0EUR
425    Professor Gleeson Fergus    Oxford University Hospitals NHS Foundation Trust    2017-10-01    2021-09-30    IDEAL: Artificial Intelligence and Big Data for Early Lung Cancer Diagnosis    None    National Institute for Health Research (Department of Health)    Full Grant    1425634.0GBP
426    Dr. WASSERMANN Demian    National Institute for Research in Computer Science and Automatic Control (INRIA)    2018-03-01    2023-02-28    Accelerating Neuroscience Research by Unifying Knowledge Representation and Analysis Through a Domain Specific Language    Neuroscience is at an inflection point. The 150-year old cortical specialization paradigm, in which cortical brain areas have a distinct set of functions, is experiencing an unprecedented momentum with over 1000 articles being published every year. However, this paradigm is reaching its limits. Recent studies show that current approaches to atlas brain areas, like relative location, cellular population type, or connectivity, are not enough on their own to characterize a cortical area and its function unequivocally. This hinders the reproducibility and advancement of neuroscience. Neuroscience is thus in dire need of a universal standard to specify neuroanatomy and function: a novel formal language allowing neuroscientists to simultaneously specify tissue characteristics, relative location, known function and connectional topology for the unequivocal identification of a given brain region. The vision of NeuroLang is that a unified formal language for neuroanatomy will boost our understanding of the brain. By defining brain regions, networks, and cognitive tasks through a set of formal criteria, researchers will be able to synthesize and integrate data within and across diverse studies. NeuroLang will accelerate the development of neuroscience by providing a way to evaluate anatomical specificity, test current theories, and develop new hypotheses. NeuroLang will lead to a new generation of computational tools for neuroscience research. In doing so, we will be shedding a novel light onto neurological research and possibly disease treatment and palliative care. Our project complements current developments in large multimodal studies across different databases. This project will bring the power of Domain Specific Languages to neuroscience research, driving the field towards a new paradigm articulating classical neuroanatomy with current statistical and machine learning-based approaches.    European Research Council    Starting Grant    1497045.0EUR
427    Dr. WASSERMANN Demian    University of Zurich    2017-11-01    2019-01-31    EEG based microsleep episode detection in the maintenance of wakefulness test and the driving simulator using a machine learning approach    Road traffic injuries are the leading cause of death for those aged 15-29 years. Excessive daytime sleepiness (EDS) is estimated to be the underlying cause in up to 15-20% of motor vehicle accidents (MVA), and is most often caused by socially induced sleep deprivation or poor sleep hygiene in otherwise healthy individuals, followed by medical disorders, or the intake of drugs. Methods for reliably objectifying sleepiness are urgently sought, primarily for sleepiness detection while driving but also for predicting the risk of sleepiness induced accidents during laboratory assessments of patients. The EEG is widely recognized as the gold standard for determining sleep-wake stages and their sudden, as well as gradual, changes. In the clinical and scientific context, standard EEG scoring criteria are generally applied, in which sleep is scored based on 30-s epochs. These sleep criteria consider neither the occurrence of microsleep episodes (MSE) nor the local aspects of sleep demonstrated in both animals and humans. Falling asleep is a gradual, not a sudden, process and shows fluctuations between waking and sleep. Particularly when assessing objective sleepiness in the context of driving ability, MSE of short duration originating in any brain area become an important criterion.We aim to characterize and identify MSE by defining visual scoring rules for MSE as short as 1 s, extracting relevant features based on quantitative EEG analyses, and by developing machine learning algorithms to detect MSE. First, we will only consider occipital EEG leads but will extend our analyses to central EEG leads in a second step. Our algorithms will be trained and verified on MWT data of patients and subsequently applied to MWT data of sleep deprived healthy individuals, and simulated driving conditions. In the driving simulator, we will investigate the association of MSE with impaired driving performance and spontaneously perceived sleepiness (SPS). In the context of fitness to drive assessments, one generally assumes that the perception of sleepiness precedes the occurrence of MSE. Our approach should allow to validate or disprove such an assumption. Previous studies mainly assessed relationships based on mean values or pooled data. Our aim is to take inter-individual variations into account and relate single events (i.e. MSE) with quantitative EEG measures. Therefore, we intend to track the sleep-wake transition zone (e.g. occurrence of MSE, vigilance fluctuations) with high temporal resolution on a second-by-second basis. In summary, we intend to develop and formulate the first standardized MSE scoring rules worldwide and to establish reliable automatic detection algorithms. This will have a major impact in sleep medicine and research and open new areas of research and diagnostic procedures. Since sleepiness is among the most frequent causes of car accidents, and an important risk factor for train and truck drivers and in many surveillance tasks such as air traffic or nuclear power plant control, reliable diagnostic tools to judge fitness to drive or fitness on the job become essential for reducing car and work accidents, catastrophic incidents, and the immense costs related to excessive daytime sleepiness.    Swiss National Science Foundation    Project funding (Div. I-III)    217848.0CHF
428    Dr. WASSERMANN Demian    ETH Zurich    2010-08-01    2013-07-31    The evolution of proteins with tandem repeats: a large-scale study of rates, selective forces, complex patterns and associations with human disease    Protein repeats are predominantly found in muscle, brain, synaptic cell adhesion proteins, but underrepresented in very basic cellular functions. Over the last years, the important and versatile roles of tandem repeats have been documented by an increasing number of studies. Repeat lengths vary from homorepeats (eg. in the Huntington disease gene) to long repeats with multiple domains (eg. the cytoskeletal protein titin). Functional classification of proteins with tandem repeats suggests that they are often involved in multiple binding, and so facilitate protein-protein interactions and are required for the formation of multi-protein complexes. Protein repeats tend to have structural roles in proteins with fundamental biological functions, including survival facilitation. Antigenic and other virulence related proteins such as toxins and allergens may also be encoded by sequences with repeats. Proteins with tandem repeats are frequently associated with infectious, neurodegenerative diseases. Along this line, the discovery of repeat-containing proteins and their structure-function study promise to be a fertile direction for research leading to the identification of targets for new medicaments and vaccines. The systematic bioinformatics analysis of protein repeats in genomes can provide a global view on these motifs, their structures, functions and evolution. This should facilitate a significant improvement of our understanding of structural and functional changes during the evolution of proteins with repeats and their protein-protein interaction networks.Despite several studies of protein repeats, the fundamental questions about the evolution and roles of repeats are far from being answered. Hardly any previous studies went beyond the classification-type summaries of single repeats and proteins containing them. This project aims to make a major contribution to the understanding of repeat-containing proteins, their structure, function and the dynamics of protein-protein interactions. • The project will assemble the most complete set of protein repeats using a novel algorithm for the identification of repeats that have diverged via substitution and indel events, based on the K-clustering-based approach. Data will be made publicly available through our Protein Repeat DataBase (PRDB). • We will study biological forces shaping the mutational landscape of proteins with repeats. Several purpose-built novel codon substitution models will be developed for this task. For example, using these models we will assess selective forces modulating the number and length of repeats. We will obtain estimates of rates of repeat duplications (and mutations) relative to point mutation. The project will seek to compare and characterize the selective forces acting on the globular and the repetitive parts of the proteins as well as the composition bias, recombination and co-evolutionary forces. PI’s expertise in codon models will be important for the proposed development of novel tailor-made Markov models, which will consequently be used to assess selective forces modulating the number and length of repeats, and the rates of evolution of the globular part vs the periodic part. The developed methods will be implemented in a user-friendly software package and made available to the users.• Complementary information (expression, disease associations, biochemical pathways, etc.) will be assembled for each repeat protein. Together with evolutionary rates and selection estimates, these data will be analyzed using machine learning and pattern discovery techniques. A particular emphasis will be on the trends observed for disease-associated genes. Do the disease genes exhibit unusual evolutionary patterns, selective pressures, expression profiles, etc. compared to genes lacking such associations? Prediction of genes with roles in disease will have invaluable medical applications. • Important examples of disease-related genes will be analyzed in more detail, applying most suitable evolutionary models to the expanded datasets. In particular, codon models that we will develop will be used to evaluate the selective pressure acting on the length of homorepeats during their evolution. These models will assess protein fitness changes as a result of altered numbers of repeats or their lengths. While the conservation is observed in protein coding homorepeats, their length and variability in population is not conserved, suggesting differential selection. The codon models will provide powerful means to evaluate various selection scenarios. Structural modeling techniques will be applied to a set of important proteins (collaboration with Dr Kajava, CRBM-CNRS, Montpellier, France), which will also be studied further using experimental techniques. For the empirical studies we will work in collaboration with biophysicists, experimental biologists and biochemists (Dr Padilla, CBS-CNRS, Montpellier, France).• We will study repeat-containing proteins in pathogenic organisms. Due to their binding properties, proteins with repeats are potential candidate genes influencing the pathogeneicity and disease progression. Indeed, many repeat-containing proteins are expressed on the surface of rapidly evolving pathogens, such as bacteria, viruses that are serious human pathogens or have agricultural or environmental importance. Their antigenic proteins are under strong selective pressure to escape host immune response. A combination of adaptive mutations in such genes may be at the origin of emerging infectious diseases. We will aim to identify the genes and residues that may be used as drug/vaccine targets, essential for controlling and preventing epidemics. Studies of protein with repeats in plant or animal pathogens, are of agricultural and environmental importance. • All assembled data on repeat-containing proteins will be available via mirror web-server. Our database PRDB will be regularly updated, and will incorporate basic tools for categorized searches of repeats, their basic analysis and organization. The goal is to create an integrated data resource for proteins with repeats. Such approach will enable researchers to combine the multitude of available resources and consider them in their integrity. This will facilitate a more efficient and rapid progress in studying structure-function relationship of proteins with tandem repeats and subsequent medical, agricultural and environmental applications. The project will lay a sound foundation for further work on structure of proteins with repeats. The discovery process has high potential and may open new aspects of theoretical biology, molecular evolution, protein structure, and medical related research.    Swiss National Science Foundation    Project funding (Div. I-III)    260278.0CHF
429    Dr. WASSERMANN Demian    University of Fribourg    2012-10-01    2013-09-30    Coestimating selection and demography    Detecting signatures of past selective events provides insights into the evolutionary history of a species by evidencing adaptive events. The identification of molecular targets of selection in humans, for instance, pinpoints biologically relevant differences between us and other apes. In addition, inferences regarding selection provide important functional information by elucidating the interaction between genotype and phenotype. Since positions in the genome that are under selection are functionally important by definition, detecting signatures of selection has also been used extensively to identify functional regions or protein residues. Finally, inferring the molecular locations at which selection is acting may help us to predict responses to selective pressures in organisms such as viruses, which would revolutionize the management of pandemics and the development of drugs. Unfortunately, the demographic history of a population or species is a major confounding factor when inferring past selective events. Indeed, neutrality tests are very sensitive to violations of the underlying demographic assumption of constant population size - and it is often impossible to distinguish between adaptive or demographic processes in putatively identified regions. After a population bottleneck, for instance, false positive rates of up to 90\% are not uncommon. Current approaches to deal with this problem rely on the assumption that selection is acting on a few loci only, while demography affects all loci equally. A two step procedure has been proposed in which a set of selectively neutral loci are used to calibrate a demographic model against which putatively selected loci are compared. However, recent evidence suggests that selection may be common in the genome of many organisms and a priori knowledge on the neutrality of markers is often difficult to obtain. As a result, this approach suffers from high false negative rates - not to mention relying on very strong assumptions regarding the pervasiveness of adaptive mutations.There is currently no flexible approach to estimate demography and selection jointly. However, recent advances in computational approaches offer new hopes to tackle such an inference. A particularly promising approach is Approximate Bayesian Computation (ABC), a technique to sidestep analytical likelihood calculations with simulations. To this end, ABC has been used to infer a wide range of evolutionary scenarios such as population bottlenecks, population splits and migration, but also to distinguish between a classic selective sweep and recurrent selective events.Here I propose to develop an ABC framework to estimate demography and selection jointly, and to apply it to a variety of organisms with very different evolutionary histories. Major new developments are needed to reach this goal. Firstly, the application of ABC to large scale data sets is tenuous without novel techniques to reduce the computational effort required. I will address this through various innovations including new ABC-MCMC algorithms with increased performance, an efficient recycling of simulations, and extending recent approaches to hybridize ABC with traditional full-likelihood methods. Such a hybridization will enable us to profit from the rich literature on full-likelihood solutions to simpler problems and to take linkage properly into account. Secondly, the joint inference of demography and selection calls for new models and new sets of informative summary statistics. I will approach this challenge by integrating models established to infer either selection or demography alone and through current techniques to explore a large space of summary statistics, such as PLS or machine learning algorithms. The proposed innovations will allow me to work towards answering some of the most controversial questions in evolutionary biology, namely the importance of adaptation in shaping genomic variation. I will approach these questions by inferring genome-wide selection coefficients from unique data sets of four organisms representing various selective and demographic histories: Deer mice (Peromyscus maniculatus), HCM viruses, Drosophila melanogaster and humans. These estimates will not only have broad implications for the management of pandemics or the development of new drugs, but will also greatly improve our understanding of the mode and tempo of the process of adaptation.    Swiss National Science Foundation    Ambizione    214250.0CHF
430    Dr. WASSERMANN Demian    Langer lab Department of Chemical Engineering Massachusetts Institute of Technology    2016-07-01    2017-12-31    Active learning for late-stage drug design    Challenges in solubility, stability, and absorption require formulation development that can significantly delay the introduction of promising drugs to patients. Active machine learning allows for rapid model development by implementing a feedback-driven artificial intelligence that puts the machine in charge of requesting additional data for iteratively improving predictive accuracy. Coupled to novel organ-on-a-chip assay systems, active learning workflows have the potential to transform formulation development and generate in silico models that help streamlining translational drug design. This proposal aims to establish active learning workflows for rapidly generating models for various objectives relevant to formulation development using intestine-on-a-chip systems. In addition to predictive models, such workflows will generate vast amounts of biological data to deepen our understanding of organ-on-a-chip systems as well as provide theoretical insights into the learning patterns of artificial intelligence.    Swiss National Science Foundation    Early Postdoc.Mobility    214250.0CHF
431    Dr. WASSERMANN Demian    University of Zurich    2017-02-01    2019-01-31    ICU-Cockpit: IT platform for multimodal patient monitoring and therapy support in intensive care and emergency medicine    In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.    Swiss National Science Foundation    NRP 75 Big Data    573853.0CHF
432    Mr Schultesz Ferenc    City, University of London    2016-07-04    2016-09-03    Pattern Classification of attention deficit hyperactivity disorder: Integrating functional magnetic resonance imaging and genetics data    In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.    Wellcome Trust    Vacation Scholarships    2000.0GBP
433    Dr de Goede Christan    Lancashire Teaching Hospitals NHS Foundation Trust    2017-12-01    2021-01-31    MyPad – Intelligent Bladder Pre-void Alerting System    In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.    National Institute for Health Research (Department of Health)    Full Grant    477200.0GBP
434    Dr Caminati Marco    University of St Andrews    2018-02-14    2021-02-13    Stochastic models to enable tailoring of medications to patients with multiple morbidities    In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.    Medical Research Council    Fellowship    289274.0GBP
435    Dr Desrivieres Sylvane    King's College London    2017-09-01    2019-08-31    Neurobiological underpinning of eating disorders: integrative biopsychosocial longitudinal analyses in adolescents    In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.    Medical Research Council    Research Grant    230792.0GBP
436    Professor Kadioglu Aras    University of Liverpool    2017-07-10    2020-12-31    Mechanisms for acquisition and transmission of successful antibiotic resistant pneumococcal clones pre- and post-vaccination    In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.    Medical Research Council    Research Grant    273163.0GBP
437    Dr Meyer Nicholas    King's College London    2016-10-01    2020-10-31    Detecting early signs of relapse in psychosis using remote monitoring technology    In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.    Medical Research Council    Fellowship    243384.0GBP
438    Dr Mukherjee Sach    MRC Biostatistics Unit    2014-03-01    2016-11-30    Statistics and machine learning for precision medicine    In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.    Medical Research Council    Unit    243384.0GBP
439    Dr Waithe Dominic    University of Oxford    2018-04-01    2021-03-31    Quantitative and Real-Time Image Analysis for Advanced Light Microscopy.    In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.    Medical Research Council    Fellowship    345783.0GBP
440    Dr Hirst Yasemin    University College London    2016-05-01    2017-12-31    CRUK Guardian Angels    Background: Most patients with cancer present with symptoms / via symptomatic routes (including emergency, urgent two week GP referrals or other – non-urgent- referrals). One year survival following the cancer diagnosis relies on early detection.Helping doctors to suspect cancer after patients have presented with symptoms is obviously crucial – but at the same time it is also crucial to support patients to seek help soon after symptoms have developed. Delays in patients' symptom appraisal and help-seeking have usually been studied retrospectively in surveys and interviews. However, there is vast amount of existing data available in the digital and commercial platforms to be used in prospective and complex study designs. To some extent, individual (e.g. google searches, symptom checkers), consumer (e.g. loyalty card) and clinical data (GP visits, referral etc) have previously been a focus of research but have not yet been investigated collectively to understand delays in symptom appraisal and help-seeking in cancer alarm symptoms. Aims: The proposed research aims to investigate delays in symptom appraisal and subsequent help-seeking behaviour using 'Big Data'. The future aim is to link individual, consumer and clinical datasets via a secure mobile phone application, and assess whether it is possible to identify signs of cancer looking at behaviour change and self-assessment using machine learning algorithms. Methods: The project includes three multidisciplinary (Health Psychology, Computing, Advanced Statistics and Cancer Epidemiology) studies that will (a) explore the lay language that is being used to articulate cancer symptoms in order to generate a dictionary of possible online search terms, (b) investigate the associations between symptom perception and behaviour change using consumer data, e.g. continuous purchase of cough medicine etc., and (c) explore public perceptions on data linkage to monitor cancer risk/warning signs. The first two studies will be using data-mining techniques, and the last study will conduct focus groups using inductive thematic analysis to assess the acceptability of using data linkage to monitor indicators of cancer symptoms. How the results of this research will be used; Beyond our future ambition, this project aims to produce a glossary based on lay expressions of cancer symptoms in order to better understand online information-seeking behaviours. We also aim to provide a detailed report on the availability of data from different resources, and evaluate the feasibility of the prospective studies. The results of this project will initiate further cancer research into understanding the ever increasing volumes of data across disciplines.    Cancer Research UK    PRC - Early Diagnosis - Innovation Grant    345783.0GBP
441    Dr Baranowski Elizabeth    University of Birmingham    2018-09-11    2020-12-10    Steroid Metabolomics for Diagnosis and Monitoring of Inborn Steroidogenesis Disorders    Background: Most patients with cancer present with symptoms / via symptomatic routes (including emergency, urgent two week GP referrals or other – non-urgent- referrals). One year survival following the cancer diagnosis relies on early detection.Helping doctors to suspect cancer after patients have presented with symptoms is obviously crucial – but at the same time it is also crucial to support patients to seek help soon after symptoms have developed. Delays in patients' symptom appraisal and help-seeking have usually been studied retrospectively in surveys and interviews. However, there is vast amount of existing data available in the digital and commercial platforms to be used in prospective and complex study designs. To some extent, individual (e.g. google searches, symptom checkers), consumer (e.g. loyalty card) and clinical data (GP visits, referral etc) have previously been a focus of research but have not yet been investigated collectively to understand delays in symptom appraisal and help-seeking in cancer alarm symptoms. Aims: The proposed research aims to investigate delays in symptom appraisal and subsequent help-seeking behaviour using 'Big Data'. The future aim is to link individual, consumer and clinical datasets via a secure mobile phone application, and assess whether it is possible to identify signs of cancer looking at behaviour change and self-assessment using machine learning algorithms. Methods: The project includes three multidisciplinary (Health Psychology, Computing, Advanced Statistics and Cancer Epidemiology) studies that will (a) explore the lay language that is being used to articulate cancer symptoms in order to generate a dictionary of possible online search terms, (b) investigate the associations between symptom perception and behaviour change using consumer data, e.g. continuous purchase of cough medicine etc., and (c) explore public perceptions on data linkage to monitor cancer risk/warning signs. The first two studies will be using data-mining techniques, and the last study will conduct focus groups using inductive thematic analysis to assess the acceptability of using data linkage to monitor indicators of cancer symptoms. How the results of this research will be used; Beyond our future ambition, this project aims to produce a glossary based on lay expressions of cancer symptoms in order to better understand online information-seeking behaviours. We also aim to provide a detailed report on the availability of data from different resources, and evaluate the feasibility of the prospective studies. The results of this project will initiate further cancer research into understanding the ever increasing volumes of data across disciplines.    Medical Research Council    Fellowship    167889.0GBP
442    Dr Fujita Andre    University of Sao Paulo    2018-03-31    2020-03-20    A model-based graph clustering approach for autism stratification    Autism spectrum disorder (ASD) is usually diagnosed by behavioral analyses and currently there is no objective test. Even the latest computational methods (e.g. support vector machine and deep learning) based on analyses of bloodoxygen-level dependent (BOLD) signal yield classification accuracies of about 60 to 70%, which are unsatisfactory for clinical use. However, if we focus on a group of individuals sharing a specific phenotype, we obtain better results, probably because there is more than one “type” of ASD. Thus, our main goal is to stratify the ASD into subgroups, not by the direct analysis of the BOLD signal as it is usually done, but by the functional brain network (FBN) based on the BOLD signal. The rationale is that ASD can be explained by the differences in how neurons interact. However, there are at least three technical drawbacks with this approach: (i) to the best of our knowledge, there is no method to cluster FBNs (note that this problem is different of the clustering of the vertices of the network as done by the spectral clustering algorithm); (ii) even individuals belonging to the same group present different FBNs (intrinsic fluctuation), which makes the analysis using standard computational methods unfruitful; and (iii) confounders such as age, gender, and other clinical parameters may affect the clustering structure. To overcome these problems, we will represent the FBNs as random graphs and assume that probabilistic models (e.g. Watts-Strogatz and Barabási-Albert models) generate the FBNs. Then, we will define that FBNs generated by the same model belong to the same group while FBNs generated by different models belong to different groups (similar to the Gaussian mixture model). Confounders’ effects will be removed by covarying the probabilistic distributions. We hope this stratification contribute for a better understanding of the mechanisms underlying ASD. To develop this method, Dr. Fujita of USP contacted Dr. Mourao-Miranda of UCL to complement his expertise in classification methods. Dr. Fujita works on statistical methods on graphs while Dr. Mourao-Miranda is specialist in machine learning with applications in psychiatric disorders. To train Dr. Fujita’s group in the field of machine learning, the partners will organize two courses, one at UCL and another at USP. Moreover, two Ph.D. candidates from USP will be sent for six months each one to UCL to study cutting-edge classification techniques. This project is essential to obtain novel insights, solidify this cooperation, and improve the quality of this research.    The Academy of Medical Sciences    Newton Advanced Fellowship    61814.0GBP
443    Professor Zaikin Alexey    University College London    2019-01-15    2022-01-14    HSM: Construction of graph-based network longitudinal algorithms to identify screening and prognostic biomarkers and therapeutic targets (GBNLA)    Autism spectrum disorder (ASD) is usually diagnosed by behavioral analyses and currently there is no objective test. Even the latest computational methods (e.g. support vector machine and deep learning) based on analyses of bloodoxygen-level dependent (BOLD) signal yield classification accuracies of about 60 to 70%, which are unsatisfactory for clinical use. However, if we focus on a group of individuals sharing a specific phenotype, we obtain better results, probably because there is more than one “type” of ASD. Thus, our main goal is to stratify the ASD into subgroups, not by the direct analysis of the BOLD signal as it is usually done, but by the functional brain network (FBN) based on the BOLD signal. The rationale is that ASD can be explained by the differences in how neurons interact. However, there are at least three technical drawbacks with this approach: (i) to the best of our knowledge, there is no method to cluster FBNs (note that this problem is different of the clustering of the vertices of the network as done by the spectral clustering algorithm); (ii) even individuals belonging to the same group present different FBNs (intrinsic fluctuation), which makes the analysis using standard computational methods unfruitful; and (iii) confounders such as age, gender, and other clinical parameters may affect the clustering structure. To overcome these problems, we will represent the FBNs as random graphs and assume that probabilistic models (e.g. Watts-Strogatz and Barabási-Albert models) generate the FBNs. Then, we will define that FBNs generated by the same model belong to the same group while FBNs generated by different models belong to different groups (similar to the Gaussian mixture model). Confounders’ effects will be removed by covarying the probabilistic distributions. We hope this stratification contribute for a better understanding of the mechanisms underlying ASD. To develop this method, Dr. Fujita of USP contacted Dr. Mourao-Miranda of UCL to complement his expertise in classification methods. Dr. Fujita works on statistical methods on graphs while Dr. Mourao-Miranda is specialist in machine learning with applications in psychiatric disorders. To train Dr. Fujita’s group in the field of machine learning, the partners will organize two courses, one at UCL and another at USP. Moreover, two Ph.D. candidates from USP will be sent for six months each one to UCL to study cutting-edge classification techniques. This project is essential to obtain novel insights, solidify this cooperation, and improve the quality of this research.    Medical Research Council    Research Grant    469754.0GBP
444    Ao. Univ. Prof.Dr. KAUTZKY-WILLER Alexandra    Medical University of Vienna    2019-01-21    2022-01-20    Gender Outcomes and Well-being Development Group (GOING-FWD GNP78)    Gender Outcomes INternational Group: to Further Well-being Development (GOING-FWD) Background: Beyond biological sex, gender is increasingly recognized as a pivotal determinant of health. However, there are no standardized gender measurements. We hypothesize that gender-related factors and their effect will vary substantially between countries and diseases. Aims: The overarching aims of this large Consortium are to integrate sex and gender dimensions in applied health research, to evaluate their impact on clinical cost-sensitive outcomes and patients reported outcomes related to quality of life in noncommunicable diseases including cardiovascular disease, metabolic disease, chronic kidney disease and neurological disease. We also aim to construct innovative ways to disseminate the application of gender measurement towards personalized approaches to chronic disease prevention, diagnosis and treatment. Methods: With a five-country transatlantic network comprised of 30 investigators, we will benchmark innovative solutions to measure gender in retrospective cohorts. Based on consensus, we will develop a framework to identify gender-related factors, as well as cost-sensitive and patients reported outcomes and measure their associations in 32 accessible cohorts of patients affected by cardiovascular, chronic kidney and neurological diseases and metabolic syndrome. Large database analysis and when appropriate machine learning approaches will allow the derivation of pan and within country disease specific gender scores which will be validated through e-Health and m-Health applications in prospective disease groups. Educational modules will be developed to promote awareness, implementation and dissemination. Innovation: As a five-country multidisciplinary Consortium with access to granular large databases, we are uniquely positioned to harness an innovative methodology that will provide a framework to close gender gaps in chronic disease management and promote knowledge transfer in the scientific community and clinical practice.    Austrian Science Fund FWF    02 International programmes    298552.8EUR
445    Dr Li Ke    University of Exeter    2019-05-01    2023-04-30    Transfer Optimisation System for Adaptive Automated Nature-Inspired Optimisation    Hard optimisation problems are ubiquitous across the breadth of science, engineering and economics. For example, in water system planning and management, water companies are often interested in optimising several system performance measures of their infrastructures in order to provide sustainable and resilient water/wastewater services that are able to cope with and recover from disruption, as well as wider challenges brought by climate change and population increase. As a classic discipline, significant advances in both theory and algorithms have been achieved in optimisation. However, almost all traditional optimisation solvers, ranging from classic methods to nature-inspired computational intelligence techniques, ignore some important facts: (i) real-world optimisation problems seldom exist in isolation; and (ii) artificial systems are designed to tackle a large number of problems over their lifetime, many of which are repetitive or inherently related. Instead, optimisation is run as a 'one-off' process, i.e. it is started from scratch by assuming zero prior knowledge each time. Therefore, knowledge/experience from solving different (but possibly related) optimisation exercises (either previously completed or currently underway), which can be useful for enhancing the target optimisation task at hand, will be wasted. Although the Bayesian optimisation considers incorporating some decision maker's knowledge as a prior, the gathered experience during the optimisation process is discarded afterwards. In this case, we cannot expect any automatic growth of their capability with experience. This practice is counter-intuitive from the cognitive perspective where humans routinely grow from a novice to domain experts by gradually accumulating problem-solving experience and making use of existing knowledge to tackle new unseen tasks. In machine learning, leveraging knowledge gained from related source tasks to improve the learning of the new task is known as transfer learning, an emerging field that considerable success has been witnessed in a wide range of application domains. There have been some attempts on applying transfer learning in evolutionary computation, but they do not consider the optimisation as a closed-loop system. Moreover, the recurrent patterns within problem-solving exercises have been discarded after optimisation, thus experience cannot be accumulated over time. The proposed research will develop a revolutionary general-purpose optimiser (as known as transfer optimisation system) that will be able to learn knowledge/experience from previous optimisation process and then autonomously and selectively transfer such knowledge to new unseen optimisation tasks. The transfer optimisation system places adaptive automation at the heart of the development process and explores novel synergies at the crossroads of several disciplines including nature-inspired computation, machine learning, human-computer interaction and high-performance parallel computing. The outputs will bring automation in industry, including an optimised/shortened production cycle, reduced resource consumption and more balanced and innovative products, which have great potentials to result in economic savings and an increase of turnover. The proposed methods will be rigorously evaluated by the industrial partners, first in water industry and will be expanded to a boarder range of sectors which put the optimisation at the heart of their regular production/management process (e.g. renewable energy, healthcare, automotive, appliance and medicine manufacturers).    Medical Research Council    Fellowship    1097642.0GBP
446    Professor Hogg Claire    Royal Brompton and Harefield NHS Foundation Trust    2019-03-01    2021-02-28    Improving Primary Ciliary Dyskinesia diagnosis using artificial intelligence.    Hard optimisation problems are ubiquitous across the breadth of science, engineering and economics. For example, in water system planning and management, water companies are often interested in optimising several system performance measures of their infrastructures in order to provide sustainable and resilient water/wastewater services that are able to cope with and recover from disruption, as well as wider challenges brought by climate change and population increase. As a classic discipline, significant advances in both theory and algorithms have been achieved in optimisation. However, almost all traditional optimisation solvers, ranging from classic methods to nature-inspired computational intelligence techniques, ignore some important facts: (i) real-world optimisation problems seldom exist in isolation; and (ii) artificial systems are designed to tackle a large number of problems over their lifetime, many of which are repetitive or inherently related. Instead, optimisation is run as a 'one-off' process, i.e. it is started from scratch by assuming zero prior knowledge each time. Therefore, knowledge/experience from solving different (but possibly related) optimisation exercises (either previously completed or currently underway), which can be useful for enhancing the target optimisation task at hand, will be wasted. Although the Bayesian optimisation considers incorporating some decision maker's knowledge as a prior, the gathered experience during the optimisation process is discarded afterwards. In this case, we cannot expect any automatic growth of their capability with experience. This practice is counter-intuitive from the cognitive perspective where humans routinely grow from a novice to domain experts by gradually accumulating problem-solving experience and making use of existing knowledge to tackle new unseen tasks. In machine learning, leveraging knowledge gained from related source tasks to improve the learning of the new task is known as transfer learning, an emerging field that considerable success has been witnessed in a wide range of application domains. There have been some attempts on applying transfer learning in evolutionary computation, but they do not consider the optimisation as a closed-loop system. Moreover, the recurrent patterns within problem-solving exercises have been discarded after optimisation, thus experience cannot be accumulated over time. The proposed research will develop a revolutionary general-purpose optimiser (as known as transfer optimisation system) that will be able to learn knowledge/experience from previous optimisation process and then autonomously and selectively transfer such knowledge to new unseen optimisation tasks. The transfer optimisation system places adaptive automation at the heart of the development process and explores novel synergies at the crossroads of several disciplines including nature-inspired computation, machine learning, human-computer interaction and high-performance parallel computing. The outputs will bring automation in industry, including an optimised/shortened production cycle, reduced resource consumption and more balanced and innovative products, which have great potentials to result in economic savings and an increase of turnover. The proposed methods will be rigorously evaluated by the industrial partners, first in water industry and will be expanded to a boarder range of sectors which put the optimisation at the heart of their regular production/management process (e.g. renewable energy, healthcare, automotive, appliance and medicine manufacturers).    National Institute for Health Research (Department of Health)    Full Grant    341942.0GBP
447    Professor Jefferson Emily    University of Dundee    2019-08-01    2024-07-31    MICA: InterdisciPlInary Collaboration for efficienT and effective Use of clinical images in big data health care RESearch: PICTURES    Hard optimisation problems are ubiquitous across the breadth of science, engineering and economics. For example, in water system planning and management, water companies are often interested in optimising several system performance measures of their infrastructures in order to provide sustainable and resilient water/wastewater services that are able to cope with and recover from disruption, as well as wider challenges brought by climate change and population increase. As a classic discipline, significant advances in both theory and algorithms have been achieved in optimisation. However, almost all traditional optimisation solvers, ranging from classic methods to nature-inspired computational intelligence techniques, ignore some important facts: (i) real-world optimisation problems seldom exist in isolation; and (ii) artificial systems are designed to tackle a large number of problems over their lifetime, many of which are repetitive or inherently related. Instead, optimisation is run as a 'one-off' process, i.e. it is started from scratch by assuming zero prior knowledge each time. Therefore, knowledge/experience from solving different (but possibly related) optimisation exercises (either previously completed or currently underway), which can be useful for enhancing the target optimisation task at hand, will be wasted. Although the Bayesian optimisation considers incorporating some decision maker's knowledge as a prior, the gathered experience during the optimisation process is discarded afterwards. In this case, we cannot expect any automatic growth of their capability with experience. This practice is counter-intuitive from the cognitive perspective where humans routinely grow from a novice to domain experts by gradually accumulating problem-solving experience and making use of existing knowledge to tackle new unseen tasks. In machine learning, leveraging knowledge gained from related source tasks to improve the learning of the new task is known as transfer learning, an emerging field that considerable success has been witnessed in a wide range of application domains. There have been some attempts on applying transfer learning in evolutionary computation, but they do not consider the optimisation as a closed-loop system. Moreover, the recurrent patterns within problem-solving exercises have been discarded after optimisation, thus experience cannot be accumulated over time. The proposed research will develop a revolutionary general-purpose optimiser (as known as transfer optimisation system) that will be able to learn knowledge/experience from previous optimisation process and then autonomously and selectively transfer such knowledge to new unseen optimisation tasks. The transfer optimisation system places adaptive automation at the heart of the development process and explores novel synergies at the crossroads of several disciplines including nature-inspired computation, machine learning, human-computer interaction and high-performance parallel computing. The outputs will bring automation in industry, including an optimised/shortened production cycle, reduced resource consumption and more balanced and innovative products, which have great potentials to result in economic savings and an increase of turnover. The proposed methods will be rigorously evaluated by the industrial partners, first in water industry and will be expanded to a boarder range of sectors which put the optimisation at the heart of their regular production/management process (e.g. renewable energy, healthcare, automotive, appliance and medicine manufacturers).    Medical Research Council    Research Grant    2851878.0GBP
448    Dr Walsh Simon    Imperial College of Science, Technology and Medicine    2019-02-01    2024-01-31    Fibrotic lung disease on high-resolution computed tomography: predicting disease behaviour using computer algorithms.    Hard optimisation problems are ubiquitous across the breadth of science, engineering and economics. For example, in water system planning and management, water companies are often interested in optimising several system performance measures of their infrastructures in order to provide sustainable and resilient water/wastewater services that are able to cope with and recover from disruption, as well as wider challenges brought by climate change and population increase. As a classic discipline, significant advances in both theory and algorithms have been achieved in optimisation. However, almost all traditional optimisation solvers, ranging from classic methods to nature-inspired computational intelligence techniques, ignore some important facts: (i) real-world optimisation problems seldom exist in isolation; and (ii) artificial systems are designed to tackle a large number of problems over their lifetime, many of which are repetitive or inherently related. Instead, optimisation is run as a 'one-off' process, i.e. it is started from scratch by assuming zero prior knowledge each time. Therefore, knowledge/experience from solving different (but possibly related) optimisation exercises (either previously completed or currently underway), which can be useful for enhancing the target optimisation task at hand, will be wasted. Although the Bayesian optimisation considers incorporating some decision maker's knowledge as a prior, the gathered experience during the optimisation process is discarded afterwards. In this case, we cannot expect any automatic growth of their capability with experience. This practice is counter-intuitive from the cognitive perspective where humans routinely grow from a novice to domain experts by gradually accumulating problem-solving experience and making use of existing knowledge to tackle new unseen tasks. In machine learning, leveraging knowledge gained from related source tasks to improve the learning of the new task is known as transfer learning, an emerging field that considerable success has been witnessed in a wide range of application domains. There have been some attempts on applying transfer learning in evolutionary computation, but they do not consider the optimisation as a closed-loop system. Moreover, the recurrent patterns within problem-solving exercises have been discarded after optimisation, thus experience cannot be accumulated over time. The proposed research will develop a revolutionary general-purpose optimiser (as known as transfer optimisation system) that will be able to learn knowledge/experience from previous optimisation process and then autonomously and selectively transfer such knowledge to new unseen optimisation tasks. The transfer optimisation system places adaptive automation at the heart of the development process and explores novel synergies at the crossroads of several disciplines including nature-inspired computation, machine learning, human-computer interaction and high-performance parallel computing. The outputs will bring automation in industry, including an optimised/shortened production cycle, reduced resource consumption and more balanced and innovative products, which have great potentials to result in economic savings and an increase of turnover. The proposed methods will be rigorously evaluated by the industrial partners, first in water industry and will be expanded to a boarder range of sectors which put the optimisation at the heart of their regular production/management process (e.g. renewable energy, healthcare, automotive, appliance and medicine manufacturers).    National Institute for Health Research (Department of Health)    Full Grant    1053348.0GBP
449    Dr Saxe Andrew    University of Oxford    2019-09-01    2024-08-31    Principles of Learning in Distributed Brain Networks    Hard optimisation problems are ubiquitous across the breadth of science, engineering and economics. For example, in water system planning and management, water companies are often interested in optimising several system performance measures of their infrastructures in order to provide sustainable and resilient water/wastewater services that are able to cope with and recover from disruption, as well as wider challenges brought by climate change and population increase. As a classic discipline, significant advances in both theory and algorithms have been achieved in optimisation. However, almost all traditional optimisation solvers, ranging from classic methods to nature-inspired computational intelligence techniques, ignore some important facts: (i) real-world optimisation problems seldom exist in isolation; and (ii) artificial systems are designed to tackle a large number of problems over their lifetime, many of which are repetitive or inherently related. Instead, optimisation is run as a 'one-off' process, i.e. it is started from scratch by assuming zero prior knowledge each time. Therefore, knowledge/experience from solving different (but possibly related) optimisation exercises (either previously completed or currently underway), which can be useful for enhancing the target optimisation task at hand, will be wasted. Although the Bayesian optimisation considers incorporating some decision maker's knowledge as a prior, the gathered experience during the optimisation process is discarded afterwards. In this case, we cannot expect any automatic growth of their capability with experience. This practice is counter-intuitive from the cognitive perspective where humans routinely grow from a novice to domain experts by gradually accumulating problem-solving experience and making use of existing knowledge to tackle new unseen tasks. In machine learning, leveraging knowledge gained from related source tasks to improve the learning of the new task is known as transfer learning, an emerging field that considerable success has been witnessed in a wide range of application domains. There have been some attempts on applying transfer learning in evolutionary computation, but they do not consider the optimisation as a closed-loop system. Moreover, the recurrent patterns within problem-solving exercises have been discarded after optimisation, thus experience cannot be accumulated over time. The proposed research will develop a revolutionary general-purpose optimiser (as known as transfer optimisation system) that will be able to learn knowledge/experience from previous optimisation process and then autonomously and selectively transfer such knowledge to new unseen optimisation tasks. The transfer optimisation system places adaptive automation at the heart of the development process and explores novel synergies at the crossroads of several disciplines including nature-inspired computation, machine learning, human-computer interaction and high-performance parallel computing. The outputs will bring automation in industry, including an optimised/shortened production cycle, reduced resource consumption and more balanced and innovative products, which have great potentials to result in economic savings and an increase of turnover. The proposed methods will be rigorously evaluated by the industrial partners, first in water industry and will be expanded to a boarder range of sectors which put the optimisation at the heart of their regular production/management process (e.g. renewable energy, healthcare, automotive, appliance and medicine manufacturers).    Wellcome Trust    Sir Henry Dale Fellowship    771226.0GBP
450    Dr Nellaker Christoffer    University of Oxford    2015-07-01    2019-07-30    Developing diagnostic methods for clinical genetics - phenotyping from faces in photos.    None    Medical Research Council    Fellowship    457586.0GBP
451    Dr Kiddle Steven    King's College London    2014-04-01    2017-03-31    Early identification of Alzheimer's disease: dynamic biomarkers for enrichment of trials    None    Medical Research Council    Fellowship    253519.0GBP
452    Dr Khastgir Siddartha    University of Warwick    2020-01-30    2024-01-29    Enabling a Novel Evaluation Continuum for Connected & Autonomous Vehicles (CAV)    The global Connected & Autonomous Vehicles (CAV) industry is estimated to be worth over £50billion (by 2035), with the UK CAV industry being projected over £3billion. Additionally, the UK Government's Industrial Strategy aims to bring fully autonomous cars without a human operator on the UK roads by 2021, one of the first countries in the world to achieve this. However, in order to realise this vision and the market potential, safe introduction of CAV is necessary, requiring significant research to overcome diverse barriers (technological, legislative and societal) associated with public deployment of CAV. While prototype CAV technologies have existed for some time now, ensuring the safety level of these technologies has been proving to be a hindrance to the commercialization of CAV technologies. The vision for CAV is coupled with the challenge of testing and safety analysis as it needs complex solutions to include interactions between a large number of variables and the environment. It is suggested that in order to prove that CAV are safer than human drivers, they will need to be driven for more than 11 billion miles. The vision for this fellowship is to support positioning the UK as the world leader in CAV research and innovation for a long lasting societal and economic benefit. This fellowship will develop pioneering testing methodologies and standards to enable robust and safe use of CAV with a focus on creating both fundamental knowledge and applied research methods and tools. At WMG, University of Warwick, UK, we have created a concept of the "evaluation continuum" for CAV, which involves using various environment like digital world, simulated environment, test track testing and real-world for testing. There are two aspects which are common to each of the evaluation continuum environments and also the focus areas of the fellowship research 1) Test Scenarios (input to the environment) 2) Safety Evidence (output of the environment). On the scenarios theme, ile the 11-billion-miles requirement has garnered a lot of publicity, the focus needs to be on what happens in those miles (i.e., smart miles which expose failures in CAV) and not on the number of miles themselves. As a part of this fellowship, three approaches will be explored to identify these smart miles. These include 1) using Machine Learning (ML) based methods including Bayesian Optimisation to create test cases for test scenarios, 2) Safety Of The Intended Functionality (SOTIF) (Innovative safety analysis of CAV) based test scenarios and 3) translating real-world data into executable test scenarios for a simulation tool. All these approaches will together contribute to the creation of a UK's National CAV Test Scenario Database, which will help coordinate the research work in various CAV projects part-funded by the UK Government and will prevent "reinventing of the wheel" in each of the projects with respect to test scenario identification. Industry trends in CAV suggest the widespread adoption of machine learning (ML) in the autonomous control systems. ML-systems by their structure are non-deterministic in nature, making the CAV system highly opaque in nature. Therefore, it is difficult to identify the reason of a failure in such ML-based systems and take the corrective measures. Thus, on the safety strand, fundamental research will be conducted as a part of this fellowship to explore how to make ML-based systems interpretable enabling us to explain the results. This is an essential requirement for safety of CAV due to the critical nature of their deployment and the mitigation of risk. In addition, the fellowship will also benefit from the fellow's first-hand experience as the UK's technical representative on the ISO standards committees, providing further insight and a clear route to deliver impact from the proposed research through the development of international standards, while also ensuring that the UK becomes a world leader in this area.    Medical Research Council    Fellowship    1110158.0GBP
453    Dr Oxtoby Neil    University College London    2020-01-01    2023-12-31    I-AIM: Individualised Artificial Intelligence for Medicine    Management and treatment of complex, chronic diseases such as Alzheimer's disease is one of the biggest challenges facing modern medicine. All clinical trials of investigational treatments for slowing or stopping the progression of Alzheimer's disease since 2003 have failed. This is likely due to the complexity and duration (decades) of Alzheimer's disease, coupled with the highly individual nature of the disease and its progression. Combined, this works against clinical trials by making it extremely difficult to identify and recruit a large group of individuals who are at the same stage of the same trajectory, and so who might benefit from a potential treatment. In principle, this challenge can be met by a set of modern computational approaches called data-driven disease progression modelling (D3PM), but some technological development is required first. D3PM aims to combine statistics with the latest developments in AI and data science to estimate disease signatures that describe how a progressive disease plays out from beginning to end. This active research field grew from basic supervised machine learning (pattern learning/recognition) to a range of phenomenological (top-down) models, and mechanistic (bottom-up) models that incorporate a range of AI tools including unsupervised machine learning (pattern discovery). D3PM signatures have shown promise for estimating severity and predicting progression in neurodegenerative diseases such as Alzheimer's disease, but they currently lack in individual level precision, and mechanistic rigour. This research and innovation project is a unique combination of technology development and translational product development: a series of novel technological developments for individualising D3PM and expanding mechanistic modelling; and translational efforts to develop drug-development tools based on this next-generation technology. In combination, this work will speed up drug-development by increasing the efficiency of clinical trials: recruiting smaller cohorts of suitable individuals will reduce costs and lead to fewer false-negative results - where a drug works on a fraction of the population, but the trial cannot detect it because the majority did not respond to treatment. The chosen application is Alzheimer's disease, but the ideas are fit-for-purpose for similarly complex, progressive diseases. This fellowship is a significant launchpad for my career. My ambition is to benefit patients and society by providing robust computational solutions to complex healthcare challenges. My vision for achieving this ambition starts by targeting the global epidemic of dementia, where I have identified an unmet need (improving clinical trials) and proposed a viable solution in the form of this research and innovation project. The fellowship provides essential resources to capitalise on my recent progress in the field and to personally develop into a UK-based future leader in using AI for medicine and health.    Medical Research Council    Fellowship    838376.0GBP
454    Dr Steeden Jennifer    University College London    2020-02-01    2024-01-31    Towards 10-minute Magnetic Resonance Scanning in Children - Developing Accelerated Imaging Using Machine Learning    Magnetic Resonance Imaging (MRI) scans play a vital role in helping many ill children, by finding out what the problem is and helping plan their treatment. MRI is safe because it does not use radiation. MRI scans produce good-quality pictures or images of many parts of the body, including the brain, heart, spine, joints and other organs. The main problem is they take a long time - often over an hour. During the scan, the child has to keep very still and may even need to hold their breath many times. This is especially hard for children and unwell patients. Hence, younger children under 8 years old need a general anaesthetic, to put them to sleep during the scan. In many childhood diseases, for example in cancer, children may need many MRI scans to follow up disease progression and treatment. Being put to sleep for all of these scans is not pleasant for the child and may occasionally cause problems. It also puts a lot of pressure on hospitals who need to find the doctors, beds, equipment and funds for this. One way of overcoming these problems would be to speed up the MRI scans so the children do not have to keep still or hold their breath. The simplest way of doing this is to collect less data for each image, but this causes so much distortion in the images that they cannot be used. There are some ways of converting these into useful images, but these are complicated and take too long to use in a hospital. Machine Learning is an upcoming way of teaching computers to find complicated patterns in large amounts of information. Recent advances mean that computers are now so powerful that they can learn effectively. Machine Learning has been successfully used for analysing many types of images, for example to perform de-noising, interpolation, image classification and border identification. Despite its popularity, only a few recent studies have shown its potential for reconstruction of MRI images. This is partly due to the greater complexity of the problem and importantly, the large amounts of data required to 'learn' the solution. At Great Ormond Street Hospital, we have MRI images from over 100,000 children and scan an additional 10,000 children each year, all of which we could use to help train and test Machine Learning technologies. I have already shown that basic Machine Learning techniques can remove distortions from MRI scans of the heart, so I am well placed to develop Machine Learning techniques to reconstruct MRI images from other children's diseases, as well as developing more advanced Machine Learning techniques. I showed Machine Learning to be faster than existing reconstruction methods and the images were of better quality than more conventional state-of-the-art techniques. However, much more work is needed to get Machine Learning working reliably in children's scans and to make the most of the possible benefits. If we can use fast scanning with Machine Learning we could shorten scan times from 1 hour to about 10 minutes for children having MRI scans. They would not have to keep completely still for the scan and would not have to hold their breath, therefore reducing the need to put patients to sleep. This would make MRI scanning far less difficult and daunting for children, and would eliminate the cost and side effects from the anaesthetic. Quicker scans would help reduce waiting lists and costs for the NHS. It would also mean that MRI scanning would be used far more often, so it could help many more children. Additionally, these techniques could enable MRI scans to become affordable in some countries for the first time.    Medical Research Council    Fellowship    989996.0GBP
455    Dr Steeden Jennifer    University of Tennessee Knoxville    2020-05-01    2021-04-30    RAPID: Impacts of Design and Operation Attributes of Mass-Gathering Civil Infrastructure Systems on Pathogen Transmission and Exposure    Engineering - This Rapid Response Research (RAPID) grant will support fundamental research to reveal how the design attributes and operation strategies will influence the transmission of and exposure to infectious pathogens within mass-gathering civil infrastructure systems. During the pandemic of 2019 novel coronavirus, the mass-gathering civil infrastructure systems, such as schools, airports, and public transit systems, can become hot spots for spreading the infectious disease. There remains a striking knowledge gap in understanding the impacts of infrastructure design and operation on the occurrence, distribution, transport, and viability of pathogens. It is imperative to address this knowledge gap. Results from this project will lead to bio-informed guidelines for managing critical civil infrastructure systems to prevent exposure of facility users to pathogenic microorganisms, and reduce risks of spreading infectious diseases, and thus alleviating burdens on healthcare systems and citizens. This project will provide much needed insights for infrastructure design reconfigurations and operation practices during the pandemic, in the recovery, and beyond to prevent further disease outbreaks and support healthy, resilient, and smart communities. As a result, this project will help promote public health, national security, and economic prosperity. In addition, this project will raise public science literacy and awareness of infectious diseases, and improve student education and training, as well as K-12 outreach and engagement activities. <br/><br/>The specific objective of this research is to parameterize relevant design attributes and operation strategies of infrastructure systems, and subsequently evaluate their impacts on pathogen transmission and exposure from spatiotemporal microbiome profiles. Three aims will be pursued: 1) identify and quantify the design attributes and operation strategies that may impact pathogen dynamics; 2) audit the types, abundance, and co-occurrence patterns, as well as spatiotemporal dynamics of microorganisms, particularly pathogens, associated with spatially and functionally distributed system components; and 3) characterize the impacts of design and operation on the transmission and exposure pathways of microorganisms in infrastructure systems. The spatial and functional interdependence of system components will be considered to parameterize design attributes based on building information modeling and syntactic analysis. The operation strategies will be modeled using integrated data sensing and simulation techniques. A model-informed sampling approach will be developed with molecular and metagenomics techniques to characterize spatiotemporal microbiome dynamics. The impacts of design and operation on microbial transmission and exposure pathways will be assessed using integrated source tracking and machine learning methods. At the nexus of infrastructure system engineering and environmental microbiology, this convergence research will provide unique insights into design attributes and operation practices impacting pathogen transmission and exposure in mass-gathering civil infrastructure systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    199809.0USD
456    Dr Steeden Jennifer    Massachusetts Institute of Technology    2020-04-01    2021-03-31    RAPID: Immunogenicity of SARS-CoV2 to Human T Cells    Mathematical and Physical Sciences - Pandemics caused by infectious pathogens have plagued humanity since antiquity. The Coronavirus Disease 2019 (COVID-19) caused by the SARS-CoV-2 virus is currently spreading across the world rapidly, including in the United States, with major adverse impact on health and the economy. The SARSCoV-2 outbreak has led to several urgent efforts to develop vaccines that may offer protection against this virus. It is unknown as to whether the current approaches being pursued will elicit protective immune responses in humans. While vaccines have been very effective against many pathogens, the empirical methods for vaccine development pioneered by Pasteur and Jenner over two centuries ago have failed to produce effective vaccines against Human Immune Deficiency Virus, Malaria, Tuberculosis, and many other pathogens. Therefore, rational design of vaccines based on a mechanistic understanding of the pertinent virology and immunology is being pursued, and these efforts include work that is rooted in statistical physics. SARSCoV-2 is phylogenetically most similar to SARS-CoV. This project will use a machine learning approach to understand how the SARS-CoV-2 virus interacts with the immune T cells. This work will directly impact the design of SARS-CoV-2 vaccines and vaccines against future endemic-causing pathogens.<br/><br/>Analyses of patients who have recovered from SARS-CoV shows that antibody responses are not prevalent a few years later, but memory T cell responses are durable and may offer long-term protection. The main questions addressed by this project are 1. Will the SARS-CoV peptides targeted by human T cells that are mutated in SARS-CoV-2 still elicit human T cell responses - i.e. are they immunogenic? 2: Are the 102 peptides identified by host major histocompatibility molecules binding assays alone that are common between SARS-CoV and SARS-CoV-2 immunogenic in humans? If not, they are irrelevant from vaccine design perspective. The goal of the work proposed here is to take a physics-based machine learning approach to determine the immunogenicity of SARS-CoV-2 proteins to human T cell responses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    124472.0USD
457    Dr Steeden Jennifer    University of Southern California    2020-05-01    2021-04-30    RAPID: ReCOVER: Accurate Predictions and Resource Allocation for COVID-19 Epidemic Response    Computer and Information Science and Engineering - The recent outbreak of COVID-19 and its world-wide impact calls for urgent measures to contain the epidemic. Predicting the speed and severity of infectious diseases like COVID-19 and allocating medical resources appropriately is central to dealing with epidemics. Epidemics like COVID-19 not only affect world-wide health, but also have profound economic and social impact. Containing the epidemic, providing informed predictions and preventing future epidemics is essential for the global population to resume their day-to-day work and travel without fear. Shortage of resources puts undue stress on healthcare system further risking health of the community. Preparedness and better management of available resources would require specific predictions at the level of cities and counties around the world rather than solely at the level of countries. The project will provide a predictive understanding of the spread of the virus by developing machine learning based computational models to study the transmission of the virus and evaluate the impact of various interventions on disease spread. The project will learn infection prediction models for COVID-19 considering the following. (i) Predicting at state/county/city-level rather than country-level as finer granularity is essential in planning and managing resources. (ii) How infectious a person is changes over time. Learning the model through observed data will help in understanding of the temporal nature of the virality. (iii) At such granularity travel is a significant reason for the spread and needs to be accounted for. (iv) Available data needs to be ?corrected? by finding the number of underlying unreported cases that are not observed and yet influence the epidemic dynamics. The project will also solve the resource allocation problem based on the prediction ? for instance if a certain number of masks will be available next week in a certain state, how should they be distributed across different hospitals in the state (which hospitals and how many in each state)?<br/><br/>Proposed project ReCOVER will use a novel fine-grained, heterogeneous infection rate model to perform predictions at various granularities (hospital/airports, city, state, country) while accounting for human mobility. ReCOVER will integrate data from various sources to build highly accurate models for prediction of the epidemic across the world at various granularity. Due to the ability to capture temporal heterogeneity in infection rate, the approach has the potential to provide insights into infectious nature of COVID-19 which are not fully understood yet. The project will address the issue of unreported cases through temporal analysis of historical infections and correct the data. The right granularities of modeling will be automatically identified, e.g., when to model a state over its cities to trade-off precision for higher reliability in predictions. The proposed project also formulates and solves a resource allocation problem that can guide the response to contain the epidemic and prevent future outbreaks. This is provided by optimal solutions to resource allocation over a network where each node (representing a region) has a function that captures probabilistic response. While the project obtains data with COVID-19 in consideration, the model and algorithms developed under the project are applicable to a wide class of contagious diseases. The project will culminate into an interactive customizable tool that can be used to perform predictions and resource management by a qualified user such as a government entity tasked with managing the epidemic response. The data and code will also be shared with research community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    158592.0USD
458    Dr Steeden Jennifer    Colorado State University    2020-05-01    2021-04-30    RAPID: ENSURING INTEGRITY OF COVID-19 DATA AND NEWS ACROSS REGIONS    Computer and Information Science and Engineering - Large amounts of epidemiological data are being generated and collected from a variety of sources to understand the impact and propagation of COVID-19. Similarly, huge amounts of news articles are generated and disseminated about the pandemic to keep the population informed. The appropriateness of the actions taken by individuals, corporations, and governments are often based on the quality of data and news. Thus, ensuring the quality of data and news is important. However, malicious actors can alter the attributes of data records, insert spurious records, or suppress records causing any analysis to be inadequate and misinformation to be propagated. This project addresses the critical problem of defining and identifying spurious data and news concerning COVID-19, and tracking the source of misinformation. The project novelty lies in the development of an approach and associated toolset that adapts and combines Machine Learning technologies to detect spurious data and misinformation and presents the results in a manner that is easy for end users to understand and interpret. The approach detects discrepancies in COVID-19 data and traces the flagged discrepancies back to the data sources. The results obtained from the news sources and those obtained from the medical data analysis are compared to determine correlations between the quality of news and the degree and type of data manipulation performed at any region. The project?s impacts are on significantly enhancing the ability to perform accurate scientific analysis, and detecting and explaining news manipulation with respect to COVID-19. The scientific principles developed in the project are expected to be useful outside the medical domain. The PI and the students identified for this project are minorities. The project will be carried out in the Computer Science Department at Colorado State University which is a BRAID affiliate.<br/><br/>COVID-19 data discrepancies are related to (1) single records, where some field is modified, (2) sequence of records over time forming a temporal dimension, where spurious records have been inserted or records have been suppressed, and (3) sequences of records across regions forming a spatial dimension, where there is a pattern of manipulation or information disclosure across regions. The approach determines the appropriate combination of autoencoders, Long Short-Term Memory (LSTM), Temporal Convolution Network (TCNs), and Convolution Neural Networks (CNNs) that can work with data obtained from medical sources and news containing both spatial and temporal dimensions. The tools help the investigators? collaborators at the University of Colorado Anschutz Medical Center and Center for Disease Control and Prevention to perform data integrity checking of medical records and to provide explanations of integrity violations. The tools also handle different types of data and news alterations pertaining to COVID-19.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    199748.0USD
459    Dr Steeden Jennifer    University of California-Irvine    2020-05-01    2021-04-30    RAPID: The Role of Emerging Virtual Cultures in the Prevention of COVID-19 Transmission    Social, Behavioral and Economic Sciences - The COVID-19 pandemic has transformed our relationship to the physical world. Social distancing guidelines have led many people to avoid all forms of public life, from concerts and restaurants to everyday interaction in parks, neighborhoods, and the homes of family and friends. In response there has been a massive increase in online interaction: the internet has suddenly become the primary way that many Americans socialize, labor, and learn. It is crucial to gain a better understanding of how the emergence of these changes is related to the pandemic. Even if a vaccine is discovered, preventing catastrophic levels of COVID-19 transmission into the next few years will depend on social distancing that can be sustained and integrated with work, education, and community. This means going online. The starting point for addressing this global challenge is thus the fact that what we call ?social distancing? is really physical distancing. Successful physical distancing will rely on new forms of social closeness online. Yet there is not just one ?online.? A rapid and effective response requires clarifying the impact of virtual worlds as part of different forms of online interaction that comprise a virtual culture: social network sites, streaming websites, and multiplayer platforms. The project will also train graduate student researchers in methodological approaches for studying online cultures. <br/><br/>This research will be conducted in a densely trafficked virtual world. Virtual worlds are places where individuals interact with avatars in online environments. The investigators have conducted research in a virtual world context for over a decade, and thus have detailed baseline data with which to examine what is happening as a large number of individuals enter that virtual world due to the COVID-19 pandemic. What is the sudden move to virtual worlds doing in terms of social closeness and interaction? How does co-presence in virtual place transform intimacy and collaboration? How might this provide innovative strategies for preventing viral transmission, by forging new forms of social closeness in the context of physical distancing? To investigate these questions, the researchers will conduct participant observation, individual interviews, and group interviews. The study will compare individuals who have spent time in the virtual world for years with individuals who have entered the virtual world after COVID-19. Findings from this research will provide insight into the specific possibilities virtual worlds are providing in the circumstances of societies reshaped by COVID-19. In these new circumstances, virtual worlds will be one element of an online ecosystem linking drones, robots, and autonomous vehicles to mobile devices, social network sites, online games and streaming, augmented reality, artificial intelligence, machine learning, and data analytics. The research will thus provide a better understanding of the place of virtual worlds in this emerging online ecosystem.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    195619.0USD
460    Dr Steeden Jennifer    WASHINGTON UNIVERSITY    2020-05-15    2021-04-30    RAPID: A multiscale approach to dissect SARS-CoV-2 attachment to host cells and detect viruses on surfaces    Biological Sciences - The 2019 novel coronavirus, identified as the cause for the pneumonia pathology reported in Wuhan, spread quickly and became a global pandemic. The project will employ experimental methods to develop sensors for the detection of SARSCoV-2 from environmental samples and develop predictive models for virus attachment to cells by applying computational machine learning methods. The outcome of this project will contribute to the development of proactive measures to identify viruses with pandemic potential before they are able to transmit and spread broadly among humans. The graduate students involved in this research will gain experience in protein biochemistry, fluorescence microscopy, and computational simulations and experience utilizing those skills to problems of societal importance.<br/><br/>This NSF Rapid response Research (RAPID) project will support a project that is aimed to characterize receptor interactions mediated by the Spike protein (S) of SARS-CoV-2. Development of fluorescence-based assays to characterize SARSCoV-2 attachment to Angiotensin converting enzyme (ACE2)-functionalized surfaces with controlled density and mobility, identifying peptide mimics of the ACE2 ectodomain for the development of sensors to detect SARSCoV-2 from environmental samples, and develop and validate predictive models of CoV attachment from primary sequence using machine learning constitute the specific goals of this project.<br/><br/>This RAPID award is made by the Molecular Biophysics Program in the Division of Molecular and Cellular Biosciences, using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    200000.0USD
461    Dr Steeden Jennifer    Columbus State University    2020-05-15    2021-04-30    RAPID/Collaborative Research: Quantifying Social Media Data for Improved Modeling of Mitigation Strategies for the COVID-19 Pandemic    Engineering - This Rapid Response Research (RAPID) grant will support research that will contribute new knowledge related to modeling social behavior and community activity during the COVID-19 pandemic, as well as future pandemics with COVID-19 characteristics. The model focuses on compliance with mitigation strategies and public health guidelines, thus enabling the selection of policies that are most effective in promoting both the progress of science and advancing national health and prosperity. Various pandemic models are currently being used to predict the spread of a virus and establish which mitigation strategies are the most effective. These models are heavily based on assumptions and may include an oversimplified reality of how populations react and behave. This research will provide needed knowledge and methods for the development of a model of how individuals in the U.S. react to certain mitigation strategies, such as social-distancing, stay-at-home orders, quarantines, and travel advisories, by mining and analyzing social media data during the COVID-19 crisis. This enhanced modeling approach and its resultant model will be of great value to disaster response managers and policy/decision makers to understand human social behavior. This work allows assessment of the effectiveness of mitigation strategies and public health guidelines during pandemics (and other crises). This project will also form the basis of a publicly available case study suitable for university level students that can be widely incorporated in courses. <br/><br/>Although individual-based and homogeneous mixing pandemic models provide useful insights and predictive capabilities within a range of possibilities, they are highly sensitive to people?s actions. This research aims to provide an enhanced approach to model social behavior and community activity during a pandemic in terms of compliance with mitigation strategies and public health guidelines. Social media data present a brief window of opportunity for research on how, and to what extent, the public does or does not comply with the recommended mitigation strategies and public health guidelines. The research team will collect real-time data from social media related to COVID19-exposed regional populations in the U.S. The data will be analyzed using machine learning techniques to identify non-mutually exclusive clusters of people based on similarity of their demographic, geographic, and time information, and establish relationships among clusters. The analyzed data will form the basis of a data-driven multi-paradigm simulation model that captures changes in public sentiment over time, quantifies the resistance/compliance with mitigation strategies and health guidelines, and gauges overall effectiveness of various mitigation strategies and advice over time.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    13406.0USD
462    Dr Steeden Jennifer    Northeastern University    2020-05-15    2021-04-30    RAPID: D3SC: Identification of Chemical Probes and Inhibitors Targeting Novel Sites on SARS-CoV-2 Proteins for COVID-19 Intervention    Mathematical and Physical Sciences - The life cycle of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) involves a number of viral proteins and enzymes required for infectivity and replication. Inhibitors that target these enzymes serve as potential therapeutic interventions against coronavirus disease 2019 (COVID-19). With this award, the Chemistry of Life Processes program in the Chemistry Division is supporting the research of Drs. Mary Jo Ondrechen and Penny J. Beuning from Northeastern University to apply computational methods to identify sites in SARS-CoV-2 proteins that would be good targets for binding inhibitors. The project uses artificial intelligence methods developed at Northeastern University to identify pockets and crevices in the structures of viral proteins that may serve as new targets for the development of antiviral agents. Large datasets of natural and synthetic compounds are computationally searched for molecules that fit into these alternative sites, and any compounds that fit will be experimentally tested for their ability to inhibit the functions of these viral enzymes. The project provides training in computational chemistry and biochemical analysis to graduate students and postdoctoral associates.<br/><br/>This project uses the unique Partial Order Optimum Likelihood (POOL) machine learning (ML) method developed by Dr. Ondrechen?s group to predict multiple types of binding sites in SARS-CoV-2 proteins, including catalytic sites, allosteric sites, and other interaction sites. The goals of this project are to apply the POOL-ML method to identify the binding sites on viral pathogen SARS-CoV-2 proteins using the three-dimensional protein structures as input. Molecular dynamics simulations are used to generate conformations for ensemble docking. Compounds from the large molecular databases are computationally docked into the predicted sites to identify potentially strong binding ligands. Candidate ligands to selected SARS-CoV-2 proteins, including the main protease and 2?-O-ribose RNA methyltransferase, are experimentally tested in vitro for binding affinity and the effect of the best predicted inhibitors on catalytic activities determined by direct biochemical assays. All the SARS-CoV-2 protein structures in the Protein Data Bank (PDB) are studied. Compound libraries for the study include: a) selected 2600+ compounds from the ZINC and Enamine databases that are already being manufactured; b) a library of 20,000+ compounds found in foods that the team recently gained access to; these potentially hold some special advantages, including ready availability in the public domain and low cost; and c) the March 2020 open access CAS (American Chemical Society) database of 50,000 compounds with known or potential anti-viral activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    165808.0USD
463    Dr Steeden Jennifer    Harvard University    2020-05-01    2021-04-30    RAPID:Collaborative Research: Computational Drug Repurposing for COVID-19    Computer and Information Science and Engineering - With the disruptive nature of the COVID-19 pandemic, effective treatments could save the lives of severely ill patients, protect individuals with a high risk of infection, and reduce the time patients spend in hospital beds. However, there are currently no effective treatments for COVID-19. Traditional methodologies take years to develop and test compounds from scratch. Machine learning provides promising new approaches to repurpose drugs that are safe and already approved for other diseases. This project will develop a machine learning toolset to expedite the development of safe and effective medicines for COVID-19. The toolset will rapidly identify safe repurposing opportunities for approved and experimental drugs. It will predict whether treatments may have therapeutic effects in COVID-19 patients, allowing the identification of drugs and drug cocktails that are safe and plentiful enough to treat a substantial number of patients. By putting tools in the hand of practitioners, the activities in this project will have an immediate impact. They will result in actionable predictions that are accurate and interpretable. <br/><br/>Recently, the principal investigators have developed a series of machine learning tools to identify drug repurposing opportunities. Building on foundational previous work, in this project, the principal investigators will first build a large COVID-19 focused knowledge graph that will capture fundamental and COVID-19-specific biological knowledge. The graph learning methods will be adapted to identify safe drugs and drug cocktails for COVID-19. To predict the safety of cocktails with two or more drugs, the methods will generalize to an exponentially large space of high-order drug combinations. In addition to drug safety, efficacy is a crucial endpoint for drug development. The project will develop a novel graph neural network (GNN) method to identify efficacious drug repurposing opportunities, even for diseases, such as COVID-19, that do not yet have any drug treatments and thereby, no label, supervised information. The method will predict what drugs and drug combinations may have a therapeutic effect on COVID-19. Finally, the principal investigators will integrate the developed tools into a complete, explainable framework that will generate predictions, provide explanations, and incorporate human feedback into the machine learning loop. This project will provide new, open tools for rapid drug repurposing that will be relevant for COVID-19 and other emerging pathogens. Additionally, the project will provide unique opportunities for multi-disciplinary curriculum development, training and advising, and professional activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    99863.0USD
464    Dr Steeden Jennifer    University of Minnesota-Twin Cities    2020-05-15    2021-04-30    RAPID: The effect of contact network structure on the spread of COVID-19: balancing disease mitigation and socioeconomic well-being    Biological Sciences - What makes COVID-19 spread rapidly in some places, yet slowly in others? How should society lessen social distancing while limiting an increase in infections? To answer these questions, this Rapid Response Research (RAPID) project seeks to understand how patterns of interpersonal interaction (?structure?) in social contact networks affect disease spread in a population. The researchers will simulate a disease spreading through a variety of social contact networks, and use machine learning to relate each network?s structure to the number and timing of new infections. By limiting structures related to increased disease, societies may be able to reopen other parts of their economies while still curbing overall disease spread. The researchers will produce an interactive web application for the public and decision-makers to visualize trade-offs between reducing disease and maintaining social cohesion. This research will support the professional development of an early career scientist.<br/><br/>This research aims to determine the inherent risk of SARS-CoV-2 spread based on contact network structure. The researchers will use machine learning to 1) identify network structures that influence disease spread and 2) predict disease spread on empirical contact networks. Important network structures will serve as targets for simulated disease mitigation interventions (e.g. reducing structures that increase levels of disease or increasing structures that reduce disease levels). Finally, the researchers will investigate whether future outbreaks of COVID-19 or other diseases could be alleviated through optimizing social contact networks ahead of time. The outcomes of this research will inform and facilitate quick, efficient interventions to reduce the social and economic costs of COVID-19. This research will develop a general framework for relating disease to network structure. Thus, results can be generalized beyond the current pandemic, serving to further our understanding of potential future waves of COVID-19, as well as other directly-transmitted diseases in humans, livestock, and wildlife.<br/><br/>This RAPID award is made by the Ecology and Evolution of Infectious Diseases Program in the Division of Environmental Biology, using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    199136.0USD
465    Dr Steeden Jennifer    New Mexico Highlands University    2020-05-15    2021-04-30    RAPID: Machine Learning Methods to Understand, Predict and Reduce the Spread of COVID-19 in Small Communities    Mathematical and Physical Sciences - The ongoing COVID-19 outbreak has recently reached pandemic status spreading all around the world. The severity of the pandemic, along with an enormous impact on world?s economy and society, has forced governments to introduce emergency measures. It is essential to utilize the available statistical data from trusted sources in order to model and evaluate the dynamics of the pandemic spread, to not only better understand such complex systems, but to learn and develop possible solutions to prevent further spread of current and/or similar future outbreaks. Thus, this research, devoted to the development of mathematical models of COVID-19 pandemic spread, addresses an urgent national need. Faculty and students in computer science, anthropology, and computational chemistry at New Mexico Highlands University have formed a diverse group for finding a solution to the complicated problems of the description and prediction of COVID-19 spread. This multidisciplinary project is expected to yield a better understanding of the interconnections among many factors that contribute to the spread of COVID-19. Statistical data will be collected in regions of Northern New Mexico, including San Juan and McKinley Counties in the Navajo Nation and Los Alamos county outside of the Navajo Nation. Analysis of the collected statistical data along with socio-cultural assessment from this project will be presented to New Mexico (NM) tribal and health authorities. The project will aim to provide a scientific basis for the prediction of disease spread and will consider scenarios associated with the possibility of another wave of the pandemic. Students from this minority-serving institution involved in the project will obtain valuable experience in the application of advanced machine learning models and methods in providing fast robust reaction to a national health, economic, and societal crisis.<br/><br/>In this study, machine learning methods will be used to analyze pandemic spread scenarios in different regions and to glean the most important features of the data characterizing the spread. The research team will use both traditional machine learning techniques and advanced methods, such as artificial neural networks, allowing development of virus incidence model capturing dependencies in both linear and nonlinear domains. The work will concentrate on understanding disease spread with regard to multiple socioeconomic factors. The problem can be treated as a sequence modeling one; so, recurrent neural networks and more complex models based on their recurrent cells might be one promising direction. The next step will be to assemble datasets for small isolated communities with different socioeconomic backgrounds and ethnicities ? comparing Navajo Indians living on the Navajo reservation to Los Alamos County (NM) ? and to test the applicability of the developed model to these regions. The spatiotemporal data available on the spread is heterogeneous in character. An important goal of this research is to classify the collected data with respect to the similarity in the epidemic curve behavior and then build separate models for different regions according to this classification. The proposed model will be used for prediction of future incidents and to produce the most effective non-medical recommendations for suppression and prevention of future viral outbreaks.<br/><br/>This research is supported by the Partnerships for Research and Education in Materials (PREM) program and the Condensed Matter and Materials Theory (CMMT) program in the Division of Materials Research in the Directorate for Mathematical and Physical Science using supplemental funds made available by the Coronavirus Aid, Relief, and Economic Security (CARES) Act.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.    National Science Foundation    Research Grant    185747.0USD
466    Dr Steeden Jennifer    University of British Columbia    2020-05-15    2021-04-30    Digital Virtual Support of Cases and Contacts to Novel Coronavirus (COVID-19): Readiness and Knowledge Sharing for Global Outbreaks (WelTel PHM)    The global outbreak of COVID-19 is the latest example of a rapidly spreading infectious outbreak with global impact. Infected patients with mild symptoms and asymptomatic contacts need to be isolated, ideally without overwhelming health facilities. WelTel, an integrated virtual care and patient engagement solution, emerged as an innovation initially to support the global HIV pandemic through a Canadian-Kenyan partnership over a decade ago. Co-founded by the lead investigator and registered in British Columbia, WelTel has continued to integrate research into a richly featured virtual care platform that can be used on the frontlines of healthcare delivery. The study aims to: 1-Deploy and co-optimize WelTel to assist in home monitoring and support of COVID-19 cases and contacts; 2- Determine essential linkages and technical demands of the digital health ecosystem for data security purposes and integration into other electronic health records (EHR) & health information management systems (HIMS); 3-Evaluate communication and other metadata captured by the system for public health quality improvement to better understand and reduce barriers (such as stigma); 4-Use novel computing approaches such as natural language processing (NLP) and machine learning to harness artificial intelligence (AI) capabilities to model, predict, and provide insights into future precision public health approaches. Collaborators have necessary expert skills in quantitative and qualitative research methods for rigorous assessment, and come from the countries targeted for the research deployment (Canada, UK, US, Kenya, and Rwanda). A rapid digital landscape analysis will also be done as a part of this research. Virtual care may be an efficient, cost-effective way to provide the necessary public health monitoring and support for patients and contacts of COVID-19 and future emerging communicable pathogens, as well as can inform public health quality improvement and precision care.    Canadian Institutes of Health Research    Research Grant    500000.0CAD
467    Prof. Cotton Matthew    MRC/UVRI & LSHTM Uganda Research Unit    2020-05-15    2021-04-30    African COVID-19 Preparedness (AFRICO19)    Our project, AFRICO19, will enhance capacity to understand SARS-CoV-2/hCoV-19 infection in three regions of Africa and globally. Building on existing infrastructures and collaborations we will create a network to share knowledge on next generation sequencing (NGS), including Oxford Nanopore Technology (MinION), coronavirus biology and COVID-19 disease control. Our consortium links three African sites combined with genomics and informatics support from the University of Glasgow to achieve the following key goals: 1. Support East and West African capacities for rapid diagnosis and sequencing of SARS-CoV-2 to help with contact tracing and quarantine measures. Novel diagnostic tools optimized for this virus will be deployed. An African COVID-19 case definition will be refined using machine learning for identification of SARS-CoV-2 infections. 2. Surveillance of SARS-CoV-2 will be performed in one cohort at each African site. This will use established cohorts to ensure that sampling begins quickly. A sampling plan optimized to detect initial moderate and severe cases followed by household contact tracing will be employed to obtain both mild to severe COVID-19 cases. 3. Provide improved understanding of SARS-CoV-2 biology/evolution using machine learning and novel bioinformatics analyses. Our results will be shared via a real-time analysis platform using the newly developed CoV-GLUE resource.    Wellcome/Department for International Development    Research Grant    2001990.0GBP
468    Professor Herten Dirk-Peter    University of Birmingham    2020-08-01    2023-08-01    AMS Professorship Award for Professor Dirk-Peter Herten, University of Birmingham    I want to establish a world-leading research group in receptor signalling by providing tools and support for quantitative studies with molecular sensitivity for biomedical research. The prestigious award of the AMS fellowship will enable the purchase of cutting-edge microscopy tools and allow further automation of our microscopes to increase throughput and provide the capacity for super-resolved 3D reconstructions of whole cells. My position in COMPARE aligned to the Institute of Cardiovascular Sciences (ICVS) and the School of Chemistry at the University of Birmingham (UoB) adds expertise in methods development to the COMPARE advanced microscopy facility at UoB and will allow me to work on my quantitative microscopy methods. My appointment creates a unique opportunity to support researchers and clinicians in their search for solutions to urgent biomedical questions, like HIV infection or thrombosis, by use of existing techniques and novel approaches. I will involve Jeremy Pike (UoB, Data Analysis Officer) and Iain Styles (Computer Sciences, Turing Institute Fellow and Deputy Director of COMPARE, UoB) in machine learning approaches for the fast processing of 3D microscopy data. Additionally, I will initiate a planned industry collaboration with IRIS Biotech (Germany) to elaborate on the potential commercialisation of fluorescent probes for super-resolution microscopy, chemical multiplexing and metal cation sensing based on my patent (DE 10 2016 012 162.9). A challenge in methods development is to identify suitable targets and questions for the novel approach. The fellowship will enable collaborations with colleagues on campus and within COMPARE to increase the number of biomedical targets and facilitate translational application of my techniques which focus on T cell receptor signalling and more recently on GPCRs. Examples of new collaborations include the platelet immunoglobulin C-type lectin-like receptors GPVI and CLEC-2 (with Steve Watson, Steve Thomas and Natalie Poulter, ICVS) which are novel targets for a variety of thrombosis and thrombo-inflammatory disorders, and in the chemokine GPCR family (with Dimity Veprintsev, University of Nottingham - UoN) that play an important role in diseases like hypertension, hypoxia, or hypoglycaemia. In this context, I envision a key training centre for advanced microscopy at UoB also addressing problems like fluorescence labelling and probe development. This will involve colleagues in the School of Chemistry as well as COMPARE researchers working on fluorophores (Jon Preece, UoB) and bioactive molecules (Liam Cox, UoB; Barrie Kellam, UoN). COMPARE offered me a generous starting budget COMPARE (£ 450K) and the institute is in the process of refurbishing the labs to my needs, to move 3 co-workers together with my equipment by February 2020. Since I am unable to bring any overseas funding, the award will allow me to get accommodated with the UK grant application system and plan projects to strengthen my translational research (MRC) and the development of novel probes and techniques (BBSRC and EPSCR) without losing any momentum in my ongoing research activities. I have already applied for a translational research grant (BBSCR TRDF) in collaboration with Robert Henderson (University of Edinburgh), which involves quantitative imaging microscopy through use of his 256x256 APD camera.    The Academy of Medical Sciences    AMS Professorship Scheme Round 2    470621.92GBP
469    Professor Huang Xiaolin    Tongji Hospital, Tongji Medical College, Huazhong University of Science and Technology    2018-02-01    2019-06-30    Low cost robotic orthosis for stroke treatment in rural China    China is the worst affected developing country with 2.5M new stroke cases each year and 11.1M stroke survivors at any given time. In the past decades, due to lower socioeconomic status, less stroke awareness, and inequitable distribution of medical resources to rural areas, the incidence rate and burden of stroke in China has increased disproportionately in rural areas of the northeast and central regions. Rehabilitation programmes have been shown to be extremely effective in reducing the disability and restoring walking ability through early training. However, in most rural areas of China no such medical rehabilitation centres or hospitals exist leaving the rural population without the therapies which will allow them to regain mobility functions after stroke. This leads to disability and has broader negative socioeconomic impacts. This project seeks to gather evidence directly from the key stakeholders in the beneficiary ODA countries (China and Kazakhstan) about what their problems and requirements are. The networking activities will allow us to build a full proposal based around actual rather than assumed need, and therefore maximise the impact achieved. The project aims to establish a multi-disciplinary consortium including experts from rehabilitation robotics, sensing, machine learning, physiotherapy and rehabilitation medicine. We aim to provide low-cost robotic solutions to stroke survivors in remote home and community environments. This would reduce therapists’/stroke carers' demand in rural areas of ODA countries whilst maximizing the stroke survivor’s sense of intention, involvement, interaction and achievement of limb movement, with greater likelihood of successful rehabilitation and improved quality of life.    The Academy of Medical Sciences    AMS Professorship Scheme Round 2    22000.0GBP
470    Professor Huang Xiaolin    TWO WORLDS CONSULTING LIMITED    2018-02-01    2019-06-30    udu: AI Platform for Pandemic Intelligence    The UK, amongst others, lacks a coherent infrastructure to support effective direct and timely collection and analysis of pandemic data, about both the progressIon of Covid-19 itself and the population response to public policy aimed at mitigating and profiling its progress. Refining policy and informing the judgement calls required to navigate the balance between lockdown and economic damage requires both accurate data and the ability to rapidly model multiple, 'What if?' scenarios. Current data intelligence systems are partial, fragmented, incomplete, lag reality and, in most cases can only surface what they have specifically been asked to look for. AI systems used to look for patterns are often constrained by the quality and range of data available to them. Existing models tend to look at single factors in isolation, e.g. not taking into account multiple sources of mortality data or failing to take account factors such as population mobility and behaviour, the impact of events such as Cheltenham races, sunny bank holiday weather or other regional and seasonal variations. This can only be addressed through a more holistic approach to data collection and integration. This project therefore uses an advanced data intelligence platform, udu, which is capable of integrating a wide range of data from multiple sources and of multiple types and of actively discovering new data online. It then uses software that can self-organise itself around a task to discover relationships between and patterns in the collected data to provide an inferential view of pandemic impact, policy effectiveness and population behaviour. udu has been established for several years in niche markets. Here, we are building on previous experience by Two Worlds in using udu to create systems for the predictive analysis of environmental change to public health for the first time. The resulting system is intended to be capable of supporting direct exploration by human users, providing an interface (API) to allow other teams to test their own analytic models against the datascape created by udu and supporting local and external machine learning systems with a wider range of high quality data and analysis.    UK Research and Innovation    AMS Professorship Scheme Round 2    49984.0GBP
471    Professor Huang Xiaolin    University of Basel    2009-10-01    2013-03-31    Inference of post-transcriptional regulatory codes involving miRNAs and RNA binding proteins    Although for many years transcription factors held the center stage inthe regulation of gene expression, a very complex post-transcriptionalregulatory layer implemented by miRNAs and RNA-binding proteins andfrequently acting precisely on the mRNAs encoding transcriptionfactors, has been uncovered. Far from passively carrying the geneticinformation to the ribosomal translation machines, messenger RNAs havea life of their own and this can be extended or shortened depending oninteractions with various protein and ribonucleoprotein complexes inthe cell. Although the miRNAs have initially been described asregulators of the mRNA translation rate, it is now clear that animportant mechanism behind the miRNA-based regulation is target mRNAdegradation. The activity of miRNA-containing ribonucleoproteincomplexes can itself be modulated for example by RNA-binding proteins(RBPs) that recognize sites in the vicinity of miRNA-binding sites andact as competitors for miRNA binding.Much effort in the past years has been devoted to identifying newplayers in post-transcriptional control. Many groups, including ourown, contributed to the catalog of miRNA genes in species ranging fromviruses to human. Similarly, many groups, ours included, contributedthrough computational or experimental approaches to the list ofputative miRNA-target interactions. Yet although it is clear that onaverage a miRNA targets hundreds of genes, much remains to be done tounderstand the determinants of miRNA targeting specificity. Based onthe observation that miRNAs destabilize their mRNA targets manyexperimental studies aimed to identify miRNA targets by miRNAtransfection and microarray profiling. In spite of the fact thathundreds to thousands of transcripts respond in a typical experiment,only a fraction of the predicted miRNA targets do. Moreover, forreasons that are so far unclear, the magnitude of the response varieswidely. In a recent analysis of over seventy experimental data sets ofmRNA or protein level changes that occurred upon perturbation of miRNAexpression, we found that functional miRNA target sites reside inaccessible regions, and that the degree of mRNA degradation is relatedto the U (and to some extent A) nucleotide content of the miRNA targetsite environment. This suggests that the fate of miRNA targets dependsnot only on their interaction with the RNA-induced silencing complex(RISC), but also on the presence of modulatory protein co-factors. Incollaboration with the group of Tom Tuschl (The RockefellerUniversity) we have recently identified a number of RBPs that interactwith the RISC complex and we characterized their cognate mRNAs byimmunoprecipitation and microarray expression profiling. We havefurther developed a novel cross-linking and immunoprecipitation methodto identify RBP targets at the resolution of individual bindingsites. Building on this work, we here propose to carry out the following projects:1. Computational modeling of miRNA-target interactions.The starting point of this study will be the set of sequences that we found, with the CLIP method mentioned above, to be bound by Argonaute protein-containing complexes. We will attempt to use machine learning approaches to identify the location of miRNA binding sites in these data and to characterize the mode of binding of miRNAs to their targets. We will also thoroughly characterize the properties of these binding sites and we will use the results to develop improved methods for miRNA target prediction.2. Experimental identification of the targets of U-rich element binding proteins.This project will build on one hand on the experimental method that we developed in collaboration with the group of Tom Tuschl (The Rockefeller University), and on the other hand on our computational results that indicate that U-rich elements are associated with the degradation of miRNA targets. We have selected a set of U-rich element binding proteins whose targets we would like to identify by CLIP, and then study in relationship with the miRNA targets that we already obtained.3. Development of computational models of RBP-mRNA interactions.This projects aims to again use the CLIP data that we have and will obtain in the near future to start developing models to specifically describe RBP-mRNA interactions, taking into account the particularities of this class of proteins. These projects will make an important contribution to a quantitativeunderstanding of miRNA-based post-transcriptional regulation and tothe inference of the elusive 'RNA code'.    Swiss National Science Foundation    Project funding (Div. I-III)    600000.0CHF
472    Dr Sampson Stephen    University of Oxford    2020-01-01    2023-06-30    Modelling the cellular causes of HLA-B*27 associated spondyloarthritis with single-cell genomics    Although for many years transcription factors held the center stage inthe regulation of gene expression, a very complex post-transcriptionalregulatory layer implemented by miRNAs and RNA-binding proteins andfrequently acting precisely on the mRNAs encoding transcriptionfactors, has been uncovered. Far from passively carrying the geneticinformation to the ribosomal translation machines, messenger RNAs havea life of their own and this can be extended or shortened depending oninteractions with various protein and ribonucleoprotein complexes inthe cell. Although the miRNAs have initially been described asregulators of the mRNA translation rate, it is now clear that animportant mechanism behind the miRNA-based regulation is target mRNAdegradation. The activity of miRNA-containing ribonucleoproteincomplexes can itself be modulated for example by RNA-binding proteins(RBPs) that recognize sites in the vicinity of miRNA-binding sites andact as competitors for miRNA binding.Much effort in the past years has been devoted to identifying newplayers in post-transcriptional control. Many groups, including ourown, contributed to the catalog of miRNA genes in species ranging fromviruses to human. Similarly, many groups, ours included, contributedthrough computational or experimental approaches to the list ofputative miRNA-target interactions. Yet although it is clear that onaverage a miRNA targets hundreds of genes, much remains to be done tounderstand the determinants of miRNA targeting specificity. Based onthe observation that miRNAs destabilize their mRNA targets manyexperimental studies aimed to identify miRNA targets by miRNAtransfection and microarray profiling. In spite of the fact thathundreds to thousands of transcripts respond in a typical experiment,only a fraction of the predicted miRNA targets do. Moreover, forreasons that are so far unclear, the magnitude of the response varieswidely. In a recent analysis of over seventy experimental data sets ofmRNA or protein level changes that occurred upon perturbation of miRNAexpression, we found that functional miRNA target sites reside inaccessible regions, and that the degree of mRNA degradation is relatedto the U (and to some extent A) nucleotide content of the miRNA targetsite environment. This suggests that the fate of miRNA targets dependsnot only on their interaction with the RNA-induced silencing complex(RISC), but also on the presence of modulatory protein co-factors. Incollaboration with the group of Tom Tuschl (The RockefellerUniversity) we have recently identified a number of RBPs that interactwith the RISC complex and we characterized their cognate mRNAs byimmunoprecipitation and microarray expression profiling. We havefurther developed a novel cross-linking and immunoprecipitation methodto identify RBP targets at the resolution of individual bindingsites. Building on this work, we here propose to carry out the following projects:1. Computational modeling of miRNA-target interactions.The starting point of this study will be the set of sequences that we found, with the CLIP method mentioned above, to be bound by Argonaute protein-containing complexes. We will attempt to use machine learning approaches to identify the location of miRNA binding sites in these data and to characterize the mode of binding of miRNAs to their targets. We will also thoroughly characterize the properties of these binding sites and we will use the results to develop improved methods for miRNA target prediction.2. Experimental identification of the targets of U-rich element binding proteins.This project will build on one hand on the experimental method that we developed in collaboration with the group of Tom Tuschl (The Rockefeller University), and on the other hand on our computational results that indicate that U-rich elements are associated with the degradation of miRNA targets. We have selected a set of U-rich element binding proteins whose targets we would like to identify by CLIP, and then study in relationship with the miRNA targets that we already obtained.3. Development of computational models of RBP-mRNA interactions.This projects aims to again use the CLIP data that we have and will obtain in the near future to start developing models to specifically describe RBP-mRNA interactions, taking into account the particularities of this class of proteins. These projects will make an important contribution to a quantitativeunderstanding of miRNA-based post-transcriptional regulation and tothe inference of the elusive 'RNA code'.    Versus Arthritis    Full Application Disease    716522.74GBP
473    Professor Rubinsztein David    University of Cambridge    2011-10-12    2013-10-11    IDENTIFICATION OF GENERIC SUPRESSORS OF PROTEINOPATHIES    Although for many years transcription factors held the center stage inthe regulation of gene expression, a very complex post-transcriptionalregulatory layer implemented by miRNAs and RNA-binding proteins andfrequently acting precisely on the mRNAs encoding transcriptionfactors, has been uncovered. Far from passively carrying the geneticinformation to the ribosomal translation machines, messenger RNAs havea life of their own and this can be extended or shortened depending oninteractions with various protein and ribonucleoprotein complexes inthe cell. Although the miRNAs have initially been described asregulators of the mRNA translation rate, it is now clear that animportant mechanism behind the miRNA-based regulation is target mRNAdegradation. The activity of miRNA-containing ribonucleoproteincomplexes can itself be modulated for example by RNA-binding proteins(RBPs) that recognize sites in the vicinity of miRNA-binding sites andact as competitors for miRNA binding.Much effort in the past years has been devoted to identifying newplayers in post-transcriptional control. Many groups, including ourown, contributed to the catalog of miRNA genes in species ranging fromviruses to human. Similarly, many groups, ours included, contributedthrough computational or experimental approaches to the list ofputative miRNA-target interactions. Yet although it is clear that onaverage a miRNA targets hundreds of genes, much remains to be done tounderstand the determinants of miRNA targeting specificity. Based onthe observation that miRNAs destabilize their mRNA targets manyexperimental studies aimed to identify miRNA targets by miRNAtransfection and microarray profiling. In spite of the fact thathundreds to thousands of transcripts respond in a typical experiment,only a fraction of the predicted miRNA targets do. Moreover, forreasons that are so far unclear, the magnitude of the response varieswidely. In a recent analysis of over seventy experimental data sets ofmRNA or protein level changes that occurred upon perturbation of miRNAexpression, we found that functional miRNA target sites reside inaccessible regions, and that the degree of mRNA degradation is relatedto the U (and to some extent A) nucleotide content of the miRNA targetsite environment. This suggests that the fate of miRNA targets dependsnot only on their interaction with the RNA-induced silencing complex(RISC), but also on the presence of modulatory protein co-factors. Incollaboration with the group of Tom Tuschl (The RockefellerUniversity) we have recently identified a number of RBPs that interactwith the RISC complex and we characterized their cognate mRNAs byimmunoprecipitation and microarray expression profiling. We havefurther developed a novel cross-linking and immunoprecipitation methodto identify RBP targets at the resolution of individual bindingsites. Building on this work, we here propose to carry out the following projects:1. Computational modeling of miRNA-target interactions.The starting point of this study will be the set of sequences that we found, with the CLIP method mentioned above, to be bound by Argonaute protein-containing complexes. We will attempt to use machine learning approaches to identify the location of miRNA binding sites in these data and to characterize the mode of binding of miRNAs to their targets. We will also thoroughly characterize the properties of these binding sites and we will use the results to develop improved methods for miRNA target prediction.2. Experimental identification of the targets of U-rich element binding proteins.This project will build on one hand on the experimental method that we developed in collaboration with the group of Tom Tuschl (The Rockefeller University), and on the other hand on our computational results that indicate that U-rich elements are associated with the degradation of miRNA targets. We have selected a set of U-rich element binding proteins whose targets we would like to identify by CLIP, and then study in relationship with the miRNA targets that we already obtained.3. Development of computational models of RBP-mRNA interactions.This projects aims to again use the CLIP data that we have and will obtain in the near future to start developing models to specifically describe RBP-mRNA interactions, taking into account the particularities of this class of proteins. These projects will make an important contribution to a quantitativeunderstanding of miRNA-based post-transcriptional regulation and tothe inference of the elusive 'RNA code'.    Medical Research Council    Research Grant    407427.0GBP
474    Dr Courtin Emilie    London Sch of Hygiene and Trop Medicine    2020-05-01    2023-04-30    Heterogeneous and long-term effects of social interventions on mortality and psychological health    Although for many years transcription factors held the center stage inthe regulation of gene expression, a very complex post-transcriptionalregulatory layer implemented by miRNAs and RNA-binding proteins andfrequently acting precisely on the mRNAs encoding transcriptionfactors, has been uncovered. Far from passively carrying the geneticinformation to the ribosomal translation machines, messenger RNAs havea life of their own and this can be extended or shortened depending oninteractions with various protein and ribonucleoprotein complexes inthe cell. Although the miRNAs have initially been described asregulators of the mRNA translation rate, it is now clear that animportant mechanism behind the miRNA-based regulation is target mRNAdegradation. The activity of miRNA-containing ribonucleoproteincomplexes can itself be modulated for example by RNA-binding proteins(RBPs) that recognize sites in the vicinity of miRNA-binding sites andact as competitors for miRNA binding.Much effort in the past years has been devoted to identifying newplayers in post-transcriptional control. Many groups, including ourown, contributed to the catalog of miRNA genes in species ranging fromviruses to human. Similarly, many groups, ours included, contributedthrough computational or experimental approaches to the list ofputative miRNA-target interactions. Yet although it is clear that onaverage a miRNA targets hundreds of genes, much remains to be done tounderstand the determinants of miRNA targeting specificity. Based onthe observation that miRNAs destabilize their mRNA targets manyexperimental studies aimed to identify miRNA targets by miRNAtransfection and microarray profiling. In spite of the fact thathundreds to thousands of transcripts respond in a typical experiment,only a fraction of the predicted miRNA targets do. Moreover, forreasons that are so far unclear, the magnitude of the response varieswidely. In a recent analysis of over seventy experimental data sets ofmRNA or protein level changes that occurred upon perturbation of miRNAexpression, we found that functional miRNA target sites reside inaccessible regions, and that the degree of mRNA degradation is relatedto the U (and to some extent A) nucleotide content of the miRNA targetsite environment. This suggests that the fate of miRNA targets dependsnot only on their interaction with the RNA-induced silencing complex(RISC), but also on the presence of modulatory protein co-factors. Incollaboration with the group of Tom Tuschl (The RockefellerUniversity) we have recently identified a number of RBPs that interactwith the RISC complex and we characterized their cognate mRNAs byimmunoprecipitation and microarray expression profiling. We havefurther developed a novel cross-linking and immunoprecipitation methodto identify RBP targets at the resolution of individual bindingsites. Building on this work, we here propose to carry out the following projects:1. Computational modeling of miRNA-target interactions.The starting point of this study will be the set of sequences that we found, with the CLIP method mentioned above, to be bound by Argonaute protein-containing complexes. We will attempt to use machine learning approaches to identify the location of miRNA binding sites in these data and to characterize the mode of binding of miRNAs to their targets. We will also thoroughly characterize the properties of these binding sites and we will use the results to develop improved methods for miRNA target prediction.2. Experimental identification of the targets of U-rich element binding proteins.This project will build on one hand on the experimental method that we developed in collaboration with the group of Tom Tuschl (The Rockefeller University), and on the other hand on our computational results that indicate that U-rich elements are associated with the degradation of miRNA targets. We have selected a set of U-rich element binding proteins whose targets we would like to identify by CLIP, and then study in relationship with the miRNA targets that we already obtained.3. Development of computational models of RBP-mRNA interactions.This projects aims to again use the CLIP data that we have and will obtain in the near future to start developing models to specifically describe RBP-mRNA interactions, taking into account the particularities of this class of proteins. These projects will make an important contribution to a quantitativeunderstanding of miRNA-based post-transcriptional regulation and tothe inference of the elusive 'RNA code'.    Medical Research Council    Fellowship    306714.0GBP
475    Mr Holdsworth Ed    Practical Control Limited    2016-01-01    2018-09-30    Speech Rehabilitation from Articulator Movement (SRAM)    None    National Institute for Health Research (Department of Health)    Full Award    655678.0GBP
476    Dr Hennequin Guillaume    Department of Electrical Engineering University of Cambridge    2013-02-01    2014-07-31    Fast but not furious: rapid Bayesian inference in balanced cortical circuits    Sensory perception is fast in human, and nonetheless almost Bayes-optimal. I propose a theoretical approach to this puzzle in which circuit mechanisms for fast dynamics (which I have explored as part of my PhD thesis) will be related to the speed of perception, for wich the computational aspects will be cast in the framework of probabilistic Bayesian inference. The working hypothesis will be that the brain represents posterior uncertainty via sampling, for which the host laboratory has found experimental evidence recently. We will thus combine methods from machine learning and statistical physics to understand how inference via sampling can be performed in the cortex. The project has expected outcomes both in the field of machine learning and in neuroscience.    Swiss National Science Foundation    Fellowships for prospective researchers    655678.0GBP
477    Dr Churcher Thomas    Imperial College London    2017-03-01    2020-02-29    Near-Infrared Spectroscopy: A One-Stop-Shop for Mosquito Epidemiological Monitoring?    Sensory perception is fast in human, and nonetheless almost Bayes-optimal. I propose a theoretical approach to this puzzle in which circuit mechanisms for fast dynamics (which I have explored as part of my PhD thesis) will be related to the speed of perception, for wich the computational aspects will be cast in the framework of probabilistic Bayesian inference. The working hypothesis will be that the brain represents posterior uncertainty via sampling, for which the host laboratory has found experimental evidence recently. We will thus combine methods from machine learning and statistical physics to understand how inference via sampling can be performed in the cortex. The project has expected outcomes both in the field of machine learning and in neuroscience.    Medical Research Council    Research Grant    643682.0GBP
478    Dr Enshaei Amir    University of Newcastle    2016-08-01    2020-08-01    Development of machine learning system as a prediction tool in acute lymphoblastic leukaemia    Sensory perception is fast in human, and nonetheless almost Bayes-optimal. I propose a theoretical approach to this puzzle in which circuit mechanisms for fast dynamics (which I have explored as part of my PhD thesis) will be related to the speed of perception, for wich the computational aspects will be cast in the framework of probabilistic Bayesian inference. The working hypothesis will be that the brain represents posterior uncertainty via sampling, for which the host laboratory has found experimental evidence recently. We will thus combine methods from machine learning and statistical physics to understand how inference via sampling can be performed in the cortex. The project has expected outcomes both in the field of machine learning and in neuroscience.    Blood Cancer UK    Integrative Biology Fellowship    643682.0GBP
479    Mr Schultesz Ferenc    City, University of London    2016-07-04    2016-09-03    Pattern Classification of attention deficit hyperactivity disorder: Integrating functional magnetic resonance imaging and genetics data    Sensory perception is fast in human, and nonetheless almost Bayes-optimal. I propose a theoretical approach to this puzzle in which circuit mechanisms for fast dynamics (which I have explored as part of my PhD thesis) will be related to the speed of perception, for wich the computational aspects will be cast in the framework of probabilistic Bayesian inference. The working hypothesis will be that the brain represents posterior uncertainty via sampling, for which the host laboratory has found experimental evidence recently. We will thus combine methods from machine learning and statistical physics to understand how inference via sampling can be performed in the cortex. The project has expected outcomes both in the field of machine learning and in neuroscience.    Wellcome Trust    Vacation Scholarships    2000.0GBP
480    Professor Low Nicola Minling    University of Berne    2017-11-01    2021-10-31    Zika virus: causality, open science and risks of emerging infectious diseases    BackgroundZika virus infection was established as a cause of congenital abnormalities, including microcephaly, and of Guillain-Barré syndrome during a Public Health Emergency of International Concern that the World Health Organization (WHO) announced in February 2016. The Public Health Emergency ended in November 2016 but substantial gaps remain in the causality framework of Zika complications, knowledge about population level susceptibility to Zika virus infection and the risks of the newly recognised route of sexual transmission of Zika virus. Objectives1. To produce a web platform that will allow the production and updating of living systematic reviews of evidence about Zika virus infection; 2. To estimate key parameters that will allow refined inferences about the sexual transmissibility of Zika virus in endemic and non-endemic settings; 3. To investigate the seroprevalence of antibodies to Zika virus in different geographic settings and to use seroprevalence data to allow estimation of the duration of immunity after Zika virus infection. Methods1. We will produce an open access web application to produce living systematic reviews that allow continual updating of evidence about causal associations between Zika virus and its complications, and emerging research questions. The application will automate searching and deduplication, use text mining and machine learning to assist screening and allow automated updates of review output for rapid publication. 2. We have developed a sexual transmission framework to identify key parameters needed to understand the potential for ongoing spread of Zika virus through sexual transmission. We will analyse data to determine the duration of persistence of Zika virus in semen, vaginal fluid, urine, breast milk and other bodily fluids. We will then use a transmission model to estimate the per sex act probability of Zika virus transmission. 3. We will use data from ongoing longitudinal studies and repeated cross-sectional studies in Nicaragua that will determine antibody levels to Zika virus using new diagnostic tests (taking into account exposure to dengue and chikungunya). We will apply “back-calculation” methods to determine the duration of immunity of Zika virus infection. We will also pilot a method for the collection and assessment of seroprevalence data collected in a range of settings that have experienced new Zika transmission since 2013 and where Zika is presumed to be endemic to improve understanding of population level susceptibility to Zika virus infection.Timeline: The project will last four yearsImportance and impactThis project has considerable importance for research on Zika virus infection and transmission. Whilst vaccine development is advancing rapidly, there are still important gaps in our knowledge about vulnerability to Zika virus in large proportion of the world’s population that lives in areas where Aedes mosquito vectors are distributed. The project objectives are aligned with the research agenda of the WHO and with international initiatives to increase capacity for preparedness for infectious disease pandemics. The outputs are therefore relevant to current research priorities. By working within a culture of open science and with the living systematic review network, our research outputs, including publications and software will be publicly available as quickly as possible.    Swiss National Science Foundation    Project funding (Div. I-III)    700000.0CHF
481    Prof Arlt Wiebke    University of Birmingham    2009-03-01    2012-02-29    Steroid profiling as a biomarker tool in the diagnosis and monitoring of adrenal tumours    BackgroundZika virus infection was established as a cause of congenital abnormalities, including microcephaly, and of Guillain-Barré syndrome during a Public Health Emergency of International Concern that the World Health Organization (WHO) announced in February 2016. The Public Health Emergency ended in November 2016 but substantial gaps remain in the causality framework of Zika complications, knowledge about population level susceptibility to Zika virus infection and the risks of the newly recognised route of sexual transmission of Zika virus. Objectives1. To produce a web platform that will allow the production and updating of living systematic reviews of evidence about Zika virus infection; 2. To estimate key parameters that will allow refined inferences about the sexual transmissibility of Zika virus in endemic and non-endemic settings; 3. To investigate the seroprevalence of antibodies to Zika virus in different geographic settings and to use seroprevalence data to allow estimation of the duration of immunity after Zika virus infection. Methods1. We will produce an open access web application to produce living systematic reviews that allow continual updating of evidence about causal associations between Zika virus and its complications, and emerging research questions. The application will automate searching and deduplication, use text mining and machine learning to assist screening and allow automated updates of review output for rapid publication. 2. We have developed a sexual transmission framework to identify key parameters needed to understand the potential for ongoing spread of Zika virus through sexual transmission. We will analyse data to determine the duration of persistence of Zika virus in semen, vaginal fluid, urine, breast milk and other bodily fluids. We will then use a transmission model to estimate the per sex act probability of Zika virus transmission. 3. We will use data from ongoing longitudinal studies and repeated cross-sectional studies in Nicaragua that will determine antibody levels to Zika virus using new diagnostic tests (taking into account exposure to dengue and chikungunya). We will apply “back-calculation” methods to determine the duration of immunity of Zika virus infection. We will also pilot a method for the collection and assessment of seroprevalence data collected in a range of settings that have experienced new Zika transmission since 2013 and where Zika is presumed to be endemic to improve understanding of population level susceptibility to Zika virus infection.Timeline: The project will last four yearsImportance and impactThis project has considerable importance for research on Zika virus infection and transmission. Whilst vaccine development is advancing rapidly, there are still important gaps in our knowledge about vulnerability to Zika virus in large proportion of the world’s population that lives in areas where Aedes mosquito vectors are distributed. The project objectives are aligned with the research agenda of the WHO and with international initiatives to increase capacity for preparedness for infectious disease pandemics. The outputs are therefore relevant to current research priorities. By working within a culture of open science and with the living systematic review network, our research outputs, including publications and software will be publicly available as quickly as possible.    Medical Research Council    Research Grant    727149.0GBP
482    Dr Gavara Nuria    QUEEN MARY UNIVERSITY OF LONDON    2016-10-24    2018-04-23    Effect of cell age on cell migration and cytoskeletal reorganization    BackgroundZika virus infection was established as a cause of congenital abnormalities, including microcephaly, and of Guillain-Barré syndrome during a Public Health Emergency of International Concern that the World Health Organization (WHO) announced in February 2016. The Public Health Emergency ended in November 2016 but substantial gaps remain in the causality framework of Zika complications, knowledge about population level susceptibility to Zika virus infection and the risks of the newly recognised route of sexual transmission of Zika virus. Objectives1. To produce a web platform that will allow the production and updating of living systematic reviews of evidence about Zika virus infection; 2. To estimate key parameters that will allow refined inferences about the sexual transmissibility of Zika virus in endemic and non-endemic settings; 3. To investigate the seroprevalence of antibodies to Zika virus in different geographic settings and to use seroprevalence data to allow estimation of the duration of immunity after Zika virus infection. Methods1. We will produce an open access web application to produce living systematic reviews that allow continual updating of evidence about causal associations between Zika virus and its complications, and emerging research questions. The application will automate searching and deduplication, use text mining and machine learning to assist screening and allow automated updates of review output for rapid publication. 2. We have developed a sexual transmission framework to identify key parameters needed to understand the potential for ongoing spread of Zika virus through sexual transmission. We will analyse data to determine the duration of persistence of Zika virus in semen, vaginal fluid, urine, breast milk and other bodily fluids. We will then use a transmission model to estimate the per sex act probability of Zika virus transmission. 3. We will use data from ongoing longitudinal studies and repeated cross-sectional studies in Nicaragua that will determine antibody levels to Zika virus using new diagnostic tests (taking into account exposure to dengue and chikungunya). We will apply “back-calculation” methods to determine the duration of immunity of Zika virus infection. We will also pilot a method for the collection and assessment of seroprevalence data collected in a range of settings that have experienced new Zika transmission since 2013 and where Zika is presumed to be endemic to improve understanding of population level susceptibility to Zika virus infection.Timeline: The project will last four yearsImportance and impactThis project has considerable importance for research on Zika virus infection and transmission. Whilst vaccine development is advancing rapidly, there are still important gaps in our knowledge about vulnerability to Zika virus in large proportion of the world’s population that lives in areas where Aedes mosquito vectors are distributed. The project objectives are aligned with the research agenda of the WHO and with international initiatives to increase capacity for preparedness for infectious disease pandemics. The outputs are therefore relevant to current research priorities. By working within a culture of open science and with the living systematic review network, our research outputs, including publications and software will be publicly available as quickly as possible.    The Dunhill Medical Trust    Research Project & Programme Grants    74930.0GBP
483    Dr Miller Crispin    University of Manchester    2016-10-18    2020-10-17    noncoding RNA derived classifiers as biomarkers of patient response to therapy    BackgroundZika virus infection was established as a cause of congenital abnormalities, including microcephaly, and of Guillain-Barré syndrome during a Public Health Emergency of International Concern that the World Health Organization (WHO) announced in February 2016. The Public Health Emergency ended in November 2016 but substantial gaps remain in the causality framework of Zika complications, knowledge about population level susceptibility to Zika virus infection and the risks of the newly recognised route of sexual transmission of Zika virus. Objectives1. To produce a web platform that will allow the production and updating of living systematic reviews of evidence about Zika virus infection; 2. To estimate key parameters that will allow refined inferences about the sexual transmissibility of Zika virus in endemic and non-endemic settings; 3. To investigate the seroprevalence of antibodies to Zika virus in different geographic settings and to use seroprevalence data to allow estimation of the duration of immunity after Zika virus infection. Methods1. We will produce an open access web application to produce living systematic reviews that allow continual updating of evidence about causal associations between Zika virus and its complications, and emerging research questions. The application will automate searching and deduplication, use text mining and machine learning to assist screening and allow automated updates of review output for rapid publication. 2. We have developed a sexual transmission framework to identify key parameters needed to understand the potential for ongoing spread of Zika virus through sexual transmission. We will analyse data to determine the duration of persistence of Zika virus in semen, vaginal fluid, urine, breast milk and other bodily fluids. We will then use a transmission model to estimate the per sex act probability of Zika virus transmission. 3. We will use data from ongoing longitudinal studies and repeated cross-sectional studies in Nicaragua that will determine antibody levels to Zika virus using new diagnostic tests (taking into account exposure to dengue and chikungunya). We will apply “back-calculation” methods to determine the duration of immunity of Zika virus infection. We will also pilot a method for the collection and assessment of seroprevalence data collected in a range of settings that have experienced new Zika transmission since 2013 and where Zika is presumed to be endemic to improve understanding of population level susceptibility to Zika virus infection.Timeline: The project will last four yearsImportance and impactThis project has considerable importance for research on Zika virus infection and transmission. Whilst vaccine development is advancing rapidly, there are still important gaps in our knowledge about vulnerability to Zika virus in large proportion of the world’s population that lives in areas where Aedes mosquito vectors are distributed. The project objectives are aligned with the research agenda of the WHO and with international initiatives to increase capacity for preparedness for infectious disease pandemics. The outputs are therefore relevant to current research priorities. By working within a culture of open science and with the living systematic review network, our research outputs, including publications and software will be publicly available as quickly as possible.    Prostate Cancer UK    PhD Studentships    139722.0GBP
