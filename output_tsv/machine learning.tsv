0	Professor Bath Philip	Nottingham,University of	None	None	Assessment of modern machine learning methods and conventional statistical regression techniques in diagnosis and prediction of outcome after acute stroke using big data	None	British Heart Foundation	None	145079.0GBP
1	Professor Jayson Gordon	University of Manchester	2005-01-01	2008-06-30	The human serum metabolome in health and disease	Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high- resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.	Biotechnology and Biological Sciences Research Council	LINK project	168047.0GBP
2	Professor Rubinsztein David	University of Cambridge	2011-10-12	2013-10-11	IDENTIFICATION OF GENERIC SUPRESSORS OF PROTEINOPATHIES	Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high- resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.	Medical Research Council	Research Grant	407427.0GBP
3	Professor Herrick Ariane	University of Manchester	2019-09-01	2022-08-05	Development of a measuring app for finger lesions as an outcome measure for systemic sclerosis-related digital ulceration (SALVE: Scleroderma App for Lesion VErification)’	Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high- resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.	Versus Arthritis	Full Application Treatment	216773.0GBP
4	Professor Kell Douglas	The University of Manchester	2019-09-01	2022-08-05	The Rothamsted Metabolomics Centre (MeT-RO)	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20482)	Biotechnology and Biological Sciences Research Council	Full Application Treatment	134363.0GBP
5	Professor Collins Andrew Richard	University of Southampton	2011-10-01	2014-09-30	Using machine learning methods to characterise the role of genetic factors in early onset breast cancer	Currently only about 30% of the genetic contribution to breast cancer risk has been identified. These risk factors comprise a small number of rare and moderate penetrance disease genes along with common variants identified through genome-wide association studies. Some of the 'missing heritability' is likely to remain within the phenotype: focus on 'simple' case and control disease phenotypes may be less powerful than testing for genes underlying disease sub-types. Furthermore, analytical methods have not adequately modelled interactions between genes, phenotypic variables and other factors. Finally, a proportion of genetic variation is likely to arise through rarer moderate penetrance genes that have not been tested for through association studies, but justify the effort towards sequencing. This studentship develops novel analyses in a large early onset breast cancer phenotype and genotype sample. Through collaborative supervision between the Southampton Schools of Medical and Mathematics, machine learning methods will be developed, evaluated and employed for the analyses, thereby avoiding the limitations of conventional parametric models. A particular focus will be the characterisation of the genetic basis of breast cancer sub-phenotypes (including for example, categories based on tumour grade, type and tumour biomarkers). Modelling the role of genetic variation using SNP genotype data to identify novel genetic variation underlying early onset disease is the basis of the studentship. An understanding of the role of this variation on breast cancer phenotypes is essential for the development of novel therapies in the future.	Breast Cancer Now	PhD	61400.0GBP
6	Professor Ananiadou Sophia	The University of Manchester	2011-10-01	2014-09-30	Tools for the text mining-based visualisation of the provenance of biochemical networks	Systems biology is concerned with the modelling, visualisation and analysis of biochemical networks in which, for instance, metabolites are 'linked' by arrows representing the enzymes which turn one molecule into another or which are modified by particular substances. SBML provides a computer- readable 'standard' for describing such biochemical or signalling networks. However, these diagrams (and thus the SBML models) are divorced from the scientific evidence on which they are based, represented by the scientific literature (and increasingly by online databases). In order to overcome the problems of reading the burgeoning scientific literature, we shall deploy Text Mining TM. TM involves named entity recognition (i.e. semantic annotation of enzymes, metabolites, etc) and information extraction (i.e. relationship extraction between named entities). An important part of this proposal is to find solutions for the terminology problem in systems biology, by developing techniques for recognising synonym terms.Based on our efficient parsing techniques, we shall extract relationships between entities that will form the basis by which we shall can discover, index, store and display the scientific evidence for such linkages. The selection of the most pertinent relationships will be performed using our preferred methods of advanced machine learning (Support Vector Machines and Genetic Programming). The overall aim of the project is thus to develop and deploy the necessary TM tools and to use them to display the different relationships to the user together with the literature from which they have been extracted. The different types (and strength) of evidence for these interactions will then be visualised directly and linked to a dynamic website of the literature. This will thus give users a direct linkage between the systems biology diagrams encoded in (an advanced form of) SBML and the scientific evidence for them. Where available, linkages to kinetic data will also be made.	Biotechnology and Biological Sciences Research Council	PhD	549458.0GBP
7	Professor Kell Douglas	The University of Manchester	2011-10-01	2014-09-30	Tools for the text mining-based visualisation of the provenance of biochemical networks	Systems biology is concerned with the modelling, visualisation and analysis of biochemical networks in which, for instance, metabolites are 'linked' by arrows representing the enzymes which turn one molecule into another or which are modified by particular substances. SBML provides a computer- readable 'standard' for describing such biochemical or signalling networks. However, these diagrams (and thus the SBML models) are divorced from the scientific evidence on which they are based, represented by the scientific literature (and increasingly by online databases). In order to overcome the problems of reading the burgeoning scientific literature, we shall deploy Text Mining TM. TM involves named entity recognition (i.e. semantic annotation of enzymes, metabolites, etc) and information extraction (i.e. relationship extraction between named entities). An important part of this proposal is to find solutions for the terminology problem in systems biology, by developing techniques for recognising synonym terms.Based on our efficient parsing techniques, we shall extract relationships between entities that will form the basis by which we shall can discover, index, store and display the scientific evidence for such linkages. The selection of the most pertinent relationships will be performed using our preferred methods of advanced machine learning (Support Vector Machines and Genetic Programming). The overall aim of the project is thus to develop and deploy the necessary TM tools and to use them to display the different relationships to the user together with the literature from which they have been extracted. The different types (and strength) of evidence for these interactions will then be visualised directly and linked to a dynamic website of the literature. This will thus give users a direct linkage between the systems biology diagrams encoded in (an advanced form of) SBML and the scientific evidence for them. Where available, linkages to kinetic data will also be made.	Biotechnology and Biological Sciences Research Council	PhD	549458.0GBP
8	Professor Kell Douglas	The University of Manchester	2011-10-01	2014-09-30	Inference and learning in machine vision	Systems biology is concerned with the modelling, visualisation and analysis of biochemical networks in which, for instance, metabolites are 'linked' by arrows representing the enzymes which turn one molecule into another or which are modified by particular substances. SBML provides a computer- readable 'standard' for describing such biochemical or signalling networks. However, these diagrams (and thus the SBML models) are divorced from the scientific evidence on which they are based, represented by the scientific literature (and increasingly by online databases). In order to overcome the problems of reading the burgeoning scientific literature, we shall deploy Text Mining TM. TM involves named entity recognition (i.e. semantic annotation of enzymes, metabolites, etc) and information extraction (i.e. relationship extraction between named entities). An important part of this proposal is to find solutions for the terminology problem in systems biology, by developing techniques for recognising synonym terms.Based on our efficient parsing techniques, we shall extract relationships between entities that will form the basis by which we shall can discover, index, store and display the scientific evidence for such linkages. The selection of the most pertinent relationships will be performed using our preferred methods of advanced machine learning (Support Vector Machines and Genetic Programming). The overall aim of the project is thus to develop and deploy the necessary TM tools and to use them to display the different relationships to the user together with the literature from which they have been extracted. The different types (and strength) of evidence for these interactions will then be visualised directly and linked to a dynamic website of the literature. This will thus give users a direct linkage between the systems biology diagrams encoded in (an advanced form of) SBML and the scientific evidence for them. Where available, linkages to kinetic data will also be made.	Biotechnology and Biological Sciences Research Council	PhD	549458.0GBP
9	Professor Wernisch Lorenz	University of Cambridge	2016-12-01	2019-03-31	Statistical bioinformatics and genetics	Systems biology is concerned with the modelling, visualisation and analysis of biochemical networks in which, for instance, metabolites are 'linked' by arrows representing the enzymes which turn one molecule into another or which are modified by particular substances. SBML provides a computer- readable 'standard' for describing such biochemical or signalling networks. However, these diagrams (and thus the SBML models) are divorced from the scientific evidence on which they are based, represented by the scientific literature (and increasingly by online databases). In order to overcome the problems of reading the burgeoning scientific literature, we shall deploy Text Mining TM. TM involves named entity recognition (i.e. semantic annotation of enzymes, metabolites, etc) and information extraction (i.e. relationship extraction between named entities). An important part of this proposal is to find solutions for the terminology problem in systems biology, by developing techniques for recognising synonym terms.Based on our efficient parsing techniques, we shall extract relationships between entities that will form the basis by which we shall can discover, index, store and display the scientific evidence for such linkages. The selection of the most pertinent relationships will be performed using our preferred methods of advanced machine learning (Support Vector Machines and Genetic Programming). The overall aim of the project is thus to develop and deploy the necessary TM tools and to use them to display the different relationships to the user together with the literature from which they have been extracted. The different types (and strength) of evidence for these interactions will then be visualised directly and linked to a dynamic website of the literature. This will thus give users a direct linkage between the systems biology diagrams encoded in (an advanced form of) SBML and the scientific evidence for them. Where available, linkages to kinetic data will also be made.	Medical Research Council	Unit	549458.0GBP
10	Professor Wernisch Lorenz	Rothamsted Research	2016-12-01	2019-03-31	The Rothamsted Metabolomics Centre (MeT-RO)	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)	Biotechnology and Biological Sciences Research Council	Unit	1461648.0GBP
11	Professor Wernisch Lorenz	University College London	2016-12-01	2019-03-31	Prediction of protein-protein interaction hot spots using a combination of physics and machine learning	Protein-protein interactions are central to most biological processes, from signal transduction to immune response. Understanding these functional associations requires knowledge of the three-dimensional structure of the complex as this reveal the underlying molecular mechanism. However, determining experimentally the 3D structure of a protein complex present considerable difficulties. There is therefore a need for accurate and reliable computational methods. Several experiments have shown that protein interactions are critically dependent on just a few residues, or hot spots, at the binding interface. Hot spots make a dominant contribution to the free energy of binding and if mutated they can disrupt the interaction. In this project we aim to develop a computational method that can identify hot spot residues (and the contacts they form across the interface) in unbound proteins (i.e. without prior knowledge of the complex). This would significantly improve our ability at predicting the overall structure of the complex (the so-called docking problem). We plan to combine and integrate the basic energetic terms that contribute to the stability of protein complexes (e.g. van der Waals potential, hydrogen bonds,etc.) using state of the art machine learning techniques. In the first part of the project, we will develop a method to predict hot-spot residues at protein protein interfaces when the structure of complex is available. In the second part, we plan to systematically dock structural fragments of the two unbound proteins and test them for the presence of potential hot spots (using the classifier developed in the first part). Eventually, we will combine different sources of information (energetic, evolutionary and structural) to predict few important contacts across the interface of two proteins.	Biotechnology and Biological Sciences Research Council	Unit	310931.0GBP
12	Professor McKeigue Paul	University of Edinburgh	2009-03-16	2012-03-15	Development of Bayesian methods for genetic epidemiology	Protein-protein interactions are central to most biological processes, from signal transduction to immune response. Understanding these functional associations requires knowledge of the three-dimensional structure of the complex as this reveal the underlying molecular mechanism. However, determining experimentally the 3D structure of a protein complex present considerable difficulties. There is therefore a need for accurate and reliable computational methods. Several experiments have shown that protein interactions are critically dependent on just a few residues, or hot spots, at the binding interface. Hot spots make a dominant contribution to the free energy of binding and if mutated they can disrupt the interaction. In this project we aim to develop a computational method that can identify hot spot residues (and the contacts they form across the interface) in unbound proteins (i.e. without prior knowledge of the complex). This would significantly improve our ability at predicting the overall structure of the complex (the so-called docking problem). We plan to combine and integrate the basic energetic terms that contribute to the stability of protein complexes (e.g. van der Waals potential, hydrogen bonds,etc.) using state of the art machine learning techniques. In the first part of the project, we will develop a method to predict hot-spot residues at protein protein interfaces when the structure of complex is available. In the second part, we plan to systematically dock structural fragments of the two unbound proteins and test them for the presence of potential hot spots (using the classifier developed in the first part). Eventually, we will combine different sources of information (energetic, evolutionary and structural) to predict few important contacts across the interface of two proteins.	Medical Research Council	Research Grant	317659.0GBP
13	Professor Draper John	Aberystwyth University	2009-03-16	2012-03-15	Analysis of Magnaporthe grisea pathogenicity by insertion mutagenesis and hierarchical metabolomics	The project aims to utilise metabolomics approaches to identify metabolic processes associated with pathogenicity in the fungus Magnaporthe grisea, a major disease of a range of cereals and grasses. The genome sequence of the fungus has been determined and tools are available for generating targeted gene replacement mutants, studying gene expression using genome microarrays, and carrying out detailed cell biological studies of plant infection (for review see Talbot, 2003). M. grisea is being subjected to intensive functional genomics analysis, including large-scale insertion mutagenesis projects. To date, mutants affecting pathogenicity are almost without exception impaired in ability to form infection structures (appressoria) and penetrate host epidermal cells. However, in new mutant screens carried out at Exeter and elsewhere, several new classes of mutant are emerging where the timing of lesion formation and subsequent lesion expansion is impaired. We hypothesize that the corresponding genes may make important contributions to plant tissue colonization and disease symptom formation by M. grisea. Molecular genetic analysis of early-phase infection mutants in M. grisea have largely been carried out ex planta by germination of spores on inert plastic surfaces, providing large synchronous populations of infection structures for biochemical analysis. In parallel, we have developed an accurate sampling system for in planta infection sites based on microscopy and GFP-tagging of the pathogen, and by using such sampling approaches have shown that metabolomic fingerprinting and supervised data analysis can detect reproducible major changes in metabolome during lesion development. We will carry out detailed metabolome phenotyping of M. grisea in order to understand the precise roles of genes involved in the development of fungal infection structures using already available mutants. We will also refine and carry out a screen for mutants affected in the timing, rate of growth and sporulation of disease lesions. Mutants representative of different phenotypic classes will be inoculated onto hosts in controlled environments and lesion material collected at several time points. Metabolome analysis will follow a hierarchical procedure initiated with high-throughput, low-resolution ESI- MS fingerprinting (LTQ linear ion trap) and GC-tof-MS fingerprinting (LECO Pegasus II). Discrimination of appropriate sample combinations will be determined by supervised data analysis. If there is evidence for metabolome differences (e.g. comparing mutants with an isogenic wild type strain at the same stage of infection) then a further subset of the same samples will be subjected to ESI-FTMS fingerprinting to generate high resolution peak tables. Corresponding GC-tof-MS chromatograms will also be processed to deconvolute and annotate peaks for data mining. Explanatory metabolite signals between different sample classes will be determined using machine learning procedures. In ESI-MS data, high ranked m/z signals will be examined in chromatograms of further LC-FTMS analyses to predict the mass of possible parent ions which will be further fragmented to obtain MS(n) spectral data. Metabolite mass tables and spectral libraries will be searched for matches with spectra representing discriminatory peaks, and predicted metabolites will be quantified against standards using targeted GC-tof-MS or LC-MS as appropriate. Metabolome differences centred on specific metabolites in the various mutants will be used to determine areas of metabolism that may impact on fungal pathogenicity in future experiments. Parallel genetic analysis of insertional mutant collections of M. grisea will focus on those in which metabolome differences are apparent during plant tissue invasion. Gene isolation by inverse PCR, complementation and validation by targeted gene replacement experiments will be used to define genes associated with disease lesion formation by M. grisea.	Biotechnology and Biological Sciences Research Council	Research Grant	203644.0GBP
14	Professor Draper John	Aberystwyth University	2009-03-16	2012-03-15	The Rothamsted Metabolomics Centre (MeT-RO)	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20482 and MET20484)	Biotechnology and Biological Sciences Research Council	Research Grant	462262.0GBP
15	Prof Goodacre Roy	The University of Manchester	2009-03-16	2012-03-15	The human serum metabolome in health and disease	Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high-resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.	Biotechnology and Biological Sciences Research Council	Research Grant	1083263.0GBP
16	Professor Kell Douglas	The University of Manchester	2009-03-16	2012-03-15	The human serum metabolome in health and disease	Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high-resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.	Biotechnology and Biological Sciences Research Council	Research Grant	1083263.0GBP
17	Professor King Ross	Aberystwyth University	2009-03-16	2012-03-15	The Modelling Apprentice: A tool to aid the formation of cell signalling models	This proposal aims to exploit and enhance the newly developed software system Justaid" to create the "Modelling Apprentice", a tool to assist biological researchers interested in developing accurate and understandable mathematical models of cell signalling networks. Justaid is a general purpose programming assistant for scientific discovery that uses techniques from Qualitative Reasoning (QR), Machine Learning and User Interface design to to simplify the task of model construction, testing and updating. QR is chosen as the knowledge representation because domain knowledge can be acquired easily and quickly from experimental observations that lack precision. Machine Learning is used to determine whether a given model can fully explain a set of experimental observations, and suggest model updates, extending the explanatory power of the model to hitherto unexplained observations. Justaid has been designed to remove the need for the scientist to understand the underlying knowledge representation and machine learning, so that he/she can concentrate on the scientific domain rather than on mathematical concepts. This is achieved by an intuitive graphical user interface and a modular architecture whereby construction of a model for a new scientific domain simply involves creating a new Justaid library. The first task involves the construction of a Justaid library for cell signalling, using the yeast MAPK signalling pathway as a test case. This will involve expert biologists from Centre for Systems Biology, University of Cambridge and the SABR centre at the University of Aberdeen. The Modelling Apprentice and the new MAPK model will then be used by the biologists, in conjunction with the results of wet lab experiments, to test the utility an appropriateness of Justaid as a scientific discovery tool. The insight from these tests will be used to further improve the user interface design and model updating techniques of the Modelling Apprentice"	Biotechnology and Biological Sciences Research Council	Research Grant	99554.0GBP
18	Professor Stuart David	Diamond Light Source Ltd	2018-10-01	2021-09-30	SuRVoS Workbench: Enhanced machine learning for segmentation across structural biology	This proposal aims to exploit and enhance the newly developed software system Justaid" to create the "Modelling Apprentice", a tool to assist biological researchers interested in developing accurate and understandable mathematical models of cell signalling networks. Justaid is a general purpose programming assistant for scientific discovery that uses techniques from Qualitative Reasoning (QR), Machine Learning and User Interface design to to simplify the task of model construction, testing and updating. QR is chosen as the knowledge representation because domain knowledge can be acquired easily and quickly from experimental observations that lack precision. Machine Learning is used to determine whether a given model can fully explain a set of experimental observations, and suggest model updates, extending the explanatory power of the model to hitherto unexplained observations. Justaid has been designed to remove the need for the scientist to understand the underlying knowledge representation and machine learning, so that he/she can concentrate on the scientific domain rather than on mathematical concepts. This is achieved by an intuitive graphical user interface and a modular architecture whereby construction of a model for a new scientific domain simply involves creating a new Justaid library. The first task involves the construction of a Justaid library for cell signalling, using the yeast MAPK signalling pathway as a test case. This will involve expert biologists from Centre for Systems Biology, University of Cambridge and the SABR centre at the University of Aberdeen. The Modelling Apprentice and the new MAPK model will then be used by the biologists, in conjunction with the results of wet lab experiments, to test the utility an appropriateness of Justaid as a scientific discovery tool. The insight from these tests will be used to further improve the user interface design and model updating techniques of the Modelling Apprentice"	Wellcome Trust	Biomedical Resources Grant	605412.0GBP
19	Dr Falciani Francesco	University of Birmingham	2008-08-01	2011-11-30	Modelling cell-to-cell communication networks:an integrated approach to studying cell interactions during tissue angiogenesis.	The recent development of functional genomics technologies, particularly gene expression profiling, has provided the scientific community with the tools to characterize the molecular state of cells and tissues at a genomic level. The analysis of these datasets using statistical modelling and more advanced machine learning techniques designed to reconstruct and model the structure of biologically relevant pathways has contributed to reshaping biological investigation. Despite the paramount importance of this work, very little has been done to apply such network inference methodologies to reconstruct and model the structure of networks representing cell-to-cell communication events. The project we propose develops from preliminary work performed in the applicants research groups, aiming to develop the mathematical and experimental framework to infer the structure of regulatory pathways from both steady-state and temporal data. From an experimental prospective we will focus on the understanding of the general mechanisms involved in endothelia-tumor cell interactions. The project involves an initial component of genome-wide expression profiling and proteomics data generation followed by iterative cycles of computational modelling, hypothesis formulation and experimental verification using a set of state of the art gene modulation tools applicable to both in vitro and in vivo systems. Initially, we will identify informative genes and map these on known functional networks and protein-protein interaction networks in order to identify functionally interesting gene modules (for example derived from KEGG, BioGrid, REACTOME, etc). This modularization process will help in the selection of the genes to model using ARACNE, Bayesian networks and other similar computational approaches. The project will ultimately lead to the development of computational models representing cell-to-cell interaction networks from both time course and steady-state datasets. The analysis and Biological interpretation of these networks will allow us to develop hypothesis and experimentally verify them using gene manipulation techniques in both in vitro and in vivo systems. We expect this project to lead to the identification of regulatory circuits that are important in the interaction between endothelial and tumour cells during tumour growth and to a better understanding of the biological and molecular events associated to known factors involved in the tumour vascularization.	Cancer Research UK	Project Award	605412.0GBP
20	Dr Walsh Claire	University College London	2018-08-01	2021-07-31	Creating high fidelity digital tissue substrates for the development of non-invasive microstructural MRI	The recent development of functional genomics technologies, particularly gene expression profiling, has provided the scientific community with the tools to characterize the molecular state of cells and tissues at a genomic level. The analysis of these datasets using statistical modelling and more advanced machine learning techniques designed to reconstruct and model the structure of biologically relevant pathways has contributed to reshaping biological investigation. Despite the paramount importance of this work, very little has been done to apply such network inference methodologies to reconstruct and model the structure of networks representing cell-to-cell communication events. The project we propose develops from preliminary work performed in the applicants research groups, aiming to develop the mathematical and experimental framework to infer the structure of regulatory pathways from both steady-state and temporal data. From an experimental prospective we will focus on the understanding of the general mechanisms involved in endothelia-tumor cell interactions. The project involves an initial component of genome-wide expression profiling and proteomics data generation followed by iterative cycles of computational modelling, hypothesis formulation and experimental verification using a set of state of the art gene modulation tools applicable to both in vitro and in vivo systems. Initially, we will identify informative genes and map these on known functional networks and protein-protein interaction networks in order to identify functionally interesting gene modules (for example derived from KEGG, BioGrid, REACTOME, etc). This modularization process will help in the selection of the genes to model using ARACNE, Bayesian networks and other similar computational approaches. The project will ultimately lead to the development of computational models representing cell-to-cell interaction networks from both time course and steady-state datasets. The analysis and Biological interpretation of these networks will allow us to develop hypothesis and experimentally verify them using gene manipulation techniques in both in vitro and in vivo systems. We expect this project to lead to the identification of regulatory circuits that are important in the interaction between endothelial and tumour cells during tumour growth and to a better understanding of the biological and molecular events associated to known factors involved in the tumour vascularization.	Medical Research Council	Fellowship	289629.0GBP
21	Dr Hentges Kathryn	Manchester, University of	2018-08-01	2021-07-31	Identification of genes associated with cardiac development using "Machine Learning"	The recent development of functional genomics technologies, particularly gene expression profiling, has provided the scientific community with the tools to characterize the molecular state of cells and tissues at a genomic level. The analysis of these datasets using statistical modelling and more advanced machine learning techniques designed to reconstruct and model the structure of biologically relevant pathways has contributed to reshaping biological investigation. Despite the paramount importance of this work, very little has been done to apply such network inference methodologies to reconstruct and model the structure of networks representing cell-to-cell communication events. The project we propose develops from preliminary work performed in the applicants research groups, aiming to develop the mathematical and experimental framework to infer the structure of regulatory pathways from both steady-state and temporal data. From an experimental prospective we will focus on the understanding of the general mechanisms involved in endothelia-tumor cell interactions. The project involves an initial component of genome-wide expression profiling and proteomics data generation followed by iterative cycles of computational modelling, hypothesis formulation and experimental verification using a set of state of the art gene modulation tools applicable to both in vitro and in vivo systems. Initially, we will identify informative genes and map these on known functional networks and protein-protein interaction networks in order to identify functionally interesting gene modules (for example derived from KEGG, BioGrid, REACTOME, etc). This modularization process will help in the selection of the genes to model using ARACNE, Bayesian networks and other similar computational approaches. The project will ultimately lead to the development of computational models representing cell-to-cell interaction networks from both time course and steady-state datasets. The analysis and Biological interpretation of these networks will allow us to develop hypothesis and experimentally verify them using gene manipulation techniques in both in vitro and in vivo systems. We expect this project to lead to the identification of regulatory circuits that are important in the interaction between endothelial and tumour cells during tumour growth and to a better understanding of the biological and molecular events associated to known factors involved in the tumour vascularization.	British Heart Foundation	Fellowship	246410.0GBP
22	Professor Tomaszewski Maciej	Manchester, University of	2018-08-01	2021-07-31	The BHF-Turing Cardiovascular Data Science Awards (Second Call): Molecular causal networks of hypertension – a machine learning approach (joint funding with The Alan Turing Institute)	The recent development of functional genomics technologies, particularly gene expression profiling, has provided the scientific community with the tools to characterize the molecular state of cells and tissues at a genomic level. The analysis of these datasets using statistical modelling and more advanced machine learning techniques designed to reconstruct and model the structure of biologically relevant pathways has contributed to reshaping biological investigation. Despite the paramount importance of this work, very little has been done to apply such network inference methodologies to reconstruct and model the structure of networks representing cell-to-cell communication events. The project we propose develops from preliminary work performed in the applicants research groups, aiming to develop the mathematical and experimental framework to infer the structure of regulatory pathways from both steady-state and temporal data. From an experimental prospective we will focus on the understanding of the general mechanisms involved in endothelia-tumor cell interactions. The project involves an initial component of genome-wide expression profiling and proteomics data generation followed by iterative cycles of computational modelling, hypothesis formulation and experimental verification using a set of state of the art gene modulation tools applicable to both in vitro and in vivo systems. Initially, we will identify informative genes and map these on known functional networks and protein-protein interaction networks in order to identify functionally interesting gene modules (for example derived from KEGG, BioGrid, REACTOME, etc). This modularization process will help in the selection of the genes to model using ARACNE, Bayesian networks and other similar computational approaches. The project will ultimately lead to the development of computational models representing cell-to-cell interaction networks from both time course and steady-state datasets. The analysis and Biological interpretation of these networks will allow us to develop hypothesis and experimentally verify them using gene manipulation techniques in both in vitro and in vivo systems. We expect this project to lead to the identification of regulatory circuits that are important in the interaction between endothelial and tumour cells during tumour growth and to a better understanding of the biological and molecular events associated to known factors involved in the tumour vascularization.	British Heart Foundation	Fellowship	24434.0GBP
23	Professor Tomaszewski Maciej	The University of Manchester	2018-08-01	2021-07-31	Target practice: informatic and metabolomic assessment of biological network changes and of drug-cell interactions	There are many occasions where one may wish to know the site of interaction of an effector molecule with a complex biological system (i.e. network), typically by measuring changes in the accessible state variables. These are usually ill-conditioned problems, in the sense that many models can account for the observable data, and to make progress it is necessary to apply constraints and simplifications of various kinds. In contrast to cognate analyses of signalling and gene regulatory networks, the analysis of METABOLIC networks and their fluxes is attractive since they NECESSARILY possess stoichiometric and thermodynamic constraints, which are known, and measurement of the molecules they excrete as end products creates further constraints on the fluxes through the different parts of the network. Initially using baker's yeast as a model organism, we wish to demonstrate that this strategy does indeed work. The necessary simplifications include the use of mass action and lin-log kinetics, while we shall develop and exploit modern methods of multivariate statistical optimisation and machine learning for parameter estimation. These include multi-objective evolutionary algorithms, and the exploitation of probabilistic graphical methods and Gaussian process models. We shall initially develop and test these strategies in baker's yeast, Saccharomyces cerevisiae, since this is a well understood organism. However, our collaborative partner Unilever are extremely interested in Corynebacterium jeikeium, for which a genome sequence and network model exist, and using resources made available by them for this project we shall also exploit these methods in the analysis of metabolic fluxes in this organism. The deliverable will be a suite of novel methods with which to infer the site of action of any effector in a reasonably well understood metabolic network.	Biotechnology and Biological Sciences Research Council	Fellowship	1287697.0GBP
24	Professor Kell Douglas	The University of Manchester	2018-08-01	2021-07-31	Target practice: informatic and metabolomic assessment of biological network changes and of drug-cell interactions	There are many occasions where one may wish to know the site of interaction of an effector molecule with a complex biological system (i.e. network), typically by measuring changes in the accessible state variables. These are usually ill-conditioned problems, in the sense that many models can account for the observable data, and to make progress it is necessary to apply constraints and simplifications of various kinds. In contrast to cognate analyses of signalling and gene regulatory networks, the analysis of METABOLIC networks and their fluxes is attractive since they NECESSARILY possess stoichiometric and thermodynamic constraints, which are known, and measurement of the molecules they excrete as end products creates further constraints on the fluxes through the different parts of the network. Initially using baker's yeast as a model organism, we wish to demonstrate that this strategy does indeed work. The necessary simplifications include the use of mass action and lin-log kinetics, while we shall develop and exploit modern methods of multivariate statistical optimisation and machine learning for parameter estimation. These include multi-objective evolutionary algorithms, and the exploitation of probabilistic graphical methods and Gaussian process models. We shall initially develop and test these strategies in baker's yeast, Saccharomyces cerevisiae, since this is a well understood organism. However, our collaborative partner Unilever are extremely interested in Corynebacterium jeikeium, for which a genome sequence and network model exist, and using resources made available by them for this project we shall also exploit these methods in the analysis of metabolic fluxes in this organism. The deliverable will be a suite of novel methods with which to infer the site of action of any effector in a reasonably well understood metabolic network.	Biotechnology and Biological Sciences Research Council	Fellowship	1287697.0GBP
25	Professor McKeigue Paul	University of Edinburgh	2009-03-16	2012-03-15	Development of Bayesian methods for genetic epidemiology	None	Medical Research Council	Research Grant	317659.0GBP
26	Professor Low Nicola Minling	University of Berne	2017-11-01	2021-10-31	Zika virus: causality, open science and risks of emerging infectious diseases	BackgroundZika virus infection was established as a cause of congenital abnormalities, including microcephaly, and of Guillain-Barré syndrome during a Public Health Emergency of International Concern that the World Health Organization (WHO) announced in February 2016. The Public Health Emergency ended in November 2016 but substantial gaps remain in the causality framework of Zika complications, knowledge about population level susceptibility to Zika virus infection and the risks of the newly recognised route of sexual transmission of Zika virus. Objectives1. To produce a web platform that will allow the production and updating of living systematic reviews of evidence about Zika virus infection; 2. To estimate key parameters that will allow refined inferences about the sexual transmissibility of Zika virus in endemic and non-endemic settings; 3. To investigate the seroprevalence of antibodies to Zika virus in different geographic settings and to use seroprevalence data to allow estimation of the duration of immunity after Zika virus infection. Methods1. We will produce an open access web application to produce living systematic reviews that allow continual updating of evidence about causal associations between Zika virus and its complications, and emerging research questions. The application will automate searching and deduplication, use text mining and machine learning to assist screening and allow automated updates of review output for rapid publication. 2. We have developed a sexual transmission framework to identify key parameters needed to understand the potential for ongoing spread of Zika virus through sexual transmission. We will analyse data to determine the duration of persistence of Zika virus in semen, vaginal fluid, urine, breast milk and other bodily fluids. We will then use a transmission model to estimate the per sex act probability of Zika virus transmission. 3. We will use data from ongoing longitudinal studies and repeated cross-sectional studies in Nicaragua that will determine antibody levels to Zika virus using new diagnostic tests (taking into account exposure to dengue and chikungunya). We will apply “back-calculation” methods to determine the duration of immunity of Zika virus infection. We will also pilot a method for the collection and assessment of seroprevalence data collected in a range of settings that have experienced new Zika transmission since 2013 and where Zika is presumed to be endemic to improve understanding of population level susceptibility to Zika virus infection.Timeline: The project will last four yearsImportance and impactThis project has considerable importance for research on Zika virus infection and transmission. Whilst vaccine development is advancing rapidly, there are still important gaps in our knowledge about vulnerability to Zika virus in large proportion of the world’s population that lives in areas where Aedes mosquito vectors are distributed. The project objectives are aligned with the research agenda of the WHO and with international initiatives to increase capacity for preparedness for infectious disease pandemics. The outputs are therefore relevant to current research priorities. By working within a culture of open science and with the living systematic review network, our research outputs, including publications and software will be publicly available as quickly as possible.	Swiss National Science Foundation	Project funding (Div. I-III)	700000.0CHF
27	Professor Ananiadou Sophia	The University of Manchester	2017-11-01	2021-10-31	Tools for the text mining-based visualisation of the provenance of biochemical networks	Systems biology is concerned with the modelling, visualisation and analysis of biochemical networks in which, for instance, metabolites are 'linked' by arrows representing the enzymes which turn one molecule into another or which are modified by particular substances. SBML provides a computer- readable 'standard' for describing such biochemical or signalling networks. However, these diagrams (and thus the SBML models) are divorced from the scientific evidence on which they are based, represented by the scientific literature (and increasingly by online databases). In order to overcome the problems of reading the burgeoning scientific literature, we shall deploy Text Mining TM. TM involves named entity recognition (i.e. semantic annotation of enzymes, metabolites, etc) and information extraction (i.e. relationship extraction between named entities). An important part of this proposal is to find solutions for the terminology problem in systems biology, by developing techniques for recognising synonym terms.Based on our efficient parsing techniques, we shall extract relationships between entities that will form the basis by which we shall can discover, index, store and display the scientific evidence for such linkages. The selection of the most pertinent relationships will be performed using our preferred methods of advanced machine learning (Support Vector Machines and Genetic Programming). The overall aim of the project is thus to develop and deploy the necessary TM tools and to use them to display the different relationships to the user together with the literature from which they have been extracted. The different types (and strength) of evidence for these interactions will then be visualised directly and linked to a dynamic website of the literature. This will thus give users a direct linkage between the systems biology diagrams encoded in (an advanced form of) SBML and the scientific evidence for them. Where available, linkages to kinetic data will also be made.	Biotechnology and Biological Sciences Research Council	Project funding (Div. I-III)	549458.0GBP
28	Professor Kell Douglas	The University of Manchester	2017-11-01	2021-10-31	Tools for the text mining-based visualisation of the provenance of biochemical networks	Systems biology is concerned with the modelling, visualisation and analysis of biochemical networks in which, for instance, metabolites are 'linked' by arrows representing the enzymes which turn one molecule into another or which are modified by particular substances. SBML provides a computer- readable 'standard' for describing such biochemical or signalling networks. However, these diagrams (and thus the SBML models) are divorced from the scientific evidence on which they are based, represented by the scientific literature (and increasingly by online databases). In order to overcome the problems of reading the burgeoning scientific literature, we shall deploy Text Mining TM. TM involves named entity recognition (i.e. semantic annotation of enzymes, metabolites, etc) and information extraction (i.e. relationship extraction between named entities). An important part of this proposal is to find solutions for the terminology problem in systems biology, by developing techniques for recognising synonym terms.Based on our efficient parsing techniques, we shall extract relationships between entities that will form the basis by which we shall can discover, index, store and display the scientific evidence for such linkages. The selection of the most pertinent relationships will be performed using our preferred methods of advanced machine learning (Support Vector Machines and Genetic Programming). The overall aim of the project is thus to develop and deploy the necessary TM tools and to use them to display the different relationships to the user together with the literature from which they have been extracted. The different types (and strength) of evidence for these interactions will then be visualised directly and linked to a dynamic website of the literature. This will thus give users a direct linkage between the systems biology diagrams encoded in (an advanced form of) SBML and the scientific evidence for them. Where available, linkages to kinetic data will also be made.	Biotechnology and Biological Sciences Research Council	Project funding (Div. I-III)	549458.0GBP
29	Professor Kell Douglas	University College London	2017-11-01	2021-10-31	Cerebellum as a neuronal machine: Behavioural, electrophysiological and computational analysis of classical conditioning	The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified and accurately measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single- unit electrophysiology and computational methods. (Joint with BBS/B/1700X and BBS/B/17026).	Biotechnology and Biological Sciences Research Council	Project funding (Div. I-III)	330321.0GBP
30	Professor Kell Douglas	University of Cambridge	2017-11-01	2021-10-31	Cerebellum as a neuronal machine: Behavioural, electrophysiological and computational analysis of classical conditioning	The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single-unit electrophysiology and computational methods. (Joint with BBS/B/17026 and BBS/B/16984).	Biotechnology and Biological Sciences Research Council	Project funding (Div. I-III)	272021.0GBP
31	Professor Kell Douglas	University of Sheffield	2017-11-01	2021-10-31	Cerebellum as a neuronal machine: Behavioural, electrophysiological and computational analysis of classical conditioning	The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single-unit electrophysiology and computational methods. (Joint with BBS/B/1700X and BBS/B/16984).	Biotechnology and Biological Sciences Research Council	Project funding (Div. I-III)	184698.0GBP
32	Professor Bath Philip	Nottingham,University of	2017-11-01	2021-10-31	Assessment of modern machine learning methods and conventional statistical regression techniques in diagnosis and prediction of outcome after acute stroke using big data	The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single-unit electrophysiology and computational methods. (Joint with BBS/B/1700X and BBS/B/16984).	British Heart Foundation	Project funding (Div. I-III)	145079.0GBP
33	Professor Wardlaw Joanna	Edinburgh, University of	2017-11-01	2021-10-31	The BHF-Turing Cardiovascular Data Science Awards (Second Call): Uncovering retinal microvascular predictors of compromised brain haemodynamics in small vessel disease (joint funding with The Alan Turing Institute)	The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single-unit electrophysiology and computational methods. (Joint with BBS/B/1700X and BBS/B/16984).	British Heart Foundation	Project funding (Div. I-III)	66000.0GBP
34	Professor Wardlaw Joanna	Edinburgh, University of	2017-11-01	2021-10-31	Inference and learning in machine vision	The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single-unit electrophysiology and computational methods. (Joint with BBS/B/1700X and BBS/B/16984).	Biotechnology and Biological Sciences Research Council	Project funding (Div. I-III)	66000.0GBP
35	Professor Wernisch Lorenz	University of Cambridge	2016-12-01	2019-03-31	Statistical bioinformatics and genetics	The cerebellum has been likened to a neuronal machine making critical contributions to sensory-motor control, motor learning and cognition. Its cortical neurons are in a regularly repeating, geometrical array that suggests an information-processing algorithm consistent across every region. Classical conditioning of the eyeblink response is an excellent model of associative learning and recent work has defined cerebellar circuitry essential for its learning and expression. These advances provide an outstanding opportunity to analyse an identified neural network operating under natural conditions to develop a specified measurable learned behaviour. Three laboratories will work closely together to characterise the cerebellar algorithm using behavioural/pharmacological, multiple single-unit electrophysiology and computational methods. (Joint with BBS/B/1700X and BBS/B/16984).	Medical Research Council	Unit	66000.0GBP
36	Professor Wernisch Lorenz	Rothamsted Research	2016-12-01	2019-03-31	The Rothamsted Metabolomics Centre (MeT-RO)	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)	Biotechnology and Biological Sciences Research Council	Unit	1461648.0GBP
37	Professor Razavi Reza	King's College London	2019-09-01	2022-12-31	Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)	Wellcome Trust	Innovations Priority Project	4030521.01GBP
38	Dr Yacoub Sophie	Oxford University Clinical Research Unit	2019-09-01	2022-12-31	Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)	Wellcome Trust	Innovations Priority Project	4030521.01GBP
39	Dr Thwaites Catherine	University of Oxford	2019-09-01	2022-12-31	Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)	Wellcome Trust	Innovations Priority Project	4030521.01GBP
40	Dr Modat Marc	King's College London	2019-09-01	2022-12-31	Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)	Wellcome Trust	Innovations Priority Project	4030521.01GBP
41	Prof Dondorp Arjen	University of Oxford	2019-09-01	2022-12-31	Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)	Wellcome Trust	Innovations Priority Project	4030521.01GBP
42	Dr Nguyen Vinh Chau	Oxford University Clinical Research Unit	2019-09-01	2022-12-31	Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)	Wellcome Trust	Innovations Priority Project	4030521.01GBP
43	Prof Karlen Walter	ETH Zurich	2019-09-01	2022-12-31	Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)	Wellcome Trust	Innovations Priority Project	4030521.01GBP
44	Dr Georgiou Pantelis	Imperial College London	2019-09-01	2022-12-31	Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)	Wellcome Trust	Innovations Priority Project	4030521.01GBP
45	Prof Denehy Linda	University of Melbourne	2019-09-01	2022-12-31	Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)	Wellcome Trust	Innovations Priority Project	4030521.01GBP
46	Prof Clifton David	University of Oxford	2019-09-01	2022-12-31	Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)	Wellcome Trust	Innovations Priority Project	4030521.01GBP
47	Prof Day Nicholas	University of Oxford	2019-09-01	2022-12-31	Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)	Wellcome Trust	Innovations Priority Project	4030521.01GBP
48	Prof Thwaites Guy	University of Oxford	2019-09-01	2022-12-31	Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)	Wellcome Trust	Innovations Priority Project	4030521.01GBP
49	Prof Leff Alexander	University College London	2016-12-01	2021-11-30	Digital neuro-interventions to enhance re-learning in patients with acquired and degenerative brain diseases	Metabolomics is one of several technologies that will contribute to our quest to understand the function of all genes. MeT-RO will build and operate a high-throughput primary and secondary metabolite fingerprinting service for plants and microbes. We will develop and utilise a large- scale analytical chemistry facility generating NMR, GC, GC- MS, HPLC and LC-MS data at Rothamsted. Data will be managed in LIMS system at Rothamsted and transferred electronically to our partners at the University of Aberystwyth and UMIST and used to construct a metabolomics database from which comparative information will be extracted with chemometric, machine learning and other bioinformatic tools. MeT-RO and its partners will act as foci for research training in metabolomic technology while contributing data and interpretation tools towards systematic functional genomics, as well as to many other applications in the food, pharmaceutical and agrochemical industries. (Joint with MET20483 and MET20484)	National Institute for Health Research (Department of Health)	Full Award	2074551.0GBP
50	Professor King Ross	Aberystwyth University	None	None	The Modelling Apprentice: A tool to aid the formation of cell signalling models	This proposal aims to exploit and enhance the newly developed software system Justaid" to create the "Modelling Apprentice", a tool to assist biological researchers interested in developing accurate and understandable mathematical models of cell signalling networks. Justaid is a general purpose programming assistant for scientific discovery that uses techniques from Qualitative Reasoning (QR), Machine Learning and User Interface design to to simplify the task of model construction, testing and updating. QR is chosen as the knowledge representation because domain knowledge can be acquired easily and quickly from experimental observations that lack precision. Machine Learning is used to determine whether a given model can fully explain a set of experimental observations, and suggest model updates, extending the explanatory power of the model to hitherto unexplained observations. Justaid has been designed to remove the need for the scientist to understand the underlying knowledge representation and machine learning, so that he/she can concentrate on the scientific domain rather than on mathematical concepts. This is achieved by an intuitive graphical user interface and a modular architecture whereby construction of a model for a new scientific domain simply involves creating a new Justaid library. The first task involves the construction of a Justaid library for cell signalling, using the yeast MAPK signalling pathway as a test case. This will involve expert biologists from Centre for Systems Biology, University of Cambridge and the SABR centre at the University of Aberdeen. The Modelling Apprentice and the new MAPK model will then be used by the biologists, in conjunction with the results of wet lab experiments, to test the utility an appropriateness of Justaid as a scientific discovery tool. The insight from these tests will be used to further improve the user interface design and model updating techniques of the Modelling Apprentice"	Biotechnology and Biological Sciences Research Council	None	99554.0GBP
51	Professor Rockall Andrea	The Royal Marsden NHS Foundation Trust	2018-06-01	2021-08-31	MAchine Learning In MyelomA Response (MALIMAR study): Development of machine learning support for reading whole body diffusion weighted magnetic resonance imaging (WB-MRI) in myeloma for the detection and quantification of the extent of disease before and after treatment	This proposal aims to exploit and enhance the newly developed software system Justaid" to create the "Modelling Apprentice", a tool to assist biological researchers interested in developing accurate and understandable mathematical models of cell signalling networks. Justaid is a general purpose programming assistant for scientific discovery that uses techniques from Qualitative Reasoning (QR), Machine Learning and User Interface design to to simplify the task of model construction, testing and updating. QR is chosen as the knowledge representation because domain knowledge can be acquired easily and quickly from experimental observations that lack precision. Machine Learning is used to determine whether a given model can fully explain a set of experimental observations, and suggest model updates, extending the explanatory power of the model to hitherto unexplained observations. Justaid has been designed to remove the need for the scientist to understand the underlying knowledge representation and machine learning, so that he/she can concentrate on the scientific domain rather than on mathematical concepts. This is achieved by an intuitive graphical user interface and a modular architecture whereby construction of a model for a new scientific domain simply involves creating a new Justaid library. The first task involves the construction of a Justaid library for cell signalling, using the yeast MAPK signalling pathway as a test case. This will involve expert biologists from Centre for Systems Biology, University of Cambridge and the SABR centre at the University of Aberdeen. The Modelling Apprentice and the new MAPK model will then be used by the biologists, in conjunction with the results of wet lab experiments, to test the utility an appropriateness of Justaid as a scientific discovery tool. The insight from these tests will be used to further improve the user interface design and model updating techniques of the Modelling Apprentice"	National Institute for Health Research (Department of Health)	Full Award	646787.62GBP
52	Professor Rockall Andrea	University of Leeds	2007-02-01	2010-09-30	Cognitive Systems Foresight: Human Attention and Machine Learning	This proposal aims to exploit and enhance the newly developed software system Justaid" to create the "Modelling Apprentice", a tool to assist biological researchers interested in developing accurate and understandable mathematical models of cell signalling networks. Justaid is a general purpose programming assistant for scientific discovery that uses techniques from Qualitative Reasoning (QR), Machine Learning and User Interface design to to simplify the task of model construction, testing and updating. QR is chosen as the knowledge representation because domain knowledge can be acquired easily and quickly from experimental observations that lack precision. Machine Learning is used to determine whether a given model can fully explain a set of experimental observations, and suggest model updates, extending the explanatory power of the model to hitherto unexplained observations. Justaid has been designed to remove the need for the scientist to understand the underlying knowledge representation and machine learning, so that he/she can concentrate on the scientific domain rather than on mathematical concepts. This is achieved by an intuitive graphical user interface and a modular architecture whereby construction of a model for a new scientific domain simply involves creating a new Justaid library. The first task involves the construction of a Justaid library for cell signalling, using the yeast MAPK signalling pathway as a test case. This will involve expert biologists from Centre for Systems Biology, University of Cambridge and the SABR centre at the University of Aberdeen. The Modelling Apprentice and the new MAPK model will then be used by the biologists, in conjunction with the results of wet lab experiments, to test the utility an appropriateness of Justaid as a scientific discovery tool. The insight from these tests will be used to further improve the user interface design and model updating techniques of the Modelling Apprentice"	Wellcome Trust	Project Grant	67918.0GBP
53	Professor Rockall Andrea	University of Bristol	2007-02-01	2010-09-30	Cognitive Systems Foresight: Human Attention and Machine Learning	This proposal aims to exploit and enhance the newly developed software system Justaid" to create the "Modelling Apprentice", a tool to assist biological researchers interested in developing accurate and understandable mathematical models of cell signalling networks. Justaid is a general purpose programming assistant for scientific discovery that uses techniques from Qualitative Reasoning (QR), Machine Learning and User Interface design to to simplify the task of model construction, testing and updating. QR is chosen as the knowledge representation because domain knowledge can be acquired easily and quickly from experimental observations that lack precision. Machine Learning is used to determine whether a given model can fully explain a set of experimental observations, and suggest model updates, extending the explanatory power of the model to hitherto unexplained observations. Justaid has been designed to remove the need for the scientist to understand the underlying knowledge representation and machine learning, so that he/she can concentrate on the scientific domain rather than on mathematical concepts. This is achieved by an intuitive graphical user interface and a modular architecture whereby construction of a model for a new scientific domain simply involves creating a new Justaid library. The first task involves the construction of a Justaid library for cell signalling, using the yeast MAPK signalling pathway as a test case. This will involve expert biologists from Centre for Systems Biology, University of Cambridge and the SABR centre at the University of Aberdeen. The Modelling Apprentice and the new MAPK model will then be used by the biologists, in conjunction with the results of wet lab experiments, to test the utility an appropriateness of Justaid as a scientific discovery tool. The insight from these tests will be used to further improve the user interface design and model updating techniques of the Modelling Apprentice"	Wellcome Trust	Project Grant	67918.0GBP
54	Professor Rockall Andrea	University of Liverpool	2007-02-01	2010-09-30	The application of support vector machine feature selection to cross sectional studies in epidemiology	This proposal builds on an EPSRC small grant GR/S73631/01, graded tending to outstanding" in its final report. It is a resubmission from March 2005 of application GR/EP/DO3O684 sent EPRSC Life Sciences Interface as a continuation of GR/S73631/01 and recommended for submission to BBSRC with a contribution in financial support from EPSRC if successful. Epidemiologists from Liverpool Veterinary School and computer scientist from DIMACS, a National Science Foundations Institute at Rutgers, New Jersey, USA, have recently reported the use of Support Vector Machine learning as a method of identifying risk factors for disease from observational epidemiological data. Support Vector Machine classification was developed in the mid 90's and although related to neural networks, the technique is simpler more robust and founded on statistical learning theory. In particular the use of SVM overcomes overfitting associated with the empirical risk minimisation, (ERM) which aims to minimise the error on the training data set but results in poor generalisation (i.e. performance on unseen datasets). SVM are arguably the single most important development in supervised classification in recent years. They are known to generalise well in high dimensional space even with small training sample conditions, when the data are noisy. SVM are not only good classifiers but are also good feature selection techniques. SVM has been used for the verification and recognition of faces, speech, handwriting and such diverse events as goal detection in football matches and financial forecasting. In life sciences it has been applied to gene expression, proteomics and disease diagnosis With the exception of a recent report, using single nucleotide polymorphisms (SNPs) to predict an increased risk of breast cancer there have been no published reports of the application of SVM in epidemiology. This project aims to further develop the application of SVM, to improve kernel selection and to produce a user-friendly SVM program for wider epidemiological use Data for the development of this program will be provided from an epidemiological study of an emerging disease of poultry (wet litter). Because meat birds live for only 6-7 weeks broiler flocks provide the opportunity to validate the classifications made by SVM during the period of study. The final program will provide a new paradigm in epidemiology and act as an easily applicable "second opinion" for statistical models generated using the "epidemiological standard" of logisitic regression. It will also, as a by-product, improve our understanding of wet litter in poultry. The final program will be applicable to observational studies of non-infectious and infectious human and animal disease."	Biotechnology and Biological Sciences Research Council	Project Grant	400793.0GBP
55	Professor Crook Derrick	University of Oxford	2016-10-01	2020-10-01	Comprehensive Resistance Prediction for Tuberculosis: an International Consortium (CRyPTIC)	This proposal builds on an EPSRC small grant GR/S73631/01, graded tending to outstanding" in its final report. It is a resubmission from March 2005 of application GR/EP/DO3O684 sent EPRSC Life Sciences Interface as a continuation of GR/S73631/01 and recommended for submission to BBSRC with a contribution in financial support from EPSRC if successful. Epidemiologists from Liverpool Veterinary School and computer scientist from DIMACS, a National Science Foundations Institute at Rutgers, New Jersey, USA, have recently reported the use of Support Vector Machine learning as a method of identifying risk factors for disease from observational epidemiological data. Support Vector Machine classification was developed in the mid 90's and although related to neural networks, the technique is simpler more robust and founded on statistical learning theory. In particular the use of SVM overcomes overfitting associated with the empirical risk minimisation, (ERM) which aims to minimise the error on the training data set but results in poor generalisation (i.e. performance on unseen datasets). SVM are arguably the single most important development in supervised classification in recent years. They are known to generalise well in high dimensional space even with small training sample conditions, when the data are noisy. SVM are not only good classifiers but are also good feature selection techniques. SVM has been used for the verification and recognition of faces, speech, handwriting and such diverse events as goal detection in football matches and financial forecasting. In life sciences it has been applied to gene expression, proteomics and disease diagnosis With the exception of a recent report, using single nucleotide polymorphisms (SNPs) to predict an increased risk of breast cancer there have been no published reports of the application of SVM in epidemiology. This project aims to further develop the application of SVM, to improve kernel selection and to produce a user-friendly SVM program for wider epidemiological use Data for the development of this program will be provided from an epidemiological study of an emerging disease of poultry (wet litter). Because meat birds live for only 6-7 weeks broiler flocks provide the opportunity to validate the classifications made by SVM during the period of study. The final program will provide a new paradigm in epidemiology and act as an easily applicable "second opinion" for statistical models generated using the "epidemiological standard" of logisitic regression. It will also, as a by-product, improve our understanding of wet litter in poultry. The final program will be applicable to observational studies of non-infectious and infectious human and animal disease."	Wellcome Trust	Collaborative Award in Science	4095865.0GBP
56	Professor Sir Bodmer Walter	University of Oxford	2009-07-12	2016-07-11	Genetics of the people of the british isles and their faces.	We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.	Wellcome Trust	Programme Grant	2067797.0GBP
57	Professor Ciccarelli Olga	University College London	2019-01-01	2023-12-31	Predicting individual treatment responses towards personalised medicine in Multiple Sclerosis	We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.	National Institute for Health Research (Department of Health)	Full Grant	1853695.0GBP
58	Professor Macleod Malcolm	University of Edinburgh	2016-04-01	2018-03-31	Pilot study of the utility of text mining and machine learning tools to accelerate systematic review and meta-analysis of findings of in vivo research	We will add 1,000 samples from the Ancient British western fringe, and from the eastern areas that most reflect the Anglo-Saxon influence, to the 3,500 already collected (rural areas, all four grandparents from the same area). A further 1,500 samples will be collected from populations surrounding the British Isles. Genetic variation determining facial features will be identified through association analysis of candidate gene markers with statistically determined facial characteristics obtained from photographs of the newly collected volunteers and of MZ and DZ twins from the TRU study. Lymphoblastoid cell lines will be grown out from a subset of 1,000 individuals to enable study of the genetic control of tissue specific protein and mRNA levels. Surname distributions will be used to enhance information about the locality of volunteers. Extensive genotyping will be done on the new samples. Analysis of the results of this and previous genotype data, using machine learning algorithms a nd second level haplotyping, together with the facial genetic and certain other markers, will be used to construct a panel of highly informative AIMs. These will be typed on all samples and form the basis for an extensive analysis of population substructure using a wide variety of statistical approaches.	Medical Research Council	Research Grant	352793.0GBP
59	Professor Macleod Malcolm	University of Oxford	2010-05-01	2014-04-30	Dissecting the contribution of anterior prefrontal cortex to decision making with computational, statistical and neuroimaging approaches.	The first experiments would combine computational modelling from machine learning and mathematical economics with functional MRI to examine the neural computations underlying decision-making between multiple uncertain prospects in a changing environment. We would then directly compare competing computational models with subject behaviour and neural data. This would enable us to address several pivotal questions for the first time, such as how the values of different decision variables are repre sented in the vmPFC and FPC and whether there are dissociable neural routes guiding exploratory decision-making. To examine the causal contribution of FPC to decision-making, I propose to combine interference and recording methodologies. Specifically, I would interfere with FPC processing and record the effects from downstream brain regions whose signals are hypothesized to depend on FPC. This would reveal precisely when the FPC is essential for value-based decision-making. Recent ad vances in diffusion-weighted imaging (DWI) have made it feasible for the first time to examine the trajectories of anatomical pathways in the brain in vivo. I propose to combine the approaches outlined above with DWI to examine how inter-individual variability in functional interactions between the FPC and other brain regions during decision-making relates to the anatomical connectivity of the FPC.	Wellcome Trust	Sir Henry Wellcome Postdoctoral Fellowship	250000.0GBP
60	Professor Rockall Andrea	Imperial College London	2015-02-01	2019-09-30	Development and evaluation of machine learning methods in whole body magnetic resonance imaging with diffusion weighted imaging for staging of patients with cancer. (MAchine Learning In whole Body Oncology, MALIBO)	The first experiments would combine computational modelling from machine learning and mathematical economics with functional MRI to examine the neural computations underlying decision-making between multiple uncertain prospects in a changing environment. We would then directly compare competing computational models with subject behaviour and neural data. This would enable us to address several pivotal questions for the first time, such as how the values of different decision variables are repre sented in the vmPFC and FPC and whether there are dissociable neural routes guiding exploratory decision-making. To examine the causal contribution of FPC to decision-making, I propose to combine interference and recording methodologies. Specifically, I would interfere with FPC processing and record the effects from downstream brain regions whose signals are hypothesized to depend on FPC. This would reveal precisely when the FPC is essential for value-based decision-making. Recent ad vances in diffusion-weighted imaging (DWI) have made it feasible for the first time to examine the trajectories of anatomical pathways in the brain in vivo. I propose to combine the approaches outlined above with DWI to examine how inter-individual variability in functional interactions between the FPC and other brain regions during decision-making relates to the anatomical connectivity of the FPC.	National Institute for Health Research (Department of Health)	Full Grant	578090.0GBP
61	Prof Kourtzi Zoe	University of Birmingham	2015-02-01	2019-09-30	Classification decisions in machines and human brains	Our ability to extract abstract information from our experiences and group it into meaningful units (categories) is a fundamental cognitive skill for interpreting the complex environments we inhabit. How does the human brain learn about the regularities and context of novel perceptual experiences that have not been honed by evolution and development and decide on their interpretation and classification? We propose a novel interdisciplinary approach that integrates advanced multimodal imaging (fMRI, MEG, EEG) methods and state-of-the art machine learning algorithms to examine the neural architecture that underlies classification learning and decisions in the human brain. We aim to a) create an electrical-haemodynamic signal space in which neuronal assemblies and their interactions can be characterised, and b) to develop a unified algorithmic method for efficiently analyzing neural imaging and behavioural data. In particular, we will use machine pattern classifiers to define perceptual decision images that reveal the critical stimulus features on which the observers base their perceptual classifications, and neural decision images that reveal the neural selectivity, plasticity and dynamics with which these features are encoded and learnt by the human brain. Our methodological and theoretical developments will provide a) novel and sensitive tools for the assessment of the behavioural and neural signatures of perceptual decisions in neuroscience, and b) novel challenges and insights in machine learning for the optimisation of biologically-constrained algorithms with direct applications for expert recognition systems. Further, our findings will advance our understanding of the link between sensory input, neural code and human behaviour and have potential applications for studying the development of perceptual decision processes across the life span, and their impairment and potential for recovery of function in ageing and disorders of visual and social cognition.	Biotechnology and Biological Sciences Research Council	Full Grant	936093.0GBP
62	Professor Munroe Patricia	Queen Mary, University of London	2020-10-05	2023-10-04	Predicting hypertension mediated subclinical left ventricular hypertrophy using machine learning techniques	Our ability to extract abstract information from our experiences and group it into meaningful units (categories) is a fundamental cognitive skill for interpreting the complex environments we inhabit. How does the human brain learn about the regularities and context of novel perceptual experiences that have not been honed by evolution and development and decide on their interpretation and classification? We propose a novel interdisciplinary approach that integrates advanced multimodal imaging (fMRI, MEG, EEG) methods and state-of-the art machine learning algorithms to examine the neural architecture that underlies classification learning and decisions in the human brain. We aim to a) create an electrical-haemodynamic signal space in which neuronal assemblies and their interactions can be characterised, and b) to develop a unified algorithmic method for efficiently analyzing neural imaging and behavioural data. In particular, we will use machine pattern classifiers to define perceptual decision images that reveal the critical stimulus features on which the observers base their perceptual classifications, and neural decision images that reveal the neural selectivity, plasticity and dynamics with which these features are encoded and learnt by the human brain. Our methodological and theoretical developments will provide a) novel and sensitive tools for the assessment of the behavioural and neural signatures of perceptual decisions in neuroscience, and b) novel challenges and insights in machine learning for the optimisation of biologically-constrained algorithms with direct applications for expert recognition systems. Further, our findings will advance our understanding of the link between sensory input, neural code and human behaviour and have potential applications for studying the development of perceptual decision processes across the life span, and their impairment and potential for recovery of function in ageing and disorders of visual and social cognition.	British Heart Foundation	Fellowship	251134.0GBP
63	Professor Jayson Gordon	University of Manchester	2005-01-01	2008-06-30	The human serum metabolome in health and disease	Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high- resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.	Biotechnology and Biological Sciences Research Council	LINK project	168047.0GBP
64	Professor L. Barreto Mauricio	Fiocruz (Oswaldo Cruz Foundation)	2020-04-01	2021-09-30	The risk of a chronic clinical condition following a previous hospitalisation by a psychiatric disorder: a linkage nationwide study in Brazil	Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high- resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.	Medical Research Council	P&Cs	161780.0GBP
65	Professor Herrick Ariane	University of Manchester	2019-09-01	2022-08-05	Development of a measuring app for finger lesions as an outcome measure for systemic sclerosis-related digital ulceration (SALVE: Scleroderma App for Lesion VErification)’	Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high- resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.	Versus Arthritis	Full Application Treatment	216773.0GBP
66	Professor McMahon Stephen	King's College London	2017-01-01	2021-12-31	Stratifying Chronic Pain Patients By Pathological Mechanism- A Multimodal Investigation Using Functional MRI, Psychometric And Clinical Assessment	Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high- resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.	Medical Research Council	Research Grant	2718044.0GBP
67	Professor Ciccarelli Olga	Institute of Neurology, UCL	2019-01-01	2021-12-31	Assessing treatment responses using machine learning	Thirteen disease-modifying treatments (DMTs) are approved in the UK to reduce the risk of relapses. Patients can switch from one first-line DMT to a more effective medication, if they present with relapses. The consequences of this policy, that requires "failure" of DMTs before using another DMT are continuous relapses and disability accumulation. We cannot currently predict which DMT will work best for an individual patient. The goal of this project is to predict the individual treatment response in MS by translating machine learning from the computer science field into clinical practice. All patients (adults and children) currently on DMTs and due to start a treatment at UCLH NHS Trust and in the UK paediatric neuroinflammation centres will be studied. Demographic factors, diet quality and life-style, clinical scales, comorbidities, MRI scans and blood tests for safety data, neurofilaments-light levels, and genetic analysis will be collected. This information will be used to predict treatment response in each individual using a high-dimensional model. The model will be validated on independent, international cohorts. This project will bridge the gap between clinical trials, which focus on the "average" response to a therapy, to clinical practice, where the focus should be on the individual treatment response.	Multiple Sclerosis Society	Project grant application	355293.0GBP
68	Professor Lotery Andrew	University of Southampton	2019-01-01	2024-01-01	Deciphering AMD by deep phenotyping and machine learning	Thirteen disease-modifying treatments (DMTs) are approved in the UK to reduce the risk of relapses. Patients can switch from one first-line DMT to a more effective medication, if they present with relapses. The consequences of this policy, that requires "failure" of DMTs before using another DMT are continuous relapses and disability accumulation. We cannot currently predict which DMT will work best for an individual patient. The goal of this project is to predict the individual treatment response in MS by translating machine learning from the computer science field into clinical practice. All patients (adults and children) currently on DMTs and due to start a treatment at UCLH NHS Trust and in the UK paediatric neuroinflammation centres will be studied. Demographic factors, diet quality and life-style, clinical scales, comorbidities, MRI scans and blood tests for safety data, neurofilaments-light levels, and genetic analysis will be collected. This information will be used to predict treatment response in each individual using a high-dimensional model. The model will be validated on independent, international cohorts. This project will bridge the gap between clinical trials, which focus on the "average" response to a therapy, to clinical practice, where the focus should be on the individual treatment response.	Wellcome Trust	Collaborative Award in Science	3980169.0GBP
69	Professor Crook Derrick	University of Oxford	2016-10-01	2020-03-31	Comprehensive Resistance Prediction for Tuberculosis: an International Consortium (CRyPTIC)	Our aim is to achieve sufficiently accurate genetic prediction of resistance to all anti-tuberculosis drugs for whole genome sequencing (WGS) to replace slow, cumbersome, culture-based drug susceptibility testing (DST) for Mycobacterium tuberculosis complex (MTBC). This would enable rapid-turnaround near-to-patient assays to revolutionize drug-resistant TB identification and management. This multidisciplinary collaboration including TB experts from five continents, WHO, statisticians/mathematicians and software engineers will integrate machine-learning, statistical genetics and molecular genetics methods to uncover all genomic variation causing at least 1% resistance to first- and second-line anti-TB drugs with >1% resistance prevalence. We will use largescale global and clade-representative WGS (>90,000), with initially >37,000 isolates with extended DST. Approaches include better WGS assembly to identify more variants with more precision, improved statistical methods to detect associations between variants and DST, and selected molecular validation of predicted resistant variants. The project will create an automatically-updating, free, publically accessible, comprehensive data repository of resistance-conferring variants. This will provide accurate genetic resistance prediction for all drugs for any new MTBC WGS; superior design of near-to-patient amplification-based molecular drug-resistance tests as alternatives to ‘WGS-only’ solutions; and better, faster and more targeted drug-resistant TB treatment, facilitating improved control and WHO’s initiative to eliminate TB by 2050.	Medical Research Council	P&Cs	1999934.0GBP
70	Dr Hentges Kathryn	Manchester, University of	2016-10-01	2020-03-31	Identification of genes associated with cardiac development using "Machine Learning"	Our aim is to achieve sufficiently accurate genetic prediction of resistance to all anti-tuberculosis drugs for whole genome sequencing (WGS) to replace slow, cumbersome, culture-based drug susceptibility testing (DST) for Mycobacterium tuberculosis complex (MTBC). This would enable rapid-turnaround near-to-patient assays to revolutionize drug-resistant TB identification and management. This multidisciplinary collaboration including TB experts from five continents, WHO, statisticians/mathematicians and software engineers will integrate machine-learning, statistical genetics and molecular genetics methods to uncover all genomic variation causing at least 1% resistance to first- and second-line anti-TB drugs with >1% resistance prevalence. We will use largescale global and clade-representative WGS (>90,000), with initially >37,000 isolates with extended DST. Approaches include better WGS assembly to identify more variants with more precision, improved statistical methods to detect associations between variants and DST, and selected molecular validation of predicted resistant variants. The project will create an automatically-updating, free, publically accessible, comprehensive data repository of resistance-conferring variants. This will provide accurate genetic resistance prediction for all drugs for any new MTBC WGS; superior design of near-to-patient amplification-based molecular drug-resistance tests as alternatives to ‘WGS-only’ solutions; and better, faster and more targeted drug-resistant TB treatment, facilitating improved control and WHO’s initiative to eliminate TB by 2050.	British Heart Foundation	P&Cs	246410.0GBP
71	Professor Slade Mike	Nottinghamshire Healthcare NHS Foundation Trust	2017-04-01	2022-03-31	Personal experience as a recovery resource in psychosis: Narrative Experiences ONline (NEON) Programme	Our aim is to achieve sufficiently accurate genetic prediction of resistance to all anti-tuberculosis drugs for whole genome sequencing (WGS) to replace slow, cumbersome, culture-based drug susceptibility testing (DST) for Mycobacterium tuberculosis complex (MTBC). This would enable rapid-turnaround near-to-patient assays to revolutionize drug-resistant TB identification and management. This multidisciplinary collaboration including TB experts from five continents, WHO, statisticians/mathematicians and software engineers will integrate machine-learning, statistical genetics and molecular genetics methods to uncover all genomic variation causing at least 1% resistance to first- and second-line anti-TB drugs with >1% resistance prevalence. We will use largescale global and clade-representative WGS (>90,000), with initially >37,000 isolates with extended DST. Approaches include better WGS assembly to identify more variants with more precision, improved statistical methods to detect associations between variants and DST, and selected molecular validation of predicted resistant variants. The project will create an automatically-updating, free, publically accessible, comprehensive data repository of resistance-conferring variants. This will provide accurate genetic resistance prediction for all drugs for any new MTBC WGS; superior design of near-to-patient amplification-based molecular drug-resistance tests as alternatives to ‘WGS-only’ solutions; and better, faster and more targeted drug-resistant TB treatment, facilitating improved control and WHO’s initiative to eliminate TB by 2050.	National Institute for Health Research (Department of Health)	Full Award	2295609.0GBP
72	Professor Tomaszewski Maciej	Manchester, University of	2017-04-01	2022-03-31	The BHF-Turing Cardiovascular Data Science Awards (Second Call): Molecular causal networks of hypertension – a machine learning approach (joint funding with The Alan Turing Institute)	Our aim is to achieve sufficiently accurate genetic prediction of resistance to all anti-tuberculosis drugs for whole genome sequencing (WGS) to replace slow, cumbersome, culture-based drug susceptibility testing (DST) for Mycobacterium tuberculosis complex (MTBC). This would enable rapid-turnaround near-to-patient assays to revolutionize drug-resistant TB identification and management. This multidisciplinary collaboration including TB experts from five continents, WHO, statisticians/mathematicians and software engineers will integrate machine-learning, statistical genetics and molecular genetics methods to uncover all genomic variation causing at least 1% resistance to first- and second-line anti-TB drugs with >1% resistance prevalence. We will use largescale global and clade-representative WGS (>90,000), with initially >37,000 isolates with extended DST. Approaches include better WGS assembly to identify more variants with more precision, improved statistical methods to detect associations between variants and DST, and selected molecular validation of predicted resistant variants. The project will create an automatically-updating, free, publically accessible, comprehensive data repository of resistance-conferring variants. This will provide accurate genetic resistance prediction for all drugs for any new MTBC WGS; superior design of near-to-patient amplification-based molecular drug-resistance tests as alternatives to ‘WGS-only’ solutions; and better, faster and more targeted drug-resistant TB treatment, facilitating improved control and WHO’s initiative to eliminate TB by 2050.	British Heart Foundation	Full Award	24434.0GBP
73	Prof Rees Jonathan	University of Edinburgh	2008-09-15	2011-11-14	Dermofit: A cognitive prosthesis to aid focal skin lesion diagnosis.	We want to allow non-experts to diagnose skin lesions by taking advantage of the ability of humans to make visual matches even when they are not able to describe the lesions (using words) in a consistent way. In order to attach semantics to images we have to discover the basis of similarity between different lesions such that we can construct a database in which those lesion images that are similar are tagged as similar. If the search space can be structured in this way then it becomes possible for non-experts to search it efficiently, and match an index case with a tagged-reference case. We will acquire images, construct a user interface, and use iterative testing and user interaction coupled with machine vision and machine learning techniques, to order the database. Just as the pattern of hypertext links reflects a webpage s importance, so does the pattern of clicks from one image to another reveal what users consider as similar. The approach is therefore that of computer based i mage retrieval. Our goal is a device that assists non-experts achieve the correct diagnosis suitable for use wherever PCs are available.	Wellcome Trust	Project Grant	348877.0GBP
74	Professor Haydon Daniel	University of Glasgow	2007-03-12	2010-09-11	Predicting immunological cross-reactivity: from genotype to antigenic phenotype	An important goal of both epidemiological and viral evolutionary studies is to predict the antigenic similarity of different viral genotypes. The ability to easily determine antigenic similarity would greatly facilitate the empirical study of the evolution of antigenic novelty, informing us about when and how fast we can expect viruses to exhibit antigenic change. In this proposal I lay out a research program that aims to provide tools that will enable prediction of the antigenic similarity of different strains of FMDV from their capsid gene sequences alone. In stage 1 we will develop simplified 'in silico' models of immune reactions that simulate a polyclonal antibody response to different viral strains as represented by complete amino acid sequences of their capsid proteins. This immune model will exploit the substantial amount that is known about the structure and distribution of epitopes across the FMDV capsid, and will use as input existing capsid genotypes and additional strains predicted to derive from them. This immune model will enable the reactivity of the polyclonal response to one viral strain to be measured against another, thereby allowing pairwise antigenic similarity of different viral strains to be predicted. In stage 2 we will use these simulated data sets containing full-length capsid genes, and matrices containing estimates of their antigenic similarity to develop bioinformatic algorithms that will be able to predict the antigenic similarity of new pairs of capsid sequences for which antigenic data is lacking. We propose to try two different approaches: artificial neural networks, and kernel based machine learning methods. The performance of these algorithms can be assessed using simulated data, and pre-existing empirical data.	Biotechnology and Biological Sciences Research Council	Standard grant	265364.0GBP
75	Prof Denehy Linda	University of Melbourne	2019-09-01	2022-12-31	Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings	None	Wellcome Trust	Innovations Priority Project	4030521.01GBP
76	Prof Clifton David	University of Oxford	2019-09-01	2022-12-31	Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings	None	Wellcome Trust	Innovations Priority Project	4030521.01GBP
77	Prof Day Nicholas	University of Oxford	2019-09-01	2022-12-31	Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings	None	Wellcome Trust	Innovations Priority Project	4030521.01GBP
78	Prof Thwaites Guy	University of Oxford	2019-09-01	2022-12-31	Innovative biomedical engineering and computational science to improve the management of critical illness in resource-limited settings	None	Wellcome Trust	Innovations Priority Project	4030521.01GBP
79	Prof Leff Alexander	University College London	2016-12-01	2021-11-30	Digital neuro-interventions to enhance re-learning in patients with acquired and degenerative brain diseases	None	National Institute for Health Research (Department of Health)	Full Award	2074551.0GBP
80	Prof Leff Alexander	University of Liverpool	2016-12-01	2021-11-30	The application of support vector machine feature selection to cross sectional studies in epidemiology	This proposal builds on an EPSRC small grant GR/S73631/01, graded tending to outstanding" in its final report. It is a resubmission from March 2005 of application GR/EP/DO3O684 sent EPRSC Life Sciences Interface as a continuation of GR/S73631/01 and recommended for submission to BBSRC with a contribution in financial support from EPSRC if successful. Epidemiologists from Liverpool Veterinary School and computer scientist from DIMACS, a National Science Foundations Institute at Rutgers, New Jersey, USA, have recently reported the use of Support Vector Machine learning as a method of identifying risk factors for disease from observational epidemiological data. Support Vector Machine classification was developed in the mid 90's and although related to neural networks, the technique is simpler more robust and founded on statistical learning theory. In particular the use of SVM overcomes overfitting associated with the empirical risk minimisation, (ERM) which aims to minimise the error on the training data set but results in poor generalisation (i.e. performance on unseen datasets). SVM are arguably the single most important development in supervised classification in recent years. They are known to generalise well in high dimensional space even with small training sample conditions, when the data are noisy. SVM are not only good classifiers but are also good feature selection techniques. SVM has been used for the verification and recognition of faces, speech, handwriting and such diverse events as goal detection in football matches and financial forecasting. In life sciences it has been applied to gene expression, proteomics and disease diagnosis With the exception of a recent report, using single nucleotide polymorphisms (SNPs) to predict an increased risk of breast cancer there have been no published reports of the application of SVM in epidemiology. This project aims to further develop the application of SVM, to improve kernel selection and to produce a user-friendly SVM program for wider epidemiological use Data for the development of this program will be provided from an epidemiological study of an emerging disease of poultry (wet litter). Because meat birds live for only 6-7 weeks broiler flocks provide the opportunity to validate the classifications made by SVM during the period of study. The final program will provide a new paradigm in epidemiology and act as an easily applicable "second opinion" for statistical models generated using the "epidemiological standard" of logisitic regression. It will also, as a by-product, improve our understanding of wet litter in poultry. The final program will be applicable to observational studies of non-infectious and infectious human and animal disease."	Biotechnology and Biological Sciences Research Council	Full Award	400793.0GBP
81	Univ.Prof. Dr. SPEICHER Michael	Medical University of Graz	2019-10-01	2022-09-30	Breast cancer liquid biopsy stratification	Breast cancer is the most common cancer in Austrian women. Estimation of prognosis and treatment strategies is increasingly being dependent on stratification of tumors into different entities or classes. Currently, clinical routine stratification of tumors is mostly based on hormone receptor, HER2 status, and estimation of proliferation. However, a more robust and objective classification of tumors can be achieved by elucidation of further biological properties, which is also of increasing significance, as novel anticancer therapies are based on biological mechanisms. Consequently, available information from molecular analyses is increasingly being implemented in routine diagnostic assays with the aim to improve stratification for optimal treatment selection. To date the most extensive molecular-based taxonomy of breast cancer has been achieved by a classification based on combining gene expression and somatic copy number alterations (SCNAs), referred to as integrative clusters. Tissue biopsies are the current gold standard to attain such a classification. However, they can often be difficult to obtain in the metastatic setting and are subject to sampling bias due to intratumor heterogeneity. “Liquid biopsies” are, among other analytes, based on the analysis of cell-free DNA (cfDNA) which contains circulating tumor DNA (ctDNA), i.e. DNA fragments shed from normal and tumor cells into the blood, in patients with cancer. cfDNA can be obtained minimally invasive with a blood draw, allows for the “real time” analysis of tumor DNA from the circulation, and blood samples can be repeated at any time point, which is especially important for monitoring response to therapy. Our group has extensive expertise in the analysis of cfDNA and has developed a plethora of approaches for ctDNA analysis. Recently, we have developed a new approach, which relates to nucleosome positions and gene expression. cfDNA fragments have been associated with the release of DNA from apoptotic cells after enzymatic processing and hence consist mainly of mono-nucleosomal DNA. By performing whole-genome sequencing of cfDNA we could demonstrate that at transcriptional start sites, the nucleosome occupancy results in different read-depth coverage patterns in expressed and silent genes. By employing machine learning for gene classification, we were able to classify genes in cells releasing their DNA into the circulation as expressed. Our main hypothesis is that integrative breast cancer clusters can be established from directly blood without the need for an invasive tissue biopsy. Hence, our aims include refining stratification of patients for an improved selection of treatment strategies. Furthermore, we will obtain novel insights into the biology of metastatic breast cancer, so that this project will have important implications for patients, clinical oncologists, pathologists, pharmacologists, and all basic researchers interested in cancer.	Austrian Science Fund FWF	Full Award	387166.38EUR
82	Professor Gibbins Jonathan	Reading, University of	2019-10-01	2022-09-30	Understanding differences in platelet function and regulation in health and cardiometabolic disease: towards personalised and more effective anti-platelet treatment (renewal)	Breast cancer is the most common cancer in Austrian women. Estimation of prognosis and treatment strategies is increasingly being dependent on stratification of tumors into different entities or classes. Currently, clinical routine stratification of tumors is mostly based on hormone receptor, HER2 status, and estimation of proliferation. However, a more robust and objective classification of tumors can be achieved by elucidation of further biological properties, which is also of increasing significance, as novel anticancer therapies are based on biological mechanisms. Consequently, available information from molecular analyses is increasingly being implemented in routine diagnostic assays with the aim to improve stratification for optimal treatment selection. To date the most extensive molecular-based taxonomy of breast cancer has been achieved by a classification based on combining gene expression and somatic copy number alterations (SCNAs), referred to as integrative clusters. Tissue biopsies are the current gold standard to attain such a classification. However, they can often be difficult to obtain in the metastatic setting and are subject to sampling bias due to intratumor heterogeneity. “Liquid biopsies” are, among other analytes, based on the analysis of cell-free DNA (cfDNA) which contains circulating tumor DNA (ctDNA), i.e. DNA fragments shed from normal and tumor cells into the blood, in patients with cancer. cfDNA can be obtained minimally invasive with a blood draw, allows for the “real time” analysis of tumor DNA from the circulation, and blood samples can be repeated at any time point, which is especially important for monitoring response to therapy. Our group has extensive expertise in the analysis of cfDNA and has developed a plethora of approaches for ctDNA analysis. Recently, we have developed a new approach, which relates to nucleosome positions and gene expression. cfDNA fragments have been associated with the release of DNA from apoptotic cells after enzymatic processing and hence consist mainly of mono-nucleosomal DNA. By performing whole-genome sequencing of cfDNA we could demonstrate that at transcriptional start sites, the nucleosome occupancy results in different read-depth coverage patterns in expressed and silent genes. By employing machine learning for gene classification, we were able to classify genes in cells releasing their DNA into the circulation as expressed. Our main hypothesis is that integrative breast cancer clusters can be established from directly blood without the need for an invasive tissue biopsy. Hence, our aims include refining stratification of patients for an improved selection of treatment strategies. Furthermore, we will obtain novel insights into the biology of metastatic breast cancer, so that this project will have important implications for patients, clinical oncologists, pathologists, pharmacologists, and all basic researchers interested in cancer.	British Heart Foundation	Full Award	1399398.0GBP
83	Professor Slade Mike	Nottinghamshire Healthcare NHS Foundation Trust	2017-04-01	2022-03-31	Personal experience as a recovery resource in psychosis: Narrative Experiences ONline (NEON) Programme	Breast cancer is the most common cancer in Austrian women. Estimation of prognosis and treatment strategies is increasingly being dependent on stratification of tumors into different entities or classes. Currently, clinical routine stratification of tumors is mostly based on hormone receptor, HER2 status, and estimation of proliferation. However, a more robust and objective classification of tumors can be achieved by elucidation of further biological properties, which is also of increasing significance, as novel anticancer therapies are based on biological mechanisms. Consequently, available information from molecular analyses is increasingly being implemented in routine diagnostic assays with the aim to improve stratification for optimal treatment selection. To date the most extensive molecular-based taxonomy of breast cancer has been achieved by a classification based on combining gene expression and somatic copy number alterations (SCNAs), referred to as integrative clusters. Tissue biopsies are the current gold standard to attain such a classification. However, they can often be difficult to obtain in the metastatic setting and are subject to sampling bias due to intratumor heterogeneity. “Liquid biopsies” are, among other analytes, based on the analysis of cell-free DNA (cfDNA) which contains circulating tumor DNA (ctDNA), i.e. DNA fragments shed from normal and tumor cells into the blood, in patients with cancer. cfDNA can be obtained minimally invasive with a blood draw, allows for the “real time” analysis of tumor DNA from the circulation, and blood samples can be repeated at any time point, which is especially important for monitoring response to therapy. Our group has extensive expertise in the analysis of cfDNA and has developed a plethora of approaches for ctDNA analysis. Recently, we have developed a new approach, which relates to nucleosome positions and gene expression. cfDNA fragments have been associated with the release of DNA from apoptotic cells after enzymatic processing and hence consist mainly of mono-nucleosomal DNA. By performing whole-genome sequencing of cfDNA we could demonstrate that at transcriptional start sites, the nucleosome occupancy results in different read-depth coverage patterns in expressed and silent genes. By employing machine learning for gene classification, we were able to classify genes in cells releasing their DNA into the circulation as expressed. Our main hypothesis is that integrative breast cancer clusters can be established from directly blood without the need for an invasive tissue biopsy. Hence, our aims include refining stratification of patients for an improved selection of treatment strategies. Furthermore, we will obtain novel insights into the biology of metastatic breast cancer, so that this project will have important implications for patients, clinical oncologists, pathologists, pharmacologists, and all basic researchers interested in cancer.	National Institute for Health Research (Department of Health)	Full Award	2295609.0GBP
84	Professor Akerman Colin	University of Oxford	2014-06-01	2015-05-31	A biologically inspired algorithm for training deep neural networks	'In machine learning, deep neural networks are powerful computer-based models that use layers of computational units. Current commercial applications for these models include a wide array of software tasks such as image classification, identification of potential drugs, market predictions and speech recognition. Network models must be ‘trained’ using data, and their success hinges critically on the quality of the learning algorithm that is employed. We have recently discovered a novel, biologically inspired algorithm for training deep neural networks that is simpler to implement, more flexible and finds better solutions than existing techniques on bench-mark tests. Thus, our system has the potential to improve performance widely across the many fields that make use of machine learning in software tasks. Furthermore, the simplicity and flexibility of our method means that it could be more easily exploited in hardware devices such as mobile phones and cameras. The central aim of this proposal is to move our new algorithm to a stage where it is ready for commercialization. To do this we plan to accomplish two main areas of work. First, we will research the optimal way to employ our algorithm, establish its performance on a comprehensive set of industry-accepted bench-mark tasks, and compile our research into a manuscript for publication in a leading machine learning journal. Second, we will secure any arising intellectual property in line with the preliminary US patent application that we have already filed, assess application of the algorithm to the different commercial sectors identified through market research, and generate commercial interest in the technology through targeted marketing to relevant companies. This plan of work will confirm the innovation potential of our new algorithm and will establish the technical and commercial feasibility of our discovery.'	European Research Council	Proof-of-Concept Grant	146761.0EUR
85	Dr. MÜLLER-MANG Christina	Medical University of Vienna	2010-12-01	2014-11-30	Computerized 3D pulmonary architecture analysis	'In machine learning, deep neural networks are powerful computer-based models that use layers of computational units. Current commercial applications for these models include a wide array of software tasks such as image classification, identification of potential drugs, market predictions and speech recognition. Network models must be ‘trained’ using data, and their success hinges critically on the quality of the learning algorithm that is employed. We have recently discovered a novel, biologically inspired algorithm for training deep neural networks that is simpler to implement, more flexible and finds better solutions than existing techniques on bench-mark tests. Thus, our system has the potential to improve performance widely across the many fields that make use of machine learning in software tasks. Furthermore, the simplicity and flexibility of our method means that it could be more easily exploited in hardware devices such as mobile phones and cameras. The central aim of this proposal is to move our new algorithm to a stage where it is ready for commercialization. To do this we plan to accomplish two main areas of work. First, we will research the optimal way to employ our algorithm, establish its performance on a comprehensive set of industry-accepted bench-mark tasks, and compile our research into a manuscript for publication in a leading machine learning journal. Second, we will secure any arising intellectual property in line with the preliminary US patent application that we have already filed, assess application of the algorithm to the different commercial sectors identified through market research, and generate commercial interest in the technology through targeted marketing to relevant companies. This plan of work will confirm the innovation potential of our new algorithm and will establish the technical and commercial feasibility of our discovery.'	Austrian Science Fund FWF	Stand-Alone Project	146761.0EUR
86	Prof Murphy David	University of Bristol	2009-03-09	2012-05-08	Gene networks involved in hypothalamic plasticity in response to dehydration; assessing the in vivo functions of candidate nodal genes.	We have used array technology to comprehensively describe the pattern of gene expression in the hypothalamus, and how this changes following the physiological challenge of dehydration. We now wish to study the functions of key differentially expressed genes in vivo. We have employed a rational and unbiased approach to gene selection. We have utilised machine-learning algorithms to describe a gene network that, we hypothesise, might be involved in regulating and mediating hypothalamic plasticity. Of particular interest are those genes with many connections. Such genes may represent crucial functional hubs, or nodes. We will now test this hypothesis in vivo, focusing on 4 genes with 4 or more connections. We will now determine the functional and regulatory roles of these four key signalling nodes within a hypothetical gene network activated in the SON as a consequence of dehydration. To test this hypothesis we will: - validate the transcriptome data by determining the expression patterns of our candidate genes in the brain, hypothalamus and HNS at both the RNA and protein levels in terms of both specific brain cell-types and responses to dehydration - assess the functions of these genes in basal hypothalamic activity and stress-induced remodelling using in vivo gene manipulation techniques. Three systems will be exploited - knockout transgenic mice, transgenic rats and somatic gene delivery using viral vectors. Gene activity will be manipulated by over-expression of wild-type proteins, or inhibition using RNAi. This will be followed by expression analysis of putative interacting genes, and by robust, but wherever possible, non-invasive, quantification of water balance, vasopressin release, the electrical activity of hypothalamic neurons, and hypothalamic morphology.	Biotechnology and Biological Sciences Research Council	Standard grant	970501.0GBP
87	Prof Al-Chalabi Ammar	King's College London	2018-02-01	2021-01-31	JPND Biological Resource Analysis to Identify New MEchanisms and phenotypes in Neurodegenerative Diseases (BRAIN-MEND)	We have used array technology to comprehensively describe the pattern of gene expression in the hypothalamus, and how this changes following the physiological challenge of dehydration. We now wish to study the functions of key differentially expressed genes in vivo. We have employed a rational and unbiased approach to gene selection. We have utilised machine-learning algorithms to describe a gene network that, we hypothesise, might be involved in regulating and mediating hypothalamic plasticity. Of particular interest are those genes with many connections. Such genes may represent crucial functional hubs, or nodes. We will now test this hypothesis in vivo, focusing on 4 genes with 4 or more connections. We will now determine the functional and regulatory roles of these four key signalling nodes within a hypothetical gene network activated in the SON as a consequence of dehydration. To test this hypothesis we will: - validate the transcriptome data by determining the expression patterns of our candidate genes in the brain, hypothalamus and HNS at both the RNA and protein levels in terms of both specific brain cell-types and responses to dehydration - assess the functions of these genes in basal hypothalamic activity and stress-induced remodelling using in vivo gene manipulation techniques. Three systems will be exploited - knockout transgenic mice, transgenic rats and somatic gene delivery using viral vectors. Gene activity will be manipulated by over-expression of wild-type proteins, or inhibition using RNAi. This will be followed by expression analysis of putative interacting genes, and by robust, but wherever possible, non-invasive, quantification of water balance, vasopressin release, the electrical activity of hypothalamic neurons, and hypothalamic morphology.	Medical Research Council	Research Grant	549837.0GBP
88	Professor Lord Darzi Ara	Imperial College London	2017-04-01	2022-03-31	Cancer Research UK Imperial Centre	The Cancer Research UK Imperial Centre will enhance cancer prevention, expedite early cancer detection, and improve the precision of cancer treatments and outcomes for patients, by leveraging Imperial’s core strengths in engineering, technology, physical sciences, imaging and systems medicine. Our vision is to unify technologies and platforms and re-orientate them in a co-ordinated effort to tackle cancer treatment and prevention. We will focus on reducing the burden of cancer by: improving the identification of high-risk populations; capitalising on advances in metabolic phenotyping; developing novel screening tests; enhancing screening uptake; improving clinical decision-making through machine learning and digital health systems. Our efforts to improve precision of cancer care will lead to improvements in: tumour boundary identification; cancer resectability via medical robotics, augmented reality, and intraoperative tissue characterisation; identification of risk for relapse through lab-on-chip technologies enabling interrogation of single cells, cell free DNA, and microRNAs; prediction of stage and spread by exploiting changes in microbiome composition; treatment monitoring of response and resistance by imaging apoptosis and studying epigenetic reprogramming. The Cancer Research UK Imperial Centre and Imperial Experimental Cancer Research Centre will utilise a broad and unique set of strengths across engineering and the physical sciences, surgery, imaging and diagnostics - underpinned with high-quality clinical practice - to realise a transformative research programme aimed at improving cancer survival for patients and the public.	Cancer Research UK	Research Grant	549837.0GBP
89	Ms Wehrspaun Claudia	University of Oxford	2011-10-01	2015-09-30	Development of novel meta-machine learning algorithms for the integration of genomic and neuroimaging data for genetic pathway discovery: application to Parkinson's disease and schizophrenia.	No Data Entered	Wellcome Trust	WT/NIH Four Year PhD Studentship	70000.0GBP
90	Professor Haydon Daniel	University of Glasgow	2007-03-12	2010-09-11	Predicting immunological cross-reactivity: from genotype to antigenic phenotype	An important goal of both epidemiological and viral evolutionary studies is to predict the antigenic similarity of different viral genotypes. The ability to easily determine antigenic similarity would greatly facilitate the empirical study of the evolution of antigenic novelty, informing us about when and how fast we can expect viruses to exhibit antigenic change. In this proposal I lay out a research program that aims to provide tools that will enable prediction of the antigenic similarity of different strains of FMDV from their capsid gene sequences alone. In stage 1 we will develop simplified 'in silico' models of immune reactions that simulate a polyclonal antibody response to different viral strains as represented by complete amino acid sequences of their capsid proteins. This immune model will exploit the substantial amount that is known about the structure and distribution of epitopes across the FMDV capsid, and will use as input existing capsid genotypes and additional strains predicted to derive from them. This immune model will enable the reactivity of the polyclonal response to one viral strain to be measured against another, thereby allowing pairwise antigenic similarity of different viral strains to be predicted. In stage 2 we will use these simulated data sets containing full-length capsid genes, and matrices containing estimates of their antigenic similarity to develop bioinformatic algorithms that will be able to predict the antigenic similarity of new pairs of capsid sequences for which antigenic data is lacking. We propose to try two different approaches: artificial neural networks, and kernel based machine learning methods. The performance of these algorithms can be assessed using simulated data, and pre-existing empirical data.	Biotechnology and Biological Sciences Research Council	Standard grant	265364.0GBP
91	Dr Mourao-Miranda Janaina	University College London	2014-09-15	2020-03-14	Learning from neuroimaging and clinical data: a multiple-source machine learning approach for mental health disorders	This proposal aims to develop Multiple-Source Machine Learning models to investigate complex relationships between multivariate measures of brain anatomy or function (e.g. functional/structural Magnetic Resonance Imaging) and multidimensional descriptions of the mental health disorder and individual differences (e.g. clinical assessments, personality traits). Neuroimaging and machine learning techniques show potential as tools to identify biological measures that may help diagnosis and prognosi s of mental health disorders. However, so far, most of the studies using these techniques have focused on binary classification problems using a single imaging modality, i.e. they summarize the clinical assessment into a single measure and the output of the models is limited to a probability value and in most cases a binary decision (patients/healthy control). Although these studies represent an important advance in the field, they do not enable patient stratification and provide limited informa tion about the underlying biological mechanisms of the diseases. Considering the complexity of mental health disorders, it is potentially beneficial to embed a multidimensional description of the disorder into the models. The aim of this proposal is to move away from treating neuroimaging-based diagnosis as a binary classification problem towards models that: (i) are able to merge multi-modal neuroimaging and clinical information for diagnoses and prognoses of psychiatric disorders; (ii) can im prove stratification of patients with mental health disorders (e.g. identify subgroups of patients for helping treatment allocation or illness course prediction); (iii) are able to deal with large multi-center datasets; (iv) provide insights about underlying biological mechanisms of the diseases (e.g. biological markers).	Wellcome Trust	Senior Research Fellowship Basic	1295649.0GBP
92	Dr Kempton Matthew	King's College London	2012-10-01	2018-06-30	Trajectory of Brain Structure and Function before and after the Onset of Psychosis: a Longitudinal Multicentre Study	This proposal aims to develop Multiple-Source Machine Learning models to investigate complex relationships between multivariate measures of brain anatomy or function (e.g. functional/structural Magnetic Resonance Imaging) and multidimensional descriptions of the mental health disorder and individual differences (e.g. clinical assessments, personality traits). Neuroimaging and machine learning techniques show potential as tools to identify biological measures that may help diagnosis and prognosi s of mental health disorders. However, so far, most of the studies using these techniques have focused on binary classification problems using a single imaging modality, i.e. they summarize the clinical assessment into a single measure and the output of the models is limited to a probability value and in most cases a binary decision (patients/healthy control). Although these studies represent an important advance in the field, they do not enable patient stratification and provide limited informa tion about the underlying biological mechanisms of the diseases. Considering the complexity of mental health disorders, it is potentially beneficial to embed a multidimensional description of the disorder into the models. The aim of this proposal is to move away from treating neuroimaging-based diagnosis as a binary classification problem towards models that: (i) are able to merge multi-modal neuroimaging and clinical information for diagnoses and prognoses of psychiatric disorders; (ii) can im prove stratification of patients with mental health disorders (e.g. identify subgroups of patients for helping treatment allocation or illness course prediction); (iii) are able to deal with large multi-center datasets; (iv) provide insights about underlying biological mechanisms of the diseases (e.g. biological markers).	Medical Research Council	Fellowship	1227953.0GBP
93	Professor Custovic Adnan	Imperial College London	2015-09-15	2017-07-14	MICA: STELAR (Study Team for Early Life Asthma Research) consortium - Asthma e-lab and identification of novel endotypes of childhood asthma	This proposal aims to develop Multiple-Source Machine Learning models to investigate complex relationships between multivariate measures of brain anatomy or function (e.g. functional/structural Magnetic Resonance Imaging) and multidimensional descriptions of the mental health disorder and individual differences (e.g. clinical assessments, personality traits). Neuroimaging and machine learning techniques show potential as tools to identify biological measures that may help diagnosis and prognosi s of mental health disorders. However, so far, most of the studies using these techniques have focused on binary classification problems using a single imaging modality, i.e. they summarize the clinical assessment into a single measure and the output of the models is limited to a probability value and in most cases a binary decision (patients/healthy control). Although these studies represent an important advance in the field, they do not enable patient stratification and provide limited informa tion about the underlying biological mechanisms of the diseases. Considering the complexity of mental health disorders, it is potentially beneficial to embed a multidimensional description of the disorder into the models. The aim of this proposal is to move away from treating neuroimaging-based diagnosis as a binary classification problem towards models that: (i) are able to merge multi-modal neuroimaging and clinical information for diagnoses and prognoses of psychiatric disorders; (ii) can im prove stratification of patients with mental health disorders (e.g. identify subgroups of patients for helping treatment allocation or illness course prediction); (iii) are able to deal with large multi-center datasets; (iv) provide insights about underlying biological mechanisms of the diseases (e.g. biological markers).	Medical Research Council	Research Grant	568951.0GBP
94	Dr Patel Rashmi	King's College London	2013-01-14	2016-01-13	Predicting clinical and functional outcomes in psychosis using machine learning	This proposal aims to develop Multiple-Source Machine Learning models to investigate complex relationships between multivariate measures of brain anatomy or function (e.g. functional/structural Magnetic Resonance Imaging) and multidimensional descriptions of the mental health disorder and individual differences (e.g. clinical assessments, personality traits). Neuroimaging and machine learning techniques show potential as tools to identify biological measures that may help diagnosis and prognosi s of mental health disorders. However, so far, most of the studies using these techniques have focused on binary classification problems using a single imaging modality, i.e. they summarize the clinical assessment into a single measure and the output of the models is limited to a probability value and in most cases a binary decision (patients/healthy control). Although these studies represent an important advance in the field, they do not enable patient stratification and provide limited informa tion about the underlying biological mechanisms of the diseases. Considering the complexity of mental health disorders, it is potentially beneficial to embed a multidimensional description of the disorder into the models. The aim of this proposal is to move away from treating neuroimaging-based diagnosis as a binary classification problem towards models that: (i) are able to merge multi-modal neuroimaging and clinical information for diagnoses and prognoses of psychiatric disorders; (ii) can im prove stratification of patients with mental health disorders (e.g. identify subgroups of patients for helping treatment allocation or illness course prediction); (iii) are able to deal with large multi-center datasets; (iv) provide insights about underlying biological mechanisms of the diseases (e.g. biological markers).	Medical Research Council	Fellowship	174832.0GBP
95	Dr Patel Rashmi	University of Oxford	2010-05-01	2014-04-30	Dissecting the contribution of anterior prefrontal cortex to decision making with computational, statistical and neuroimaging approaches.	The first experiments would combine computational modelling from machine learning and mathematical economics with functional MRI to examine the neural computations underlying decision-making between multiple uncertain prospects in a changing environment. We would then directly compare competing computational models with subject behaviour and neural data. This would enable us to address several pivotal questions for the first time, such as how the values of different decision variables are repre sented in the vmPFC and FPC and whether there are dissociable neural routes guiding exploratory decision-making. To examine the causal contribution of FPC to decision-making, I propose to combine interference and recording methodologies. Specifically, I would interfere with FPC processing and record the effects from downstream brain regions whose signals are hypothesized to depend on FPC. This would reveal precisely when the FPC is essential for value-based decision-making. Recent ad vances in diffusion-weighted imaging (DWI) have made it feasible for the first time to examine the trajectories of anatomical pathways in the brain in vivo. I propose to combine the approaches outlined above with DWI to examine how inter-individual variability in functional interactions between the FPC and other brain regions during decision-making relates to the anatomical connectivity of the FPC.	Wellcome Trust	Sir Henry Wellcome Postdoctoral Fellowship	250000.0GBP
96	Professor Baralle Diana	University of Southampton	2017-12-01	2022-11-30	Translational genomics- maximising potential for NHS patient care.	The first experiments would combine computational modelling from machine learning and mathematical economics with functional MRI to examine the neural computations underlying decision-making between multiple uncertain prospects in a changing environment. We would then directly compare competing computational models with subject behaviour and neural data. This would enable us to address several pivotal questions for the first time, such as how the values of different decision variables are repre sented in the vmPFC and FPC and whether there are dissociable neural routes guiding exploratory decision-making. To examine the causal contribution of FPC to decision-making, I propose to combine interference and recording methodologies. Specifically, I would interfere with FPC processing and record the effects from downstream brain regions whose signals are hypothesized to depend on FPC. This would reveal precisely when the FPC is essential for value-based decision-making. Recent ad vances in diffusion-weighted imaging (DWI) have made it feasible for the first time to examine the trajectories of anatomical pathways in the brain in vivo. I propose to combine the approaches outlined above with DWI to examine how inter-individual variability in functional interactions between the FPC and other brain regions during decision-making relates to the anatomical connectivity of the FPC.	National Institute for Health Research (Department of Health)	Full award	1870764.0GBP
97	Dr Ballester Aristin Pedro	EMBL - European Bioinformatics Institute	2010-07-01	2014-06-30	New computational methods for protein function prediction using structural, binding and sequence data	The first experiments would combine computational modelling from machine learning and mathematical economics with functional MRI to examine the neural computations underlying decision-making between multiple uncertain prospects in a changing environment. We would then directly compare competing computational models with subject behaviour and neural data. This would enable us to address several pivotal questions for the first time, such as how the values of different decision variables are repre sented in the vmPFC and FPC and whether there are dissociable neural routes guiding exploratory decision-making. To examine the causal contribution of FPC to decision-making, I propose to combine interference and recording methodologies. Specifically, I would interfere with FPC processing and record the effects from downstream brain regions whose signals are hypothesized to depend on FPC. This would reveal precisely when the FPC is essential for value-based decision-making. Recent ad vances in diffusion-weighted imaging (DWI) have made it feasible for the first time to examine the trajectories of anatomical pathways in the brain in vivo. I propose to combine the approaches outlined above with DWI to examine how inter-individual variability in functional interactions between the FPC and other brain regions during decision-making relates to the anatomical connectivity of the FPC.	Medical Research Council	Fellowship	400905.0GBP
98	Prof. Dr. TRINKA Eugen	SALK	2011-11-01	2016-10-31	Physiological Markers for the Prognosis of Memory Decline	The first experiments would combine computational modelling from machine learning and mathematical economics with functional MRI to examine the neural computations underlying decision-making between multiple uncertain prospects in a changing environment. We would then directly compare competing computational models with subject behaviour and neural data. This would enable us to address several pivotal questions for the first time, such as how the values of different decision variables are repre sented in the vmPFC and FPC and whether there are dissociable neural routes guiding exploratory decision-making. To examine the causal contribution of FPC to decision-making, I propose to combine interference and recording methodologies. Specifically, I would interfere with FPC processing and record the effects from downstream brain regions whose signals are hypothesized to depend on FPC. This would reveal precisely when the FPC is essential for value-based decision-making. Recent ad vances in diffusion-weighted imaging (DWI) have made it feasible for the first time to examine the trajectories of anatomical pathways in the brain in vivo. I propose to combine the approaches outlined above with DWI to examine how inter-individual variability in functional interactions between the FPC and other brain regions during decision-making relates to the anatomical connectivity of the FPC.	Austrian Science Fund FWF	Clinical Research	312295.2EUR
99	Prof. Dr. TRINKA Eugen	Aberystwyth University	2011-11-01	2016-10-31	Multivariate data-mining for determining biological principles underlying sustainable livestock-based land management	The project will establish a coordinated machine-learning approach to integrating datasets across the range of IGERs research into sustainable land management. Appropriate hardware and software will be evaluated and commissioned. A training framework will be developed for providers of datasets to exploit enhanced bioinformatics resources. Software and data-mining approaches will be applied to selected separate but biologically interrelated datasets to identify emergent properties and higher-order factors within the complexity of the overall system. Machine-learning approaches for high-performance computing (local cluster, Grid) will be made available. IGERs informatics strategy will be coordinated with those of other organisations.	Biotechnology and Biological Sciences Research Council	Clinical Research	251853.0GBP
100	Dr Cusack Rhodri	MRC Cognition and Brain Sciences Unit	2008-04-01	2011-06-30	ACA4: Selective attention, short-term memory and the parietal lobe	None	Medical Research Council	Unit	None
101	Prof Murphy David	University of Bristol	2009-03-09	2012-05-08	Gene networks involved in hypothalamic plasticity in response to dehydration; assessing the in vivo functions of candidate nodal genes.	We have used array technology to comprehensively describe the pattern of gene expression in the hypothalamus, and how this changes following the physiological challenge of dehydration. We now wish to study the functions of key differentially expressed genes in vivo. We have employed a rational and unbiased approach to gene selection. We have utilised machine-learning algorithms to describe a gene network that, we hypothesise, might be involved in regulating and mediating hypothalamic plasticity. Of particular interest are those genes with many connections. Such genes may represent crucial functional hubs, or nodes. We will now test this hypothesis in vivo, focusing on 4 genes with 4 or more connections. We will now determine the functional and regulatory roles of these four key signalling nodes within a hypothetical gene network activated in the SON as a consequence of dehydration. To test this hypothesis we will: - validate the transcriptome data by determining the expression patterns of our candidate genes in the brain, hypothalamus and HNS at both the RNA and protein levels in terms of both specific brain cell-types and responses to dehydration - assess the functions of these genes in basal hypothalamic activity and stress-induced remodelling using in vivo gene manipulation techniques. Three systems will be exploited - knockout transgenic mice, transgenic rats and somatic gene delivery using viral vectors. Gene activity will be manipulated by over-expression of wild-type proteins, or inhibition using RNAi. This will be followed by expression analysis of putative interacting genes, and by robust, but wherever possible, non-invasive, quantification of water balance, vasopressin release, the electrical activity of hypothalamic neurons, and hypothalamic morphology.	Biotechnology and Biological Sciences Research Council	Standard grant	970501.0GBP
102	Univ.Prof. Dipl.Ing. Dr. TRAJANOSKI Zlatko	MEDIZINISCHE UNIVERSITAT INNSBRUCK	2018-10-01	2023-09-30	Enabling Precision Immuno-oncology in Colorectal cancer	Immunotherapy with checkpoints blockers is transforming the treatment of advanced cancers. Colorectal cancer (CRC), a cancer with 1.4 million new cases diagnosed annually worldwide, is refractory to immunotherapy (with the exception of a minority of tumors with microsatellite instability). This is somehow paradoxical as CRC is a cancer for which we have shown that it is under immunological control and that tumor infiltrating lymphocytes represent a strong independent predictor of survival. Thus, there is an urgent need to broaden the clinical benefits of immune checkpoint blockers to CRC by combining agents with synergistic mechanisms of action. An attractive approach to sensitize tumors to immunotherapy is to harness immunogenic effects induced by approved conventional or targeted agents. Here I propose a new paradigm to identify molecular determinants of resistance to immunotherapy and develop personalized in silico and in vitro models for predicting response to combination therapy in CRC. The EPIC concept is based on three pillars: 1) emphasis on antitumor T cell activity; 2) systematic interrogation of tumor-immune cell interactions using data-driven modeling and knowledge-based mechanistic modeling, and 3) generation of key quantitative data to train and validate algorithms using perturbation experiments with patient-derived tumor organoids and cutting-edge technologies for multidimensional profiling. We will investigate three immunomodulatory processes: 1) immunostimulatory effects of chemotherapeutics, 2) rewiring of signaling networks induced by targeted drugs and their interference with immunity, and 3) metabolic reprogramming of T cells to enhance antitumor immunity. The anticipated outcome of EPIC is a precision immuno-oncology platform that integrates tumor organoids with high-throughput and high-content data for testing drug combinations, and machine learning for making therapeutic recommendations for individual patients.	European Research Council	Advanced Grant	2460500.0EUR
103	Dr. GOEBL Werner	University of Music and Performing Arts Vienna	2012-11-01	2016-09-30	Performing Together:Synchronisation and Communication in Music Ensembles	Immunotherapy with checkpoints blockers is transforming the treatment of advanced cancers. Colorectal cancer (CRC), a cancer with 1.4 million new cases diagnosed annually worldwide, is refractory to immunotherapy (with the exception of a minority of tumors with microsatellite instability). This is somehow paradoxical as CRC is a cancer for which we have shown that it is under immunological control and that tumor infiltrating lymphocytes represent a strong independent predictor of survival. Thus, there is an urgent need to broaden the clinical benefits of immune checkpoint blockers to CRC by combining agents with synergistic mechanisms of action. An attractive approach to sensitize tumors to immunotherapy is to harness immunogenic effects induced by approved conventional or targeted agents. Here I propose a new paradigm to identify molecular determinants of resistance to immunotherapy and develop personalized in silico and in vitro models for predicting response to combination therapy in CRC. The EPIC concept is based on three pillars: 1) emphasis on antitumor T cell activity; 2) systematic interrogation of tumor-immune cell interactions using data-driven modeling and knowledge-based mechanistic modeling, and 3) generation of key quantitative data to train and validate algorithms using perturbation experiments with patient-derived tumor organoids and cutting-edge technologies for multidimensional profiling. We will investigate three immunomodulatory processes: 1) immunostimulatory effects of chemotherapeutics, 2) rewiring of signaling networks induced by targeted drugs and their interference with immunity, and 3) metabolic reprogramming of T cells to enhance antitumor immunity. The anticipated outcome of EPIC is a precision immuno-oncology platform that integrates tumor organoids with high-throughput and high-content data for testing drug combinations, and machine learning for making therapeutic recommendations for individual patients.	Austrian Science Fund FWF	Stand-Alone Project	351237.09EUR
104	Univ.Prof. Dr. SCHMIDT Reinhold	Medical University of Graz	2012-03-01	2016-02-29	Mechanisms of Small Vessel Related Brain Damage and Cognitive Impairment	Immunotherapy with checkpoints blockers is transforming the treatment of advanced cancers. Colorectal cancer (CRC), a cancer with 1.4 million new cases diagnosed annually worldwide, is refractory to immunotherapy (with the exception of a minority of tumors with microsatellite instability). This is somehow paradoxical as CRC is a cancer for which we have shown that it is under immunological control and that tumor infiltrating lymphocytes represent a strong independent predictor of survival. Thus, there is an urgent need to broaden the clinical benefits of immune checkpoint blockers to CRC by combining agents with synergistic mechanisms of action. An attractive approach to sensitize tumors to immunotherapy is to harness immunogenic effects induced by approved conventional or targeted agents. Here I propose a new paradigm to identify molecular determinants of resistance to immunotherapy and develop personalized in silico and in vitro models for predicting response to combination therapy in CRC. The EPIC concept is based on three pillars: 1) emphasis on antitumor T cell activity; 2) systematic interrogation of tumor-immune cell interactions using data-driven modeling and knowledge-based mechanistic modeling, and 3) generation of key quantitative data to train and validate algorithms using perturbation experiments with patient-derived tumor organoids and cutting-edge technologies for multidimensional profiling. We will investigate three immunomodulatory processes: 1) immunostimulatory effects of chemotherapeutics, 2) rewiring of signaling networks induced by targeted drugs and their interference with immunity, and 3) metabolic reprogramming of T cells to enhance antitumor immunity. The anticipated outcome of EPIC is a precision immuno-oncology platform that integrates tumor organoids with high-throughput and high-content data for testing drug combinations, and machine learning for making therapeutic recommendations for individual patients.	Austrian Science Fund FWF	International programmes	191457.0EUR
105	Ms Wehrspaun Claudia	University of Oxford	2011-10-01	2015-09-30	Development of novel meta-machine learning algorithms for the integration of genomic and neuroimaging data for genetic pathway discovery: application to Parkinson's disease and schizophrenia.	No Data Entered	Wellcome Trust	WT/NIH Four Year PhD Studentship	70000.0GBP
106	Dr Nachev Parashkev	University College London	2019-06-01	2023-10-01	Programme for High Dimensional Translation in Neurology	No Data Entered	Wellcome Trust	Innovations Priority Project	4557675.0GBP
107	Prof Ourselin Sebastien	King's College London	2019-06-01	2023-10-01	Programme for High Dimensional Translation in Neurology	No Data Entered	Wellcome Trust	Innovations Priority Project	4557675.0GBP
108	Dr Cardoso M Jorge	King's College London	2019-06-01	2023-10-01	Programme for High Dimensional Translation in Neurology	No Data Entered	Wellcome Trust	Innovations Priority Project	4557675.0GBP
109	Prof Rees Geraint	University College London	2019-06-01	2023-10-01	Programme for High Dimensional Translation in Neurology	No Data Entered	Wellcome Trust	Innovations Priority Project	4557675.0GBP
110	Dr Colwell Lucy	University of Cambridge	2019-10-01	2021-10-01	Next Generation Drug Discovery Enabled by Digital Molecular Technologies: Machine learning enabled high throughput synthesis to repurpose drugs and failed clinical candidates	No Data Entered	Wellcome Trust	Innovator Award: Digital Technologies	575926.0GBP
111	Dr Gaunt Matthew	University of Cambridge	2019-10-01	2021-10-01	Next Generation Drug Discovery Enabled by Digital Molecular Technologies: Machine learning enabled high throughput synthesis to repurpose drugs and failed clinical candidates	No Data Entered	Wellcome Trust	Innovator Award: Digital Technologies	575926.0GBP
112	Dr Morales Daniel	University of Dundee	2015-04-01	2016-03-31	Application Of Support Vector Machine Learning To Predict The Risk Of Death From Chronic Obstructive Pulmonary Disease Using Electronic Primary Care Medical Records. (Health Informatics Grant)	To use a support vector machine learning approach to predict death from COPD using electronic primary care medical records and to compare its performance with standard regression. 4. Research questions Question 1: Can routinely collected data in electronic primary care medical records be used to predict death from COPD? Question 2: Does a support vector machine learning approach perform better than standard logistic regression?	Chief Scientist Office	Innovator Award: Digital Technologies	29525.0GBP
113	Mr Barnes Chris	University College London	2013-05-01	2018-04-30	A statistical approach to the understanding of mutational processes in the human genome and their impact on evolution, health and disease.	Germline genomic instability is the hypothesis that the architecture of the genome can increase the local mutation rate mostly through homologous regions promoting structural variation (SV) through errors in double strand break (DSB) repair and replication processes. Genomic instability has important consequences for genomic disorders, neuro-degenerative disease and primate evolution. There are three main aims of this research. 1) Develop stochastic models for the different mechanisms of SV for mation and use Approximate Bayesian Computation (ABC) methods to estimate parameters such as relative contributions and length-dependent mutation rates from published datasets. 2) Test the hypothesis that local genome architecture contributes to genomic instability by using a combination of the developed ABC framework and machine-learning methods to relate variant position and mechanism to genomic features. Use this to predict interesting regions of the genome for further study. 3) Examine the effect of genome instability in complex disease (metabolic, cardiovascular and autoimmune), neurodegenerative disease, population differentiation and evolution through collaborative efforts with geneticists and clinicians using both human and great ape sequencing.	Wellcome Trust	Research Career Development Fellowship	774472.0GBP
114	Dr Kudla Grzegorz	University of Edinburgh	2018-04-01	2023-03-31	RNA synthetic biology	Germline genomic instability is the hypothesis that the architecture of the genome can increase the local mutation rate mostly through homologous regions promoting structural variation (SV) through errors in double strand break (DSB) repair and replication processes. Genomic instability has important consequences for genomic disorders, neuro-degenerative disease and primate evolution. There are three main aims of this research. 1) Develop stochastic models for the different mechanisms of SV for mation and use Approximate Bayesian Computation (ABC) methods to estimate parameters such as relative contributions and length-dependent mutation rates from published datasets. 2) Test the hypothesis that local genome architecture contributes to genomic instability by using a combination of the developed ABC framework and machine-learning methods to relate variant position and mechanism to genomic features. Use this to predict interesting regions of the genome for further study. 3) Examine the effect of genome instability in complex disease (metabolic, cardiovascular and autoimmune), neurodegenerative disease, population differentiation and evolution through collaborative efforts with geneticists and clinicians using both human and great ape sequencing.	Medical Research Council	Unit	1205000.0GBP
115	Professor Emberton Mark	University College London	2018-06-01	2022-05-31	MICA: The exploitation of a novel image-based risk stratification tool in early prostate cancer - the Re-IMAGINE Consortium	Germline genomic instability is the hypothesis that the architecture of the genome can increase the local mutation rate mostly through homologous regions promoting structural variation (SV) through errors in double strand break (DSB) repair and replication processes. Genomic instability has important consequences for genomic disorders, neuro-degenerative disease and primate evolution. There are three main aims of this research. 1) Develop stochastic models for the different mechanisms of SV for mation and use Approximate Bayesian Computation (ABC) methods to estimate parameters such as relative contributions and length-dependent mutation rates from published datasets. 2) Test the hypothesis that local genome architecture contributes to genomic instability by using a combination of the developed ABC framework and machine-learning methods to relate variant position and mechanism to genomic features. Use this to predict interesting regions of the genome for further study. 3) Examine the effect of genome instability in complex disease (metabolic, cardiovascular and autoimmune), neurodegenerative disease, population differentiation and evolution through collaborative efforts with geneticists and clinicians using both human and great ape sequencing.	Medical Research Council	Research Grant	5179122.0GBP
116	Dr. MECHTLER Karl	IMP - Research Institute of Molecular Pathology	2013-03-01	2016-02-29	Self-Learning Search Algorithms for High-Res Mass Spectra	To identify proteins in biological samples mass spectrometry (MS) is most often applied: proteins are digested to peptides which are subsequently analyzed. Within the last decade, a new generation of mass spectrometers has been developed that are capable of acquiring mass spectra with high resolution and high mass accuracy. This has significantly changed the characteristics of mass spectra; however, this development has not been accompanied by a corresponding progress in peptide identification algorithms capable of fully exploiting the available information. We therefore propose to develop a set of novel identification algorithms that are specifically designed for the analysis of modern mass spectra and incorporate multiple sources of information in the here proposed bioinformatics research project. Preliminary research results are promising: The project consortium consisting of the Proteomics Group at IMP Vienna and the Bioinformatics Research Group at FH OÖ (Campus Hagenberg) has already conducted successful joint research in the analysis of MS data: Identification rates comparable or even superior to Mascot, the current gold-standard, have been achieved using a first version of a scoring function designed by the proposing consortium. Encouraged by these preliminary research results, we are convinced that considering additional sources of information will further improve identification rates of mass spectra – therefore this project is dedicated to research on a combination of the following novel approaches: We plan to use machine learning techniques to analyze peptide elution times, fragmentation patterns and mass accuracy characteristics specific to the instrument; in addition, observed m/z values will be recalibrated based on the mass error of highly reliable identifications, and the remaining mass error with regard to the learned distribution will be incorporated into the scoring function. Sophisticated peak picking strategies will also be designed using machine learning. These improvements will help increase identification rates in challenging situations such as hybrid spectra and exhaustive searches for a wide range of post-translational modifications. The latter approach leads to exponentially growing search spaces and an accompanying drop in spectra identification rates because the information in MS spectra on its own is not sufficient to cope with the increased search space. Instead of applying brute force methods we plan to solve this problem using construction heuristics, i.e., evolutionary algorithms that realize intelligent search strategies for large numbers of unknown post-translational modifications based on a combination of database search and de novo identification. All research results achieved in this project will be published and made freely available to the bioinformatics and proteomics communities. Improving identification rates of peptides in general and of unknown modifications in particular will permit a deeper insight into the proteome; computer science shall thus form a new basis for finding answers to important medical and biological questions.	Austrian Science Fund FWF	Translational Research	419816.26EUR
117	Dr Kempton Matthew	King's College London	2012-10-01	2018-06-30	Trajectory of Brain Structure and Function before and after the Onset of Psychosis: a Longitudinal Multicentre Study	To identify proteins in biological samples mass spectrometry (MS) is most often applied: proteins are digested to peptides which are subsequently analyzed. Within the last decade, a new generation of mass spectrometers has been developed that are capable of acquiring mass spectra with high resolution and high mass accuracy. This has significantly changed the characteristics of mass spectra; however, this development has not been accompanied by a corresponding progress in peptide identification algorithms capable of fully exploiting the available information. We therefore propose to develop a set of novel identification algorithms that are specifically designed for the analysis of modern mass spectra and incorporate multiple sources of information in the here proposed bioinformatics research project. Preliminary research results are promising: The project consortium consisting of the Proteomics Group at IMP Vienna and the Bioinformatics Research Group at FH OÖ (Campus Hagenberg) has already conducted successful joint research in the analysis of MS data: Identification rates comparable or even superior to Mascot, the current gold-standard, have been achieved using a first version of a scoring function designed by the proposing consortium. Encouraged by these preliminary research results, we are convinced that considering additional sources of information will further improve identification rates of mass spectra – therefore this project is dedicated to research on a combination of the following novel approaches: We plan to use machine learning techniques to analyze peptide elution times, fragmentation patterns and mass accuracy characteristics specific to the instrument; in addition, observed m/z values will be recalibrated based on the mass error of highly reliable identifications, and the remaining mass error with regard to the learned distribution will be incorporated into the scoring function. Sophisticated peak picking strategies will also be designed using machine learning. These improvements will help increase identification rates in challenging situations such as hybrid spectra and exhaustive searches for a wide range of post-translational modifications. The latter approach leads to exponentially growing search spaces and an accompanying drop in spectra identification rates because the information in MS spectra on its own is not sufficient to cope with the increased search space. Instead of applying brute force methods we plan to solve this problem using construction heuristics, i.e., evolutionary algorithms that realize intelligent search strategies for large numbers of unknown post-translational modifications based on a combination of database search and de novo identification. All research results achieved in this project will be published and made freely available to the bioinformatics and proteomics communities. Improving identification rates of peptides in general and of unknown modifications in particular will permit a deeper insight into the proteome; computer science shall thus form a new basis for finding answers to important medical and biological questions.	Medical Research Council	Fellowship	1227953.0GBP
118	Professor Custovic Adnan	Imperial College London	2015-09-15	2017-07-14	MICA: STELAR (Study Team for Early Life Asthma Research) consortium - Asthma e-lab and identification of novel endotypes of childhood asthma	To identify proteins in biological samples mass spectrometry (MS) is most often applied: proteins are digested to peptides which are subsequently analyzed. Within the last decade, a new generation of mass spectrometers has been developed that are capable of acquiring mass spectra with high resolution and high mass accuracy. This has significantly changed the characteristics of mass spectra; however, this development has not been accompanied by a corresponding progress in peptide identification algorithms capable of fully exploiting the available information. We therefore propose to develop a set of novel identification algorithms that are specifically designed for the analysis of modern mass spectra and incorporate multiple sources of information in the here proposed bioinformatics research project. Preliminary research results are promising: The project consortium consisting of the Proteomics Group at IMP Vienna and the Bioinformatics Research Group at FH OÖ (Campus Hagenberg) has already conducted successful joint research in the analysis of MS data: Identification rates comparable or even superior to Mascot, the current gold-standard, have been achieved using a first version of a scoring function designed by the proposing consortium. Encouraged by these preliminary research results, we are convinced that considering additional sources of information will further improve identification rates of mass spectra – therefore this project is dedicated to research on a combination of the following novel approaches: We plan to use machine learning techniques to analyze peptide elution times, fragmentation patterns and mass accuracy characteristics specific to the instrument; in addition, observed m/z values will be recalibrated based on the mass error of highly reliable identifications, and the remaining mass error with regard to the learned distribution will be incorporated into the scoring function. Sophisticated peak picking strategies will also be designed using machine learning. These improvements will help increase identification rates in challenging situations such as hybrid spectra and exhaustive searches for a wide range of post-translational modifications. The latter approach leads to exponentially growing search spaces and an accompanying drop in spectra identification rates because the information in MS spectra on its own is not sufficient to cope with the increased search space. Instead of applying brute force methods we plan to solve this problem using construction heuristics, i.e., evolutionary algorithms that realize intelligent search strategies for large numbers of unknown post-translational modifications based on a combination of database search and de novo identification. All research results achieved in this project will be published and made freely available to the bioinformatics and proteomics communities. Improving identification rates of peptides in general and of unknown modifications in particular will permit a deeper insight into the proteome; computer science shall thus form a new basis for finding answers to important medical and biological questions.	Medical Research Council	Research Grant	568951.0GBP
119	Dr Patel Rashmi	King's College London	2013-01-14	2016-01-13	Predicting clinical and functional outcomes in psychosis using machine learning	To identify proteins in biological samples mass spectrometry (MS) is most often applied: proteins are digested to peptides which are subsequently analyzed. Within the last decade, a new generation of mass spectrometers has been developed that are capable of acquiring mass spectra with high resolution and high mass accuracy. This has significantly changed the characteristics of mass spectra; however, this development has not been accompanied by a corresponding progress in peptide identification algorithms capable of fully exploiting the available information. We therefore propose to develop a set of novel identification algorithms that are specifically designed for the analysis of modern mass spectra and incorporate multiple sources of information in the here proposed bioinformatics research project. Preliminary research results are promising: The project consortium consisting of the Proteomics Group at IMP Vienna and the Bioinformatics Research Group at FH OÖ (Campus Hagenberg) has already conducted successful joint research in the analysis of MS data: Identification rates comparable or even superior to Mascot, the current gold-standard, have been achieved using a first version of a scoring function designed by the proposing consortium. Encouraged by these preliminary research results, we are convinced that considering additional sources of information will further improve identification rates of mass spectra – therefore this project is dedicated to research on a combination of the following novel approaches: We plan to use machine learning techniques to analyze peptide elution times, fragmentation patterns and mass accuracy characteristics specific to the instrument; in addition, observed m/z values will be recalibrated based on the mass error of highly reliable identifications, and the remaining mass error with regard to the learned distribution will be incorporated into the scoring function. Sophisticated peak picking strategies will also be designed using machine learning. These improvements will help increase identification rates in challenging situations such as hybrid spectra and exhaustive searches for a wide range of post-translational modifications. The latter approach leads to exponentially growing search spaces and an accompanying drop in spectra identification rates because the information in MS spectra on its own is not sufficient to cope with the increased search space. Instead of applying brute force methods we plan to solve this problem using construction heuristics, i.e., evolutionary algorithms that realize intelligent search strategies for large numbers of unknown post-translational modifications based on a combination of database search and de novo identification. All research results achieved in this project will be published and made freely available to the bioinformatics and proteomics communities. Improving identification rates of peptides in general and of unknown modifications in particular will permit a deeper insight into the proteome; computer science shall thus form a new basis for finding answers to important medical and biological questions.	Medical Research Council	Fellowship	174832.0GBP
120	Dr Ball Gareth	King's College London	2012-07-01	2015-06-30	Machine learning to understand brain development	To identify proteins in biological samples mass spectrometry (MS) is most often applied: proteins are digested to peptides which are subsequently analyzed. Within the last decade, a new generation of mass spectrometers has been developed that are capable of acquiring mass spectra with high resolution and high mass accuracy. This has significantly changed the characteristics of mass spectra; however, this development has not been accompanied by a corresponding progress in peptide identification algorithms capable of fully exploiting the available information. We therefore propose to develop a set of novel identification algorithms that are specifically designed for the analysis of modern mass spectra and incorporate multiple sources of information in the here proposed bioinformatics research project. Preliminary research results are promising: The project consortium consisting of the Proteomics Group at IMP Vienna and the Bioinformatics Research Group at FH OÖ (Campus Hagenberg) has already conducted successful joint research in the analysis of MS data: Identification rates comparable or even superior to Mascot, the current gold-standard, have been achieved using a first version of a scoring function designed by the proposing consortium. Encouraged by these preliminary research results, we are convinced that considering additional sources of information will further improve identification rates of mass spectra – therefore this project is dedicated to research on a combination of the following novel approaches: We plan to use machine learning techniques to analyze peptide elution times, fragmentation patterns and mass accuracy characteristics specific to the instrument; in addition, observed m/z values will be recalibrated based on the mass error of highly reliable identifications, and the remaining mass error with regard to the learned distribution will be incorporated into the scoring function. Sophisticated peak picking strategies will also be designed using machine learning. These improvements will help increase identification rates in challenging situations such as hybrid spectra and exhaustive searches for a wide range of post-translational modifications. The latter approach leads to exponentially growing search spaces and an accompanying drop in spectra identification rates because the information in MS spectra on its own is not sufficient to cope with the increased search space. Instead of applying brute force methods we plan to solve this problem using construction heuristics, i.e., evolutionary algorithms that realize intelligent search strategies for large numbers of unknown post-translational modifications based on a combination of database search and de novo identification. All research results achieved in this project will be published and made freely available to the bioinformatics and proteomics communities. Improving identification rates of peptides in general and of unknown modifications in particular will permit a deeper insight into the proteome; computer science shall thus form a new basis for finding answers to important medical and biological questions.	Medical Research Council	Fellowship	299148.0GBP
121	Professor Custovic Adnan	The University of Manchester	2013-01-25	2015-09-13	MICA: STELAR (Study Team for Early Life Asthma Research) consortium - Asthma e-lab and identification of novel endotypes of childhood asthma	To identify proteins in biological samples mass spectrometry (MS) is most often applied: proteins are digested to peptides which are subsequently analyzed. Within the last decade, a new generation of mass spectrometers has been developed that are capable of acquiring mass spectra with high resolution and high mass accuracy. This has significantly changed the characteristics of mass spectra; however, this development has not been accompanied by a corresponding progress in peptide identification algorithms capable of fully exploiting the available information. We therefore propose to develop a set of novel identification algorithms that are specifically designed for the analysis of modern mass spectra and incorporate multiple sources of information in the here proposed bioinformatics research project. Preliminary research results are promising: The project consortium consisting of the Proteomics Group at IMP Vienna and the Bioinformatics Research Group at FH OÖ (Campus Hagenberg) has already conducted successful joint research in the analysis of MS data: Identification rates comparable or even superior to Mascot, the current gold-standard, have been achieved using a first version of a scoring function designed by the proposing consortium. Encouraged by these preliminary research results, we are convinced that considering additional sources of information will further improve identification rates of mass spectra – therefore this project is dedicated to research on a combination of the following novel approaches: We plan to use machine learning techniques to analyze peptide elution times, fragmentation patterns and mass accuracy characteristics specific to the instrument; in addition, observed m/z values will be recalibrated based on the mass error of highly reliable identifications, and the remaining mass error with regard to the learned distribution will be incorporated into the scoring function. Sophisticated peak picking strategies will also be designed using machine learning. These improvements will help increase identification rates in challenging situations such as hybrid spectra and exhaustive searches for a wide range of post-translational modifications. The latter approach leads to exponentially growing search spaces and an accompanying drop in spectra identification rates because the information in MS spectra on its own is not sufficient to cope with the increased search space. Instead of applying brute force methods we plan to solve this problem using construction heuristics, i.e., evolutionary algorithms that realize intelligent search strategies for large numbers of unknown post-translational modifications based on a combination of database search and de novo identification. All research results achieved in this project will be published and made freely available to the bioinformatics and proteomics communities. Improving identification rates of peptides in general and of unknown modifications in particular will permit a deeper insight into the proteome; computer science shall thus form a new basis for finding answers to important medical and biological questions.	Medical Research Council	Research Grant	1498108.0GBP
122	Professor Custovic Adnan	Imperial College London	2020-08-01	2023-07-31	Endotypes of childhood wheezing after severe RSV lower respiratory tract illness in infancy in socially vulnerable Argentinian children	To identify proteins in biological samples mass spectrometry (MS) is most often applied: proteins are digested to peptides which are subsequently analyzed. Within the last decade, a new generation of mass spectrometers has been developed that are capable of acquiring mass spectra with high resolution and high mass accuracy. This has significantly changed the characteristics of mass spectra; however, this development has not been accompanied by a corresponding progress in peptide identification algorithms capable of fully exploiting the available information. We therefore propose to develop a set of novel identification algorithms that are specifically designed for the analysis of modern mass spectra and incorporate multiple sources of information in the here proposed bioinformatics research project. Preliminary research results are promising: The project consortium consisting of the Proteomics Group at IMP Vienna and the Bioinformatics Research Group at FH OÖ (Campus Hagenberg) has already conducted successful joint research in the analysis of MS data: Identification rates comparable or even superior to Mascot, the current gold-standard, have been achieved using a first version of a scoring function designed by the proposing consortium. Encouraged by these preliminary research results, we are convinced that considering additional sources of information will further improve identification rates of mass spectra – therefore this project is dedicated to research on a combination of the following novel approaches: We plan to use machine learning techniques to analyze peptide elution times, fragmentation patterns and mass accuracy characteristics specific to the instrument; in addition, observed m/z values will be recalibrated based on the mass error of highly reliable identifications, and the remaining mass error with regard to the learned distribution will be incorporated into the scoring function. Sophisticated peak picking strategies will also be designed using machine learning. These improvements will help increase identification rates in challenging situations such as hybrid spectra and exhaustive searches for a wide range of post-translational modifications. The latter approach leads to exponentially growing search spaces and an accompanying drop in spectra identification rates because the information in MS spectra on its own is not sufficient to cope with the increased search space. Instead of applying brute force methods we plan to solve this problem using construction heuristics, i.e., evolutionary algorithms that realize intelligent search strategies for large numbers of unknown post-translational modifications based on a combination of database search and de novo identification. All research results achieved in this project will be published and made freely available to the bioinformatics and proteomics communities. Improving identification rates of peptides in general and of unknown modifications in particular will permit a deeper insight into the proteome; computer science shall thus form a new basis for finding answers to important medical and biological questions.	Medical Research Council	Research Grant	913705.0GBP
123	Professor Mills Nicholas	Edinburgh, University of	2020-08-01	2023-07-31	High-sensitivity cardiac troponin beyond the acute coronary syndrome	To identify proteins in biological samples mass spectrometry (MS) is most often applied: proteins are digested to peptides which are subsequently analyzed. Within the last decade, a new generation of mass spectrometers has been developed that are capable of acquiring mass spectra with high resolution and high mass accuracy. This has significantly changed the characteristics of mass spectra; however, this development has not been accompanied by a corresponding progress in peptide identification algorithms capable of fully exploiting the available information. We therefore propose to develop a set of novel identification algorithms that are specifically designed for the analysis of modern mass spectra and incorporate multiple sources of information in the here proposed bioinformatics research project. Preliminary research results are promising: The project consortium consisting of the Proteomics Group at IMP Vienna and the Bioinformatics Research Group at FH OÖ (Campus Hagenberg) has already conducted successful joint research in the analysis of MS data: Identification rates comparable or even superior to Mascot, the current gold-standard, have been achieved using a first version of a scoring function designed by the proposing consortium. Encouraged by these preliminary research results, we are convinced that considering additional sources of information will further improve identification rates of mass spectra – therefore this project is dedicated to research on a combination of the following novel approaches: We plan to use machine learning techniques to analyze peptide elution times, fragmentation patterns and mass accuracy characteristics specific to the instrument; in addition, observed m/z values will be recalibrated based on the mass error of highly reliable identifications, and the remaining mass error with regard to the learned distribution will be incorporated into the scoring function. Sophisticated peak picking strategies will also be designed using machine learning. These improvements will help increase identification rates in challenging situations such as hybrid spectra and exhaustive searches for a wide range of post-translational modifications. The latter approach leads to exponentially growing search spaces and an accompanying drop in spectra identification rates because the information in MS spectra on its own is not sufficient to cope with the increased search space. Instead of applying brute force methods we plan to solve this problem using construction heuristics, i.e., evolutionary algorithms that realize intelligent search strategies for large numbers of unknown post-translational modifications based on a combination of database search and de novo identification. All research results achieved in this project will be published and made freely available to the bioinformatics and proteomics communities. Improving identification rates of peptides in general and of unknown modifications in particular will permit a deeper insight into the proteome; computer science shall thus form a new basis for finding answers to important medical and biological questions.	British Heart Foundation	Research Grant	1270434.0GBP
124	Professor Mills Nicholas	University of Sheffield	2015-08-01	2020-08-01	Distributed Algorithms for Optimal Decision-Making	This grant will develop and translate a unifying framework for optimal decision-theory, and observations of natural systems, to design distributed algorithms for decentralised decision-making. This will enable a technological step-change in techniques for controlling distributed systems, primarily demonstrated during the grant by decentralised control of robot swarms. These algorithms and associated methodology will also provide hypotheses and tools to change the way scientists think about and interrogate natural decision mechanisms, from intracellular regulatory networks, via neural decision circuits, to decision-making populations of animals. Specific objectives are: 1. Distributed value-sensitive decision-making: undertake optimality analyses of the applicant's existing decentralised decision-making algorithms based on observations of collective iterated voting-processes in honeybees, and extend these. 2. Distributed sampling and decision-making: design distributed mechanisms that implement optimal compromises between sampling information and making decisions based on that information. 3. Individual-confidence and distributed decision-making: translate machine learning theory to collective behaviour models, designing mechanisms in which weak decision-makers optimally combine their decisions to optimise group performance. 4. Optimal distributed decision-making in collective robotics: translate theory from objective 1 to 3 towards practical applications in artificial systems, demonstrated using collectively-deciding robots. 5. Development of tools for life scientists and validation of theoretical predictions in natural systems: interact with named collaborators to investigate identified decision mechanisms in single cells, in neural circuits, and in social groups. Develop accessible modelling tools to facilitate investigations by life scientists.	European Research Council	Consolidator Grant	1413705.0EUR
125	Dr Robinson Emma	King's College London	2019-06-01	2024-05-31	Integrative imaging of brain structure and function in populations and individuals	None	Wellcome Trust	Collaborative Award in Science	4106203.0GBP
126	Professor Williams Steven	King's College London	2012-08-06	2014-07-05	Cerebral Blood Flow Imaging - Towards an Efficient, Automated Assay of Ongoing Pain and its Treatment	None	Medical Research Council	Research Grant	394246.0GBP
127	Prof Sir Lovestone Simon	University of Oxford	2016-10-01	2021-09-30	Deep and Frequent Phenotyping; combinatorial biomarkers for dementia experimental medicine	None	Medical Research Council	Research Grant	6301078.0GBP
128	Prof. Dehaene Stanislas Pierre Joseph	French Alternative Energies and Atomic Energy Commission	2016-10-01	2021-09-30	The cerebral representation of sequences and roles : investigating the origins of human uniqueness.	What are the origins of humans’ remarkable capacities to grasp, memorize, and produce complex sequences and rules, as manifested in language and mathematics? During its evolution, the human brain may have acquired a capacity to represent nested rules, based in part on the expansion of circuits involving the inferior frontal gyrus. This hypothesis will be tested using behavioral measures, functional MRI, magneto-encephalography (MEG), electro-corticography (ECOG) and machine learning techniques in human and non-human primates tested in identical paradigms. (1) We will design a hierarchy of non-linguistic visual and auditory sequences that place increasing demands on abstract rule coding.(2) Behavioral studies of pointing time and eye tracking will investigate the memory for such sequences in human adults, children, and macaque monkeys, and their extrapolation to future items. (3) Functional MRI, MEG, and ECOG will probe the localization, time course, and neural coding of such non-linguistic sequences in human adults. (4) In the same subjects, we will investigate the representation of linguistic and mathematical structures and determine if they involve the same areas and coding principles. (5) We will also record fMRI and ECOG responses to this hierarchy of non-linguistic sequences in macaque monkeys, in search of both correspondences and sharp differences with humans. (6) The same non-linguistic materials will be used in fMRI and EEG studies of human children and infants. Our hypothesis predicts that human children may perform better than adult monkeys. (7) We will formulate and test mathematical models that propose that the human brain “compresses” incoming sequences using nested rules (Kolmogorov complexity), uses predictive codes to anticipate on future inputs, and encodes syntax via tensor-product representations. The results will clarify the brain mechanisms of human language and abstraction abilities, and shed light on their ontogeny and phylogeny.	European Research Council	Advanced Grant	2499747.0EUR
129	Prof. Dehaene Stanislas Pierre Joseph	Eberhard Karls Universität Tübingen	2013-04-01	2018-04-01	Language Evolution: The Empirical Turn	This proposal describes a highly interdisciplinary approach to the empirical study of cultural language evolution. It draws on ideas and methods from *historical linguistics and typology*, *natural language processing*, *biology*, *bioinformatics*, *computer science*, and *statistics*.The computer aided study of cultural language evolution has seen a tremendous upturn over the past fifteen years. This comprises both model-driven approaches - studying the consequences of design assumptions regarding language production, comprehension, and learning for their long-term population-wide consequences - and data-driven approaches that employ algorithmic techniques from bioinformatics to recover otherwise inaccessible information about language history. At the current junction, the field faces two challenges:- The specifics of language evolution - which includes parallels with but also key differences to biological evolution - require central attention.- Model-driven and data-driven approaches need to inform each other to achieve explanatory power and to assess the statistical significance of the findings.The project will establish a radically data-oriented framework for the study of language evolution. This includes three aspects:- replacing the off-the-shelf tools from bioinformatics that are currently in use in computational language classification by linguistically informed algorithms, esp.\ *multiple sequence alignment techniques*,- identifying characteristic traits of language evolution via *exploratory data analysis*, guided by the theory of *complex systems* and employing cutting-edge methods from *machine learning* such as *kernel methods* and *causal inference*, and- developing, implementing and testing models of language evolution that correctly predict the *statistical fingerprints of language evolution*, i.e. pay sufficient attention to the domain specific features of language evolution that have no counterpart in biological evolution.	European Research Council	Advanced Grant	2003580.0EUR
130	Doctor Dobson Richard	King's College London	2013-10-01	2016-09-30	Creating an early diagnostic blood test for Alzheimer's Disease	In this PhD we aim to validate, refine and extend a panel of biomarkers of early Alzheimer's disease pathology. One of the earliest known markers of Alzheimer's disease pathology is the level of amyloid beta in the brain; another early marker is hippocampal volume. Our previous work has shown that the level of some proteins in plasma associate with brain amyloid burden across independent cohorts and proteomic platforms. In this study these plasma protein markers will be validated, and novel plasma protein markers of brain amyloid burden identified. In addition, inexpensive and non-invasive multi-modal biomarkers of early Alzheimer's disease pathology will be investigated for the first time. Data on the level of plasma proteins, and/or other blood analytes or cognitive measures, will be compared to brain amyloid burden and/or hippocampal volume as derived from brain scans. Regression, classification and machine learning approaches will be used to study the ability of these biomarkers to predict brain amyloid burden or hippocampal atrophy. Cross-validation will be used to test the robustness of these biomarkers over datasets derived from independent cohorts and proteomic platforms. These results will reveal the clinical utility of inexpensive and non-invasive biomarkers of early Alzheimer's disease, a potentially transformative technology.	Alzheimer's Society	PhD Studentship	79457.0GBP
131	Dr Gibbons Chris	University of Cambridge	2017-10-01	2018-09-30	Development and testing of an intelligent system for the personalised reporting and evaluation of patient-reported data (INSPiRED).	In this PhD we aim to validate, refine and extend a panel of biomarkers of early Alzheimer's disease pathology. One of the earliest known markers of Alzheimer's disease pathology is the level of amyloid beta in the brain; another early marker is hippocampal volume. Our previous work has shown that the level of some proteins in plasma associate with brain amyloid burden across independent cohorts and proteomic platforms. In this study these plasma protein markers will be validated, and novel plasma protein markers of brain amyloid burden identified. In addition, inexpensive and non-invasive multi-modal biomarkers of early Alzheimer's disease pathology will be investigated for the first time. Data on the level of plasma proteins, and/or other blood analytes or cognitive measures, will be compared to brain amyloid burden and/or hippocampal volume as derived from brain scans. Regression, classification and machine learning approaches will be used to study the ability of these biomarkers to predict brain amyloid burden or hippocampal atrophy. Cross-validation will be used to test the robustness of these biomarkers over datasets derived from independent cohorts and proteomic platforms. These results will reveal the clinical utility of inexpensive and non-invasive biomarkers of early Alzheimer's disease, a potentially transformative technology.	National Institute for Health Research (Department of Health)	Full award	115285.0GBP
132	Dr Gibbons Chris	Università degli Studi di Trento	2014-08-01	2019-08-01	Transfer Learning within and between brains	The neural bases of adaptive behavior in social environments are far from being understood. We propose to use both computational and neuroscientific methodologies to provide new and more accurate models of learning in interactive settings. The long-term objective is to develop a neural theory of learning: a mathematical framework that describes the computations mediating social learning in terms of neural signals, structures and plasticity. We plan to develop a model of adaptive learning based on three basic principles: (1) the observation of the outcome of un-chosen options improves the decisions taken in the learning process, (2) learning can be transferred from one domain to another, and (3) learning can be transferred from one agent to another (i. e. social learning). In all three cases, humans appear able to construct and transfer knowledge from sources other than their own direct experience, an underappreciated though we believe critical aspect of learning. Our approach will combine neural and behavioral data with computational models of learning. The hypotheses will be formalized into machine learning algorithms and neural networks of "regret" learning, to quantify the evolution of the learning computations on a trial-by-trial basis from the sequence of stimuli, choices and outcomes. The existence and accuracy of the predicted computations will be then tested on neural signals recorded with functional magnetic resonance imaging (fMRI). The potential findings of this project could lead us to suggest general principles of social learning, and we will be able to measure and model neural activation to show those general principles in action. In addition, our results could have important implications into policy-making - by revealing what type of information agents are naturally inclined to better learn from - and clinical practice - by outlining potential diagnostic procedures and behavioral therapies for disorders affecting social behavior.	European Research Council	Consolidator Grant	1999998.0EUR
133	Professor Koh Dow-Mu	The Royal Marsden NHS Foundation Trust	2017-01-02	2020-01-01	Advanced computer diagnostics for whole body magnetic resonance imaging to improve management of patients with metastatic bone cancer	The neural bases of adaptive behavior in social environments are far from being understood. We propose to use both computational and neuroscientific methodologies to provide new and more accurate models of learning in interactive settings. The long-term objective is to develop a neural theory of learning: a mathematical framework that describes the computations mediating social learning in terms of neural signals, structures and plasticity. We plan to develop a model of adaptive learning based on three basic principles: (1) the observation of the outcome of un-chosen options improves the decisions taken in the learning process, (2) learning can be transferred from one domain to another, and (3) learning can be transferred from one agent to another (i. e. social learning). In all three cases, humans appear able to construct and transfer knowledge from sources other than their own direct experience, an underappreciated though we believe critical aspect of learning. Our approach will combine neural and behavioral data with computational models of learning. The hypotheses will be formalized into machine learning algorithms and neural networks of "regret" learning, to quantify the evolution of the learning computations on a trial-by-trial basis from the sequence of stimuli, choices and outcomes. The existence and accuracy of the predicted computations will be then tested on neural signals recorded with functional magnetic resonance imaging (fMRI). The potential findings of this project could lead us to suggest general principles of social learning, and we will be able to measure and model neural activation to show those general principles in action. In addition, our results could have important implications into policy-making - by revealing what type of information agents are naturally inclined to better learn from - and clinical practice - by outlining potential diagnostic procedures and behavioral therapies for disorders affecting social behavior.	National Institute for Health Research (Department of Health)	Full Award	1142228.0GBP
134	Dr Poh Norman	University of Surrey	2015-09-01	2017-07-12	Modelling and Predicting CKD Progression	The neural bases of adaptive behavior in social environments are far from being understood. We propose to use both computational and neuroscientific methodologies to provide new and more accurate models of learning in interactive settings. The long-term objective is to develop a neural theory of learning: a mathematical framework that describes the computations mediating social learning in terms of neural signals, structures and plasticity. We plan to develop a model of adaptive learning based on three basic principles: (1) the observation of the outcome of un-chosen options improves the decisions taken in the learning process, (2) learning can be transferred from one domain to another, and (3) learning can be transferred from one agent to another (i. e. social learning). In all three cases, humans appear able to construct and transfer knowledge from sources other than their own direct experience, an underappreciated though we believe critical aspect of learning. Our approach will combine neural and behavioral data with computational models of learning. The hypotheses will be formalized into machine learning algorithms and neural networks of "regret" learning, to quantify the evolution of the learning computations on a trial-by-trial basis from the sequence of stimuli, choices and outcomes. The existence and accuracy of the predicted computations will be then tested on neural signals recorded with functional magnetic resonance imaging (fMRI). The potential findings of this project could lead us to suggest general principles of social learning, and we will be able to measure and model neural activation to show those general principles in action. In addition, our results could have important implications into policy-making - by revealing what type of information agents are naturally inclined to better learn from - and clinical practice - by outlining potential diagnostic procedures and behavioral therapies for disorders affecting social behavior.	Medical Research Council	Research Grant	519780.0GBP
135	Professor Collier Nigel	University of Cambridge	2015-11-13	2018-11-28	PheneBank: automatic extraction and validation of a database of human phenotype-disease associations in the scientific literature	The neural bases of adaptive behavior in social environments are far from being understood. We propose to use both computational and neuroscientific methodologies to provide new and more accurate models of learning in interactive settings. The long-term objective is to develop a neural theory of learning: a mathematical framework that describes the computations mediating social learning in terms of neural signals, structures and plasticity. We plan to develop a model of adaptive learning based on three basic principles: (1) the observation of the outcome of un-chosen options improves the decisions taken in the learning process, (2) learning can be transferred from one domain to another, and (3) learning can be transferred from one agent to another (i. e. social learning). In all three cases, humans appear able to construct and transfer knowledge from sources other than their own direct experience, an underappreciated though we believe critical aspect of learning. Our approach will combine neural and behavioral data with computational models of learning. The hypotheses will be formalized into machine learning algorithms and neural networks of "regret" learning, to quantify the evolution of the learning computations on a trial-by-trial basis from the sequence of stimuli, choices and outcomes. The existence and accuracy of the predicted computations will be then tested on neural signals recorded with functional magnetic resonance imaging (fMRI). The potential findings of this project could lead us to suggest general principles of social learning, and we will be able to measure and model neural activation to show those general principles in action. In addition, our results could have important implications into policy-making - by revealing what type of information agents are naturally inclined to better learn from - and clinical practice - by outlining potential diagnostic procedures and behavioral therapies for disorders affecting social behavior.	Medical Research Council	Research Grant	464013.0GBP
136	Dr Nirantharakumar Krishnarajah	University of Birmingham	2020-06-01	2020-11-30	Multimorbid Pregnancy: Determinants, Clusters, Consequences and Trajectories (MuM-PreDiCCT)	The neural bases of adaptive behavior in social environments are far from being understood. We propose to use both computational and neuroscientific methodologies to provide new and more accurate models of learning in interactive settings. The long-term objective is to develop a neural theory of learning: a mathematical framework that describes the computations mediating social learning in terms of neural signals, structures and plasticity. We plan to develop a model of adaptive learning based on three basic principles: (1) the observation of the outcome of un-chosen options improves the decisions taken in the learning process, (2) learning can be transferred from one domain to another, and (3) learning can be transferred from one agent to another (i. e. social learning). In all three cases, humans appear able to construct and transfer knowledge from sources other than their own direct experience, an underappreciated though we believe critical aspect of learning. Our approach will combine neural and behavioral data with computational models of learning. The hypotheses will be formalized into machine learning algorithms and neural networks of "regret" learning, to quantify the evolution of the learning computations on a trial-by-trial basis from the sequence of stimuli, choices and outcomes. The existence and accuracy of the predicted computations will be then tested on neural signals recorded with functional magnetic resonance imaging (fMRI). The potential findings of this project could lead us to suggest general principles of social learning, and we will be able to measure and model neural activation to show those general principles in action. In addition, our results could have important implications into policy-making - by revealing what type of information agents are naturally inclined to better learn from - and clinical practice - by outlining potential diagnostic procedures and behavioral therapies for disorders affecting social behavior.	Medical Research Council	Research Grant	100396.0GBP
137	Professor Gaunt Tom	University of Bristol	2013-06-01	2018-03-31	Data mining and bioinformatics cross-cutting theme	The neural bases of adaptive behavior in social environments are far from being understood. We propose to use both computational and neuroscientific methodologies to provide new and more accurate models of learning in interactive settings. The long-term objective is to develop a neural theory of learning: a mathematical framework that describes the computations mediating social learning in terms of neural signals, structures and plasticity. We plan to develop a model of adaptive learning based on three basic principles: (1) the observation of the outcome of un-chosen options improves the decisions taken in the learning process, (2) learning can be transferred from one domain to another, and (3) learning can be transferred from one agent to another (i. e. social learning). In all three cases, humans appear able to construct and transfer knowledge from sources other than their own direct experience, an underappreciated though we believe critical aspect of learning. Our approach will combine neural and behavioral data with computational models of learning. The hypotheses will be formalized into machine learning algorithms and neural networks of "regret" learning, to quantify the evolution of the learning computations on a trial-by-trial basis from the sequence of stimuli, choices and outcomes. The existence and accuracy of the predicted computations will be then tested on neural signals recorded with functional magnetic resonance imaging (fMRI). The potential findings of this project could lead us to suggest general principles of social learning, and we will be able to measure and model neural activation to show those general principles in action. In addition, our results could have important implications into policy-making - by revealing what type of information agents are naturally inclined to better learn from - and clinical practice - by outlining potential diagnostic procedures and behavioral therapies for disorders affecting social behavior.	Medical Research Council	Unit	100396.0GBP
138	Dr Sproul Duncan	University of Edinburgh	2016-09-01	2022-08-31	The impact of genetic interactions on DNA methylation in breast cancer	Background:Epigenetic dysfunction is a near universal feature of carcinogenesis. DNA methylation alterations affect key breast cancer genes such as BRCA1 suggesting their importance to carcinogenesis and progression. However, the mechanisms underpinning such alterations in DNA methylation are unknown, so their significance remains unclear.My recent work suggests epigenetic marks are strongly programmed by the underlying DNA sequence. This means that sequence variants are likely to be an important and underexplored influence on breast cancer epigenomes.Aims:This project aims to understand the role of the genome in programming the epigenetic mark DNA methylation in breast cancer through two aims.1. Define the extent to which germ-line and somatic DNA sequence variants modulate local levels of DNA methylation in breast tumours in vivo.2. Understand how the effects of regulatory variants on DNA methylation and gene expression interact and how such variants impact on clinical phenotypes.Methods:I will elucidate the frequency to which germ-line and somatic sequence variants affect DNA methylation in breast cancer through machine-learning analyses of HER2-driven mouse mammary tumours and clinical high-grade luminal breast tumours. The interaction between variant-modulated alterations in DNA methylation and gene expression will be tested using genome editing technologies and the promoter of BRCA1 as a paradigm. The phenotypic impact of BRCA1 regulatory mutations will be assessed through engineering into mice. These synergistic approaches reinforce one another to uncover the global impact of sequence variation on DNA methylation, the mechanistic details underlying these effects and their clinical relevance. How the results of this research will be used:This project uses state-of-the-art computational and laboratory approaches to leverage genetics towards tackling our lack of understanding regarding the impact of DNA sequence variation on epigenetic alterations in cancer. In doing so, it impacts on our understanding of the regulation of a key breast and ovarian cancer risk gene, BRCA1, and provides new approaches to delineate the contribution of regulatory variants to carcinogenesis and progression.	Cancer Research UK	NIC - Career Development Fellowship	100396.0GBP
139	Professor Griffiths Christopher	The University of Manchester	2014-09-01	2021-05-31	MICA: Psoriasis Stratification to Optimise Relevant Therapy (PSORT)	Background:Epigenetic dysfunction is a near universal feature of carcinogenesis. DNA methylation alterations affect key breast cancer genes such as BRCA1 suggesting their importance to carcinogenesis and progression. However, the mechanisms underpinning such alterations in DNA methylation are unknown, so their significance remains unclear.My recent work suggests epigenetic marks are strongly programmed by the underlying DNA sequence. This means that sequence variants are likely to be an important and underexplored influence on breast cancer epigenomes.Aims:This project aims to understand the role of the genome in programming the epigenetic mark DNA methylation in breast cancer through two aims.1. Define the extent to which germ-line and somatic DNA sequence variants modulate local levels of DNA methylation in breast tumours in vivo.2. Understand how the effects of regulatory variants on DNA methylation and gene expression interact and how such variants impact on clinical phenotypes.Methods:I will elucidate the frequency to which germ-line and somatic sequence variants affect DNA methylation in breast cancer through machine-learning analyses of HER2-driven mouse mammary tumours and clinical high-grade luminal breast tumours. The interaction between variant-modulated alterations in DNA methylation and gene expression will be tested using genome editing technologies and the promoter of BRCA1 as a paradigm. The phenotypic impact of BRCA1 regulatory mutations will be assessed through engineering into mice. These synergistic approaches reinforce one another to uncover the global impact of sequence variation on DNA methylation, the mechanistic details underlying these effects and their clinical relevance. How the results of this research will be used:This project uses state-of-the-art computational and laboratory approaches to leverage genetics towards tackling our lack of understanding regarding the impact of DNA sequence variation on epigenetic alterations in cancer. In doing so, it impacts on our understanding of the regulation of a key breast and ovarian cancer risk gene, BRCA1, and provides new approaches to delineate the contribution of regulatory variants to carcinogenesis and progression.	Medical Research Council	Research Grant	5054543.0GBP
140	Dr Overton Ian	University of Edinburgh	2012-04-01	2017-07-31	Computational studies of phenotypic plasticity in development, metastasis and drug response; towards new clinical tools	Background:Epigenetic dysfunction is a near universal feature of carcinogenesis. DNA methylation alterations affect key breast cancer genes such as BRCA1 suggesting their importance to carcinogenesis and progression. However, the mechanisms underpinning such alterations in DNA methylation are unknown, so their significance remains unclear.My recent work suggests epigenetic marks are strongly programmed by the underlying DNA sequence. This means that sequence variants are likely to be an important and underexplored influence on breast cancer epigenomes.Aims:This project aims to understand the role of the genome in programming the epigenetic mark DNA methylation in breast cancer through two aims.1. Define the extent to which germ-line and somatic DNA sequence variants modulate local levels of DNA methylation in breast tumours in vivo.2. Understand how the effects of regulatory variants on DNA methylation and gene expression interact and how such variants impact on clinical phenotypes.Methods:I will elucidate the frequency to which germ-line and somatic sequence variants affect DNA methylation in breast cancer through machine-learning analyses of HER2-driven mouse mammary tumours and clinical high-grade luminal breast tumours. The interaction between variant-modulated alterations in DNA methylation and gene expression will be tested using genome editing technologies and the promoter of BRCA1 as a paradigm. The phenotypic impact of BRCA1 regulatory mutations will be assessed through engineering into mice. These synergistic approaches reinforce one another to uncover the global impact of sequence variation on DNA methylation, the mechanistic details underlying these effects and their clinical relevance. How the results of this research will be used:This project uses state-of-the-art computational and laboratory approaches to leverage genetics towards tackling our lack of understanding regarding the impact of DNA sequence variation on epigenetic alterations in cancer. In doing so, it impacts on our understanding of the regulation of a key breast and ovarian cancer risk gene, BRCA1, and provides new approaches to delineate the contribution of regulatory variants to carcinogenesis and progression.	Medical Research Council	Unit	5054543.0GBP
141	Dr Barry Caswell	University College London	2018-12-01	2023-11-30	Non-local computations in hippocampal circuits: neural mechanisms	Background:Epigenetic dysfunction is a near universal feature of carcinogenesis. DNA methylation alterations affect key breast cancer genes such as BRCA1 suggesting their importance to carcinogenesis and progression. However, the mechanisms underpinning such alterations in DNA methylation are unknown, so their significance remains unclear.My recent work suggests epigenetic marks are strongly programmed by the underlying DNA sequence. This means that sequence variants are likely to be an important and underexplored influence on breast cancer epigenomes.Aims:This project aims to understand the role of the genome in programming the epigenetic mark DNA methylation in breast cancer through two aims.1. Define the extent to which germ-line and somatic DNA sequence variants modulate local levels of DNA methylation in breast tumours in vivo.2. Understand how the effects of regulatory variants on DNA methylation and gene expression interact and how such variants impact on clinical phenotypes.Methods:I will elucidate the frequency to which germ-line and somatic sequence variants affect DNA methylation in breast cancer through machine-learning analyses of HER2-driven mouse mammary tumours and clinical high-grade luminal breast tumours. The interaction between variant-modulated alterations in DNA methylation and gene expression will be tested using genome editing technologies and the promoter of BRCA1 as a paradigm. The phenotypic impact of BRCA1 regulatory mutations will be assessed through engineering into mice. These synergistic approaches reinforce one another to uncover the global impact of sequence variation on DNA methylation, the mechanistic details underlying these effects and their clinical relevance. How the results of this research will be used:This project uses state-of-the-art computational and laboratory approaches to leverage genetics towards tackling our lack of understanding regarding the impact of DNA sequence variation on epigenetic alterations in cancer. In doing so, it impacts on our understanding of the regulation of a key breast and ovarian cancer risk gene, BRCA1, and provides new approaches to delineate the contribution of regulatory variants to carcinogenesis and progression.	Wellcome Trust	Senior Research Fellowship Basic	1670136.0GBP
142	Professor Dr Kirchhof Paulus	Birmingham, University of	2018-12-01	2023-11-30	Defining clusters of patients with atrial fibrillation at risk of heart failure and death	Background:Epigenetic dysfunction is a near universal feature of carcinogenesis. DNA methylation alterations affect key breast cancer genes such as BRCA1 suggesting their importance to carcinogenesis and progression. However, the mechanisms underpinning such alterations in DNA methylation are unknown, so their significance remains unclear.My recent work suggests epigenetic marks are strongly programmed by the underlying DNA sequence. This means that sequence variants are likely to be an important and underexplored influence on breast cancer epigenomes.Aims:This project aims to understand the role of the genome in programming the epigenetic mark DNA methylation in breast cancer through two aims.1. Define the extent to which germ-line and somatic DNA sequence variants modulate local levels of DNA methylation in breast tumours in vivo.2. Understand how the effects of regulatory variants on DNA methylation and gene expression interact and how such variants impact on clinical phenotypes.Methods:I will elucidate the frequency to which germ-line and somatic sequence variants affect DNA methylation in breast cancer through machine-learning analyses of HER2-driven mouse mammary tumours and clinical high-grade luminal breast tumours. The interaction between variant-modulated alterations in DNA methylation and gene expression will be tested using genome editing technologies and the promoter of BRCA1 as a paradigm. The phenotypic impact of BRCA1 regulatory mutations will be assessed through engineering into mice. These synergistic approaches reinforce one another to uncover the global impact of sequence variation on DNA methylation, the mechanistic details underlying these effects and their clinical relevance. How the results of this research will be used:This project uses state-of-the-art computational and laboratory approaches to leverage genetics towards tackling our lack of understanding regarding the impact of DNA sequence variation on epigenetic alterations in cancer. In doing so, it impacts on our understanding of the regulation of a key breast and ovarian cancer risk gene, BRCA1, and provides new approaches to delineate the contribution of regulatory variants to carcinogenesis and progression.	British Heart Foundation	Senior Research Fellowship Basic	73801.0GBP
143	Dr Savage Richard Stephen	University of Warwick	2010-06-01	2014-05-31	Applications of probabilistic machine learning to medical biostatistics	Background:Epigenetic dysfunction is a near universal feature of carcinogenesis. DNA methylation alterations affect key breast cancer genes such as BRCA1 suggesting their importance to carcinogenesis and progression. However, the mechanisms underpinning such alterations in DNA methylation are unknown, so their significance remains unclear.My recent work suggests epigenetic marks are strongly programmed by the underlying DNA sequence. This means that sequence variants are likely to be an important and underexplored influence on breast cancer epigenomes.Aims:This project aims to understand the role of the genome in programming the epigenetic mark DNA methylation in breast cancer through two aims.1. Define the extent to which germ-line and somatic DNA sequence variants modulate local levels of DNA methylation in breast tumours in vivo.2. Understand how the effects of regulatory variants on DNA methylation and gene expression interact and how such variants impact on clinical phenotypes.Methods:I will elucidate the frequency to which germ-line and somatic sequence variants affect DNA methylation in breast cancer through machine-learning analyses of HER2-driven mouse mammary tumours and clinical high-grade luminal breast tumours. The interaction between variant-modulated alterations in DNA methylation and gene expression will be tested using genome editing technologies and the promoter of BRCA1 as a paradigm. The phenotypic impact of BRCA1 regulatory mutations will be assessed through engineering into mice. These synergistic approaches reinforce one another to uncover the global impact of sequence variation on DNA methylation, the mechanistic details underlying these effects and their clinical relevance. How the results of this research will be used:This project uses state-of-the-art computational and laboratory approaches to leverage genetics towards tackling our lack of understanding regarding the impact of DNA sequence variation on epigenetic alterations in cancer. In doing so, it impacts on our understanding of the regulation of a key breast and ovarian cancer risk gene, BRCA1, and provides new approaches to delineate the contribution of regulatory variants to carcinogenesis and progression.	Medical Research Council	Fellowship	351627.0GBP
144	Professor Ananiadou Sophia	The University of Manchester	2014-03-31	2017-09-30	Supporting Evidence-based Public Health Interventions using Text Mining	Background:Epigenetic dysfunction is a near universal feature of carcinogenesis. DNA methylation alterations affect key breast cancer genes such as BRCA1 suggesting their importance to carcinogenesis and progression. However, the mechanisms underpinning such alterations in DNA methylation are unknown, so their significance remains unclear.My recent work suggests epigenetic marks are strongly programmed by the underlying DNA sequence. This means that sequence variants are likely to be an important and underexplored influence on breast cancer epigenomes.Aims:This project aims to understand the role of the genome in programming the epigenetic mark DNA methylation in breast cancer through two aims.1. Define the extent to which germ-line and somatic DNA sequence variants modulate local levels of DNA methylation in breast tumours in vivo.2. Understand how the effects of regulatory variants on DNA methylation and gene expression interact and how such variants impact on clinical phenotypes.Methods:I will elucidate the frequency to which germ-line and somatic sequence variants affect DNA methylation in breast cancer through machine-learning analyses of HER2-driven mouse mammary tumours and clinical high-grade luminal breast tumours. The interaction between variant-modulated alterations in DNA methylation and gene expression will be tested using genome editing technologies and the promoter of BRCA1 as a paradigm. The phenotypic impact of BRCA1 regulatory mutations will be assessed through engineering into mice. These synergistic approaches reinforce one another to uncover the global impact of sequence variation on DNA methylation, the mechanistic details underlying these effects and their clinical relevance. How the results of this research will be used:This project uses state-of-the-art computational and laboratory approaches to leverage genetics towards tackling our lack of understanding regarding the impact of DNA sequence variation on epigenetic alterations in cancer. In doing so, it impacts on our understanding of the regulation of a key breast and ovarian cancer risk gene, BRCA1, and provides new approaches to delineate the contribution of regulatory variants to carcinogenesis and progression.	Medical Research Council	Research Grant	655668.0GBP
145	Dr Campbell Ian	University of Bristol	2008-10-01	2012-04-30	siRNA target selection in cancer research using machine learning techniques	We wil use advanced methods from machine learning and statistics, together with prior biological knowledge, to rank candidates for gene expression knockdown using short interfering RNAs. These targets could lead through to potential therapeutic targets for further study.	Cancer Research UK	Project Award	655668.0GBP
146	Dr Campbell Ian	Royal Holloway, Univ Of London	2008-10-01	2012-04-30	Development of a graph-theoretic approach to predict protein function by integrating large scale heterogeneous data	Statistically sound large-scale protein function prediction can be obtained only by integrating evidence from different sources. Functional inference methods that exploit biological networks topologies offer good performance. But so far such methods are limited in the type of data they can integrate, while methods that can integrate a greater variety of data do not take advantage of the networks' topologies. I propose a general method that can integrate essentially any data type available taking into account the intrinsic structure of each data type: it uses graph-theoretic methods to produce functional evidence from network data, and it integrates it with evidence from one-dimensional information using machine learning techniques. Defining function in terms of the Gene Ontology, I shall collect datasets for S. cerevisiae, C. elegans, D. melanogaster, A. thaliana, H. sapiens. Algorithm development and testing will be done on S. cerevisiae. I shall then verify how these methods transfer to the other organisms. Performance on these organisms will be evaluated in silico", by means of test sets. The approach will also be tested "in vivo" by predicting the Biological Process for a group of MAP kinases that belong to the signalling pathways of A. thaliana. These predictions will be tested through functional assays: 1. an RNAi screen and quantitative measurements of MAPK signalling outputs, MAPK activities and promoter activations in cultured Arabidopsis cells 2. quantitative phenotypic tests for selected phenotypes in cell differentiation (e.g. stomata development) and stress responses. I shall design and implement stand-alone and web-based software tools incorporating the algorithms developed. These will enable the biologist to easily apply the algorithms through a user-friendly interface; visualization tools will make the functional inference process transparent to the user. All these tools will be made freely available to the scientific community."	Biotechnology and Biological Sciences Research Council	Project Award	419814.0GBP
147	Professor Relton Caroline	University of Bristol	2018-04-01	2023-03-31	Epigenetic Epidemiology	Statistically sound large-scale protein function prediction can be obtained only by integrating evidence from different sources. Functional inference methods that exploit biological networks topologies offer good performance. But so far such methods are limited in the type of data they can integrate, while methods that can integrate a greater variety of data do not take advantage of the networks' topologies. I propose a general method that can integrate essentially any data type available taking into account the intrinsic structure of each data type: it uses graph-theoretic methods to produce functional evidence from network data, and it integrates it with evidence from one-dimensional information using machine learning techniques. Defining function in terms of the Gene Ontology, I shall collect datasets for S. cerevisiae, C. elegans, D. melanogaster, A. thaliana, H. sapiens. Algorithm development and testing will be done on S. cerevisiae. I shall then verify how these methods transfer to the other organisms. Performance on these organisms will be evaluated in silico", by means of test sets. The approach will also be tested "in vivo" by predicting the Biological Process for a group of MAP kinases that belong to the signalling pathways of A. thaliana. These predictions will be tested through functional assays: 1. an RNAi screen and quantitative measurements of MAPK signalling outputs, MAPK activities and promoter activations in cultured Arabidopsis cells 2. quantitative phenotypic tests for selected phenotypes in cell differentiation (e.g. stomata development) and stress responses. I shall design and implement stand-alone and web-based software tools incorporating the algorithms developed. These will enable the biologist to easily apply the algorithms through a user-friendly interface; visualization tools will make the functional inference process transparent to the user. All these tools will be made freely available to the scientific community."	Medical Research Council	Unit	1254000.0GBP
148	Prof. Yau Christopher	The University of Manchester	2020-01-07	2020-10-08	From single cells to populations: generalized pseudotime analysis to identify patient trajectories from cross-sectional data in cancer genomics	Statistically sound large-scale protein function prediction can be obtained only by integrating evidence from different sources. Functional inference methods that exploit biological networks topologies offer good performance. But so far such methods are limited in the type of data they can integrate, while methods that can integrate a greater variety of data do not take advantage of the networks' topologies. I propose a general method that can integrate essentially any data type available taking into account the intrinsic structure of each data type: it uses graph-theoretic methods to produce functional evidence from network data, and it integrates it with evidence from one-dimensional information using machine learning techniques. Defining function in terms of the Gene Ontology, I shall collect datasets for S. cerevisiae, C. elegans, D. melanogaster, A. thaliana, H. sapiens. Algorithm development and testing will be done on S. cerevisiae. I shall then verify how these methods transfer to the other organisms. Performance on these organisms will be evaluated in silico", by means of test sets. The approach will also be tested "in vivo" by predicting the Biological Process for a group of MAP kinases that belong to the signalling pathways of A. thaliana. These predictions will be tested through functional assays: 1. an RNAi screen and quantitative measurements of MAPK signalling outputs, MAPK activities and promoter activations in cultured Arabidopsis cells 2. quantitative phenotypic tests for selected phenotypes in cell differentiation (e.g. stomata development) and stress responses. I shall design and implement stand-alone and web-based software tools incorporating the algorithms developed. These will enable the biologist to easily apply the algorithms through a user-friendly interface; visualization tools will make the functional inference process transparent to the user. All these tools will be made freely available to the scientific community."	Medical Research Council	Research Grant	117835.0GBP
149	Dr Morales Daniel	University of Dundee	2015-04-01	2016-03-31	Application Of Support Vector Machine Learning To Predict The Risk Of Death From Chronic Obstructive Pulmonary Disease Using Electronic Primary Care Medical Records. (Health Informatics Grant)	To use a support vector machine learning approach to predict death from COPD using electronic primary care medical records and to compare its performance with standard regression. 4. Research questions Question 1: Can routinely collected data in electronic primary care medical records be used to predict death from COPD? Question 2: Does a support vector machine learning approach perform better than standard logistic regression?	Chief Scientist Office	Research Grant	29525.0GBP
150	Mr Barnes Chris	University College London	2013-05-01	2018-04-30	A statistical approach to the understanding of mutational processes in the human genome and their impact on evolution, health and disease.	Germline genomic instability is the hypothesis that the architecture of the genome can increase the local mutation rate mostly through homologous regions promoting structural variation (SV) through errors in double strand break (DSB) repair and replication processes. Genomic instability has important consequences for genomic disorders, neuro-degenerative disease and primate evolution. There are three main aims of this research. 1) Develop stochastic models for the different mechanisms of SV for mation and use Approximate Bayesian Computation (ABC) methods to estimate parameters such as relative contributions and length-dependent mutation rates from published datasets. 2) Test the hypothesis that local genome architecture contributes to genomic instability by using a combination of the developed ABC framework and machine-learning methods to relate variant position and mechanism to genomic features. Use this to predict interesting regions of the genome for further study. 3) Examine the effect of genome instability in complex disease (metabolic, cardiovascular and autoimmune), neurodegenerative disease, population differentiation and evolution through collaborative efforts with geneticists and clinicians using both human and great ape sequencing.	Wellcome Trust	Research Career Development Fellowship	774472.0GBP
151	Dr Kudla Grzegorz	University of Edinburgh	2018-04-01	2023-03-31	RNA synthetic biology	Germline genomic instability is the hypothesis that the architecture of the genome can increase the local mutation rate mostly through homologous regions promoting structural variation (SV) through errors in double strand break (DSB) repair and replication processes. Genomic instability has important consequences for genomic disorders, neuro-degenerative disease and primate evolution. There are three main aims of this research. 1) Develop stochastic models for the different mechanisms of SV for mation and use Approximate Bayesian Computation (ABC) methods to estimate parameters such as relative contributions and length-dependent mutation rates from published datasets. 2) Test the hypothesis that local genome architecture contributes to genomic instability by using a combination of the developed ABC framework and machine-learning methods to relate variant position and mechanism to genomic features. Use this to predict interesting regions of the genome for further study. 3) Examine the effect of genome instability in complex disease (metabolic, cardiovascular and autoimmune), neurodegenerative disease, population differentiation and evolution through collaborative efforts with geneticists and clinicians using both human and great ape sequencing.	Medical Research Council	Unit	1205000.0GBP
152	Professor Emberton Mark	University College London	2018-06-01	2022-05-31	MICA: The exploitation of a novel image-based risk stratification tool in early prostate cancer - the Re-IMAGINE Consortium	Germline genomic instability is the hypothesis that the architecture of the genome can increase the local mutation rate mostly through homologous regions promoting structural variation (SV) through errors in double strand break (DSB) repair and replication processes. Genomic instability has important consequences for genomic disorders, neuro-degenerative disease and primate evolution. There are three main aims of this research. 1) Develop stochastic models for the different mechanisms of SV for mation and use Approximate Bayesian Computation (ABC) methods to estimate parameters such as relative contributions and length-dependent mutation rates from published datasets. 2) Test the hypothesis that local genome architecture contributes to genomic instability by using a combination of the developed ABC framework and machine-learning methods to relate variant position and mechanism to genomic features. Use this to predict interesting regions of the genome for further study. 3) Examine the effect of genome instability in complex disease (metabolic, cardiovascular and autoimmune), neurodegenerative disease, population differentiation and evolution through collaborative efforts with geneticists and clinicians using both human and great ape sequencing.	Medical Research Council	Research Grant	5179122.0GBP
153	Dr. MECHTLER Karl	IMP - Research Institute of Molecular Pathology	2013-03-01	2016-02-29	Self-Learning Search Algorithms for High-Res Mass Spectra	To identify proteins in biological samples mass spectrometry (MS) is most often applied: proteins are digested to peptides which are subsequently analyzed. Within the last decade, a new generation of mass spectrometers has been developed that are capable of acquiring mass spectra with high resolution and high mass accuracy. This has significantly changed the characteristics of mass spectra; however, this development has not been accompanied by a corresponding progress in peptide identification algorithms capable of fully exploiting the available information. We therefore propose to develop a set of novel identification algorithms that are specifically designed for the analysis of modern mass spectra and incorporate multiple sources of information in the here proposed bioinformatics research project. Preliminary research results are promising: The project consortium consisting of the Proteomics Group at IMP Vienna and the Bioinformatics Research Group at FH OÖ (Campus Hagenberg) has already conducted successful joint research in the analysis of MS data: Identification rates comparable or even superior to Mascot, the current gold-standard, have been achieved using a first version of a scoring function designed by the proposing consortium. Encouraged by these preliminary research results, we are convinced that considering additional sources of information will further improve identification rates of mass spectra – therefore this project is dedicated to research on a combination of the following novel approaches: We plan to use machine learning techniques to analyze peptide elution times, fragmentation patterns and mass accuracy characteristics specific to the instrument; in addition, observed m/z values will be recalibrated based on the mass error of highly reliable identifications, and the remaining mass error with regard to the learned distribution will be incorporated into the scoring function. Sophisticated peak picking strategies will also be designed using machine learning. These improvements will help increase identification rates in challenging situations such as hybrid spectra and exhaustive searches for a wide range of post-translational modifications. The latter approach leads to exponentially growing search spaces and an accompanying drop in spectra identification rates because the information in MS spectra on its own is not sufficient to cope with the increased search space. Instead of applying brute force methods we plan to solve this problem using construction heuristics, i.e., evolutionary algorithms that realize intelligent search strategies for large numbers of unknown post-translational modifications based on a combination of database search and de novo identification. All research results achieved in this project will be published and made freely available to the bioinformatics and proteomics communities. Improving identification rates of peptides in general and of unknown modifications in particular will permit a deeper insight into the proteome; computer science shall thus form a new basis for finding answers to important medical and biological questions.	Austrian Science Fund FWF	Translational Research	419816.26EUR
154	Dr Fu Cynthia	King's College London	2009-04-15	2010-04-14	Application of conformal predictors to functional magnetic resonance imaging research	To identify proteins in biological samples mass spectrometry (MS) is most often applied: proteins are digested to peptides which are subsequently analyzed. Within the last decade, a new generation of mass spectrometers has been developed that are capable of acquiring mass spectra with high resolution and high mass accuracy. This has significantly changed the characteristics of mass spectra; however, this development has not been accompanied by a corresponding progress in peptide identification algorithms capable of fully exploiting the available information. We therefore propose to develop a set of novel identification algorithms that are specifically designed for the analysis of modern mass spectra and incorporate multiple sources of information in the here proposed bioinformatics research project. Preliminary research results are promising: The project consortium consisting of the Proteomics Group at IMP Vienna and the Bioinformatics Research Group at FH OÖ (Campus Hagenberg) has already conducted successful joint research in the analysis of MS data: Identification rates comparable or even superior to Mascot, the current gold-standard, have been achieved using a first version of a scoring function designed by the proposing consortium. Encouraged by these preliminary research results, we are convinced that considering additional sources of information will further improve identification rates of mass spectra – therefore this project is dedicated to research on a combination of the following novel approaches: We plan to use machine learning techniques to analyze peptide elution times, fragmentation patterns and mass accuracy characteristics specific to the instrument; in addition, observed m/z values will be recalibrated based on the mass error of highly reliable identifications, and the remaining mass error with regard to the learned distribution will be incorporated into the scoring function. Sophisticated peak picking strategies will also be designed using machine learning. These improvements will help increase identification rates in challenging situations such as hybrid spectra and exhaustive searches for a wide range of post-translational modifications. The latter approach leads to exponentially growing search spaces and an accompanying drop in spectra identification rates because the information in MS spectra on its own is not sufficient to cope with the increased search space. Instead of applying brute force methods we plan to solve this problem using construction heuristics, i.e., evolutionary algorithms that realize intelligent search strategies for large numbers of unknown post-translational modifications based on a combination of database search and de novo identification. All research results achieved in this project will be published and made freely available to the bioinformatics and proteomics communities. Improving identification rates of peptides in general and of unknown modifications in particular will permit a deeper insight into the proteome; computer science shall thus form a new basis for finding answers to important medical and biological questions.	Medical Research Council	Research Grant	99973.0GBP
155	Dr Ball Gareth	King's College London	2012-07-01	2015-06-30	Machine learning to understand brain development	To identify proteins in biological samples mass spectrometry (MS) is most often applied: proteins are digested to peptides which are subsequently analyzed. Within the last decade, a new generation of mass spectrometers has been developed that are capable of acquiring mass spectra with high resolution and high mass accuracy. This has significantly changed the characteristics of mass spectra; however, this development has not been accompanied by a corresponding progress in peptide identification algorithms capable of fully exploiting the available information. We therefore propose to develop a set of novel identification algorithms that are specifically designed for the analysis of modern mass spectra and incorporate multiple sources of information in the here proposed bioinformatics research project. Preliminary research results are promising: The project consortium consisting of the Proteomics Group at IMP Vienna and the Bioinformatics Research Group at FH OÖ (Campus Hagenberg) has already conducted successful joint research in the analysis of MS data: Identification rates comparable or even superior to Mascot, the current gold-standard, have been achieved using a first version of a scoring function designed by the proposing consortium. Encouraged by these preliminary research results, we are convinced that considering additional sources of information will further improve identification rates of mass spectra – therefore this project is dedicated to research on a combination of the following novel approaches: We plan to use machine learning techniques to analyze peptide elution times, fragmentation patterns and mass accuracy characteristics specific to the instrument; in addition, observed m/z values will be recalibrated based on the mass error of highly reliable identifications, and the remaining mass error with regard to the learned distribution will be incorporated into the scoring function. Sophisticated peak picking strategies will also be designed using machine learning. These improvements will help increase identification rates in challenging situations such as hybrid spectra and exhaustive searches for a wide range of post-translational modifications. The latter approach leads to exponentially growing search spaces and an accompanying drop in spectra identification rates because the information in MS spectra on its own is not sufficient to cope with the increased search space. Instead of applying brute force methods we plan to solve this problem using construction heuristics, i.e., evolutionary algorithms that realize intelligent search strategies for large numbers of unknown post-translational modifications based on a combination of database search and de novo identification. All research results achieved in this project will be published and made freely available to the bioinformatics and proteomics communities. Improving identification rates of peptides in general and of unknown modifications in particular will permit a deeper insight into the proteome; computer science shall thus form a new basis for finding answers to important medical and biological questions.	Medical Research Council	Fellowship	299148.0GBP
156	Professor Custovic Adnan	The University of Manchester	2013-01-25	2015-09-13	MICA: STELAR (Study Team for Early Life Asthma Research) consortium - Asthma e-lab and identification of novel endotypes of childhood asthma	To identify proteins in biological samples mass spectrometry (MS) is most often applied: proteins are digested to peptides which are subsequently analyzed. Within the last decade, a new generation of mass spectrometers has been developed that are capable of acquiring mass spectra with high resolution and high mass accuracy. This has significantly changed the characteristics of mass spectra; however, this development has not been accompanied by a corresponding progress in peptide identification algorithms capable of fully exploiting the available information. We therefore propose to develop a set of novel identification algorithms that are specifically designed for the analysis of modern mass spectra and incorporate multiple sources of information in the here proposed bioinformatics research project. Preliminary research results are promising: The project consortium consisting of the Proteomics Group at IMP Vienna and the Bioinformatics Research Group at FH OÖ (Campus Hagenberg) has already conducted successful joint research in the analysis of MS data: Identification rates comparable or even superior to Mascot, the current gold-standard, have been achieved using a first version of a scoring function designed by the proposing consortium. Encouraged by these preliminary research results, we are convinced that considering additional sources of information will further improve identification rates of mass spectra – therefore this project is dedicated to research on a combination of the following novel approaches: We plan to use machine learning techniques to analyze peptide elution times, fragmentation patterns and mass accuracy characteristics specific to the instrument; in addition, observed m/z values will be recalibrated based on the mass error of highly reliable identifications, and the remaining mass error with regard to the learned distribution will be incorporated into the scoring function. Sophisticated peak picking strategies will also be designed using machine learning. These improvements will help increase identification rates in challenging situations such as hybrid spectra and exhaustive searches for a wide range of post-translational modifications. The latter approach leads to exponentially growing search spaces and an accompanying drop in spectra identification rates because the information in MS spectra on its own is not sufficient to cope with the increased search space. Instead of applying brute force methods we plan to solve this problem using construction heuristics, i.e., evolutionary algorithms that realize intelligent search strategies for large numbers of unknown post-translational modifications based on a combination of database search and de novo identification. All research results achieved in this project will be published and made freely available to the bioinformatics and proteomics communities. Improving identification rates of peptides in general and of unknown modifications in particular will permit a deeper insight into the proteome; computer science shall thus form a new basis for finding answers to important medical and biological questions.	Medical Research Council	Research Grant	1498108.0GBP
157	Professor Custovic Adnan	Imperial College London	2020-08-01	2023-07-31	Endotypes of childhood wheezing after severe RSV lower respiratory tract illness in infancy in socially vulnerable Argentinian children	To identify proteins in biological samples mass spectrometry (MS) is most often applied: proteins are digested to peptides which are subsequently analyzed. Within the last decade, a new generation of mass spectrometers has been developed that are capable of acquiring mass spectra with high resolution and high mass accuracy. This has significantly changed the characteristics of mass spectra; however, this development has not been accompanied by a corresponding progress in peptide identification algorithms capable of fully exploiting the available information. We therefore propose to develop a set of novel identification algorithms that are specifically designed for the analysis of modern mass spectra and incorporate multiple sources of information in the here proposed bioinformatics research project. Preliminary research results are promising: The project consortium consisting of the Proteomics Group at IMP Vienna and the Bioinformatics Research Group at FH OÖ (Campus Hagenberg) has already conducted successful joint research in the analysis of MS data: Identification rates comparable or even superior to Mascot, the current gold-standard, have been achieved using a first version of a scoring function designed by the proposing consortium. Encouraged by these preliminary research results, we are convinced that considering additional sources of information will further improve identification rates of mass spectra – therefore this project is dedicated to research on a combination of the following novel approaches: We plan to use machine learning techniques to analyze peptide elution times, fragmentation patterns and mass accuracy characteristics specific to the instrument; in addition, observed m/z values will be recalibrated based on the mass error of highly reliable identifications, and the remaining mass error with regard to the learned distribution will be incorporated into the scoring function. Sophisticated peak picking strategies will also be designed using machine learning. These improvements will help increase identification rates in challenging situations such as hybrid spectra and exhaustive searches for a wide range of post-translational modifications. The latter approach leads to exponentially growing search spaces and an accompanying drop in spectra identification rates because the information in MS spectra on its own is not sufficient to cope with the increased search space. Instead of applying brute force methods we plan to solve this problem using construction heuristics, i.e., evolutionary algorithms that realize intelligent search strategies for large numbers of unknown post-translational modifications based on a combination of database search and de novo identification. All research results achieved in this project will be published and made freely available to the bioinformatics and proteomics communities. Improving identification rates of peptides in general and of unknown modifications in particular will permit a deeper insight into the proteome; computer science shall thus form a new basis for finding answers to important medical and biological questions.	Medical Research Council	Research Grant	913705.0GBP
158	Professor Custovic Adnan	University of Edinburgh	2012-10-01	2017-10-01	Machine learning for computational science:statistical and formal modelling of biological systems	Computational modelling is changing the face of science. Many complex systems can be understood as embodied computational systems performing distributed computations on a massive scale. Biology is the discipline where these ideas find their most natural application: cells can be viewed as input/ output devices, with proteins and organelles behaving as finite state machines performing distributed computations inside the cell. This led to the influential framework of cell as computation, and the successful deployment of formal verification and analysis on models of biological systems. This paradigm shift in our understanding of biology has been possible due to the increasingly quantitative experimental techniques being developed in experimental biology. Formal modelling techniques, however, do not have mechanisms to directly include the information obtained from experimental observations in a statistically consistent way. This difficulty in relating the experimental and theoretical developments in biology is a central problem: without incorporating observations, it is extremely difficult to obtain reliable parametrisations of models. More importantly, it is impossible to assess the confidence of model predictions. This means that the central scientific task of falsifying hypotheses cannot be performed in a statistically meaningful way, and that it is very difficult to employ model predictions to rationally plan novel experiments.In this project we will build and develop machine learning tools for continuous time stochastic processes to obtain a principled treatment of the uncertainty at every step of the modelling pipeline. We will use and extend probabilistic programming languages to fully automate the inference tasks, and link to advanced modelling languages to allow formal analysis tools to be deployed in a data modelling framework. We will pursue twoapplications to fundamental problems in systems biology, guaranteeing impact on exciting scientific questions.	European Research Council	Starting Grant	1421944.0EUR
159	Dr Myers Nicholas	University of Oxford	2017-01-01	2021-01-01	Dynamic cortical networks for cognitive flexibility	Computational modelling is changing the face of science. Many complex systems can be understood as embodied computational systems performing distributed computations on a massive scale. Biology is the discipline where these ideas find their most natural application: cells can be viewed as input/ output devices, with proteins and organelles behaving as finite state machines performing distributed computations inside the cell. This led to the influential framework of cell as computation, and the successful deployment of formal verification and analysis on models of biological systems. This paradigm shift in our understanding of biology has been possible due to the increasingly quantitative experimental techniques being developed in experimental biology. Formal modelling techniques, however, do not have mechanisms to directly include the information obtained from experimental observations in a statistically consistent way. This difficulty in relating the experimental and theoretical developments in biology is a central problem: without incorporating observations, it is extremely difficult to obtain reliable parametrisations of models. More importantly, it is impossible to assess the confidence of model predictions. This means that the central scientific task of falsifying hypotheses cannot be performed in a statistically meaningful way, and that it is very difficult to employ model predictions to rationally plan novel experiments.In this project we will build and develop machine learning tools for continuous time stochastic processes to obtain a principled treatment of the uncertainty at every step of the modelling pipeline. We will use and extend probabilistic programming languages to fully automate the inference tasks, and link to advanced modelling languages to allow formal analysis tools to be deployed in a data modelling framework. We will pursue twoapplications to fundamental problems in systems biology, guaranteeing impact on exciting scientific questions.	Wellcome Trust	Sir Henry Wellcome Postdoctoral Fellowship	250000.0GBP
160	Dr Holt Tim	NHS Oxfordshire CCG	2019-04-01	2020-06-30	Early detection of bowel cancer in UK primary care	Computational modelling is changing the face of science. Many complex systems can be understood as embodied computational systems performing distributed computations on a massive scale. Biology is the discipline where these ideas find their most natural application: cells can be viewed as input/ output devices, with proteins and organelles behaving as finite state machines performing distributed computations inside the cell. This led to the influential framework of cell as computation, and the successful deployment of formal verification and analysis on models of biological systems. This paradigm shift in our understanding of biology has been possible due to the increasingly quantitative experimental techniques being developed in experimental biology. Formal modelling techniques, however, do not have mechanisms to directly include the information obtained from experimental observations in a statistically consistent way. This difficulty in relating the experimental and theoretical developments in biology is a central problem: without incorporating observations, it is extremely difficult to obtain reliable parametrisations of models. More importantly, it is impossible to assess the confidence of model predictions. This means that the central scientific task of falsifying hypotheses cannot be performed in a statistically meaningful way, and that it is very difficult to employ model predictions to rationally plan novel experiments.In this project we will build and develop machine learning tools for continuous time stochastic processes to obtain a principled treatment of the uncertainty at every step of the modelling pipeline. We will use and extend probabilistic programming languages to fully automate the inference tasks, and link to advanced modelling languages to allow formal analysis tools to be deployed in a data modelling framework. We will pursue twoapplications to fundamental problems in systems biology, guaranteeing impact on exciting scientific questions.	National Institute for Health Research (Department of Health)	Full Grant	78267.0GBP
161	Dr Wright Helen	University of Liverpool	2017-06-01	2022-05-31	Computational modelling and functional investigation of neutrophil activation and plasticity in inflammatory disease	Computational modelling is changing the face of science. Many complex systems can be understood as embodied computational systems performing distributed computations on a massive scale. Biology is the discipline where these ideas find their most natural application: cells can be viewed as input/ output devices, with proteins and organelles behaving as finite state machines performing distributed computations inside the cell. This led to the influential framework of cell as computation, and the successful deployment of formal verification and analysis on models of biological systems. This paradigm shift in our understanding of biology has been possible due to the increasingly quantitative experimental techniques being developed in experimental biology. Formal modelling techniques, however, do not have mechanisms to directly include the information obtained from experimental observations in a statistically consistent way. This difficulty in relating the experimental and theoretical developments in biology is a central problem: without incorporating observations, it is extremely difficult to obtain reliable parametrisations of models. More importantly, it is impossible to assess the confidence of model predictions. This means that the central scientific task of falsifying hypotheses cannot be performed in a statistically meaningful way, and that it is very difficult to employ model predictions to rationally plan novel experiments.In this project we will build and develop machine learning tools for continuous time stochastic processes to obtain a principled treatment of the uncertainty at every step of the modelling pipeline. We will use and extend probabilistic programming languages to fully automate the inference tasks, and link to advanced modelling languages to allow formal analysis tools to be deployed in a data modelling framework. We will pursue twoapplications to fundamental problems in systems biology, guaranteeing impact on exciting scientific questions.	Versus Arthritis	Career Development Fellowship	431863.0GBP
162	Professor Razavi Reza	King's College London	2009-10-01	2020-05-31	King's College London Medical Engineering Centre.	The King's College London Medical Engineering Centre aims to deliver word-class excellence in engineering and physical sciences by breaking down the barriers between physical sciences and biomedicine as well as facilitatingcloser partnership between academia and industry. Focusing on the important area of medical imaging the aim is also to extend clinical translation into outcome and efficacy studies to ensure wider dissemination of physical scienceresearch into clinical practice. The proposal has a number of key general initiatives including, a multi-disciplinary MSc and MRes programme, pump priming of joint academic industrial research initiatives and collaboration with luminary international research groups. The bulk of the proposal is six specific scientific programmes that bring together strong basic science with clinical translation in the setting of an Academic Health Science Centre. The programs are: 1- Robotic radiation-free imaging guidance system for ablation of arrhythmias; 2- Biophysical modeling of myocardial perfusion and coronary artery disease; 3- Simultaneous PET/MR imaging of atherosclerosis using novel molecular and cellular contrast agents; 4- A pretargeted nanoparticle-based platform for molecular imaging and targeted radionuclide therapy of cancer; 5-Rebuilding faces a programme of tissue engineering and 6- A machine learning approach to the analysis of neuroimaging data.	Wellcome Trust	Programme Grant	10200355GBP
163	Dr Kolehmainen Niina	Newcastle University	2020-05-01	2020-10-31	Understanding early life determinants and mechanisms to preventing life course multimorbidity	The King's College London Medical Engineering Centre aims to deliver word-class excellence in engineering and physical sciences by breaking down the barriers between physical sciences and biomedicine as well as facilitatingcloser partnership between academia and industry. Focusing on the important area of medical imaging the aim is also to extend clinical translation into outcome and efficacy studies to ensure wider dissemination of physical scienceresearch into clinical practice. The proposal has a number of key general initiatives including, a multi-disciplinary MSc and MRes programme, pump priming of joint academic industrial research initiatives and collaboration with luminary international research groups. The bulk of the proposal is six specific scientific programmes that bring together strong basic science with clinical translation in the setting of an Academic Health Science Centre. The programs are: 1- Robotic radiation-free imaging guidance system for ablation of arrhythmias; 2- Biophysical modeling of myocardial perfusion and coronary artery disease; 3- Simultaneous PET/MR imaging of atherosclerosis using novel molecular and cellular contrast agents; 4- A pretargeted nanoparticle-based platform for molecular imaging and targeted radionuclide therapy of cancer; 5-Rebuilding faces a programme of tissue engineering and 6- A machine learning approach to the analysis of neuroimaging data.	Medical Research Council	Research Grant	99268.0GBP
164	Dr Kolehmainen Niina	University of Oxford	2010-05-01	2015-05-01	From Software Verification to Everyware Verification	In the words of Adam Greenfield, ?the age of ubiquitous computing is here: a computing without computers, where information processing has diffused into everyday life, and virtually disappeared from view?. Conventional hardware and software has evolved into ?everyware' sensor-enabled electronic devices, virtually invisible and wirelessly connected on which we increasingly often rely for everyday activities and access to services such as banking and healthcare. The key component of ?everyware' is embedded software, continuously interacting with its environment by means of sensors and actuators. Ubiquitous computing must deal with the challenges posed by the complex scenario of communities of ?everyware', in presence of environmental uncertainty and resource limitations, while at the same time aiming to meet high-level expectations of autonomous operation, predictability and robustness. This calls for the use of quantitative measures, stochastic modelling, discrete and continuous dynamics and goal-driven approaches, which the emerging quantitative software verification is unable to address at present. The central premise of the proposal is that there is a need for a paradigm shift in verification to enable ?everyware' verification, which can be achieved through a model-based approach that admits discrete and continuous dynamics, the replacement of offline methods with online techniques such as machine learning, and the use of game-theoretic and planning techniques. The project will significantly advance quantitative probabilistic verification in new and previously unexplored directions. I will lead a team of researchers investigating the fundamental principles of ?everyware' verification, development of algorithms and prototype implementations, and experimenting with case studies. I will also provide continued scientific leadership in the area of ubiquitous computing.	European Research Council	Advanced Grant	2060360.0EUR
165	Dr Mirams Gary	University of Oxford	2014-02-01	2019-02-01	Improving assessment of drug-induced cardiac risk with mathematical electrophysiology models.	Drug-induced cardiac arrhythmia is a leading cause of withdrawal of drugs from the market, and risk of this (both real and perceived) is one of the leading causes of attrition during compound development. The earliest cardiac safety test consists of measuring hERG channel blockade, but its predictive power for human clinical pro-arrhythmic risk is limited. My novel approach is to use experimental data on multiple ion-channel screens for a large number of drugs. In order to integrate this info rmation I will use, and develop further, computational models of cardiac cells and tissue. I will also harness machine learning methods to quantify the predictive power of in-silico markers for safety test results and pro-arrhythmic risk. This work will require a re-calibration of cardiac electrophysiology models, and the design of optimal experiments, to establish the ion-channel conductances in different species and cell types as accurately as possible. I will also extend the existing basic models of drug/ion-channel interaction to capture more subtle, yet perhaps crucial, effects, such as heart rate-dependent blockade. Computational models including pro-arrhythmic risk factors such as age, gender and disease will be developed and utilised to improve the understanding and prediction of susceptibility to drug-induced arrhythmias.	Wellcome Trust	Sir Henry Dale Fellowship	534961.0GBP
166	Mr Dang Giang	University of Oxford	2017-09-01	2020-03-01	Forecasting dengue cases: Vietnam as a case study	Vector-borne infectious diseases continue to be a burden to Vietnam’s economy and population health. Understanding the spatiotemporal patterns of the disease transmission is thus vital for planning resources and targeting control measures. In this work, we propose a large-scale study on how different factors interact and contribute to the dynamics of dengue in Vietnam. We will employ multiple modelling techniques to analyse the dynamics of the disease and how to best predict future cases. We hypothesise that urbanisation has played a significant role in deriving the distribution of the Aedes mosquito and on dengue transmission, and will therefore incorporate factors about urbanisation gained from satellite images into predictive models. These models will use three main methods: statistical, mechanistic and machine learning. The key goal is to predict the number of new dengue cases as far as possible, and we will work in collaboration with those who use the forecasts to assess the most useful timeframe and accuracy. A particular aim will be the prediction of upcoming hotspots of dengue transmission so that hospitals can plan their resources. We will work to present the results clearly so they can best be used to help to reduce dengue burden in Vietnam.	Wellcome Trust	International Masters Fellowship	82952.0GBP
167	Professor Mills Nicholas	Edinburgh, University of	2017-09-01	2020-03-01	High-sensitivity cardiac troponin beyond the acute coronary syndrome	Vector-borne infectious diseases continue to be a burden to Vietnam’s economy and population health. Understanding the spatiotemporal patterns of the disease transmission is thus vital for planning resources and targeting control measures. In this work, we propose a large-scale study on how different factors interact and contribute to the dynamics of dengue in Vietnam. We will employ multiple modelling techniques to analyse the dynamics of the disease and how to best predict future cases. We hypothesise that urbanisation has played a significant role in deriving the distribution of the Aedes mosquito and on dengue transmission, and will therefore incorporate factors about urbanisation gained from satellite images into predictive models. These models will use three main methods: statistical, mechanistic and machine learning. The key goal is to predict the number of new dengue cases as far as possible, and we will work in collaboration with those who use the forecasts to assess the most useful timeframe and accuracy. A particular aim will be the prediction of upcoming hotspots of dengue transmission so that hospitals can plan their resources. We will work to present the results clearly so they can best be used to help to reduce dengue burden in Vietnam.	British Heart Foundation	International Masters Fellowship	1270434.0GBP
168	Professor Mills Nicholas	University of Sheffield	2015-08-01	2020-08-01	Distributed Algorithms for Optimal Decision-Making	This grant will develop and translate a unifying framework for optimal decision-theory, and observations of natural systems, to design distributed algorithms for decentralised decision-making. This will enable a technological step-change in techniques for controlling distributed systems, primarily demonstrated during the grant by decentralised control of robot swarms. These algorithms and associated methodology will also provide hypotheses and tools to change the way scientists think about and interrogate natural decision mechanisms, from intracellular regulatory networks, via neural decision circuits, to decision-making populations of animals. Specific objectives are: 1. Distributed value-sensitive decision-making: undertake optimality analyses of the applicant's existing decentralised decision-making algorithms based on observations of collective iterated voting-processes in honeybees, and extend these. 2. Distributed sampling and decision-making: design distributed mechanisms that implement optimal compromises between sampling information and making decisions based on that information. 3. Individual-confidence and distributed decision-making: translate machine learning theory to collective behaviour models, designing mechanisms in which weak decision-makers optimally combine their decisions to optimise group performance. 4. Optimal distributed decision-making in collective robotics: translate theory from objective 1 to 3 towards practical applications in artificial systems, demonstrated using collectively-deciding robots. 5. Development of tools for life scientists and validation of theoretical predictions in natural systems: interact with named collaborators to investigate identified decision mechanisms in single cells, in neural circuits, and in social groups. Develop accessible modelling tools to facilitate investigations by life scientists.	European Research Council	Consolidator Grant	1413705.0EUR
169	Dr Purandare Nitin	The University of Manchester	2005-10-01	2009-03-31	The human serum metabolome in health and disease	Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high- resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.	Biotechnology and Biological Sciences Research Council	LINK project	197915.0GBP
170	Prof. Yau Christopher	University of Birmingham	2017-10-09	2020-01-06	From single cells to populations: generalized pseudotime analysis to identify patient trajectories from cross-sectional data in cancer genomics	Many arguments, including those based on metabolic Control Analysis (MCA), indicate that the metabolome should be a particularly high-resolution method for amplifying and thereby reflecting changes in the levels of gene products, especially those which accompany disease pathogenesis. A key imperative is therefore to develop and industrialise the technology optimal for determining the serum metabolome reliably, to establish the disease-independent ranges of the many hundreds of metabolites that can be observed with such technology, to recognise the disease-independent changes that occur `simply as a function of diet, gender, age, therapeutic interventions and lifestyle, and to make these data for `normals available in a web-accessible manner. Such data will form the reference baseline that will allow prognostic and diagnostic biomarkers for disease progression to be identified reliably; this will show the high- resolution discrimination possible with metabolomic data for both diagnosis and treatment. In parallel, we shall contribute to the development of (i) robust data models for metabolome data, and (ii) machine learning methods of classification in which the `gold standard diagnosis may be imperfectly reliable. The deliverables will be both underpinning knowledge of the human serum metabolome (probably including the discovery of many novel metabolites) and the identification of candidate biomarkers for two diseases.	Medical Research Council	Research Grant	325589.0GBP
171	Professor Ananiadou Sophia	The University of Manchester	2009-09-01	2012-08-31	Automated Biological Event Extraction from the Literature for Drug Discovery	In establishing drug target confidence, it is essential to have evidence of the type of relationship between the target and key protein-bioprocesses. However, the primary starting point for target choice, and the context for interpretation of all pre-clinical observations is the literature. Text mining (TM) is ideally suited to support the discovery of reliable drug targets. But for TM systems to help researchers understand the role proteins play in biological processes, they have to extract, normalise and identify the context of complex relationships between genes, diseases and their underlying bioprocesses. Our TM techniques will recognise diverse surface forms in text describing bioprocesses and will link them with events and the proteins associated with them. Our methods are based on a combination of advanced semantic text mining (deep parsing, named entity recognition) and machine learning techniques, as we shall automatically identify events (involving proteins) such as decrease [in concentration], phosphorylation, ubiquitination, etc. Bioprocesses such as angiogenesis are composed of individual events described in the literature. We propose to identify these bioprocesses automatically and to link them with the associated events. A combination of kernel methods with knowledge resources and annotated texts (evaluated by biologists) will be used to automatically learn how bioprocesses underlying higher level processes are linked with which events. We shall concentrate on angiogenesis as an example. We shall thereby produce and make available a text mining service for researchers working in drug discovery. Both the software tools used for event extraction as well as the annotated texts used for training purposes will be made available. Co-funded by EPSRC under the RCUK Cross-Council Funding Agreement.	Biotechnology and Biological Sciences Research Council	Industrial (IPA)	288468.0GBP
172	Dr Flasche Stefan	London School of Hygiene & Tropical Medicine	2018-01-01	2023-01-01	Using mathematical modelling to re-think global pneumococcal immunisation strategies.	In establishing drug target confidence, it is essential to have evidence of the type of relationship between the target and key protein-bioprocesses. However, the primary starting point for target choice, and the context for interpretation of all pre-clinical observations is the literature. Text mining (TM) is ideally suited to support the discovery of reliable drug targets. But for TM systems to help researchers understand the role proteins play in biological processes, they have to extract, normalise and identify the context of complex relationships between genes, diseases and their underlying bioprocesses. Our TM techniques will recognise diverse surface forms in text describing bioprocesses and will link them with events and the proteins associated with them. Our methods are based on a combination of advanced semantic text mining (deep parsing, named entity recognition) and machine learning techniques, as we shall automatically identify events (involving proteins) such as decrease [in concentration], phosphorylation, ubiquitination, etc. Bioprocesses such as angiogenesis are composed of individual events described in the literature. We propose to identify these bioprocesses automatically and to link them with the associated events. A combination of kernel methods with knowledge resources and annotated texts (evaluated by biologists) will be used to automatically learn how bioprocesses underlying higher level processes are linked with which events. We shall concentrate on angiogenesis as an example. We shall thereby produce and make available a text mining service for researchers working in drug discovery. Both the software tools used for event extraction as well as the annotated texts used for training purposes will be made available. Co-funded by EPSRC under the RCUK Cross-Council Funding Agreement.	Wellcome Trust	Sir Henry Dale Fellowship	1097366.0GBP
173	Prof Boylan Geraldine	University College Cork	2018-01-08	2020-01-08	Development of a Neonatal Brain Health Index (DELPHI)	In establishing drug target confidence, it is essential to have evidence of the type of relationship between the target and key protein-bioprocesses. However, the primary starting point for target choice, and the context for interpretation of all pre-clinical observations is the literature. Text mining (TM) is ideally suited to support the discovery of reliable drug targets. But for TM systems to help researchers understand the role proteins play in biological processes, they have to extract, normalise and identify the context of complex relationships between genes, diseases and their underlying bioprocesses. Our TM techniques will recognise diverse surface forms in text describing bioprocesses and will link them with events and the proteins associated with them. Our methods are based on a combination of advanced semantic text mining (deep parsing, named entity recognition) and machine learning techniques, as we shall automatically identify events (involving proteins) such as decrease [in concentration], phosphorylation, ubiquitination, etc. Bioprocesses such as angiogenesis are composed of individual events described in the literature. We propose to identify these bioprocesses automatically and to link them with the associated events. A combination of kernel methods with knowledge resources and annotated texts (evaluated by biologists) will be used to automatically learn how bioprocesses underlying higher level processes are linked with which events. We shall concentrate on angiogenesis as an example. We shall thereby produce and make available a text mining service for researchers working in drug discovery. Both the software tools used for event extraction as well as the annotated texts used for training purposes will be made available. Co-funded by EPSRC under the RCUK Cross-Council Funding Agreement.	Wellcome Trust	Innovator Award	512574.0EUR
174	Dr Wallace Edward	University of Edinburgh	2018-01-19	2023-01-19	Dynamic regulation of mRNA processing in adapting fungi	In establishing drug target confidence, it is essential to have evidence of the type of relationship between the target and key protein-bioprocesses. However, the primary starting point for target choice, and the context for interpretation of all pre-clinical observations is the literature. Text mining (TM) is ideally suited to support the discovery of reliable drug targets. But for TM systems to help researchers understand the role proteins play in biological processes, they have to extract, normalise and identify the context of complex relationships between genes, diseases and their underlying bioprocesses. Our TM techniques will recognise diverse surface forms in text describing bioprocesses and will link them with events and the proteins associated with them. Our methods are based on a combination of advanced semantic text mining (deep parsing, named entity recognition) and machine learning techniques, as we shall automatically identify events (involving proteins) such as decrease [in concentration], phosphorylation, ubiquitination, etc. Bioprocesses such as angiogenesis are composed of individual events described in the literature. We propose to identify these bioprocesses automatically and to link them with the associated events. A combination of kernel methods with knowledge resources and annotated texts (evaluated by biologists) will be used to automatically learn how bioprocesses underlying higher level processes are linked with which events. We shall concentrate on angiogenesis as an example. We shall thereby produce and make available a text mining service for researchers working in drug discovery. Both the software tools used for event extraction as well as the annotated texts used for training purposes will be made available. Co-funded by EPSRC under the RCUK Cross-Council Funding Agreement.	Wellcome Trust	Sir Henry Dale Fellowship	1337768.0GBP
175	Dr Barry Caswell	University College London	2018-12-01	2023-11-30	Non-local computations in hippocampal circuits: neural mechanisms	None	Wellcome Trust	Senior Research Fellowship Basic	1670136.0GBP
176	Dr Poh Norman	University of Surrey	2015-09-01	2017-07-12	Modelling and Predicting CKD Progression	None	Medical Research Council	Research Grant	519780.0GBP
177	Professor Collier Nigel	University of Cambridge	2015-11-13	2018-11-28	PheneBank: automatic extraction and validation of a database of human phenotype-disease associations in the scientific literature	None	Medical Research Council	Research Grant	464013.0GBP
178	Dr Nirantharakumar Krishnarajah	University of Birmingham	2020-06-01	2020-11-30	Multimorbid Pregnancy: Determinants, Clusters, Consequences and Trajectories (MuM-PreDiCCT)	None	Medical Research Council	Research Grant	100396.0GBP
179	Professor Griffiths Christopher	The University of Manchester	2014-09-01	2021-05-31	MICA: Psoriasis Stratification to Optimise Relevant Therapy (PSORT)	None	Medical Research Council	Research Grant	5054543.0GBP
180	Professor Baralle Diana	University of Southampton	2017-12-01	2022-11-30	Translational genomics- maximising potential for NHS patient care.	None	National Institute for Health Research (Department of Health)	Full award	1870764.0GBP
181	Univ.Prof. Dr. STRASSER Ulrich	University of Innsbruck	2016-02-01	2019-01-31	Cryosphere Monitoring in the EUREGIO Region (CRYOMON-SciPro)	The cryosphere (here: snow, ice and glaciers) is the most important inter-seasonal water storage component in the Alps. Climate variability and climate change directly affects cryospheric parameters and processes related to the energy and water cycle, such as snow water equivalent, glacier mass balance or runoff. Accurate monitoring as well as understanding of such processes is still a field of scientific challenge and of utmost importance for hydropower production, agriculture, winter tourism and flood protection. Apart from direct observations, hydrological models are the most common approach to study cryospheric processes. However, particularly at larger scales (>10’000 km²), critical processes such as radiative transfer, snow albedo and the energy balance remain underdetermined due to missing spatially explicit data. Satellite remote sensing is a promising technology for generating spatially explicit information on snow for larger areas, but operational products are mostly limited to the detection of snow cover only. In view of this, the central idea of CRYOMON-SciPro is to exploit the complementary character of physically based hydrological modelling and improved satellite remote sensing products for monitoring key processes within the cryosphere by integrating both methods in an innovative approach (multi-level data fusion). The innovations expected to result from this project include: - An improved representation and understanding of the spatial and temporal dimension of key processes within the cryosphere at larger scales (> 10’00 km²) with a focus on the energy exchange (radiation, snow and ice albedo) and water cycle (mass balance, snow water equivalent, runoff) - A flexible data integration concept based on machine learning and pattern recognition techniques for a multi-level data fusion on the level of input, intermediate and output variables - The first-time application of latest ESA Sentinel 1 (radar) and 2 (optical) satellites for studying the cryosphere - The integration of data from new and innovative field measurement techniques (permanent terrestrial laser scanning, field spectrometry) CRYOMON-SciPro makes use of the EUREGIO region as a field laboratory for cryosphere research with well-instrumented test-sites, high data availability, good contact to authorities and climatological conditions representative of different Alpine zones. The results of the project will thus have a scientific value that is well beyond the EUREGIO region. CRYOMON-SciPro will form the nucleus of an Interregional Project Network (IPN) on cryosphere science with a complementary expertise of three key research institutions within the EUREGO region: - Hydroclimatological modelling and analysis of Cryosphere (Innsbruck University), - Applied Remote Sensing of Cryosphere (EURAC Bolzano), - Data driven modelling and machine learning approaches (Trento University). CRYOMON-SciPro is designed as a three year project, with three PhD-students and young postdoctoral researchers as funded stuff. A scheduled exchange program supports knowledge transfer and education of the young staff researchers.	Austrian Science Fund FWF	International Project Network Tyrol-Trentino	101682.0EUR
182	Mr Holdsworth Ed	Practical Control Limited	2016-01-01	2018-09-30	Speech Rehabilitation from Articulator Movement (SRAM)	The cryosphere (here: snow, ice and glaciers) is the most important inter-seasonal water storage component in the Alps. Climate variability and climate change directly affects cryospheric parameters and processes related to the energy and water cycle, such as snow water equivalent, glacier mass balance or runoff. Accurate monitoring as well as understanding of such processes is still a field of scientific challenge and of utmost importance for hydropower production, agriculture, winter tourism and flood protection. Apart from direct observations, hydrological models are the most common approach to study cryospheric processes. However, particularly at larger scales (>10’000 km²), critical processes such as radiative transfer, snow albedo and the energy balance remain underdetermined due to missing spatially explicit data. Satellite remote sensing is a promising technology for generating spatially explicit information on snow for larger areas, but operational products are mostly limited to the detection of snow cover only. In view of this, the central idea of CRYOMON-SciPro is to exploit the complementary character of physically based hydrological modelling and improved satellite remote sensing products for monitoring key processes within the cryosphere by integrating both methods in an innovative approach (multi-level data fusion). The innovations expected to result from this project include: - An improved representation and understanding of the spatial and temporal dimension of key processes within the cryosphere at larger scales (> 10’00 km²) with a focus on the energy exchange (radiation, snow and ice albedo) and water cycle (mass balance, snow water equivalent, runoff) - A flexible data integration concept based on machine learning and pattern recognition techniques for a multi-level data fusion on the level of input, intermediate and output variables - The first-time application of latest ESA Sentinel 1 (radar) and 2 (optical) satellites for studying the cryosphere - The integration of data from new and innovative field measurement techniques (permanent terrestrial laser scanning, field spectrometry) CRYOMON-SciPro makes use of the EUREGIO region as a field laboratory for cryosphere research with well-instrumented test-sites, high data availability, good contact to authorities and climatological conditions representative of different Alpine zones. The results of the project will thus have a scientific value that is well beyond the EUREGIO region. CRYOMON-SciPro will form the nucleus of an Interregional Project Network (IPN) on cryosphere science with a complementary expertise of three key research institutions within the EUREGO region: - Hydroclimatological modelling and analysis of Cryosphere (Innsbruck University), - Applied Remote Sensing of Cryosphere (EURAC Bolzano), - Data driven modelling and machine learning approaches (Trento University). CRYOMON-SciPro is designed as a three year project, with three PhD-students and young postdoctoral researchers as funded stuff. A scheduled exchange program supports knowledge transfer and education of the young staff researchers.	National Institute for Health Research (Department of Health)	Full Award	655678.0GBP
183	Professor Wood Stephen	University of Birmingham	2013-06-10	2018-12-31	Linear and non-linear brain changes over the transition to psychosis	The cryosphere (here: snow, ice and glaciers) is the most important inter-seasonal water storage component in the Alps. Climate variability and climate change directly affects cryospheric parameters and processes related to the energy and water cycle, such as snow water equivalent, glacier mass balance or runoff. Accurate monitoring as well as understanding of such processes is still a field of scientific challenge and of utmost importance for hydropower production, agriculture, winter tourism and flood protection. Apart from direct observations, hydrological models are the most common approach to study cryospheric processes. However, particularly at larger scales (>10’000 km²), critical processes such as radiative transfer, snow albedo and the energy balance remain underdetermined due to missing spatially explicit data. Satellite remote sensing is a promising technology for generating spatially explicit information on snow for larger areas, but operational products are mostly limited to the detection of snow cover only. In view of this, the central idea of CRYOMON-SciPro is to exploit the complementary character of physically based hydrological modelling and improved satellite remote sensing products for monitoring key processes within the cryosphere by integrating both methods in an innovative approach (multi-level data fusion). The innovations expected to result from this project include: - An improved representation and understanding of the spatial and temporal dimension of key processes within the cryosphere at larger scales (> 10’00 km²) with a focus on the energy exchange (radiation, snow and ice albedo) and water cycle (mass balance, snow water equivalent, runoff) - A flexible data integration concept based on machine learning and pattern recognition techniques for a multi-level data fusion on the level of input, intermediate and output variables - The first-time application of latest ESA Sentinel 1 (radar) and 2 (optical) satellites for studying the cryosphere - The integration of data from new and innovative field measurement techniques (permanent terrestrial laser scanning, field spectrometry) CRYOMON-SciPro makes use of the EUREGIO region as a field laboratory for cryosphere research with well-instrumented test-sites, high data availability, good contact to authorities and climatological conditions representative of different Alpine zones. The results of the project will thus have a scientific value that is well beyond the EUREGIO region. CRYOMON-SciPro will form the nucleus of an Interregional Project Network (IPN) on cryosphere science with a complementary expertise of three key research institutions within the EUREGO region: - Hydroclimatological modelling and analysis of Cryosphere (Innsbruck University), - Applied Remote Sensing of Cryosphere (EURAC Bolzano), - Data driven modelling and machine learning approaches (Trento University). CRYOMON-SciPro is designed as a three year project, with three PhD-students and young postdoctoral researchers as funded stuff. A scheduled exchange program supports knowledge transfer and education of the young staff researchers.	Medical Research Council	Research Grant	808010.0GBP
184	Dr Wright Helen	University of Liverpool	2017-06-01	2022-05-31	Computational modelling and functional investigation of neutrophil activation and plasticity in inflammatory disease	The cryosphere (here: snow, ice and glaciers) is the most important inter-seasonal water storage component in the Alps. Climate variability and climate change directly affects cryospheric parameters and processes related to the energy and water cycle, such as snow water equivalent, glacier mass balance or runoff. Accurate monitoring as well as understanding of such processes is still a field of scientific challenge and of utmost importance for hydropower production, agriculture, winter tourism and flood protection. Apart from direct observations, hydrological models are the most common approach to study cryospheric processes. However, particularly at larger scales (>10’000 km²), critical processes such as radiative transfer, snow albedo and the energy balance remain underdetermined due to missing spatially explicit data. Satellite remote sensing is a promising technology for generating spatially explicit information on snow for larger areas, but operational products are mostly limited to the detection of snow cover only. In view of this, the central idea of CRYOMON-SciPro is to exploit the complementary character of physically based hydrological modelling and improved satellite remote sensing products for monitoring key processes within the cryosphere by integrating both methods in an innovative approach (multi-level data fusion). The innovations expected to result from this project include: - An improved representation and understanding of the spatial and temporal dimension of key processes within the cryosphere at larger scales (> 10’00 km²) with a focus on the energy exchange (radiation, snow and ice albedo) and water cycle (mass balance, snow water equivalent, runoff) - A flexible data integration concept based on machine learning and pattern recognition techniques for a multi-level data fusion on the level of input, intermediate and output variables - The first-time application of latest ESA Sentinel 1 (radar) and 2 (optical) satellites for studying the cryosphere - The integration of data from new and innovative field measurement techniques (permanent terrestrial laser scanning, field spectrometry) CRYOMON-SciPro makes use of the EUREGIO region as a field laboratory for cryosphere research with well-instrumented test-sites, high data availability, good contact to authorities and climatological conditions representative of different Alpine zones. The results of the project will thus have a scientific value that is well beyond the EUREGIO region. CRYOMON-SciPro will form the nucleus of an Interregional Project Network (IPN) on cryosphere science with a complementary expertise of three key research institutions within the EUREGO region: - Hydroclimatological modelling and analysis of Cryosphere (Innsbruck University), - Applied Remote Sensing of Cryosphere (EURAC Bolzano), - Data driven modelling and machine learning approaches (Trento University). CRYOMON-SciPro is designed as a three year project, with three PhD-students and young postdoctoral researchers as funded stuff. A scheduled exchange program supports knowledge transfer and education of the young staff researchers.	Versus Arthritis	Career Development Fellowship	431863.0GBP
185	Univ.Prof. Dr. MAASS Wolfgang	Graz University of Technology	2003-10-07	2006-11-06	Computer Models for Biological Vision Systems	The cryosphere (here: snow, ice and glaciers) is the most important inter-seasonal water storage component in the Alps. Climate variability and climate change directly affects cryospheric parameters and processes related to the energy and water cycle, such as snow water equivalent, glacier mass balance or runoff. Accurate monitoring as well as understanding of such processes is still a field of scientific challenge and of utmost importance for hydropower production, agriculture, winter tourism and flood protection. Apart from direct observations, hydrological models are the most common approach to study cryospheric processes. However, particularly at larger scales (>10’000 km²), critical processes such as radiative transfer, snow albedo and the energy balance remain underdetermined due to missing spatially explicit data. Satellite remote sensing is a promising technology for generating spatially explicit information on snow for larger areas, but operational products are mostly limited to the detection of snow cover only. In view of this, the central idea of CRYOMON-SciPro is to exploit the complementary character of physically based hydrological modelling and improved satellite remote sensing products for monitoring key processes within the cryosphere by integrating both methods in an innovative approach (multi-level data fusion). The innovations expected to result from this project include: - An improved representation and understanding of the spatial and temporal dimension of key processes within the cryosphere at larger scales (> 10’00 km²) with a focus on the energy exchange (radiation, snow and ice albedo) and water cycle (mass balance, snow water equivalent, runoff) - A flexible data integration concept based on machine learning and pattern recognition techniques for a multi-level data fusion on the level of input, intermediate and output variables - The first-time application of latest ESA Sentinel 1 (radar) and 2 (optical) satellites for studying the cryosphere - The integration of data from new and innovative field measurement techniques (permanent terrestrial laser scanning, field spectrometry) CRYOMON-SciPro makes use of the EUREGIO region as a field laboratory for cryosphere research with well-instrumented test-sites, high data availability, good contact to authorities and climatological conditions representative of different Alpine zones. The results of the project will thus have a scientific value that is well beyond the EUREGIO region. CRYOMON-SciPro will form the nucleus of an Interregional Project Network (IPN) on cryosphere science with a complementary expertise of three key research institutions within the EUREGO region: - Hydroclimatological modelling and analysis of Cryosphere (Innsbruck University), - Applied Remote Sensing of Cryosphere (EURAC Bolzano), - Data driven modelling and machine learning approaches (Trento University). CRYOMON-SciPro is designed as a three year project, with three PhD-students and young postdoctoral researchers as funded stuff. A scheduled exchange program supports knowledge transfer and education of the young staff researchers.	Austrian Science Fund FWF	National Research Networks NFN	431863.0GBP
186	Univ.Prof. Dr. SCHMUTH Matthias	Medical University of Innsbruck	2019-05-01	2022-04-30	Biomolecular Analyses for Tailored Medicine in AcneiNversa (BATMAN)	Acne Inversa (AI) is a chronic inflammatory disease involving hair follicles that imposes a major physical and psychological burden on patients with significant costs for health systems. Genetic variants affecting different pathways result in wide spectrum of AI phenotypes and tracking gene variants is essential to design personalized treatments. The proposal aims to bring together medical, genetic, experimental and lifestyle data to create holistic health records (HHR), which will allow us to build a personalized model of each patient and to tailor specific treatments based on their personal characteristics. Research on animal or cellular models will be harnessed to validate hypotheses on genetic variants, generating useful information with immediate translational impact on patient stratification and therapeutic options, and also providing a wide-scale overview of previously identified and novel risk markers. DNA will be obtained from AI cases from 3 different locations in Europe. Data will be compiled from whole exome sequencing, whole genome genotyping SNPs arrays and transcriptomic signatures of hair follicle cells and novel mouse models. Genomic information will be merged with clinical evaluations and lifestyle data by advanced machine-learning and data mining algorithms. By the end of the project, our consortium intends to: • identify genetic variants associated with AI susceptibility, severity and response to treatment • design in vivo and in vitro models for investigations on the main biological pathways affected by AI and testing the impact of genetic variants on immune and cutaneous cell biology • produce an HHR to complement medical record by developing a smartphone application to remotely monitor the physical and psychological wellbeing of patients and advise them on physical activity and dietary and smoking habits • propose novel stratification methods that clinicians can use to assess severity, choose the therapy and follow the outcome	Austrian Science Fund FWF	02 International programmes	91230.13EUR
187	Dr Kinross James	Imperial College London	2018-12-01	2021-11-30	iEndoscope: Colonoscopy Using Rapid Evaporative Ionization Mass Spectrometry (REIMS) for precision phenotyping of colonic adenomas and early colorectal cancer.	Acne Inversa (AI) is a chronic inflammatory disease involving hair follicles that imposes a major physical and psychological burden on patients with significant costs for health systems. Genetic variants affecting different pathways result in wide spectrum of AI phenotypes and tracking gene variants is essential to design personalized treatments. The proposal aims to bring together medical, genetic, experimental and lifestyle data to create holistic health records (HHR), which will allow us to build a personalized model of each patient and to tailor specific treatments based on their personal characteristics. Research on animal or cellular models will be harnessed to validate hypotheses on genetic variants, generating useful information with immediate translational impact on patient stratification and therapeutic options, and also providing a wide-scale overview of previously identified and novel risk markers. DNA will be obtained from AI cases from 3 different locations in Europe. Data will be compiled from whole exome sequencing, whole genome genotyping SNPs arrays and transcriptomic signatures of hair follicle cells and novel mouse models. Genomic information will be merged with clinical evaluations and lifestyle data by advanced machine-learning and data mining algorithms. By the end of the project, our consortium intends to: • identify genetic variants associated with AI susceptibility, severity and response to treatment • design in vivo and in vitro models for investigations on the main biological pathways affected by AI and testing the impact of genetic variants on immune and cutaneous cell biology • produce an HHR to complement medical record by developing a smartphone application to remotely monitor the physical and psychological wellbeing of patients and advise them on physical activity and dietary and smoking habits • propose novel stratification methods that clinicians can use to assess severity, choose the therapy and follow the outcome	National Institute for Health Research (Department of Health)	Full Grant	1123767.0GBP
188	Dr Del Galdo Francesco	University of Leeds	2019-10-01	2021-09-30	An Artificial Intelligence enhanced, Mobile based Technology to Emposer patients in Collecting Visible Biomarkers -AIMTEC VB	Acne Inversa (AI) is a chronic inflammatory disease involving hair follicles that imposes a major physical and psychological burden on patients with significant costs for health systems. Genetic variants affecting different pathways result in wide spectrum of AI phenotypes and tracking gene variants is essential to design personalized treatments. The proposal aims to bring together medical, genetic, experimental and lifestyle data to create holistic health records (HHR), which will allow us to build a personalized model of each patient and to tailor specific treatments based on their personal characteristics. Research on animal or cellular models will be harnessed to validate hypotheses on genetic variants, generating useful information with immediate translational impact on patient stratification and therapeutic options, and also providing a wide-scale overview of previously identified and novel risk markers. DNA will be obtained from AI cases from 3 different locations in Europe. Data will be compiled from whole exome sequencing, whole genome genotyping SNPs arrays and transcriptomic signatures of hair follicle cells and novel mouse models. Genomic information will be merged with clinical evaluations and lifestyle data by advanced machine-learning and data mining algorithms. By the end of the project, our consortium intends to: • identify genetic variants associated with AI susceptibility, severity and response to treatment • design in vivo and in vitro models for investigations on the main biological pathways affected by AI and testing the impact of genetic variants on immune and cutaneous cell biology • produce an HHR to complement medical record by developing a smartphone application to remotely monitor the physical and psychological wellbeing of patients and advise them on physical activity and dietary and smoking habits • propose novel stratification methods that clinicians can use to assess severity, choose the therapy and follow the outcome	Versus Arthritis	Transitional Awards	89999.09GBP
189	Professor Razavi Reza	King's College London	2009-10-01	2020-05-31	King's College London Medical Engineering Centre.	The King's College London Medical Engineering Centre aims to deliver word-class excellence in engineering and physical sciences by breaking down the barriers between physical sciences and biomedicine as well as facilitatingcloser partnership between academia and industry. Focusing on the important area of medical imaging the aim is also to extend clinical translation into outcome and efficacy studies to ensure wider dissemination of physical scienceresearch into clinical practice. The proposal has a number of key general initiatives including, a multi-disciplinary MSc and MRes programme, pump priming of joint academic industrial research initiatives and collaboration with luminary international research groups. The bulk of the proposal is six specific scientific programmes that bring together strong basic science with clinical translation in the setting of an Academic Health Science Centre. The programs are: 1- Robotic radiation-free imaging guidance system for ablation of arrhythmias; 2- Biophysical modeling of myocardial perfusion and coronary artery disease; 3- Simultaneous PET/MR imaging of atherosclerosis using novel molecular and cellular contrast agents; 4- A pretargeted nanoparticle-based platform for molecular imaging and targeted radionuclide therapy of cancer; 5-Rebuilding faces a programme of tissue engineering and 6- A machine learning approach to the analysis of neuroimaging data.	Wellcome Trust	Programme Grant	10200355GBP
190	Professor Schumann Gunter	King's College London	2016-10-01	2021-09-30	Brain network based stratification of mental illness	To reduce the burden of mental disorders it is a formidable aim to identify widely applicable disease markers based on neural processes, which predict psychopathology and allow for targeted interventions. We will generate a neurobehavioural framework for stratification of psychopathology by characterising links between network properties of brain function and structure and reinforcement–related behaviours, which are fundamental components of some of the most prevalent mental disorders, major depression, alcohol use disorder and ADHD. We will assess if network configurations define subtypes within and if they correspond to comorbidity across these diagnoses. We will identify discriminative data modalities and characterize predictors of future psychopathology. To identify specific neurobehavioural clusters we will carry out precision phenotyping of 900 patients with major depression, ADHD and alcohol use disorders and 300 controls, which we will investigate with innovative deep machine learning methods derived from artifical intelligence research. Development of these methods will optimize exploitation of a wide range of assessment modalities, including functional and structural neuroimaging, cognitive, emotional as well as environmental measures. The neurobehavioural clusters resulting from this analysis will be validated in a longitudinal population-based imaging genomics cohort, the IMAGEN sample of over 2000 participants spanning the period from adolescence to adulthood and integrated with information generated from genomic and imaging-genomic meta-analyses of >300.000 individuals. By targeting specific neural processes the resulting stratification markers will serve as paradigmatic examples for a diagnostic classification, which is based upon quantifiable neurobiological measures, thus enabling targetted early intervention, identification of novel pharmaceutical targets and the establishment of neurobehaviourally informed endpoints for clinical trials.	European Research Council	Advanced Grant	3394215.0GBP
191	Dr Gonem Sherif	University of Nottingham	2020-01-01	2022-06-30	Detecting clinical deterioration in respiratory hospital patients using machine learning	To reduce the burden of mental disorders it is a formidable aim to identify widely applicable disease markers based on neural processes, which predict psychopathology and allow for targeted interventions. We will generate a neurobehavioural framework for stratification of psychopathology by characterising links between network properties of brain function and structure and reinforcement–related behaviours, which are fundamental components of some of the most prevalent mental disorders, major depression, alcohol use disorder and ADHD. We will assess if network configurations define subtypes within and if they correspond to comorbidity across these diagnoses. We will identify discriminative data modalities and characterize predictors of future psychopathology. To identify specific neurobehavioural clusters we will carry out precision phenotyping of 900 patients with major depression, ADHD and alcohol use disorders and 300 controls, which we will investigate with innovative deep machine learning methods derived from artifical intelligence research. Development of these methods will optimize exploitation of a wide range of assessment modalities, including functional and structural neuroimaging, cognitive, emotional as well as environmental measures. The neurobehavioural clusters resulting from this analysis will be validated in a longitudinal population-based imaging genomics cohort, the IMAGEN sample of over 2000 participants spanning the period from adolescence to adulthood and integrated with information generated from genomic and imaging-genomic meta-analyses of >300.000 individuals. By targeting specific neural processes the resulting stratification markers will serve as paradigmatic examples for a diagnostic classification, which is based upon quantifiable neurobiological measures, thus enabling targetted early intervention, identification of novel pharmaceutical targets and the establishment of neurobehaviourally informed endpoints for clinical trials.	Medical Research Council	Research Grant	158835.0GBP
192	Dr Hennequin Guillaume	Computational and Biological Learning Lab Department of Engineering University of Cambridge	2014-08-01	2016-01-31	Fast but not furious: rapid Bayesian inference in balanced cortical circuits	Sensory perception is fast in human, and nonetheless almost Bayes-optimal. I propose a theoretical approach to this puzzle in which circuit mechanisms for fast dynamics (which I have explored as part of my PhD thesis) are related to the speed of perception, for wich the computational aspects are cast in the framework of probabilistic Bayesian inference. The host laboratory has recently collected experimental evidence for a sampling-based representation of posterior uncertainty in the cortex. We are thus combining methods from machine learning and statistical physics to understand how inference via sampling can be performed through the collective dynamics of large ensembles of neurons. The project has expected outcomes both in the field of machine learning and in neuroscience.	Swiss National Science Foundation	Advanced Postdoc.Mobility	158835.0GBP
193	Dr Hennequin Guillaume	Department of Neurosurgery Stanford University	2017-11-01	2019-04-30	Behavioral Biomarker for Temporal Lobe Epilepsy	The mechanistic bases of temporal lobe epilepsy (TLE) are not well understood. A major impediment to progress towards evidence-based, rigorous preclinical, translational research into the TLE is that the experimental approach often requires labor-intensive 24/7 video-EEG monitoring of seizures and rests on the inherently subjective scoring of behavioral seizure phenotypes by human observers along the Racine scale.The aim of the proposed research is to explore the transformative potential of state of the art machine learning (ML)-assisted 3D video analysis to phenotype mice with acquired epilepsy in an automated and high-throughput manner (Aim 1) and to correlate TLE-specific neuronal circuit activity with subtle behavioral dynamics that are not captured by current technology (Aim 2). The central idea rests on the discovery that complex animal behaviors are structured in stereotyped modules (“syllables”) at sub-second timescales that are arranged according to specific rules (“grammar”) that can be detected without observer bias by a combined 3D imaging (Kinect™) and ML method. The ML-assisted 3D video analysis technology will be adopted for epilepsy research to test if there are TLE-specific behavioral syllables and associated transition probabilities that might not be discernable by a human observer. In addition, it will be determine if behavioral syllables in chronic TLE are invariant between animal models of TLE. In order to comply with current standards of research into TLE, the presence or absence of TLE will be determined using conventional 24/7 video-EEG (vEEG) recordings and analysis. Synchronous acquisition of vEEG and 3D video will be used to assess if aberrant neuronal network activity is associated with the expression of specific behavioral syllables. Intervene upon local seizures or interictal spikes using closed-loop optogenetics will help to verify if these TLE-specific behavioral syllables are related to the certain hippocampal ensemble activity.	Swiss National Science Foundation	Early Postdoc.Mobility	158835.0GBP
194	Dr Adams Rick	University College London	2018-07-31	2021-07-30	Finding psychosis subtypes using machine learning, clinical, genetic and multimodal imaging data	The mechanistic bases of temporal lobe epilepsy (TLE) are not well understood. A major impediment to progress towards evidence-based, rigorous preclinical, translational research into the TLE is that the experimental approach often requires labor-intensive 24/7 video-EEG monitoring of seizures and rests on the inherently subjective scoring of behavioral seizure phenotypes by human observers along the Racine scale.The aim of the proposed research is to explore the transformative potential of state of the art machine learning (ML)-assisted 3D video analysis to phenotype mice with acquired epilepsy in an automated and high-throughput manner (Aim 1) and to correlate TLE-specific neuronal circuit activity with subtle behavioral dynamics that are not captured by current technology (Aim 2). The central idea rests on the discovery that complex animal behaviors are structured in stereotyped modules (“syllables”) at sub-second timescales that are arranged according to specific rules (“grammar”) that can be detected without observer bias by a combined 3D imaging (Kinect™) and ML method. The ML-assisted 3D video analysis technology will be adopted for epilepsy research to test if there are TLE-specific behavioral syllables and associated transition probabilities that might not be discernable by a human observer. In addition, it will be determine if behavioral syllables in chronic TLE are invariant between animal models of TLE. In order to comply with current standards of research into TLE, the presence or absence of TLE will be determined using conventional 24/7 video-EEG (vEEG) recordings and analysis. Synchronous acquisition of vEEG and 3D video will be used to assess if aberrant neuronal network activity is associated with the expression of specific behavioral syllables. Intervene upon local seizures or interictal spikes using closed-loop optogenetics will help to verify if these TLE-specific behavioral syllables are related to the certain hippocampal ensemble activity.	Medical Research Council	Fellowship	292063.0GBP
195	Univ.Prof. Dipl.Ing. Dr. VINCZE Markus	Vienna University of Technology	2003-12-15	2006-11-14	Cognitive Vision - A Key Technology for Personal Assistance - Coordination Project	The mechanistic bases of temporal lobe epilepsy (TLE) are not well understood. A major impediment to progress towards evidence-based, rigorous preclinical, translational research into the TLE is that the experimental approach often requires labor-intensive 24/7 video-EEG monitoring of seizures and rests on the inherently subjective scoring of behavioral seizure phenotypes by human observers along the Racine scale.The aim of the proposed research is to explore the transformative potential of state of the art machine learning (ML)-assisted 3D video analysis to phenotype mice with acquired epilepsy in an automated and high-throughput manner (Aim 1) and to correlate TLE-specific neuronal circuit activity with subtle behavioral dynamics that are not captured by current technology (Aim 2). The central idea rests on the discovery that complex animal behaviors are structured in stereotyped modules (“syllables”) at sub-second timescales that are arranged according to specific rules (“grammar”) that can be detected without observer bias by a combined 3D imaging (Kinect™) and ML method. The ML-assisted 3D video analysis technology will be adopted for epilepsy research to test if there are TLE-specific behavioral syllables and associated transition probabilities that might not be discernable by a human observer. In addition, it will be determine if behavioral syllables in chronic TLE are invariant between animal models of TLE. In order to comply with current standards of research into TLE, the presence or absence of TLE will be determined using conventional 24/7 video-EEG (vEEG) recordings and analysis. Synchronous acquisition of vEEG and 3D video will be used to assess if aberrant neuronal network activity is associated with the expression of specific behavioral syllables. Intervene upon local seizures or interictal spikes using closed-loop optogenetics will help to verify if these TLE-specific behavioral syllables are related to the certain hippocampal ensemble activity.	Austrian Science Fund FWF	National Research Networks NFN	292063.0GBP
196	Dr Cole James	University College London	2019-10-07	2021-03-31	Modelling brain ageing using neuroimaging to improve brain health in older adults	The mechanistic bases of temporal lobe epilepsy (TLE) are not well understood. A major impediment to progress towards evidence-based, rigorous preclinical, translational research into the TLE is that the experimental approach often requires labor-intensive 24/7 video-EEG monitoring of seizures and rests on the inherently subjective scoring of behavioral seizure phenotypes by human observers along the Racine scale.The aim of the proposed research is to explore the transformative potential of state of the art machine learning (ML)-assisted 3D video analysis to phenotype mice with acquired epilepsy in an automated and high-throughput manner (Aim 1) and to correlate TLE-specific neuronal circuit activity with subtle behavioral dynamics that are not captured by current technology (Aim 2). The central idea rests on the discovery that complex animal behaviors are structured in stereotyped modules (“syllables”) at sub-second timescales that are arranged according to specific rules (“grammar”) that can be detected without observer bias by a combined 3D imaging (Kinect™) and ML method. The ML-assisted 3D video analysis technology will be adopted for epilepsy research to test if there are TLE-specific behavioral syllables and associated transition probabilities that might not be discernable by a human observer. In addition, it will be determine if behavioral syllables in chronic TLE are invariant between animal models of TLE. In order to comply with current standards of research into TLE, the presence or absence of TLE will be determined using conventional 24/7 video-EEG (vEEG) recordings and analysis. Synchronous acquisition of vEEG and 3D video will be used to assess if aberrant neuronal network activity is associated with the expression of specific behavioral syllables. Intervene upon local seizures or interictal spikes using closed-loop optogenetics will help to verify if these TLE-specific behavioral syllables are related to the certain hippocampal ensemble activity.	Medical Research Council	Fellowship	113938.0GBP
197	Dr Desrivieres Sylvane	King's College London	2019-09-01	2022-08-31	Establishing causal relationships between biopsychosocial predictors and correlates of eating disorders and their mediation by neural pathways	The mechanistic bases of temporal lobe epilepsy (TLE) are not well understood. A major impediment to progress towards evidence-based, rigorous preclinical, translational research into the TLE is that the experimental approach often requires labor-intensive 24/7 video-EEG monitoring of seizures and rests on the inherently subjective scoring of behavioral seizure phenotypes by human observers along the Racine scale.The aim of the proposed research is to explore the transformative potential of state of the art machine learning (ML)-assisted 3D video analysis to phenotype mice with acquired epilepsy in an automated and high-throughput manner (Aim 1) and to correlate TLE-specific neuronal circuit activity with subtle behavioral dynamics that are not captured by current technology (Aim 2). The central idea rests on the discovery that complex animal behaviors are structured in stereotyped modules (“syllables”) at sub-second timescales that are arranged according to specific rules (“grammar”) that can be detected without observer bias by a combined 3D imaging (Kinect™) and ML method. The ML-assisted 3D video analysis technology will be adopted for epilepsy research to test if there are TLE-specific behavioral syllables and associated transition probabilities that might not be discernable by a human observer. In addition, it will be determine if behavioral syllables in chronic TLE are invariant between animal models of TLE. In order to comply with current standards of research into TLE, the presence or absence of TLE will be determined using conventional 24/7 video-EEG (vEEG) recordings and analysis. Synchronous acquisition of vEEG and 3D video will be used to assess if aberrant neuronal network activity is associated with the expression of specific behavioral syllables. Intervene upon local seizures or interictal spikes using closed-loop optogenetics will help to verify if these TLE-specific behavioral syllables are related to the certain hippocampal ensemble activity.	Medical Research Council	Research Grant	507739.0GBP
198	Dr Droop Alastair	University of Leeds	2018-02-14	2019-04-07	Facilitating Deep Learning with Domain-Specific Knowledge	The mechanistic bases of temporal lobe epilepsy (TLE) are not well understood. A major impediment to progress towards evidence-based, rigorous preclinical, translational research into the TLE is that the experimental approach often requires labor-intensive 24/7 video-EEG monitoring of seizures and rests on the inherently subjective scoring of behavioral seizure phenotypes by human observers along the Racine scale.The aim of the proposed research is to explore the transformative potential of state of the art machine learning (ML)-assisted 3D video analysis to phenotype mice with acquired epilepsy in an automated and high-throughput manner (Aim 1) and to correlate TLE-specific neuronal circuit activity with subtle behavioral dynamics that are not captured by current technology (Aim 2). The central idea rests on the discovery that complex animal behaviors are structured in stereotyped modules (“syllables”) at sub-second timescales that are arranged according to specific rules (“grammar”) that can be detected without observer bias by a combined 3D imaging (Kinect™) and ML method. The ML-assisted 3D video analysis technology will be adopted for epilepsy research to test if there are TLE-specific behavioral syllables and associated transition probabilities that might not be discernable by a human observer. In addition, it will be determine if behavioral syllables in chronic TLE are invariant between animal models of TLE. In order to comply with current standards of research into TLE, the presence or absence of TLE will be determined using conventional 24/7 video-EEG (vEEG) recordings and analysis. Synchronous acquisition of vEEG and 3D video will be used to assess if aberrant neuronal network activity is associated with the expression of specific behavioral syllables. Intervene upon local seizures or interictal spikes using closed-loop optogenetics will help to verify if these TLE-specific behavioral syllables are related to the certain hippocampal ensemble activity.	Medical Research Council	Fellowship	286050.0GBP
199	Dr HUGHES DAVID	University of Liverpool	2017-11-15	2020-11-14	Variational Approximation Approaches for Efficient Clinical Predictions	The mechanistic bases of temporal lobe epilepsy (TLE) are not well understood. A major impediment to progress towards evidence-based, rigorous preclinical, translational research into the TLE is that the experimental approach often requires labor-intensive 24/7 video-EEG monitoring of seizures and rests on the inherently subjective scoring of behavioral seizure phenotypes by human observers along the Racine scale.The aim of the proposed research is to explore the transformative potential of state of the art machine learning (ML)-assisted 3D video analysis to phenotype mice with acquired epilepsy in an automated and high-throughput manner (Aim 1) and to correlate TLE-specific neuronal circuit activity with subtle behavioral dynamics that are not captured by current technology (Aim 2). The central idea rests on the discovery that complex animal behaviors are structured in stereotyped modules (“syllables”) at sub-second timescales that are arranged according to specific rules (“grammar”) that can be detected without observer bias by a combined 3D imaging (Kinect™) and ML method. The ML-assisted 3D video analysis technology will be adopted for epilepsy research to test if there are TLE-specific behavioral syllables and associated transition probabilities that might not be discernable by a human observer. In addition, it will be determine if behavioral syllables in chronic TLE are invariant between animal models of TLE. In order to comply with current standards of research into TLE, the presence or absence of TLE will be determined using conventional 24/7 video-EEG (vEEG) recordings and analysis. Synchronous acquisition of vEEG and 3D video will be used to assess if aberrant neuronal network activity is associated with the expression of specific behavioral syllables. Intervene upon local seizures or interictal spikes using closed-loop optogenetics will help to verify if these TLE-specific behavioral syllables are related to the certain hippocampal ensemble activity.	Medical Research Council	Fellowship	257981.0GBP
200	Dr Wright David	Queen's University of Belfast	2018-02-14	2021-02-13	Data driven public health approaches for diabetic retinopathy and age-related macular degeneration	None	Medical Research Council	Fellowship	279158.0GBP
201	Professor Lorincz Attila	University of London	2016-06-01	2019-05-31	Development of a highly accurate DNA methylation classifier for prevalent and incident cervical pre-cancer	Background:Cervical cancer, caused by persistent infection with high risk (hr) HPV affects ~500,000 women globally and causes ~260,000 deaths annually. Although HPV immunisation has been successfully implemented the level of protection expected from current vaccines will be quite substantially incomplete and it will take several decades to see an effect of the new and improved nonavalent vaccine. A pressing need to combat cervical cancer through improved screening remains far into the future. Highly sensitive hrHPV testing is likely to become the dominant primary screen. However, hrHPV infection is common and only a fraction of women are at risk of developing cervical cancer. 40% of hrHPV+ women are cytology negative and triage by proposed adjunctive immunostaining tests such as p16 (often in conjunction with ki67) are insufficient. A molecular test is needed to more accurately identify clinically significant HPV infection. Aims:We aim to develop a DNA methylation (DNAm) biomarker panel that has the potential to be an excellent triage tool for hrHPV+ women and may become an integral part of the primary screening test for preventing cervical cancer.Methods:We propose to measure genome-scale human DNAm as well as DNAm of predefined sites in HPV16, 18, 31 and 33 in a set of 350 hrHPV+ women with normal cytology. This will be the largest such study to date. First, we will measure DNAm with reduced representation bisulfite sequencing followed by machine learning using MS-SPCA to identify ~100 sites which consistently appear in the best ranking models. Then, the best sites will be further sifted by additional multivariate data modelling to provide us with a minimum number of required classifier sites. Finally, these selected sites, presumably 10-20 sites, will be validated in a second set of 200 well characterised cervical samples.How the results of this research will be used:We will develop a significantly improved risk classification tool to triage hrHPV+ women to colposcopy. The new classifier must come close to the high sensitivity of current hrHPV tests (90-95%) but deliver a substantially higher specificity and PPV (both ~70%) than current molecular reflex tests (30-40% and 40-50% respectively). This new algorithm would allow more efficient utilization of colposcopy services while hrHPV+ women negative for the triage classifier could be followed up at suitably frequent intervals to safely catch most, if not all, triage false negatives.	Cancer Research UK	CRC - Biomarker Project Award	279158.0GBP
202	Professor Lorincz Attila	Department of Genetics Stanford University School of Medicine	2017-10-01	2019-03-31	Dissecting the regulatory landscape of the human genome	A large number of genetic variants linked to phenotypic changes are located in the non-coding regions of the genome, suggesting that their effects are most likely the result of changes in gene expression regulation. Expression quantitative trait locus (eQTL) analyses allow finding of regulatory genetic variants, which are linked to changes in gene expression. However, the regulatory elements perturbed by the genetic variants must be identified to fully understand these mechanisms. I propose to develop a computational framework to predict enhancers and their target genes based on genome-wide chromatin and gene expression data in an eQTL study setting. First, a set of high-confidence enhancers in human lympoblastoid cell lines (LCL) will be created from published literature and available databases. A machine-learning algorithm will then be trained based on posttranslational histone modifications, transcription factor binding and DNA sequence conservation at these enhancers. This model will then be used to predict genome-wide enhancers in additional 47 LCLs. Furthermore, data on detected eQTLs and dynamics in gene expression and chromatin variability will be used to associate the predicted enhancers to target genes, thus deciphering the cis-regulatory network around eQTL genes and pinpointing candidate regulatory elements that are perturbed by genetic variants.	Swiss National Science Foundation	Early Postdoc.Mobility	279158.0GBP
203	Professor Lorincz Attila	University of Zurich	2015-05-01	2018-04-30	HIV-1 Transmission in Switzerland: viral transmission traits, superinfection and drug resistance	This translational research project is a continuation of SNF 130865. We aim to unravel traits of HIV transmission in three conceptually different situations of high clinical relevance: transmission leading to de novo infection (Aim 1), spread of HIV between infected individuals (conferred to as superinfection (SI); Aim 2) and transmission of drug resistant virus (Aim 3). The proposed studies are based on two renowned longitudinal studies with extensive biobanks: The Swiss HIV Cohort and the Zurich Primary HIV-1 infection study (ZPHI). AIM 1. Here we will determine whether specific genotypic and phenotypic viral traits of transmitted/founder viruses (T/F) exist. Characterization of T/F viruses, the earliest virus population that emerges in a newly infected individual, is of high interest as understanding their transmis-sion pathways may open avenues for prophylaxis and treatment. Current data on T/F virus features led to intriguing in-sights, but no definitive picture of genetic and phenotypic traits of T/F viruses has emerged. A limitation of previous studies was that features of T/F viruses had to be compared to unrelated viruses from chronically infected patients as transmitting individuals were in most cases not identified. Here using phylogenetically linked T/F viruses and their transmitters we will be able to study phylogenetically linked transmitter-recipient pairs. Thus far we identified 79 acutely HIV infected (recipients) and 107 chronically infected individuals with genetically related HIV strains which are their likely transmitters. Plasma samples close to estimated time of infection will be chosen for virus genomic analysis from transmitters and recipients to retrieve full length HIV-genomes employing Next Generation Sequencing (NGS) followed by haplotype reconstruction. T/F virus will be compared to variants present in the transmitter regarding, length, specific mutations, deletions/insertions, glycosylation sites, charge and frequency to define whether stochastic or non-stochastic transmission occurs. If genotypic traits are identified, gene transfer and mutagenesis experiments will be performed followed by phenotypic analysis to discern if indeed signifying characteristics of T/F viruses exist. To this end we will compare transmitter and T/F viruses for features implicated in shaping transmission including replication capacity, sensitivity to neutralizing antibodies and entry inhibitors, entry kinetics and interferon sensitivity. AIM 2: Here we will systematically explore frequency and risk factors for HIV-1 superinfection in the SHCS and ZPHI. A first phylogenetic screen based on pol sequences has revealed 150-312 potential SI cases. By retrieving longitudinal plasma samples before patients went on antiretroviral therapy we will seek to confirm SI by full length NGS and define approximate time points of SI events. In a detailed virus genomic analysis we aim to reconstruct haplotypes, study recombination events. A specific emphasis of our studies is on determining risk factors for acquiring SI using detailed patient, disease and host genomic data available. Viral fitness of initial circulating and superinfecting strains will be computed using machine learning techniques. To test whether SI is prevented by neutralizing antibody responses, those will be compared among superinfected and non-superinfected individuals. Finally, viral setpoint of cases and controls will be compared to determine the impact of SI on disease progression. AIM 3: Here we will investigate transmission of HIV-1 drug resistance mutations (TDR) within the SHCS, specifically focussing on the non-B compared to the subtype B epidemic, newer drugs, and reversion rates of TDR. We will study TDR prevalence over time for B and non- B subtypes in all drug naives and, specifically, in recently infected individuals. We will particularly focus on newer drugs. In patients with TDR we will determine reversion rates for specific TDR and fitness cost by taking into account the pol genotypic backbone indepen-dently of TDR. Furthermore, by performing phylogenetic analysis we will study whether there are differences in transmission dynamics between B- and non-B subtype TDR. This analysis will reveal whether TDR in the non-B subtypes is increasing as would be expected due to the recent, wide spread roll out of antiretroviral drugs in developing countries.	Swiss National Science Foundation	Project funding (special)	834000.0CHF
204	Professor Lorincz Attila	University of Berne	2017-10-01	2020-09-30	Stroke treatment goes personalized: Gaining added diagnostic yield by computer-assisted treatment selection (the STRAY-CATS project)	Stroke is the second most frequent cause of death and a major cause of disability in industrial countries: in patients who survive, stroke is frequently associated with high socioeconomic costs due to persistent disability. In clinical practice, advanced neuroimaging techniques are increasingly employed for a quick, reliable diagnosis and stratification for therapy. Tissue-at-risk estimation is frequently performed by MRI, with the infarct core being identified as an area of restricted diffusion on diffusion-weighted magnetic resonance imaging (DWI-MRI). The surrounding severly hypoperfused and potentially salvageable tissue tissue (i.e. the “penumbra”) is characterized by its delay in arterial transit time using perfusion-weighted MRI. The clinical image interpretation is routinely performed as a visual analysis done by neuroradiologists and/or neurological stroke experts.There is class I evidence that intravenous thrombolysis is a safe and effective therapy within an estimated time frame of 4.5 h after stroke onset. Very recently, four prospective studies demonstrated the superiority of mechanical thrombectomy in proximal vessel occlusions within a time frame of 6 h after stroke onset. Mechanical thrombectomy has thus become the treatment option of choice to achieve an early and sustained revascularization of proximally occluded vessels in specialized stroke centres2. The recent advent of mechanical thrombectomy has now raised an urgent question that needs to be answered: “can we predict advantageous tissue survival if mechanical thrombectomy is successfully applied compared to the natural course of disease in the presence of sufficient vs. insufficient collaterals?” The availability of a safe, reproducible and reliable information about the expected tissue salvage would allow not only to select patients that would benefit from mechanical thrombectomy, it would further allow to select patients for revascularization in a time window that exceeds 6 h if sufficient collateral flow enables sustained tissue survival. It is essential that indicators for further success of endovascular therapy can be calculated as soon after admission as possible, in order to save as much of the brain tissue as possible: '*time is brain*'. Computer-assisted and automated tissue segregation of the infarct core and salvageable penumbra using compound information from multimodal MRI offers a novel and robust standardized solution to this problem: while simple thresholding based on perfusion and diffusion imaging provide only a crude estimate of the tissue at risk, machine-learning approaches based on multimodal MR data overcome the limited accuracy of linear analyses. The proposed machine-learning approach incorporates thus two separate goals, i) to quantify penumbral collateral flow in the acute emergency setting and ii) to identify fingerprints that disentangle salvageable vs. non-salvageable tissue based on machine learning in a “big data” approach based on multiparametric imaging. We will provide means for i) by transforming the interpretation of image features into an interpretation of the underlying stationary flow field. This will allow us to combine information of all available MR imaging data, to quantify the collateral blood flow in the individual patient before and during intervention, and to compare 4D flow patterns at a population level. We will ii) build on our existing predictive models of stroke outcome, incorporating FLAIR and SWI maps and making use of the 'big data' that have been acquired during the last years in more than 1000 patients that underwent intraarterial thrombolysis or thrombectomy. Our overall goal is to investigate if, given sufficient training data, predictive maps of the infarction can improve on the current 'penumbra' concept as a tool for identifying patients who will have a favourable response to reperfusion therapy.	Swiss National Science Foundation	Project funding (Div. I-III)	474000.0CHF
205	Professor Lorincz Attila	Other Hospitals	2017-06-01	2017-07-31	NEWS - Newborn Early Warning Signs	Sepsis and meningitis are major causes of morbidity and mortality in theneonatal population. Despite vigilant clinical assessment of infants in the neonatalintensive care unit (NICU), diagnosis of sepsis and necrotizing enterocolitis (NEC) oftendoes not occur until an infant has significant hemodynamic compromise. Analysis of heart rae variability becomes more and more relevant in predicting outcomes over short and long time. Thus decription of apnea, bradycardia and desaturation (ABD-events) are of major importance to predict future potentially life-threatening events. Combining biomarker screening with predictive monitoring of physiologic markers for assessing risk of sepsis or NEC and for detection of early-stage illness could prevent progression to severe illness and shock.The neonatal intensive care unit's database for surveillance and storage ofcardiorespiratory parameters (Clinisoft®), allows us to prospectively characterizedevelopment of ABD-events over time.To examine the contribution of infection, acute and chronic inflammation, indevelopment of cardiorespiratory dysfunction there are ongoing prospective studies ofterm and premature infants at the NICU and paediatric intensive care units at the Karolinska University Hospital Solna. There cardiorespiratory recordings (Clinisoft®) of ABD- events are correlated with routine inflammatory biomarkers & medical records. We then further examine how physiomarkers (ABD-events) & biomarkers may act as Newborn Early Warning Signs (NEWS) for infection, inflammation and need for increased therapeutic interventions. In this project, our objective is to develop machine learning based methods for using low frequency labeled data (Clinisoft) in conjunction with the high frequency unlabeled data. Using pattern recognition we develop better physiomarkers (cardiorespiratory indexes) as NEWS that will help to predict short-term outcome and enable rapid therapeutic interventions.	Swiss National Science Foundation	International short research visits	6300.0CHF
206	Professor Lorincz Attila	University of Zurich	2017-08-01	2020-07-31	Radiomics as biomarker in multi-modality treatment of locally advanced non-small cell lung cancer	Radiomics is defined as the use of quantitative image analysis algorithms for calculation of a comprehensive set of image phenotypes also called image biomarkers. In contrast to conventional radiological image analysis, this methodology is a) objective and observer-independent and b) allows for a comprehensive description of all available information within medical images. Radiomics is therefore considered as a potential addition to the efforts currently undertaken in the field of precision medicine: to achieve a comprehensive analysis of the cancer phenotype in order to adjust the treatment to the patient-individual cancer characteristics.The aim of this project is to establish and to use comprehensive radiomics analyses in CT and FDG-PET images for outcome modelling. Prior to all image analyses, the radiomics algorithms will be evaluated regarding their robustness against non-standardized image acquisition and image reconstruction parameters. In addition, a model for quantification of loco-regional tumor spread will be established and its prognostic and predictive value will be compared to the conventional staging system. Machine learning algorithms will be established to correlate the large amount of radiomics biomarkers with clinical outcome parameters. The radiomics methodology will be tested based on images acquired within the randomized SAKK 16-00 study of multi-modality treatment for locally advanced non-small cell lung cancer (NSCLC). Radiomics biomarkers of pre-treatment CT and FDG-PET images, or post induction therapy images and changes of radiomics parameters between these two time points will be analysed. Radiomics parameters will be correlated with available histo-pathological tumor characteristics, primary (event-free survival) and secondary (overall survival, response to induction therapy, failure pattern, pulmonary toxicity) study endpoints.It is the hypothesis, that radiomics analyses will enable us to build better and more accurate prognostic models than the ones currently based on the TNM system as well as predictive models for identification of patients, who might benefit from neoadjuvant radiochemotherapy instead of neoadjuvant chemotherapy, only.The radiomics methodology and the machine learning algorithms will be evaluated in cancer patients suffering from NSCLC, a disease with currently very poor prognosis. It is planned to evaluate radiomics and the machine learning algorithms in other cancer sites as well. Additionally, the application of radiomics and the machine learning algorithms are not restricted to Oncology but can maybe successfully used in other medical questions.	Swiss National Science Foundation	Project funding (Div. I-III)	427191.0CHF
207	Professor Lorincz Attila	Brain Imaging and EEG Laboratory San Francisco VA Medical Center University of California, San Francisco	2016-03-01	2017-08-31	Neural Oscillations as Predictors of Psychosis	Aims:This study will attempt to predict the transition to psychosis on a landmark sample whose size is unmatched by any other study in the world. The study design of the North American Prodrome Longitudinal Study 2 (NAPLS-2) allows to investigate brain activity in clinical high risk (CHR) individuals longitudinally (repeatedly over time) and observe whether the anomalies identified at baseline worsen over time thereby leading to psychosis. Owing to the rich and varied set of experimental paradigms assessed in NAPLS 2, the proposed project will allow identifying neurophysiological paradigms that are most sensitive to conversion to psychosis. These paradigms could subsequently be selectively implemented in early detection clinics in Switzerland.Methodology:We will assess 242 Healthy Controls (HC), 199 CHR individuals who did not convert to psychosis (CHR-NP) and 72 CHR individuals who converted to psychosis (CHR-P) on three different EEG paradigms collected as part of NAPLS-2. NAPLS-2 is a consortium of eight universities (including prestigious universities such as Harvard, Yale and UCLA) with the largest CHR sample in the field and allows for advanced analyses and statistical power that cannot be performed in single-site studies. In particular, we will assess neural oscillations during the (1) visual and (2) auditory oddball paradigms, along with the (3) mismatch negativity response. Analyses will include: assessing neural oscillations using time-frequency analyses, phase locking value, and lagged phase synchronization across frontal and temporal regions. We will also make use of advanced machine learning algorithms (artificial intelligence) to identify multivariate patterns of brain activity predictive of transition to psychosis.Hypotheses:•Compared to both CHR-NP and HC, CHR-P individuals will demonstrate altered theta activity during both the P3a and P3b response during context-updating processes elicited by oddball target and novel stimuli.•During the P3a and P3b response, CHR-P individuals will have reduced frontal-temporal phase synchronization of theta neural oscillations compared to both CHR-NP and HC due to lower grey matter volume in both of these brain regions.•The auditory oddball paradigm will yield markers more predictive of conversion to psychosis than the visual oddball paradigm.•Compared to both CHR-NP and HC, CHR-P individuals will demonstrate lower early theta activity during the MMN response that will be associated with lower MMN amplitude.•Following the deviant tone, CHR-P individuals will have reduced frontal-temporal phase synchronization of theta neural oscillations compared to both CHR-P and HC due to lower grey matter volume in both of these brain regions.•In CHR-P and CHR-NP individuals, aberrant frontal-temporal phase-synchronization and lower theta oscillations following deviant stimuli in the MMN paradigm will be associated with psychotic symptoms and neuropsychological deficits.•Variation in brain structural (acquired via MRI) and functional data (acquired via EEG) among CHR individuals will be predictive of the individualized transition to psychosis.Expected value of the project:Currently in Switzerland, about 32,000 people are affected by schizophrenia and the average cost per patient has been estimated to be about EUR 39,000 in the year 2012 alone. Therefore, the early detection of psychosis potentially leading to enhanced treatment approaches will not only benefit the national economy in the long run but also help the patients and their families to get ready for a possible transition to psychosis. An early detection could allow for an early intervention, that is, the patients could undergo mild treatments such as a psychological intervention or administration of low-dosage antipsychotic medication. This study is unique as it is the first to investigate, repeatedly over time, a rich and varied set of experimental paradigms on the largest sample of CHR individuals worldwide. The main findings could help clarify whether brain abnormalities identified at baseline worsen overtime and allow for the individualized prediction of transition to psychosis.	Swiss National Science Foundation	Early Postdoc.Mobility	427191.0CHF
208	Professor Lorincz Attila	University of Zurich	2012-01-01	2015-07-31	Developing Bayesian Networks as a tool for Zoonotic Systems Epidemiology	A primary objective of many zoonotic epidemiological studies is to investigate hypothesized relationships between covariates of interest, and one or more outcome variables, through analyses of appropriate data. Typically, the biological, epidemiology and behavioural processes which generated this data are highly complex, resulting in multiple correlations/dependencies between covariates and also between outcome variables. Standard epidemiological and statistical approaches cannot adequately describe such inter-dependent multi-factorial relationships. Bayesian Network (BN) modelling is a generic and well established data mining/machine learning methodology, which has been demonstrated in other fields of study to be ideally suited to such analyses. The accessibility, however, of this methodology to epidemiologists is severely limited due to the sheer breadth and diversity of zoonotic epidemiological data, which is outside the established application areas of BN modelling. Two key challenges exist, one technical and one epidemiological. Firstly, no appropriate software exists for fitting the types of BN models necessary for analysing zoonotic epidemiological data, where complexities such as grouped/overdispersed/correlated observations are ubiquitous. This project will develop easy-to-use software to allow ready access to BN modelling to epidemiological practitioners, which is essential in order to make the crucial transition from merely a technically attractive methodology, to an approach which is actually used in practice. Secondly, to demonstrate and promote the use of this methodology in zoonotic epidemiology, relevant and high quality exemplar case studies will be developed showing objectively, situations in which BN models can offer the most added value, relative to existing standard statistical and epidemiological methods. Through the project's collaborators a diverse range of zoonotic data are available for analyses, including cross-sectional and longitudinal studies of antimicrobial resistance in Escherichia coli in farmed pigs in Canada, along with a wide range of different parasite field studies including Echinococcus multilocularis, Taenia, Mesocestoides, Uncinaria and Toxocara collected across eastern Europe and central Asia. This project will deliver a range of peer-reviewed publications comprising both methods orientated papers, and pathogen focused studies. The final deliverable of the project will be to provide a valuable addition to the quantitative skills base within veterinary science, by providing PhD level training in applied computational epidemiology of direct relevance to both zoonotic and animal disease research.	Swiss National Science Foundation	Project funding (Div. I-III)	193569.0CHF
209	Professor Lorincz Attila	University of Zurich	2017-09-01	2021-08-31	Predicting outcome after stroke: take a look at the other side	Despite improvements in primary prophylaxis and acute recanalization treatments, stroke remains one of the leading causes of death and disability worldwide. In order to achieve the best outcome possible for the individual patient, therapies have to be administered rapidly. However, the longer the time from symptom onset, the lower the efficacy and the higher the risk of treatment side effects. Although brain imaging is the mainstay of acute stroke diagnostics, current imaging strategies that aim to predict therapeutic success or failure in acute stroke patients remain insufficient. Here, we propose a novel prediction approach that is based on immediate and long-term vascular adaptations affecting the contralateral side of stroke. From data obtained through animal models of stroke and imaging in patients with proximal vessel occlusions, contralateral cerebral blood flow (CBF) appears to be particularly suited to predict clinical benefit from recanalization therapies. Contralateral CBF and related perfusion parameters may indicate the ability of the individual to withstand longer durations of ischemia. In the experimental part of the project, we will use a thrombin-injection stroke model that does not artificially impact collateral supply. Reperfusion will be achieved through intravenous injection of recombinant tissue plasminogen activator (rtPA). To assess CBF and ischemic tissue damage, repeated magnetic resonance imaging (MRI) combined with positron emission tomography (PET) will be performed during ischemia, after reperfusion and in the chronic phase of stroke along with behavioral assessments. In the same stroke model, processes influencing CBF on the microvascular level will be directly observed using advanced optical imaging methods. Structural adaptations of the microvascular bed as well as gene and protein expression profiles contralateral to stroke will be analyzed in brain samples. In the clinical part of the project, we will analyze stroke patient imaging data in relation to clinical outcome. In addition to a traditional region-of interest (ROI) based analysis we will apply a novel, “unbiased” machine learning algorithm to extract relevant outcome predictors from patient imaging data.Using a translational approach we aim for a mechanistic characterization of the concept of contralateral flow changes in stroke, and will generate and apply imaging predictors to clinical patient data. With the multidisciplinary study proposed here, I envision i) to deepen the understanding of the basic mechanisms regulating brain perfusion after ischemic stroke, and ii) to improve therapeutic decision-making in acute stroke patients.	Swiss National Science Foundation	SNSF Professorships	1509881.0CHF
210	Professor Lorincz Attila	Department of Neurosurgery Stanford University	2017-11-01	2019-04-30	Behavioral Biomarker for Temporal Lobe Epilepsy	The mechanistic bases of temporal lobe epilepsy (TLE) are not well understood. A major impediment to progress towards evidence-based, rigorous preclinical, translational research into the TLE is that the experimental approach often requires labor-intensive 24/7 video-EEG monitoring of seizures and rests on the inherently subjective scoring of behavioral seizure phenotypes by human observers along the Racine scale.The aim of the proposed research is to explore the transformative potential of state of the art machine learning (ML)-assisted 3D video analysis to phenotype mice with acquired epilepsy in an automated and high-throughput manner (Aim 1) and to correlate TLE-specific neuronal circuit activity with subtle behavioral dynamics that are not captured by current technology (Aim 2). The central idea rests on the discovery that complex animal behaviors are structured in stereotyped modules (“syllables”) at sub-second timescales that are arranged according to specific rules (“grammar”) that can be detected without observer bias by a combined 3D imaging (Kinect™) and ML method. The ML-assisted 3D video analysis technology will be adopted for epilepsy research to test if there are TLE-specific behavioral syllables and associated transition probabilities that might not be discernable by a human observer. In addition, it will be determine if behavioral syllables in chronic TLE are invariant between animal models of TLE. In order to comply with current standards of research into TLE, the presence or absence of TLE will be determined using conventional 24/7 video-EEG (vEEG) recordings and analysis. Synchronous acquisition of vEEG and 3D video will be used to assess if aberrant neuronal network activity is associated with the expression of specific behavioral syllables. Intervene upon local seizures or interictal spikes using closed-loop optogenetics will help to verify if these TLE-specific behavioral syllables are related to the certain hippocampal ensemble activity.	Swiss National Science Foundation	Early Postdoc.Mobility	1509881.0CHF
211	Professor Schumann Gunter	King's College London	2016-10-01	2021-09-30	Brain network based stratification of mental illness	To reduce the burden of mental disorders it is a formidable aim to identify widely applicable disease markers based on neural processes, which predict psychopathology and allow for targeted interventions. We will generate a neurobehavioural framework for stratification of psychopathology by characterising links between network properties of brain function and structure and reinforcement–related behaviours, which are fundamental components of some of the most prevalent mental disorders, major depression, alcohol use disorder and ADHD. We will assess if network configurations define subtypes within and if they correspond to comorbidity across these diagnoses. We will identify discriminative data modalities and characterize predictors of future psychopathology. To identify specific neurobehavioural clusters we will carry out precision phenotyping of 900 patients with major depression, ADHD and alcohol use disorders and 300 controls, which we will investigate with innovative deep machine learning methods derived from artifical intelligence research. Development of these methods will optimize exploitation of a wide range of assessment modalities, including functional and structural neuroimaging, cognitive, emotional as well as environmental measures. The neurobehavioural clusters resulting from this analysis will be validated in a longitudinal population-based imaging genomics cohort, the IMAGEN sample of over 2000 participants spanning the period from adolescence to adulthood and integrated with information generated from genomic and imaging-genomic meta-analyses of >300.000 individuals. By targeting specific neural processes the resulting stratification markers will serve as paradigmatic examples for a diagnostic classification, which is based upon quantifiable neurobiological measures, thus enabling targetted early intervention, identification of novel pharmaceutical targets and the establishment of neurobehaviourally informed endpoints for clinical trials.	European Research Council	Advanced Grant	3394215.0GBP
212	Professor Goh Vicky	King's College London	2015-07-01	2018-06-30	Evaluation of treatment response and resistance in metastatic renal cell cancer using integrated positron emission tomography/magnetic resonance imaging (PET/MRI) with 18F-Fluorodeoxyglucose (FDG)	Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.	Cancer Research UK	SC - Biomarker Project Award	3394215.0GBP
213	Dr Cole James	University College London	2019-10-07	2021-03-31	Modelling brain ageing using neuroimaging to improve brain health in older adults	Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.	Medical Research Council	Fellowship	113938.0GBP
214	Dr Desrivieres Sylvane	King's College London	2019-09-01	2022-08-31	Establishing causal relationships between biopsychosocial predictors and correlates of eating disorders and their mediation by neural pathways	Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.	Medical Research Council	Research Grant	507739.0GBP
215	Dr Droop Alastair	University of Leeds	2018-02-14	2019-04-07	Facilitating Deep Learning with Domain-Specific Knowledge	Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.	Medical Research Council	Fellowship	286050.0GBP
216	Dr HUGHES DAVID	University of Liverpool	2017-11-15	2020-11-14	Variational Approximation Approaches for Efficient Clinical Predictions	Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.	Medical Research Council	Fellowship	257981.0GBP
217	Dr Repapi Emmanouela	University of Oxford	2018-04-01	2022-10-28	Novel methods for the integration of high dimensional single cell proteomic and RNA data to understand cell populations in development and disease.	Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.	Medical Research Council	Fellowship	326906.0GBP
218	Dr Scholl Jacqueline	University of Oxford	2016-07-01	2021-06-30	Understanding the neural and cognitive mechanisms of attributional styles and credit assignment in depression	Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.	Medical Research Council	Fellowship	460831.0GBP
219	Ms Suel Esra	Imperial College London	2018-02-14	2021-02-13	Application of deep learning to heterogeneous open data for measuring urban environment and health	Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.	Medical Research Council	Fellowship	331573.0GBP
220	Dr Wu Honghan	University of Edinburgh	2018-02-14	2021-02-13	Deriving an actionable patient phenome from healthcare data	Background: Anti-angiogenic therapies improve outcome for metastatic renal cancer (mRCC), however, evaluation of response/resistance remains challenging. Computed tomography CT) performs poorly in differentiating response/non-response &amp; in evaluating its heterogeneity. We propose that integrated 18F-FDG PET/MRI is more sensitive than CT, by demonstrating physiological changes ahead of size change with these response biomarkers: 18F-FDG standardised uptake value, SUL (reflecting cellular metabolism), apparent diffusion coefficient, ADC (cellular volume) &amp; relative signal intensity, rSI (vascularisation). <br/><br/>Hypotheses: 1: 18F-FDG PET/MRI improves detection &amp; per-patient response classification of disease progression (PD) over CT at 12 (early) &amp; 24 weeks (late) on treatment with anti-angiogenic therapies; 2: 18F-FDG PET/MRI demonstrates the inter-lesional heterogeneity in response/non-response better than CT; 4: 18F-FDG PET/MRI is synergistic compared to PET or MRI or CT alone. <br/><br/>Aims: Primary: 1: To compare 18F-FDG PET/MRI to CT for response categorisation at 12/24 weeks on treatment; 2: To determine the sensitivity &amp; specificity of 18F-FDG PET/MRI (&amp; PET &amp; MRI separately) to detect PD after 12/24 weeks on treatment. <br/>Secondary: 1: To develop post-processing segmentation tools for whole body tumour burden; 2: To assess inter-lesional heterogeneity of imaging response/non-response; 3: To develop computational methods for predicting response using machine learning &amp; neural networks architectures.<br/><br/>Methods: Design: Pilot cohort study; Patients: 38 mRCCs, 1 or more measurable sites (2cm or greater), undergoing anti-angiogenic therapy. <br/><br/>Research imaging: 18F-FDG PET/MRI at 0/12/24 weeks. Comparator: CT at same time-points. Reference standard: Consensus panel disease status (clinical &amp; imaging by 36 weeks). <br/><br/>Imaging biomarkers:<br/>Target lesion(s): SULpeak, ADCmean, rSInormalised<br/>Exploratory whole body: WB-volume, SUVtotalvol ADCtotalvol, rSItotalvol. <br/><br/>Response categorisation (PD/non-PD): <br/>At 12/24 weeks CT RECIST 1.1 versus PET-MRI: PD - 30% increase in PET SULpeak, or ADCmean reduction 25%, or increase in rSI 40%. Sensitivity of PET/MRI to detect patients with PD calculated as percentage with PD at 12/36 weeks versus CT &amp; reference standard. <br/><br/>Using our results: 1)Improving response categorisation: Our ultimate aim is to improve therapeutic triage. Our next phase is to apply optimised 18F-FDG PET/MRI in a clinical trial to confirm clinical efficacy. 2)Validation of post-processing response tools: Our aim is to translate these tools into clinical practice. 3) Whole body response evaluation: This time-consuming read-out will be facilitated by novel machine learning methods. Our novel methods will be compared to human performance. If successful, these will be applicable to other cancers.	Medical Research Council	Fellowship	315181.0GBP
221	Dr Gordon George	University of Cambridge	2017-04-01	2019-03-31	Shedding light on cancer using nanoplasmonic metapixels	Background: Histopathology of biopsied tissue is the gold standard for diagnosis and grading of most cancers but the biopsy process is traumatic, introduces delays and is not practical for widespread early screening. Advanced optical imaging modalities hold promise for faster and less invasive methods of diagnosis, grading and precision treatment of cancer, particularly for early detection and surgical tumour margin identification. Examples include fluorescence imaging, Raman spectroscopy and polarimetric imaging. Each modality interrogates a different set of tissue properties and provides limited discrimination, but when many of these are combined, a multimodal approach, high diagnostic accuracy is achieved. This is typically achieved in a research setting, as several instruments can be applied sequentially, but in routine clinical practice this introduces unacceptable costs in terms of the instruments needed and procedure time. Further, movement occurring between changes of modality makes co-registration of images inaccurate, limiting the ability to overlay multiple tissue properties for augmented real-time vision. Aims: We propose to develop a unified multimodal optical imaging paradigm that can control all properties of light: wavelength, spatial profile, polarisation and phase. By activating different properties of illumination light and detector filtering in quick succession, we aim to develop a single system that can be dynamically reconfigured to implement many different types of imaging in a compact, low-cost fashion with high co-registration between modalities. Methods: We plan to construct a prototype using a technology we have recently developed: nanoplasmonic metapixels. A metapixel is a structure built from metal nanostructures that are small enough (~30nm) that their geometry controls the wavelength, polarisation and phase of light. We can make large arrays of metapixels that can be incorporated into existing projectors (for illumination) and image sensors. How the results will be used: A successful prototype demonstration will pave the way for numerous new research avenues and commercialisation opportunities. The reduced time and cost of multimodal imaging could enable larger screening programmes for improved early detection. The highly multimodal co-registered datasets collected could be used to develop advanced machine learning algorithms for improving cancer diagnosis. Finally, the multimodal datasets could be visualised using augmented reality systems for real-time diagnosis and improved surgical tumour resection. There is a strong base of UK companies working on the latter two aspects, creating significant commercialisation opportunities. Together, these advances will lead to improved diagnosis, treatment and hence outcome for cancer patients.	Cancer Research UK	Pioneer Award Committee - Pioneer Award	315181.0GBP
222	Dr Angelopoulou Anastasia	University of Westminster	2018-10-01	2020-03-31	Automated Diagnostic Toolkit for Dementia in Ageing Deaf Users of British Sign Language (BSL)	Background: Histopathology of biopsied tissue is the gold standard for diagnosis and grading of most cancers but the biopsy process is traumatic, introduces delays and is not practical for widespread early screening. Advanced optical imaging modalities hold promise for faster and less invasive methods of diagnosis, grading and precision treatment of cancer, particularly for early detection and surgical tumour margin identification. Examples include fluorescence imaging, Raman spectroscopy and polarimetric imaging. Each modality interrogates a different set of tissue properties and provides limited discrimination, but when many of these are combined, a multimodal approach, high diagnostic accuracy is achieved. This is typically achieved in a research setting, as several instruments can be applied sequentially, but in routine clinical practice this introduces unacceptable costs in terms of the instruments needed and procedure time. Further, movement occurring between changes of modality makes co-registration of images inaccurate, limiting the ability to overlay multiple tissue properties for augmented real-time vision. Aims: We propose to develop a unified multimodal optical imaging paradigm that can control all properties of light: wavelength, spatial profile, polarisation and phase. By activating different properties of illumination light and detector filtering in quick succession, we aim to develop a single system that can be dynamically reconfigured to implement many different types of imaging in a compact, low-cost fashion with high co-registration between modalities. Methods: We plan to construct a prototype using a technology we have recently developed: nanoplasmonic metapixels. A metapixel is a structure built from metal nanostructures that are small enough (~30nm) that their geometry controls the wavelength, polarisation and phase of light. We can make large arrays of metapixels that can be incorporated into existing projectors (for illumination) and image sensors. How the results will be used: A successful prototype demonstration will pave the way for numerous new research avenues and commercialisation opportunities. The reduced time and cost of multimodal imaging could enable larger screening programmes for improved early detection. The highly multimodal co-registered datasets collected could be used to develop advanced machine learning algorithms for improving cancer diagnosis. Finally, the multimodal datasets could be visualised using augmented reality systems for real-time diagnosis and improved surgical tumour resection. There is a strong base of UK companies working on the latter two aspects, creating significant commercialisation opportunities. Together, these advances will lead to improved diagnosis, treatment and hence outcome for cancer patients.	The Dunhill Medical Trust	Research Project & Programme Grants	65173.09GBP
223	Dr Patwardhan Ardan	European Bioinformatics Institute	2018-10-01	2023-09-30	EMDB: dealing with the cryo-EM data deluge	Background: Histopathology of biopsied tissue is the gold standard for diagnosis and grading of most cancers but the biopsy process is traumatic, introduces delays and is not practical for widespread early screening. Advanced optical imaging modalities hold promise for faster and less invasive methods of diagnosis, grading and precision treatment of cancer, particularly for early detection and surgical tumour margin identification. Examples include fluorescence imaging, Raman spectroscopy and polarimetric imaging. Each modality interrogates a different set of tissue properties and provides limited discrimination, but when many of these are combined, a multimodal approach, high diagnostic accuracy is achieved. This is typically achieved in a research setting, as several instruments can be applied sequentially, but in routine clinical practice this introduces unacceptable costs in terms of the instruments needed and procedure time. Further, movement occurring between changes of modality makes co-registration of images inaccurate, limiting the ability to overlay multiple tissue properties for augmented real-time vision. Aims: We propose to develop a unified multimodal optical imaging paradigm that can control all properties of light: wavelength, spatial profile, polarisation and phase. By activating different properties of illumination light and detector filtering in quick succession, we aim to develop a single system that can be dynamically reconfigured to implement many different types of imaging in a compact, low-cost fashion with high co-registration between modalities. Methods: We plan to construct a prototype using a technology we have recently developed: nanoplasmonic metapixels. A metapixel is a structure built from metal nanostructures that are small enough (~30nm) that their geometry controls the wavelength, polarisation and phase of light. We can make large arrays of metapixels that can be incorporated into existing projectors (for illumination) and image sensors. How the results will be used: A successful prototype demonstration will pave the way for numerous new research avenues and commercialisation opportunities. The reduced time and cost of multimodal imaging could enable larger screening programmes for improved early detection. The highly multimodal co-registered datasets collected could be used to develop advanced machine learning algorithms for improving cancer diagnosis. Finally, the multimodal datasets could be visualised using augmented reality systems for real-time diagnosis and improved surgical tumour resection. There is a strong base of UK companies working on the latter two aspects, creating significant commercialisation opportunities. Together, these advances will lead to improved diagnosis, treatment and hence outcome for cancer patients.	Wellcome Trust	Biomedical Resources Grant	1291058.0GBP
224	Dr Bunz Mercedes	King's College London	2019-01-15	2020-07-14	Public data, private collaborator: Will machine learning relocate medical knowledge?	Background: Histopathology of biopsied tissue is the gold standard for diagnosis and grading of most cancers but the biopsy process is traumatic, introduces delays and is not practical for widespread early screening. Advanced optical imaging modalities hold promise for faster and less invasive methods of diagnosis, grading and precision treatment of cancer, particularly for early detection and surgical tumour margin identification. Examples include fluorescence imaging, Raman spectroscopy and polarimetric imaging. Each modality interrogates a different set of tissue properties and provides limited discrimination, but when many of these are combined, a multimodal approach, high diagnostic accuracy is achieved. This is typically achieved in a research setting, as several instruments can be applied sequentially, but in routine clinical practice this introduces unacceptable costs in terms of the instruments needed and procedure time. Further, movement occurring between changes of modality makes co-registration of images inaccurate, limiting the ability to overlay multiple tissue properties for augmented real-time vision. Aims: We propose to develop a unified multimodal optical imaging paradigm that can control all properties of light: wavelength, spatial profile, polarisation and phase. By activating different properties of illumination light and detector filtering in quick succession, we aim to develop a single system that can be dynamically reconfigured to implement many different types of imaging in a compact, low-cost fashion with high co-registration between modalities. Methods: We plan to construct a prototype using a technology we have recently developed: nanoplasmonic metapixels. A metapixel is a structure built from metal nanostructures that are small enough (~30nm) that their geometry controls the wavelength, polarisation and phase of light. We can make large arrays of metapixels that can be incorporated into existing projectors (for illumination) and image sensors. How the results will be used: A successful prototype demonstration will pave the way for numerous new research avenues and commercialisation opportunities. The reduced time and cost of multimodal imaging could enable larger screening programmes for improved early detection. The highly multimodal co-registered datasets collected could be used to develop advanced machine learning algorithms for improving cancer diagnosis. Finally, the multimodal datasets could be visualised using augmented reality systems for real-time diagnosis and improved surgical tumour resection. There is a strong base of UK companies working on the latter two aspects, creating significant commercialisation opportunities. Together, these advances will lead to improved diagnosis, treatment and hence outcome for cancer patients.	Wellcome Trust	Seed Award in H&SS	57307.0GBP
225	Dr Hennequin Guillaume	Department of Electrical Engineering University of Cambridge	2013-02-01	2014-07-31	Fast but not furious: rapid Bayesian inference in balanced cortical circuits	Sensory perception is fast in human, and nonetheless almost Bayes-optimal. I propose a theoretical approach to this puzzle in which circuit mechanisms for fast dynamics (which I have explored as part of my PhD thesis) will be related to the speed of perception, for wich the computational aspects will be cast in the framework of probabilistic Bayesian inference. The working hypothesis will be that the brain represents posterior uncertainty via sampling, for which the host laboratory has found experimental evidence recently. We will thus combine methods from machine learning and statistical physics to understand how inference via sampling can be performed in the cortex. The project has expected outcomes both in the field of machine learning and in neuroscience.	Swiss National Science Foundation	Fellowships for prospective researchers	None
226	Dr Churcher Thomas	Imperial College London	2017-03-01	2020-02-29	Near-Infrared Spectroscopy: A One-Stop-Shop for Mosquito Epidemiological Monitoring?	Sensory perception is fast in human, and nonetheless almost Bayes-optimal. I propose a theoretical approach to this puzzle in which circuit mechanisms for fast dynamics (which I have explored as part of my PhD thesis) will be related to the speed of perception, for wich the computational aspects will be cast in the framework of probabilistic Bayesian inference. The working hypothesis will be that the brain represents posterior uncertainty via sampling, for which the host laboratory has found experimental evidence recently. We will thus combine methods from machine learning and statistical physics to understand how inference via sampling can be performed in the cortex. The project has expected outcomes both in the field of machine learning and in neuroscience.	Medical Research Council	Research Grant	643682.0GBP
227	Dr Churcher Thomas	UNIVERSITAIR MEDISCH CENTRUM UTRECHT	2013-08-01	2018-08-01	Intracranial COnnection with Neural Networks for Enabling Communication in Total paralysis	iCONNECT aims to give severely paralyzed people the means to communicate by merely imagining to talk or make hand gestures. Imagining specific movements generates spatiotemporal patterns of neuronal activity in the brain which I intend to record and decode with an intracranial Brain-Computer Interface (BCI) system. Many people suffer from partial or full loss of control over their body due to stroke, disease or trauma, and this will increase with population ageing. With both duration and quality of life beyond 60 increasing in the western world, more and more people will suffer from the consequences of function loss (mostly stroke) with the prospect of living for decades with the handicap, and will stand to benefit from restorative technology that has yet to be developed. I believe that functionality can be restored with brain implants. My goal is to develop a BCI that can interpret activity patterns on the surface of the brain in real-time. For this we need to discover how the brain codes for (imagined) actions, how codes can be captured and decoded and how an intracranial BCI system impacts on a user. I will use state of the art techniques (7 Tesla MRI and electrocorticography, ECoG) to explore brain codes and develop decoding strategies. Interactions between user and implanted device will be studied in paralyzed people. I will directly link decoded movements to animated visual feedback of the same body part, expecting to induce a feeling of ownership of the animation, and thereby a sense of actual movement. This research is only possible because of the latest developments in imaging of human brain activity, machine learning techniques, and micro systems technology. My lab is unique in bringing together all these techniques. Success of the project will lead to deeper understanding of how sensorimotor functions are represented in the human brain. The ability to ?read' the brain will add a new dimension to the field of neural prosthetics.	European Research Council	Advanced Grant	2498829.0EUR
228	Ms Bright Rebecca	Therapy Box Limited	2018-01-02	2019-01-01	ATLAS - AUTOMATED TRANSCRIPTION & LANGUAGE ANALYSIS SOFTWARE	ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.	National Institute for Health Research (Department of Health)	Full Award	149330.0GBP
229	Dr Barbosa da Silva Adriano	Queen Mary University of London	2018-03-22	2021-03-21	A translational data integration platform for the stratification of patients based on clinical, laboratory and magnetic resonances imaging	ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.	Medical Research Council	Fellowship	299527.0GBP
230	Dr Blackburn Ruth	University College London	2018-02-14	2021-11-13	Social contagion? : using data science to characterise the distribution and dispersion of health behaviours in adolescence	ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.	Medical Research Council	Fellowship	313016.0GBP
231	Dr Carson Jason	Swansea University	2018-02-14	2021-02-13	Non-invasive assessment and management of coronary heart disease - a translational, data driven approach	ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.	Medical Research Council	Fellowship	284559.0GBP
232	Dr Cole James	King's College London	2017-11-15	2019-10-06	Modelling brain ageing using neuroimaging to improve brain health in older adults	ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.	Medical Research Council	Fellowship	290658.0GBP
233	Dr Droop Alastair	Wellcome Trust Sanger Institute	2019-04-08	2021-02-13	Facilitating Deep Learning with Domain-Specific Knowledge	ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.	Medical Research Council	Fellowship	185651.0GBP
234	Dr Gurdasani Deepti	Wellcome Trust Sanger Institute	2018-02-14	2019-06-25	Predictive analytics of integrated genomic and clinical data using machine learning and complex statistical approaches	ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.	Medical Research Council	Fellowship	319508.0GBP
235	Dr Hall Benjamin	University of Cambridge	2018-12-01	2021-11-30	Deciphering and targeting cancer metabolism using executable models	ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.	Medical Research Council	Research Grant	344485.0GBP
236	Dr Hassan Lamiece	The University of Manchester	2018-02-14	2021-02-13	Social listening: Applying natural language processing methods to social media data to yield actionable analytics for health and care services	ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.	Medical Research Council	Fellowship	300577.0GBP
237	Dr Lumbers Richard	University College London	2018-02-14	2021-02-13	'Unlocking therapeutic innovation in heart failure through genomic data science	ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.	Medical Research Council	Fellowship	313017.0GBP
238	Professor Tchanturia Kate	King's College London	2017-10-09	2019-10-08	The Triple A study (Adolescents with Anorexia and Autism): A search for biomarkers	ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.	Medical Research Council	Research Grant	181988.0GBP
239	Professor Tchanturia Kate	King's College London	2019-10-01	2022-09-30	BiomaRkers for AnorexIa NErvosa and autism spectrum Disorders- longitudinal study	ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.	Medical Research Council	Research Grant	496884.0GBP
240	Dr Verity Robert	Imperial College London	2016-04-01	2019-03-31	Genetic data as a signal of changing malaria transmission	ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.	Medical Research Council	Fellowship	281369.0GBP
241	Dr Wright David	Queen's University of Belfast	2018-02-14	2021-02-13	Data driven public health approaches for diabetic retinopathy and age-related macular degeneration	ATLAS will be an app that speech and language therapists can use to screen children for developmental language disorders. It is very important that developmental language disorders are detected because providing the right help from speech and language therapists as early as possible is shown to make a signficant impact in that child's potential to succeed at school and later life. We know that the benefit of providing enhanced speech and language therapy for all children aged 6 to 10 who currently have a developmental language disorder exceeds the cost of therapy by &pound;741.8million. The challenge at the moment is that speech and language therapists do not have sufficient time to complete the screening (30 minutes) and then the transcription and analysis (90 minutes) that is needed to decide on the best therapy plan. Our app will use automated audio and language analysis tools built into the app, along with machine learning frameworks. Our project will look at which of these, or which combination of these tools, is best to detect a developmental language disorder. ATLAS will be built and trained using a very large dataset of recordings taken from the Avon Longitudinal Study of Parents and Children so that we can compare whether the technology used is comparable to the analysis done on those recordings previously by speech and language therapists. herapy Box has an established customer base for the app. It is the market leader in speech and language therapy apps; having had 197,500 apps downloaded from the iTunes AppStore across its own brand apps since 2011. The team includes the technical and commercial experts at Therapy Box - who bring clinical, technical and commercial know how to delivering this project. The team is lead by Rebecca Bright MBE who is a speech and language therapist and since 2011 has worked to deliver specialist apps to for unmet communication needs. Her co-founder Swapnil Gadgil has a technical and commercial focus and oversees the research and development of the latest technologies for the company's app range and for apps for clients such as the United Nations and universities around the world. Dr Yvonne Wren is Director of Research at Bristol Speech and Language Therapy Research Unit. Her research has focused on speech development and disorder in children and has included the development and trialing of software for use in speech and language therapy intervention. Professor Steve Renals and Dr. Korin Richmond from the University of Edinburgh have a wealth of experience in applying machine learning methods to speech data.	Medical Research Council	Fellowship	279158.0GBP
242	Professor Lorincz Attila	University of London	2016-06-01	2019-05-31	Development of a highly accurate DNA methylation classifier for prevalent and incident cervical pre-cancer	Background:Cervical cancer, caused by persistent infection with high risk (hr) HPV affects ~500,000 women globally and causes ~260,000 deaths annually. Although HPV immunisation has been successfully implemented the level of protection expected from current vaccines will be quite substantially incomplete and it will take several decades to see an effect of the new and improved nonavalent vaccine. A pressing need to combat cervical cancer through improved screening remains far into the future. Highly sensitive hrHPV testing is likely to become the dominant primary screen. However, hrHPV infection is common and only a fraction of women are at risk of developing cervical cancer. 40% of hrHPV+ women are cytology negative and triage by proposed adjunctive immunostaining tests such as p16 (often in conjunction with ki67) are insufficient. A molecular test is needed to more accurately identify clinically significant HPV infection. Aims:We aim to develop a DNA methylation (DNAm) biomarker panel that has the potential to be an excellent triage tool for hrHPV+ women and may become an integral part of the primary screening test for preventing cervical cancer.Methods:We propose to measure genome-scale human DNAm as well as DNAm of predefined sites in HPV16, 18, 31 and 33 in a set of 350 hrHPV+ women with normal cytology. This will be the largest such study to date. First, we will measure DNAm with reduced representation bisulfite sequencing followed by machine learning using MS-SPCA to identify ~100 sites which consistently appear in the best ranking models. Then, the best sites will be further sifted by additional multivariate data modelling to provide us with a minimum number of required classifier sites. Finally, these selected sites, presumably 10-20 sites, will be validated in a second set of 200 well characterised cervical samples.How the results of this research will be used:We will develop a significantly improved risk classification tool to triage hrHPV+ women to colposcopy. The new classifier must come close to the high sensitivity of current hrHPV tests (90-95%) but deliver a substantially higher specificity and PPV (both ~70%) than current molecular reflex tests (30-40% and 40-50% respectively). This new algorithm would allow more efficient utilization of colposcopy services while hrHPV+ women negative for the triage classifier could be followed up at suitably frequent intervals to safely catch most, if not all, triage false negatives.	Cancer Research UK	CRC - Biomarker Project Award	279158.0GBP
243	Dr Braga Rodrigo	Imperial College London	2014-11-01	2018-10-31	Local functional architecture and individual differences in cognitive and clinical states.	Functional connectivity within and between large-scale brain networks is disrupted in a range of mental disorders. Current network-based explanations of mental disorders have led to ambiguous conclusions, with different clinical conditions being associated with disruption of the same functional networks. My recent work has shown that heteromodal regions of the cortex can be decomposed to reveal a 'local functional architecture' (LFA) of separable subregions with distinct activation timecourses. Exploring this deeper LFA-level could disambiguate the neural basis of a range of mental illnesses. I will develop a novel analysis technique for the study of individual differences in brain activity at the LFA-level. My first goal will be to optimise a group-wise analysis technique I have developed (2,3) for use within individual subjects. I will apply the technique to large fMRI databases to explore to what extent LFA-level features such as subregion size and functional connectivity with ot her brain structures are conserved across the population. I will then use machine-learning techniques to test which LFA features relate to individual variability in a range of cognitive and personality inventories. Having identified candidate LFA features, I will test for group-wise differences in LFA properties in two psychiatric populations, schizophrenia and unipolar depression.	Wellcome Trust	Sir Henry Wellcome Postdoctoral Fellowship	250000.0GBP
244	Prof Noble Alison	University of Oxford	2016-11-01	2021-10-31	Perception Ultrasound by Learning Sonographic Experience	PULSE will develop a new generation of ultrasound imaging capabilities to revolutionize the use of this low-cost and portable imaging technology across clinical medicine worldwide. The greatest barrier to the universal implementation of ultrasound (US) in clinical medicine today is the need to train sonographers to the highest level to ensure diagnostic images are of consistently high quality and fit for purpose. Unfortunately, the non-expert finds US images very difficult to interpret by eye alone. Perception Ultrasound by Learning Sonographic Experience (PULSE) is an innovative inter-disciplinary project designed to eliminate the need for highly skilled operators of the technology. It is motivated by the observation that sonographers find it easier to interpret their own scans than review those taken by others. The innovation in PULSE is to apply the latest ideas from machine learning and computer vision to build, from real world training video data, computational models that describe how an expert sonographer performs a diagnostic study of a subject from multiple perceptual cues. Novel machine-learning based computational models will be derived based on probe and eye motion tracking, image processing, and knowledge of how to interpret real-world clinical images and videos acquired to a standardised protocol. By building models that more closely mimic how a human makes decisions from US images we believe we will build considerably more powerful assistive interpretation methods than have previously been possible from still US images and videos alone. Software demonstrators will be developed and evaluated on real world obstetric US data in collaboration with clinical experts and novices to demonstrate the new approach and its potential to move routine US scanning services from hospitals into the community which would have clear economic, healthcare and social benefits across Europe and beyond.	European Research Council	Advanced Grant	2462015.0EUR
245	Professor Leech Robert	King's College London	2018-04-01	2020-09-30	A novel adaptive sampling technique for mapping brain function	PULSE will develop a new generation of ultrasound imaging capabilities to revolutionize the use of this low-cost and portable imaging technology across clinical medicine worldwide. The greatest barrier to the universal implementation of ultrasound (US) in clinical medicine today is the need to train sonographers to the highest level to ensure diagnostic images are of consistently high quality and fit for purpose. Unfortunately, the non-expert finds US images very difficult to interpret by eye alone. Perception Ultrasound by Learning Sonographic Experience (PULSE) is an innovative inter-disciplinary project designed to eliminate the need for highly skilled operators of the technology. It is motivated by the observation that sonographers find it easier to interpret their own scans than review those taken by others. The innovation in PULSE is to apply the latest ideas from machine learning and computer vision to build, from real world training video data, computational models that describe how an expert sonographer performs a diagnostic study of a subject from multiple perceptual cues. Novel machine-learning based computational models will be derived based on probe and eye motion tracking, image processing, and knowledge of how to interpret real-world clinical images and videos acquired to a standardised protocol. By building models that more closely mimic how a human makes decisions from US images we believe we will build considerably more powerful assistive interpretation methods than have previously been possible from still US images and videos alone. Software demonstrators will be developed and evaluated on real world obstetric US data in collaboration with clinical experts and novices to demonstrate the new approach and its potential to move routine US scanning services from hospitals into the community which would have clear economic, healthcare and social benefits across Europe and beyond.	Medical Research Council	Research Grant	456799.0GBP
246	Dipl.Ing. Dr. KRANNER Gerhard	Viscovery Software GmbH	2018-07-01	2021-06-30	SystemMedicine clinical decision support for COPD patients	Chronic obstructive pulmonary disease (COPD) claimed 3.2 million lives in 2015, making it the third cause of death worldwide. It is predicted to increase in coming years due to aging populations and thus constitutes an enormous socio-economic burden. Existing assessment strategies neglect the complex, multi-component, and heterogeneous pathophysiology, as well as manifold comorbidities (cardiovascular, metabolic etc.). Therefore, improved COPD diagnosis and classification constitutes an urgent medical need for improved and personalized prevention measures and treatments strategies. The main aim of our proposal is to develop a tool that will enable effective preventive measures and personalize treatment strategies for COPD. This transnational and interdisciplinary project combines clinical scientists, experimentalists, computational and systems biology researchers, as well as a medium sized company. We will develop a systems medicine model of COPD constructed on (i) machine learning clustering of two comprehensive patient cohorts (COSYCONET, CIRO) providing long-term clinical observations, systematic outcome evaluation, biomaterial collections, multiple laboratory measurements, and extensive imaging data of more than 6,000 patients, complemented by (ii) an iterative systems biology framework of modeling and experimental analysis. Based on this multi-scale systems medicine model, we will generate a novel Clinical Decision Support (CDS) software that can be implemented in the existing IT infrastructure of hospitals and private practices for routine application. As a prototypic demonstrator of applied systems medicine, our tool will enable i) individual and comprehensive treatment and prevention measures for COPD patients ii) significant reductions of socio-economic costs due to less mortality and disability. iii) novel insights in the dysregulation of metabolism, immunology and aging in COPD from the underlying model.	Austrian Science Fund FWF	Research Grant	246692.36EUR
247	Professor Pridmore Tony	University of Nottingham	2018-09-01	2022-08-31	PhenomUK - Crop Phenotyping: from Sensors to Knowledge	Chronic obstructive pulmonary disease (COPD) claimed 3.2 million lives in 2015, making it the third cause of death worldwide. It is predicted to increase in coming years due to aging populations and thus constitutes an enormous socio-economic burden. Existing assessment strategies neglect the complex, multi-component, and heterogeneous pathophysiology, as well as manifold comorbidities (cardiovascular, metabolic etc.). Therefore, improved COPD diagnosis and classification constitutes an urgent medical need for improved and personalized prevention measures and treatments strategies. The main aim of our proposal is to develop a tool that will enable effective preventive measures and personalize treatment strategies for COPD. This transnational and interdisciplinary project combines clinical scientists, experimentalists, computational and systems biology researchers, as well as a medium sized company. We will develop a systems medicine model of COPD constructed on (i) machine learning clustering of two comprehensive patient cohorts (COSYCONET, CIRO) providing long-term clinical observations, systematic outcome evaluation, biomaterial collections, multiple laboratory measurements, and extensive imaging data of more than 6,000 patients, complemented by (ii) an iterative systems biology framework of modeling and experimental analysis. Based on this multi-scale systems medicine model, we will generate a novel Clinical Decision Support (CDS) software that can be implemented in the existing IT infrastructure of hospitals and private practices for routine application. As a prototypic demonstrator of applied systems medicine, our tool will enable i) individual and comprehensive treatment and prevention measures for COPD patients ii) significant reductions of socio-economic costs due to less mortality and disability. iii) novel insights in the dysregulation of metabolism, immunology and aging in COPD from the underlying model.	Medical Research Council	Research Grant	528567.0GBP
248	Dr Nyrup Rune	University of Cambridge	2018-10-01	2020-03-31	Understanding Medical Black Boxes: A Philosophical Analysis of AI Explainability	Chronic obstructive pulmonary disease (COPD) claimed 3.2 million lives in 2015, making it the third cause of death worldwide. It is predicted to increase in coming years due to aging populations and thus constitutes an enormous socio-economic burden. Existing assessment strategies neglect the complex, multi-component, and heterogeneous pathophysiology, as well as manifold comorbidities (cardiovascular, metabolic etc.). Therefore, improved COPD diagnosis and classification constitutes an urgent medical need for improved and personalized prevention measures and treatments strategies. The main aim of our proposal is to develop a tool that will enable effective preventive measures and personalize treatment strategies for COPD. This transnational and interdisciplinary project combines clinical scientists, experimentalists, computational and systems biology researchers, as well as a medium sized company. We will develop a systems medicine model of COPD constructed on (i) machine learning clustering of two comprehensive patient cohorts (COSYCONET, CIRO) providing long-term clinical observations, systematic outcome evaluation, biomaterial collections, multiple laboratory measurements, and extensive imaging data of more than 6,000 patients, complemented by (ii) an iterative systems biology framework of modeling and experimental analysis. Based on this multi-scale systems medicine model, we will generate a novel Clinical Decision Support (CDS) software that can be implemented in the existing IT infrastructure of hospitals and private practices for routine application. As a prototypic demonstrator of applied systems medicine, our tool will enable i) individual and comprehensive treatment and prevention measures for COPD patients ii) significant reductions of socio-economic costs due to less mortality and disability. iii) novel insights in the dysregulation of metabolism, immunology and aging in COPD from the underlying model.	Wellcome Trust	Seed Award in H&SS	86561.0GBP
249	Mr Mishra Abhishek	University of Oxford	2018-10-01	2021-10-01	Delivering Care Through AI Systems	Chronic obstructive pulmonary disease (COPD) claimed 3.2 million lives in 2015, making it the third cause of death worldwide. It is predicted to increase in coming years due to aging populations and thus constitutes an enormous socio-economic burden. Existing assessment strategies neglect the complex, multi-component, and heterogeneous pathophysiology, as well as manifold comorbidities (cardiovascular, metabolic etc.). Therefore, improved COPD diagnosis and classification constitutes an urgent medical need for improved and personalized prevention measures and treatments strategies. The main aim of our proposal is to develop a tool that will enable effective preventive measures and personalize treatment strategies for COPD. This transnational and interdisciplinary project combines clinical scientists, experimentalists, computational and systems biology researchers, as well as a medium sized company. We will develop a systems medicine model of COPD constructed on (i) machine learning clustering of two comprehensive patient cohorts (COSYCONET, CIRO) providing long-term clinical observations, systematic outcome evaluation, biomaterial collections, multiple laboratory measurements, and extensive imaging data of more than 6,000 patients, complemented by (ii) an iterative systems biology framework of modeling and experimental analysis. Based on this multi-scale systems medicine model, we will generate a novel Clinical Decision Support (CDS) software that can be implemented in the existing IT infrastructure of hospitals and private practices for routine application. As a prototypic demonstrator of applied systems medicine, our tool will enable i) individual and comprehensive treatment and prevention measures for COPD patients ii) significant reductions of socio-economic costs due to less mortality and disability. iii) novel insights in the dysregulation of metabolism, immunology and aging in COPD from the underlying model.	Wellcome Trust	PhD Studentship in H&SS	140538.0GBP
250	MSc BSc REISENHOFER Rafael	University of Vienna	2018-10-15	2020-10-14	Depth and Discriminability in Deep Learning Architectures	Deep neural networks have recently provided astounding results in a wide range of classification and regression tasks. This has sparked a renewed interest in the rigorous mathematical analysis of deep neural network architectures with the goal of uncovering the underlying principles that facilitate their groundbreaking success. Recent fundamental results already provide a better understanding of the relationship between depth and expressive power as well as a detailed analysis of the approximation properties of deep neural networks. It was also shown that certain types of convolutional neural networks exhibit desirable invariance properties such as stability with respect to small deformations. The present proposal aims at a mathematical investigation of another important property of a deep learning architecture, namely its discriminatory behavior. In most classification tasks, different classes are intertwined in a complex manner in the input space. In order to succeed, the realization of a deep neural network needs to disentangle those classes such that they can be separated in the corresponding feature space. Our goal is to better understand how the depth and other characteristics of a neural network influence its discriminatory power in the sense that they facilitate a clear separation of signal classes. Eventually, we aim to prove statements that quantify the discriminative power of a deep learning architecture with respect to distinct classes of signals as a function of depth and properties of the corresponding signals. The classification behavior of a neural network is mostly determined by its invariance properties on one side and its discriminatory properties on the other side. We will thus focus our investigation on a special class of convolutional neural networks, so-called scattering networks, for which substantial results regarding invariance and stability have already been established. We furthermore aim to utilize that when considering the modulus squared as a non-linearity, the output of each layer in a scattering network can be explicitly written as a cascade of autocorrelations in the frequency domain. In the initial phase of the project, we will investigate the discriminatory properties of scattering architectures with respect to simple template signal classes, such as signals that are sparse in the time or frequency domain. We then aim to extend our analysis to signal classes that resemble the structure of practical machine learning tasks in the sense that they are defined by shifts and time-frequency deformations of single prototype signals. Eventually, we aim to translate our theoretical findings into applicable guidelines regarding the optimal design of deep learning architectures for specific classification tasks. The research will primarily be conducted by the applicant (Rafael Reisenhofer) on a full-time basis. The work of the applicant will be supported by the invaluable expertise of the co-applicant (Philipp Grohs).	Austrian Science Fund FWF	None	156140.0EUR
251	Assoz. Prof. Dr. SCHÖFFMANN Klaus	University of Klagenfurt	2018-10-01	2021-09-30	Relevance Detection of Ophthalmic Surgery Videos	In this project, we want to investigate fundamental research questions in the field of postoperative analysis of ophthalmic surgery videos (OSVs). More precisely, three research objectives are covered: (1) Classification of OSV segments - is it possible to improve upon the state-of-the-art in automatic content classification and content segmentation of OSVs, focusing on regular and irregular operation phases? (2) Relevance prediction and relevance-driven compression - how accurately can the relevance of OSV segments be determined automatically for educational, scientific, and documentary purposes (as medical experts would do), and what compression efficiency can be achieved for OSVs when considering relevance as an additional modality? (3) Analysis of common irregularities in OSVs for medical research - we address three quantitative medical research questions related to cataract surgeries, such as: is there a statistically significant difference in duration or complication rate between cataract surgeries showing intraoperative pupil reactions and those showing no such pupil reactions? We plan to perform these investigations using data acquisition, data modelling, video content analysis, statistical analysis, and state-of-the-art machine learning methods - such as content classifiers based on deep learning. The proposed methods will be evaluated on annotated video datasets ("ground truth") created by medical field experts during the project. Beyond developing novel methods for solving the abovementioned research problems, project results are expected to have innovative effects in the emerging interdisciplinary field of automatic video-based analysis of ophthalmic surgeries. In particular, research results of this project will enable efficient permanent video documentation of ophthalmic surgeries, allowing to create OSV datasets relevant for medical education, training, and research. Moreover, archives of relevant OSVs will enable novel postoperative analysis methods for medical research questions - such as causes for irregular operation phases, for example. The research project will be a cooperation between computer scientists of AAU Klagenfurt (conducted by Prof. Klaus Schöffmann, supported and advised by Dr. Mario Taschwer and Prof. Laszlo Böszörmenyi) and ophthalmic surgeons and researchers at Klinikum Klagenfurt (Dr. Doris Putzgruber-Adamitsch, Dr. Stephanie Sarny, Prof. Yosuf El-Shabrawi).	Austrian Science Fund FWF	None	379635.38EUR
252	Prof VELDINK Jan Herman	UNIVERSITAIR MEDISCH CENTRUM UTRECHT	2018-07-01	2023-06-30	Emerging Simplex ORigins In ALS	My aim is to understand the exact genetic contribution in every patient with Amyotrophic Lateral Sclerosis (ALS), a lethal disease with a life time risk of 0.3% and an urgent unmet therapeutic need. I have recently shown a disproportionate large contribution from low-frequency genetic variants in ALS. ALS is not simply a collection of unique rare diseases with a monogenetic cause nor is it a diagnostic continuum with a complex contribution of thousands of small effect factors. ALS is in-between, which I call “simplex”, where in each patient a few, considerably strong genetic factors with or without environmental factors are at play. ALS mutations are characterized by reduced penetrance, variable clinical expressivity, have specific pleiotropic clinical features and interact with environmental factors. These phenomena are unexplained, but provide me with important and new opportunities in order to unravel the clinical, genetic and biological heterogeneity in ALS. I have created new research fields to go an important step beyond the state of the art: Splitting by lumping uses novel machine learning algorithms to reclassify patients using clinical pleiotropic features, environmental factors and blood epigenetic profiles to identify novel ALS mutations. Imaging genomics overlays patterns in ALS-associated brain morphology on MRI with brain gene-expression patterns to find ALS mutations. ALS risk in 3D integrates data on three-dimensional folding of DNA with genetic data to identify causal mutations and mutation-to-mutation interaction. ALS genomic modifiers in 3D identifies modifiers of C9orf72 mutations through the development of cellular reporter assays and CRISPR-Cas9 based screens. Genomic findings are translated using cellular models which can be used for targeted and unbiased drug screens. If successful, my approaches can be applied beyond the scope of this ERC and will have a clear impact on clinical trial design and genetic counselling in ALS in particular.	European Research Council	Consolidator Grant	1980434.0EUR
253	Professor Zhang Daoqiang	Nanjing University of Aeronautics and Astronautics	2018-03-31	2021-02-28	Big-Data Driven Intelligent Analysis of High Dimensional Multimodal Neuroimaging Data and its Application to Brain Disease Diagnosis	Neuroimaging methods based on high dimensional multimodal structural and functional imaging data have been recently proposed for objective diagnosis of brain diseases such as Alzheimer’s disease and autism. However, it is challenging to deal with multimodal data, since 1) multimodal data are typically massive in dimensionality,2) the complete set of Training Programme multimodal data is often unavailable for each subject due to data loss during canning or storage, or due to different study designs in different institutes, and 3) the number of subjects is often much smaller than the dimensionality of the multimodal data. The goal of this project is to develop advanced machine learning methods to address all these challenges in intelligent analysis of multimodal neuroimaging data. Specifically, we will develop a robust ensemble learning method for hierarchical decision fusion from multiple multi-level classifiers, through a layer-by-layer and localˇto-global fashion, to address the first challenge of high dimensionality. Moreover, we will develop a unified framework for novel multi-task semi-supervised (or transfer) learning with multimodal imaging data, to jointly address the second and third challenges of missing data and insufficient training samples, respectively. To our knowledge, the existing neuroimaging analysis methods are not able to deal effectively with these two challenges. Finally, all these proposed methods will be tested and evaluated on real neuroimaging datasets. The overarching aim of this project is to develop, train and transfer news skills to China-PI (Zhang) and his group on 1) efficient analysis of high-dimensional neuroimaging data using current advances and new knowledge in big data analytics and data science, especially in the areas of parallel and distributed computing, data analytics/image pattern recognition; 2) and transferable skills such communication, leadership and project management skills.	The Academy of Medical Sciences	Newton Advanced Fellowship	108000.0GBP
254	Dr Gavara Nuria	QUEEN MARY UNIVERSITY OF LONDON	2016-10-24	2018-04-23	Effect of cell age on cell migration and cytoskeletal reorganization	Neuroimaging methods based on high dimensional multimodal structural and functional imaging data have been recently proposed for objective diagnosis of brain diseases such as Alzheimer’s disease and autism. However, it is challenging to deal with multimodal data, since 1) multimodal data are typically massive in dimensionality,2) the complete set of Training Programme multimodal data is often unavailable for each subject due to data loss during canning or storage, or due to different study designs in different institutes, and 3) the number of subjects is often much smaller than the dimensionality of the multimodal data. The goal of this project is to develop advanced machine learning methods to address all these challenges in intelligent analysis of multimodal neuroimaging data. Specifically, we will develop a robust ensemble learning method for hierarchical decision fusion from multiple multi-level classifiers, through a layer-by-layer and localˇto-global fashion, to address the first challenge of high dimensionality. Moreover, we will develop a unified framework for novel multi-task semi-supervised (or transfer) learning with multimodal imaging data, to jointly address the second and third challenges of missing data and insufficient training samples, respectively. To our knowledge, the existing neuroimaging analysis methods are not able to deal effectively with these two challenges. Finally, all these proposed methods will be tested and evaluated on real neuroimaging datasets. The overarching aim of this project is to develop, train and transfer news skills to China-PI (Zhang) and his group on 1) efficient analysis of high-dimensional neuroimaging data using current advances and new knowledge in big data analytics and data science, especially in the areas of parallel and distributed computing, data analytics/image pattern recognition; 2) and transferable skills such communication, leadership and project management skills.	The Dunhill Medical Trust	Research Project & Programme Grants	74930.0GBP
255	Dr Gavara Nuria	ETH Zurich	2018-02-01	2019-01-31	Spectral analysis of fluorescently labeled amyloids	The self-assembly of prion precursors into oligomers and fibers has been linked to several neurodegenerative and systemic disorders. In particular, the aggregation of Amyloid-ß (Aß), tau and a-synuclein in brain tissue have been associated with the neuropathological process of Alzheimer’s (AD) and Parkinson’s disease (PD). Numerous studies have shown that not only these aggregates can propagate from cell-to-cell in a prion-like manner, but their toxicity and linked pathology strongly depends on their conformations, defining different strains of amyloid. The characterization of the biologically relevant strains remains hard to determine experimentally.To overcome this problem, I propose to implement a computational multicomponent spectral analysis pipeline that will allow to efficiently discriminate amyloid strains. Several fluorescent dyes, such as Congo red and thioflavin, are known to specifically bind to fibrils. Furthermore, their emission spectrum is modulated by the conformation of the fibril. In this project, I will use machine learning to analyze the spectral information of fluorescent microscope image and determine the strains of amyloid. The pipeline developed in this project will allow to characterize the composition of brain aggregates in vitro, in vivo and in brain tissue. Such a method could also greatly aid biophysical studies of amyloids by quickly assessing the quality and composition of samples, as well as give molecular insights linked to the different phenotypes. This understanding should lay the groundwork for the development of effective treatments for AD, PD and other neurodegenerative diseases linked to prions.	Swiss National Science Foundation	Return CH Advanced Postdoc.Mobility	110200.0CHF
256	Professor Kaiser Marcus	Newcastle University	2019-04-01	2022-03-31	Modelling dementia progression based on machine learning and simulations	The self-assembly of prion precursors into oligomers and fibers has been linked to several neurodegenerative and systemic disorders. In particular, the aggregation of Amyloid-ß (Aß), tau and a-synuclein in brain tissue have been associated with the neuropathological process of Alzheimer’s (AD) and Parkinson’s disease (PD). Numerous studies have shown that not only these aggregates can propagate from cell-to-cell in a prion-like manner, but their toxicity and linked pathology strongly depends on their conformations, defining different strains of amyloid. The characterization of the biologically relevant strains remains hard to determine experimentally.To overcome this problem, I propose to implement a computational multicomponent spectral analysis pipeline that will allow to efficiently discriminate amyloid strains. Several fluorescent dyes, such as Congo red and thioflavin, are known to specifically bind to fibrils. Furthermore, their emission spectrum is modulated by the conformation of the fibril. In this project, I will use machine learning to analyze the spectral information of fluorescent microscope image and determine the strains of amyloid. The pipeline developed in this project will allow to characterize the composition of brain aggregates in vitro, in vivo and in brain tissue. Such a method could also greatly aid biophysical studies of amyloids by quickly assessing the quality and composition of samples, as well as give molecular insights linked to the different phenotypes. This understanding should lay the groundwork for the development of effective treatments for AD, PD and other neurodegenerative diseases linked to prions.	Medical Research Council	Research Grant	304844.0GBP
257	Dr. LODE Axel Ulrich	University of Vienna	2019-01-01	2021-12-31	Numerics for many-body physics and single-shot images	Numerical models for many-body physics and single-shot images Scientific Abstract: We develop and apply improved simulation tools and numerical techniques for the time-dependent many-body Schrödinger equation for describing dynamical quantum systems used in state-of-the art experiments with ultracold atoms like the ones in the groups at the AtomInstitut, TU Wien. Our key goal is to investigate what one can really know about quantum many-body states of ultracold atoms. We go beyond models with a single Gross-Pitaevskii equation which cannot capture the phenomena in our focus like correlations and squeezing, that are observed in current experiments. A fundamental obstacle is to extract the information about correlations and squeezing from the experiment. In many experimental setups, the observations consist in so-called "single-shot images" of the atomic clouds. Theoretically, single-shot images represent a projective measurement of the many-body wavefunction. The current strategy necessitates a very large number of single-shot images to be produced and analyzed. We plan to use and improve a sophisticated numerical tool, the MCTDH-(B/F), that allows to describe correlations and squeezing. We aim to enable a direct and predictive modeling. We will implement an efficient MCTDH-(B/F) software and apply it for the simulation of also two- and three-dimensional many-body dynamics. To directly compare model predictions with experimental images, the single-shot-imaging process is also simulated. To face the key challenge of what one can really know about quantum many-body states of ultracold atoms, we develop statistical analysis and machine learning algorithms that optimally harness the information about squeezing and correlations in the single-shot images. The project core team consists of the PI, Axel Lode, a specialist on the Schrödinger equation and the MCTDH-(B/F) method, one PostDoc, that will work on the generation and analysis of single-shot images with a focus on correlations and on squeezing, and one PhD student that will apply the developed numerical tools in direct collaboration with the experimental physics project partners at the AtomInstitut, TU Wien. The core team is embedded and supported by the WPI applied math and experimental quantum physics experts N. J. Mauser, J. Schmiedmayer, T. Schumm, all WPI full members via START, Wittgenstein, ERC awards, etc..	Austrian Science Fund FWF	01 Stand-Alone Projects	389174.63EUR
258	Dr Brierley Liam	University of Liverpool	2019-10-01	2022-09-30	Ecology or genetics? Adapting machine learning approaches to understand determinants of cross-species transmission and virulence in RNA viruses	Numerical models for many-body physics and single-shot images Scientific Abstract: We develop and apply improved simulation tools and numerical techniques for the time-dependent many-body Schrödinger equation for describing dynamical quantum systems used in state-of-the art experiments with ultracold atoms like the ones in the groups at the AtomInstitut, TU Wien. Our key goal is to investigate what one can really know about quantum many-body states of ultracold atoms. We go beyond models with a single Gross-Pitaevskii equation which cannot capture the phenomena in our focus like correlations and squeezing, that are observed in current experiments. A fundamental obstacle is to extract the information about correlations and squeezing from the experiment. In many experimental setups, the observations consist in so-called "single-shot images" of the atomic clouds. Theoretically, single-shot images represent a projective measurement of the many-body wavefunction. The current strategy necessitates a very large number of single-shot images to be produced and analyzed. We plan to use and improve a sophisticated numerical tool, the MCTDH-(B/F), that allows to describe correlations and squeezing. We aim to enable a direct and predictive modeling. We will implement an efficient MCTDH-(B/F) software and apply it for the simulation of also two- and three-dimensional many-body dynamics. To directly compare model predictions with experimental images, the single-shot-imaging process is also simulated. To face the key challenge of what one can really know about quantum many-body states of ultracold atoms, we develop statistical analysis and machine learning algorithms that optimally harness the information about squeezing and correlations in the single-shot images. The project core team consists of the PI, Axel Lode, a specialist on the Schrödinger equation and the MCTDH-(B/F) method, one PostDoc, that will work on the generation and analysis of single-shot images with a focus on correlations and on squeezing, and one PhD student that will apply the developed numerical tools in direct collaboration with the experimental physics project partners at the AtomInstitut, TU Wien. The core team is embedded and supported by the WPI applied math and experimental quantum physics experts N. J. Mauser, J. Schmiedmayer, T. Schumm, all WPI full members via START, Wittgenstein, ERC awards, etc..	Medical Research Council	Fellowship	235657.0GBP
259	Dr Brierley Liam	University of Liverpool	2019-10-01	2022-09-30	Ecology or genetics? Adapting machine learning approaches to understand determinants of cross-species transmission and virulence in RNA viruses	Emerging infectious diseases remain a prominent threat to global health, e.g., Ebola virus, Zika virus. In 2015, the WHO designated 'Disease X' to indicate the serious potential of previously unknown emerging pathogens to cause public health crises. Though zoonotic RNA viruses are known to present higher risks of emergence, detailed determinants of cross-species transmission remain unclear. Zoonotic viruses also vary widely in their capability to cause severe disease. To predict public health impacts of 'Disease X', a better understanding of which traits drive this variation in infectivity and virulence is urgently needed. Whilst previous approaches have focused on ecological predictors, these traditional frameworks have been unable to capture the information within increasingly available RNA virus sequences. This research aims to capitalise upon the potential power within large genetic data resources and quantify comparative influences of genetic versus ecological traits of RNA viruses and hosts upon cross-species transmission dynamics. To fully integrate novel, high-dimensional genetic data, new analytical approaches are needed. I will apply machine learning as a state-of-the-art statistical methodology, comparing several advanced approaches, e.g. gradient boosting, a method of gradual model learning which outperforms traditional methods. Models will span all known mammal and avian RNA viruses (22 families) using the exceptional breadth of EID2, a large, host-virus infectivity dataset. This project will additionally develop further text-mining tools to capture and integrate virulence data within EID2. The proposed models will allow tests of evolutionary theory across a range of RNA viruses. Quantified model outputs will contribute to public health risk assessments by informing prioritisation for novel viruses and advancing frameworks for emergence predictions, moving towards a 'smarter', empirically-driven strategy to prevent future disease burden.	UK Research and Innovation	Fellowship	235657.0GBP
260	Dr Davis Matthew	EMBL - European Bioinformatics Institute	2014-03-24	2018-03-23	Classification and functional annotation of endogenous siRNAs and other small RNAs	Emerging infectious diseases remain a prominent threat to global health, e.g., Ebola virus, Zika virus. In 2015, the WHO designated 'Disease X' to indicate the serious potential of previously unknown emerging pathogens to cause public health crises. Though zoonotic RNA viruses are known to present higher risks of emergence, detailed determinants of cross-species transmission remain unclear. Zoonotic viruses also vary widely in their capability to cause severe disease. To predict public health impacts of 'Disease X', a better understanding of which traits drive this variation in infectivity and virulence is urgently needed. Whilst previous approaches have focused on ecological predictors, these traditional frameworks have been unable to capture the information within increasingly available RNA virus sequences. This research aims to capitalise upon the potential power within large genetic data resources and quantify comparative influences of genetic versus ecological traits of RNA viruses and hosts upon cross-species transmission dynamics. To fully integrate novel, high-dimensional genetic data, new analytical approaches are needed. I will apply machine learning as a state-of-the-art statistical methodology, comparing several advanced approaches, e.g. gradient boosting, a method of gradual model learning which outperforms traditional methods. Models will span all known mammal and avian RNA viruses (22 families) using the exceptional breadth of EID2, a large, host-virus infectivity dataset. This project will additionally develop further text-mining tools to capture and integrate virulence data within EID2. The proposed models will allow tests of evolutionary theory across a range of RNA viruses. Quantified model outputs will contribute to public health risk assessments by informing prioritisation for novel viruses and advancing frameworks for emergence predictions, moving towards a 'smarter', empirically-driven strategy to prevent future disease burden.	Medical Research Council	Fellowship	265931.0GBP
261	Dr Davis Matthew	University of Berne	2016-07-01	2019-06-30	In silico prediction of phage-bacteria infection networks as a tool to implement personalized phage therapy	The emergence and rapid dissemination of antibiotic resistance worldwide threatens medical progress. As a consequence, medicine might face a return to the pre-antibiotic era in a very soon future. The paucity of potential new anti-infectives in the pipeline of pharmaceutical industries urges the need for alternatives to fight this public health problem. Phage therapy might represent such an alternative. This re-emerging therapy uses viruses that specifically infect and kill bacteria during their life cycle to reduce/eliminate bacterial load and cure infections. These viruses, called bacteriophages or phages, have been co-evolving with bacteria for billions years, controlling bacterial populations and epidemics, and contributing to their genetic exchanges. With the advantage of having low impact on the commensal flora, as they are highly strain specific, some phages might, nevertheless, harbor virulence factors and drive horizontal gene transfer mediating dissemination of pathogenic traits including antibiotic resistance, calling for their careful selection before their therapeutic use (see below “Phage lifestyle inside the bacteria”). The success of phage therapy mainly relies on the exact matching between both the target pathogenic bacteria and the therapeutic phage. Therefore, having access to a fully characterized phage library is necessary, although not sufficient, to start with phage therapy. An essential and obligate second step to conceive personalized phage therapy treatments is the capacity to predict the interactions between the target pathogen and its potential phage. The long term goal of the proposed research is, therefore, to develop quantitative and predictive in silico models of phage-bacteria infection networks. These models will describe the interactions between phage and bacteria and will serve to fasten the selection of effective phages to propose phage therapy in a personalized fashion.To efficiently predict successful phage-bacteria interactions suitable for phage therapy, we will develop a novel in silico methodology that will, ultimately, enable the selection of phage candidates from an existing phage library to target a given pathogenic bacteria. To achieve this, we will combine genomic information with state-of-the-art bioinformatic and machine learning techniques, taking advantage of the growing amount of interaction data already available as well as of our own data to keep uncovering new phage families. We will ensure that our methodology brings explanatory power along, thereby shedding light on the relevant genomic features underscoring the interactions. To challenge our approach, we will eventually prospectively validate our methodology using paradigmatic pathogens (Pendleton et al. 2013). For this, we will construct the phage-bacterium infection networks around those pathogens, to identify single phages and/or phage cocktails with extended bactericidal activities, as assessed in different models of infections, including a Galleria melonnella model of infection and a rat endocarditis model. We expect our methodology to drive a paradigm shift in phage therapy, by offering a time-sparing and easy-to-use way to accurately select phages for each individual patient. The methodology will be made available online and several future developments of this project are already envisioned.	Swiss National Science Foundation	Interdisciplinary projects	915766.0CHF
262	Dr Carragher Raymond	University of Strathclyde	2018-02-14	2021-02-13	Precision Drug Theraputics: Risk Prediction in Pharmacoepidemiology	The emergence and rapid dissemination of antibiotic resistance worldwide threatens medical progress. As a consequence, medicine might face a return to the pre-antibiotic era in a very soon future. The paucity of potential new anti-infectives in the pipeline of pharmaceutical industries urges the need for alternatives to fight this public health problem. Phage therapy might represent such an alternative. This re-emerging therapy uses viruses that specifically infect and kill bacteria during their life cycle to reduce/eliminate bacterial load and cure infections. These viruses, called bacteriophages or phages, have been co-evolving with bacteria for billions years, controlling bacterial populations and epidemics, and contributing to their genetic exchanges. With the advantage of having low impact on the commensal flora, as they are highly strain specific, some phages might, nevertheless, harbor virulence factors and drive horizontal gene transfer mediating dissemination of pathogenic traits including antibiotic resistance, calling for their careful selection before their therapeutic use (see below “Phage lifestyle inside the bacteria”). The success of phage therapy mainly relies on the exact matching between both the target pathogenic bacteria and the therapeutic phage. Therefore, having access to a fully characterized phage library is necessary, although not sufficient, to start with phage therapy. An essential and obligate second step to conceive personalized phage therapy treatments is the capacity to predict the interactions between the target pathogen and its potential phage. The long term goal of the proposed research is, therefore, to develop quantitative and predictive in silico models of phage-bacteria infection networks. These models will describe the interactions between phage and bacteria and will serve to fasten the selection of effective phages to propose phage therapy in a personalized fashion.To efficiently predict successful phage-bacteria interactions suitable for phage therapy, we will develop a novel in silico methodology that will, ultimately, enable the selection of phage candidates from an existing phage library to target a given pathogenic bacteria. To achieve this, we will combine genomic information with state-of-the-art bioinformatic and machine learning techniques, taking advantage of the growing amount of interaction data already available as well as of our own data to keep uncovering new phage families. We will ensure that our methodology brings explanatory power along, thereby shedding light on the relevant genomic features underscoring the interactions. To challenge our approach, we will eventually prospectively validate our methodology using paradigmatic pathogens (Pendleton et al. 2013). For this, we will construct the phage-bacterium infection networks around those pathogens, to identify single phages and/or phage cocktails with extended bactericidal activities, as assessed in different models of infections, including a Galleria melonnella model of infection and a rat endocarditis model. We expect our methodology to drive a paradigm shift in phage therapy, by offering a time-sparing and easy-to-use way to accurately select phages for each individual patient. The methodology will be made available online and several future developments of this project are already envisioned.	Medical Research Council	Fellowship	300445.0GBP
263	Dr Gurdasani Deepti	Queen Mary University of London	2019-08-05	2021-03-25	Predictive analytics of integrated genomic and clinical data using machine learning and complex statistical approaches	The emergence and rapid dissemination of antibiotic resistance worldwide threatens medical progress. As a consequence, medicine might face a return to the pre-antibiotic era in a very soon future. The paucity of potential new anti-infectives in the pipeline of pharmaceutical industries urges the need for alternatives to fight this public health problem. Phage therapy might represent such an alternative. This re-emerging therapy uses viruses that specifically infect and kill bacteria during their life cycle to reduce/eliminate bacterial load and cure infections. These viruses, called bacteriophages or phages, have been co-evolving with bacteria for billions years, controlling bacterial populations and epidemics, and contributing to their genetic exchanges. With the advantage of having low impact on the commensal flora, as they are highly strain specific, some phages might, nevertheless, harbor virulence factors and drive horizontal gene transfer mediating dissemination of pathogenic traits including antibiotic resistance, calling for their careful selection before their therapeutic use (see below “Phage lifestyle inside the bacteria”). The success of phage therapy mainly relies on the exact matching between both the target pathogenic bacteria and the therapeutic phage. Therefore, having access to a fully characterized phage library is necessary, although not sufficient, to start with phage therapy. An essential and obligate second step to conceive personalized phage therapy treatments is the capacity to predict the interactions between the target pathogen and its potential phage. The long term goal of the proposed research is, therefore, to develop quantitative and predictive in silico models of phage-bacteria infection networks. These models will describe the interactions between phage and bacteria and will serve to fasten the selection of effective phages to propose phage therapy in a personalized fashion.To efficiently predict successful phage-bacteria interactions suitable for phage therapy, we will develop a novel in silico methodology that will, ultimately, enable the selection of phage candidates from an existing phage library to target a given pathogenic bacteria. To achieve this, we will combine genomic information with state-of-the-art bioinformatic and machine learning techniques, taking advantage of the growing amount of interaction data already available as well as of our own data to keep uncovering new phage families. We will ensure that our methodology brings explanatory power along, thereby shedding light on the relevant genomic features underscoring the interactions. To challenge our approach, we will eventually prospectively validate our methodology using paradigmatic pathogens (Pendleton et al. 2013). For this, we will construct the phage-bacterium infection networks around those pathogens, to identify single phages and/or phage cocktails with extended bactericidal activities, as assessed in different models of infections, including a Galleria melonnella model of infection and a rat endocarditis model. We expect our methodology to drive a paradigm shift in phage therapy, by offering a time-sparing and easy-to-use way to accurately select phages for each individual patient. The methodology will be made available online and several future developments of this project are already envisioned.	Medical Research Council	Fellowship	177596.0GBP
264	Dr Hill Yolanda	University of Exeter	2017-11-15	2021-02-17	Cardiac Positioning System (CPS) - An automated navigation system to guide catheter ablation therapy	The emergence and rapid dissemination of antibiotic resistance worldwide threatens medical progress. As a consequence, medicine might face a return to the pre-antibiotic era in a very soon future. The paucity of potential new anti-infectives in the pipeline of pharmaceutical industries urges the need for alternatives to fight this public health problem. Phage therapy might represent such an alternative. This re-emerging therapy uses viruses that specifically infect and kill bacteria during their life cycle to reduce/eliminate bacterial load and cure infections. These viruses, called bacteriophages or phages, have been co-evolving with bacteria for billions years, controlling bacterial populations and epidemics, and contributing to their genetic exchanges. With the advantage of having low impact on the commensal flora, as they are highly strain specific, some phages might, nevertheless, harbor virulence factors and drive horizontal gene transfer mediating dissemination of pathogenic traits including antibiotic resistance, calling for their careful selection before their therapeutic use (see below “Phage lifestyle inside the bacteria”). The success of phage therapy mainly relies on the exact matching between both the target pathogenic bacteria and the therapeutic phage. Therefore, having access to a fully characterized phage library is necessary, although not sufficient, to start with phage therapy. An essential and obligate second step to conceive personalized phage therapy treatments is the capacity to predict the interactions between the target pathogen and its potential phage. The long term goal of the proposed research is, therefore, to develop quantitative and predictive in silico models of phage-bacteria infection networks. These models will describe the interactions between phage and bacteria and will serve to fasten the selection of effective phages to propose phage therapy in a personalized fashion.To efficiently predict successful phage-bacteria interactions suitable for phage therapy, we will develop a novel in silico methodology that will, ultimately, enable the selection of phage candidates from an existing phage library to target a given pathogenic bacteria. To achieve this, we will combine genomic information with state-of-the-art bioinformatic and machine learning techniques, taking advantage of the growing amount of interaction data already available as well as of our own data to keep uncovering new phage families. We will ensure that our methodology brings explanatory power along, thereby shedding light on the relevant genomic features underscoring the interactions. To challenge our approach, we will eventually prospectively validate our methodology using paradigmatic pathogens (Pendleton et al. 2013). For this, we will construct the phage-bacterium infection networks around those pathogens, to identify single phages and/or phage cocktails with extended bactericidal activities, as assessed in different models of infections, including a Galleria melonnella model of infection and a rat endocarditis model. We expect our methodology to drive a paradigm shift in phage therapy, by offering a time-sparing and easy-to-use way to accurately select phages for each individual patient. The methodology will be made available online and several future developments of this project are already envisioned.	Medical Research Council	Fellowship	264230.0GBP
265	Dr Johnson Laura	University of Bristol	2019-12-31	2021-12-30	Innovating behaviour and health surveillance for cardiovascular disease prevention in Malaysia	The emergence and rapid dissemination of antibiotic resistance worldwide threatens medical progress. As a consequence, medicine might face a return to the pre-antibiotic era in a very soon future. The paucity of potential new anti-infectives in the pipeline of pharmaceutical industries urges the need for alternatives to fight this public health problem. Phage therapy might represent such an alternative. This re-emerging therapy uses viruses that specifically infect and kill bacteria during their life cycle to reduce/eliminate bacterial load and cure infections. These viruses, called bacteriophages or phages, have been co-evolving with bacteria for billions years, controlling bacterial populations and epidemics, and contributing to their genetic exchanges. With the advantage of having low impact on the commensal flora, as they are highly strain specific, some phages might, nevertheless, harbor virulence factors and drive horizontal gene transfer mediating dissemination of pathogenic traits including antibiotic resistance, calling for their careful selection before their therapeutic use (see below “Phage lifestyle inside the bacteria”). The success of phage therapy mainly relies on the exact matching between both the target pathogenic bacteria and the therapeutic phage. Therefore, having access to a fully characterized phage library is necessary, although not sufficient, to start with phage therapy. An essential and obligate second step to conceive personalized phage therapy treatments is the capacity to predict the interactions between the target pathogen and its potential phage. The long term goal of the proposed research is, therefore, to develop quantitative and predictive in silico models of phage-bacteria infection networks. These models will describe the interactions between phage and bacteria and will serve to fasten the selection of effective phages to propose phage therapy in a personalized fashion.To efficiently predict successful phage-bacteria interactions suitable for phage therapy, we will develop a novel in silico methodology that will, ultimately, enable the selection of phage candidates from an existing phage library to target a given pathogenic bacteria. To achieve this, we will combine genomic information with state-of-the-art bioinformatic and machine learning techniques, taking advantage of the growing amount of interaction data already available as well as of our own data to keep uncovering new phage families. We will ensure that our methodology brings explanatory power along, thereby shedding light on the relevant genomic features underscoring the interactions. To challenge our approach, we will eventually prospectively validate our methodology using paradigmatic pathogens (Pendleton et al. 2013). For this, we will construct the phage-bacterium infection networks around those pathogens, to identify single phages and/or phage cocktails with extended bactericidal activities, as assessed in different models of infections, including a Galleria melonnella model of infection and a rat endocarditis model. We expect our methodology to drive a paradigm shift in phage therapy, by offering a time-sparing and easy-to-use way to accurately select phages for each individual patient. The methodology will be made available online and several future developments of this project are already envisioned.	Medical Research Council	Research Grant	284830.0GBP
266	Dr Lindner Claudia	The University of Manchester	2018-02-14	2021-02-13	Fully automated system for the analysis of the efficacy of knee replacement surgery	The emergence and rapid dissemination of antibiotic resistance worldwide threatens medical progress. As a consequence, medicine might face a return to the pre-antibiotic era in a very soon future. The paucity of potential new anti-infectives in the pipeline of pharmaceutical industries urges the need for alternatives to fight this public health problem. Phage therapy might represent such an alternative. This re-emerging therapy uses viruses that specifically infect and kill bacteria during their life cycle to reduce/eliminate bacterial load and cure infections. These viruses, called bacteriophages or phages, have been co-evolving with bacteria for billions years, controlling bacterial populations and epidemics, and contributing to their genetic exchanges. With the advantage of having low impact on the commensal flora, as they are highly strain specific, some phages might, nevertheless, harbor virulence factors and drive horizontal gene transfer mediating dissemination of pathogenic traits including antibiotic resistance, calling for their careful selection before their therapeutic use (see below “Phage lifestyle inside the bacteria”). The success of phage therapy mainly relies on the exact matching between both the target pathogenic bacteria and the therapeutic phage. Therefore, having access to a fully characterized phage library is necessary, although not sufficient, to start with phage therapy. An essential and obligate second step to conceive personalized phage therapy treatments is the capacity to predict the interactions between the target pathogen and its potential phage. The long term goal of the proposed research is, therefore, to develop quantitative and predictive in silico models of phage-bacteria infection networks. These models will describe the interactions between phage and bacteria and will serve to fasten the selection of effective phages to propose phage therapy in a personalized fashion.To efficiently predict successful phage-bacteria interactions suitable for phage therapy, we will develop a novel in silico methodology that will, ultimately, enable the selection of phage candidates from an existing phage library to target a given pathogenic bacteria. To achieve this, we will combine genomic information with state-of-the-art bioinformatic and machine learning techniques, taking advantage of the growing amount of interaction data already available as well as of our own data to keep uncovering new phage families. We will ensure that our methodology brings explanatory power along, thereby shedding light on the relevant genomic features underscoring the interactions. To challenge our approach, we will eventually prospectively validate our methodology using paradigmatic pathogens (Pendleton et al. 2013). For this, we will construct the phage-bacterium infection networks around those pathogens, to identify single phages and/or phage cocktails with extended bactericidal activities, as assessed in different models of infections, including a Galleria melonnella model of infection and a rat endocarditis model. We expect our methodology to drive a paradigm shift in phage therapy, by offering a time-sparing and easy-to-use way to accurately select phages for each individual patient. The methodology will be made available online and several future developments of this project are already envisioned.	Medical Research Council	Fellowship	236193.0GBP
267	Dr Marshall Iain	King's College London	2016-07-01	2021-06-30	RobotReviewer: development and evaluation of a machine learning tool to speed up evidence synthesis in cardiovascular diseases	The emergence and rapid dissemination of antibiotic resistance worldwide threatens medical progress. As a consequence, medicine might face a return to the pre-antibiotic era in a very soon future. The paucity of potential new anti-infectives in the pipeline of pharmaceutical industries urges the need for alternatives to fight this public health problem. Phage therapy might represent such an alternative. This re-emerging therapy uses viruses that specifically infect and kill bacteria during their life cycle to reduce/eliminate bacterial load and cure infections. These viruses, called bacteriophages or phages, have been co-evolving with bacteria for billions years, controlling bacterial populations and epidemics, and contributing to their genetic exchanges. With the advantage of having low impact on the commensal flora, as they are highly strain specific, some phages might, nevertheless, harbor virulence factors and drive horizontal gene transfer mediating dissemination of pathogenic traits including antibiotic resistance, calling for their careful selection before their therapeutic use (see below “Phage lifestyle inside the bacteria”). The success of phage therapy mainly relies on the exact matching between both the target pathogenic bacteria and the therapeutic phage. Therefore, having access to a fully characterized phage library is necessary, although not sufficient, to start with phage therapy. An essential and obligate second step to conceive personalized phage therapy treatments is the capacity to predict the interactions between the target pathogen and its potential phage. The long term goal of the proposed research is, therefore, to develop quantitative and predictive in silico models of phage-bacteria infection networks. These models will describe the interactions between phage and bacteria and will serve to fasten the selection of effective phages to propose phage therapy in a personalized fashion.To efficiently predict successful phage-bacteria interactions suitable for phage therapy, we will develop a novel in silico methodology that will, ultimately, enable the selection of phage candidates from an existing phage library to target a given pathogenic bacteria. To achieve this, we will combine genomic information with state-of-the-art bioinformatic and machine learning techniques, taking advantage of the growing amount of interaction data already available as well as of our own data to keep uncovering new phage families. We will ensure that our methodology brings explanatory power along, thereby shedding light on the relevant genomic features underscoring the interactions. To challenge our approach, we will eventually prospectively validate our methodology using paradigmatic pathogens (Pendleton et al. 2013). For this, we will construct the phage-bacterium infection networks around those pathogens, to identify single phages and/or phage cocktails with extended bactericidal activities, as assessed in different models of infections, including a Galleria melonnella model of infection and a rat endocarditis model. We expect our methodology to drive a paradigm shift in phage therapy, by offering a time-sparing and easy-to-use way to accurately select phages for each individual patient. The methodology will be made available online and several future developments of this project are already envisioned.	Medical Research Council	Fellowship	330011.0GBP
268	Dr Niedzwiedz Claire	University of Glasgow	2017-11-15	2021-01-13	A machine learning approach to understanding comorbidity between mental and physical health conditions	The emergence and rapid dissemination of antibiotic resistance worldwide threatens medical progress. As a consequence, medicine might face a return to the pre-antibiotic era in a very soon future. The paucity of potential new anti-infectives in the pipeline of pharmaceutical industries urges the need for alternatives to fight this public health problem. Phage therapy might represent such an alternative. This re-emerging therapy uses viruses that specifically infect and kill bacteria during their life cycle to reduce/eliminate bacterial load and cure infections. These viruses, called bacteriophages or phages, have been co-evolving with bacteria for billions years, controlling bacterial populations and epidemics, and contributing to their genetic exchanges. With the advantage of having low impact on the commensal flora, as they are highly strain specific, some phages might, nevertheless, harbor virulence factors and drive horizontal gene transfer mediating dissemination of pathogenic traits including antibiotic resistance, calling for their careful selection before their therapeutic use (see below “Phage lifestyle inside the bacteria”). The success of phage therapy mainly relies on the exact matching between both the target pathogenic bacteria and the therapeutic phage. Therefore, having access to a fully characterized phage library is necessary, although not sufficient, to start with phage therapy. An essential and obligate second step to conceive personalized phage therapy treatments is the capacity to predict the interactions between the target pathogen and its potential phage. The long term goal of the proposed research is, therefore, to develop quantitative and predictive in silico models of phage-bacteria infection networks. These models will describe the interactions between phage and bacteria and will serve to fasten the selection of effective phages to propose phage therapy in a personalized fashion.To efficiently predict successful phage-bacteria interactions suitable for phage therapy, we will develop a novel in silico methodology that will, ultimately, enable the selection of phage candidates from an existing phage library to target a given pathogenic bacteria. To achieve this, we will combine genomic information with state-of-the-art bioinformatic and machine learning techniques, taking advantage of the growing amount of interaction data already available as well as of our own data to keep uncovering new phage families. We will ensure that our methodology brings explanatory power along, thereby shedding light on the relevant genomic features underscoring the interactions. To challenge our approach, we will eventually prospectively validate our methodology using paradigmatic pathogens (Pendleton et al. 2013). For this, we will construct the phage-bacterium infection networks around those pathogens, to identify single phages and/or phage cocktails with extended bactericidal activities, as assessed in different models of infections, including a Galleria melonnella model of infection and a rat endocarditis model. We expect our methodology to drive a paradigm shift in phage therapy, by offering a time-sparing and easy-to-use way to accurately select phages for each individual patient. The methodology will be made available online and several future developments of this project are already envisioned.	Medical Research Council	Fellowship	290658.0GBP
269	Dr Papiez Bartlomiej	University of Oxford	2018-02-14	2021-02-13	Towards quantitative image analysis of respiratory imaging data for pulmonary disease assessment.	The emergence and rapid dissemination of antibiotic resistance worldwide threatens medical progress. As a consequence, medicine might face a return to the pre-antibiotic era in a very soon future. The paucity of potential new anti-infectives in the pipeline of pharmaceutical industries urges the need for alternatives to fight this public health problem. Phage therapy might represent such an alternative. This re-emerging therapy uses viruses that specifically infect and kill bacteria during their life cycle to reduce/eliminate bacterial load and cure infections. These viruses, called bacteriophages or phages, have been co-evolving with bacteria for billions years, controlling bacterial populations and epidemics, and contributing to their genetic exchanges. With the advantage of having low impact on the commensal flora, as they are highly strain specific, some phages might, nevertheless, harbor virulence factors and drive horizontal gene transfer mediating dissemination of pathogenic traits including antibiotic resistance, calling for their careful selection before their therapeutic use (see below “Phage lifestyle inside the bacteria”). The success of phage therapy mainly relies on the exact matching between both the target pathogenic bacteria and the therapeutic phage. Therefore, having access to a fully characterized phage library is necessary, although not sufficient, to start with phage therapy. An essential and obligate second step to conceive personalized phage therapy treatments is the capacity to predict the interactions between the target pathogen and its potential phage. The long term goal of the proposed research is, therefore, to develop quantitative and predictive in silico models of phage-bacteria infection networks. These models will describe the interactions between phage and bacteria and will serve to fasten the selection of effective phages to propose phage therapy in a personalized fashion.To efficiently predict successful phage-bacteria interactions suitable for phage therapy, we will develop a novel in silico methodology that will, ultimately, enable the selection of phage candidates from an existing phage library to target a given pathogenic bacteria. To achieve this, we will combine genomic information with state-of-the-art bioinformatic and machine learning techniques, taking advantage of the growing amount of interaction data already available as well as of our own data to keep uncovering new phage families. We will ensure that our methodology brings explanatory power along, thereby shedding light on the relevant genomic features underscoring the interactions. To challenge our approach, we will eventually prospectively validate our methodology using paradigmatic pathogens (Pendleton et al. 2013). For this, we will construct the phage-bacterium infection networks around those pathogens, to identify single phages and/or phage cocktails with extended bactericidal activities, as assessed in different models of infections, including a Galleria melonnella model of infection and a rat endocarditis model. We expect our methodology to drive a paradigm shift in phage therapy, by offering a time-sparing and easy-to-use way to accurately select phages for each individual patient. The methodology will be made available online and several future developments of this project are already envisioned.	Medical Research Council	Fellowship	266536.0GBP
270	Professor Counsell Serena	King's College London	2014-09-01	2019-12-31	In vivo microstructural neuroimaging in infants at risk of developing neurocognitive delay or neurobehavioural disorders	The emergence and rapid dissemination of antibiotic resistance worldwide threatens medical progress. As a consequence, medicine might face a return to the pre-antibiotic era in a very soon future. The paucity of potential new anti-infectives in the pipeline of pharmaceutical industries urges the need for alternatives to fight this public health problem. Phage therapy might represent such an alternative. This re-emerging therapy uses viruses that specifically infect and kill bacteria during their life cycle to reduce/eliminate bacterial load and cure infections. These viruses, called bacteriophages or phages, have been co-evolving with bacteria for billions years, controlling bacterial populations and epidemics, and contributing to their genetic exchanges. With the advantage of having low impact on the commensal flora, as they are highly strain specific, some phages might, nevertheless, harbor virulence factors and drive horizontal gene transfer mediating dissemination of pathogenic traits including antibiotic resistance, calling for their careful selection before their therapeutic use (see below “Phage lifestyle inside the bacteria”). The success of phage therapy mainly relies on the exact matching between both the target pathogenic bacteria and the therapeutic phage. Therefore, having access to a fully characterized phage library is necessary, although not sufficient, to start with phage therapy. An essential and obligate second step to conceive personalized phage therapy treatments is the capacity to predict the interactions between the target pathogen and its potential phage. The long term goal of the proposed research is, therefore, to develop quantitative and predictive in silico models of phage-bacteria infection networks. These models will describe the interactions between phage and bacteria and will serve to fasten the selection of effective phages to propose phage therapy in a personalized fashion.To efficiently predict successful phage-bacteria interactions suitable for phage therapy, we will develop a novel in silico methodology that will, ultimately, enable the selection of phage candidates from an existing phage library to target a given pathogenic bacteria. To achieve this, we will combine genomic information with state-of-the-art bioinformatic and machine learning techniques, taking advantage of the growing amount of interaction data already available as well as of our own data to keep uncovering new phage families. We will ensure that our methodology brings explanatory power along, thereby shedding light on the relevant genomic features underscoring the interactions. To challenge our approach, we will eventually prospectively validate our methodology using paradigmatic pathogens (Pendleton et al. 2013). For this, we will construct the phage-bacterium infection networks around those pathogens, to identify single phages and/or phage cocktails with extended bactericidal activities, as assessed in different models of infections, including a Galleria melonnella model of infection and a rat endocarditis model. We expect our methodology to drive a paradigm shift in phage therapy, by offering a time-sparing and easy-to-use way to accurately select phages for each individual patient. The methodology will be made available online and several future developments of this project are already envisioned.	Medical Research Council	Research Grant	814464.0GBP
271	Dr Brown Benjamin	University of Manchester	2019-01-01	2023-01-01	Actionable Analytics Linking Patient, Practitioner and Population Primary Care	The emergence and rapid dissemination of antibiotic resistance worldwide threatens medical progress. As a consequence, medicine might face a return to the pre-antibiotic era in a very soon future. The paucity of potential new anti-infectives in the pipeline of pharmaceutical industries urges the need for alternatives to fight this public health problem. Phage therapy might represent such an alternative. This re-emerging therapy uses viruses that specifically infect and kill bacteria during their life cycle to reduce/eliminate bacterial load and cure infections. These viruses, called bacteriophages or phages, have been co-evolving with bacteria for billions years, controlling bacterial populations and epidemics, and contributing to their genetic exchanges. With the advantage of having low impact on the commensal flora, as they are highly strain specific, some phages might, nevertheless, harbor virulence factors and drive horizontal gene transfer mediating dissemination of pathogenic traits including antibiotic resistance, calling for their careful selection before their therapeutic use (see below “Phage lifestyle inside the bacteria”). The success of phage therapy mainly relies on the exact matching between both the target pathogenic bacteria and the therapeutic phage. Therefore, having access to a fully characterized phage library is necessary, although not sufficient, to start with phage therapy. An essential and obligate second step to conceive personalized phage therapy treatments is the capacity to predict the interactions between the target pathogen and its potential phage. The long term goal of the proposed research is, therefore, to develop quantitative and predictive in silico models of phage-bacteria infection networks. These models will describe the interactions between phage and bacteria and will serve to fasten the selection of effective phages to propose phage therapy in a personalized fashion.To efficiently predict successful phage-bacteria interactions suitable for phage therapy, we will develop a novel in silico methodology that will, ultimately, enable the selection of phage candidates from an existing phage library to target a given pathogenic bacteria. To achieve this, we will combine genomic information with state-of-the-art bioinformatic and machine learning techniques, taking advantage of the growing amount of interaction data already available as well as of our own data to keep uncovering new phage families. We will ensure that our methodology brings explanatory power along, thereby shedding light on the relevant genomic features underscoring the interactions. To challenge our approach, we will eventually prospectively validate our methodology using paradigmatic pathogens (Pendleton et al. 2013). For this, we will construct the phage-bacterium infection networks around those pathogens, to identify single phages and/or phage cocktails with extended bactericidal activities, as assessed in different models of infections, including a Galleria melonnella model of infection and a rat endocarditis model. We expect our methodology to drive a paradigm shift in phage therapy, by offering a time-sparing and easy-to-use way to accurately select phages for each individual patient. The methodology will be made available online and several future developments of this project are already envisioned.	Wellcome Trust	Clinical Research Career Development Fellowship	858035.0GBP
272	Prof de Lusignan Simon	University of Surrey	2018-10-01	2021-03-31	Royal College of General Practitioners (RCGP), Research and Surveillance Centre (RSC) quinquagenarian (QQG) practice network. Creating a longitudinal linked sentinel database of 50 years clinical and virology data and prospective research platform.	The emergence and rapid dissemination of antibiotic resistance worldwide threatens medical progress. As a consequence, medicine might face a return to the pre-antibiotic era in a very soon future. The paucity of potential new anti-infectives in the pipeline of pharmaceutical industries urges the need for alternatives to fight this public health problem. Phage therapy might represent such an alternative. This re-emerging therapy uses viruses that specifically infect and kill bacteria during their life cycle to reduce/eliminate bacterial load and cure infections. These viruses, called bacteriophages or phages, have been co-evolving with bacteria for billions years, controlling bacterial populations and epidemics, and contributing to their genetic exchanges. With the advantage of having low impact on the commensal flora, as they are highly strain specific, some phages might, nevertheless, harbor virulence factors and drive horizontal gene transfer mediating dissemination of pathogenic traits including antibiotic resistance, calling for their careful selection before their therapeutic use (see below “Phage lifestyle inside the bacteria”). The success of phage therapy mainly relies on the exact matching between both the target pathogenic bacteria and the therapeutic phage. Therefore, having access to a fully characterized phage library is necessary, although not sufficient, to start with phage therapy. An essential and obligate second step to conceive personalized phage therapy treatments is the capacity to predict the interactions between the target pathogen and its potential phage. The long term goal of the proposed research is, therefore, to develop quantitative and predictive in silico models of phage-bacteria infection networks. These models will describe the interactions between phage and bacteria and will serve to fasten the selection of effective phages to propose phage therapy in a personalized fashion.To efficiently predict successful phage-bacteria interactions suitable for phage therapy, we will develop a novel in silico methodology that will, ultimately, enable the selection of phage candidates from an existing phage library to target a given pathogenic bacteria. To achieve this, we will combine genomic information with state-of-the-art bioinformatic and machine learning techniques, taking advantage of the growing amount of interaction data already available as well as of our own data to keep uncovering new phage families. We will ensure that our methodology brings explanatory power along, thereby shedding light on the relevant genomic features underscoring the interactions. To challenge our approach, we will eventually prospectively validate our methodology using paradigmatic pathogens (Pendleton et al. 2013). For this, we will construct the phage-bacterium infection networks around those pathogens, to identify single phages and/or phage cocktails with extended bactericidal activities, as assessed in different models of infections, including a Galleria melonnella model of infection and a rat endocarditis model. We expect our methodology to drive a paradigm shift in phage therapy, by offering a time-sparing and easy-to-use way to accurately select phages for each individual patient. The methodology will be made available online and several future developments of this project are already envisioned.	Wellcome Trust	Biomedical Resources Grant	786564.0GBP
273	Dr Nellaker Christoffer	University of Oxford	2016-06-01	2019-07-30	Automated phenotyping to accurately infer functional variants in clinical genetics	The emergence and rapid dissemination of antibiotic resistance worldwide threatens medical progress. As a consequence, medicine might face a return to the pre-antibiotic era in a very soon future. The paucity of potential new anti-infectives in the pipeline of pharmaceutical industries urges the need for alternatives to fight this public health problem. Phage therapy might represent such an alternative. This re-emerging therapy uses viruses that specifically infect and kill bacteria during their life cycle to reduce/eliminate bacterial load and cure infections. These viruses, called bacteriophages or phages, have been co-evolving with bacteria for billions years, controlling bacterial populations and epidemics, and contributing to their genetic exchanges. With the advantage of having low impact on the commensal flora, as they are highly strain specific, some phages might, nevertheless, harbor virulence factors and drive horizontal gene transfer mediating dissemination of pathogenic traits including antibiotic resistance, calling for their careful selection before their therapeutic use (see below “Phage lifestyle inside the bacteria”). The success of phage therapy mainly relies on the exact matching between both the target pathogenic bacteria and the therapeutic phage. Therefore, having access to a fully characterized phage library is necessary, although not sufficient, to start with phage therapy. An essential and obligate second step to conceive personalized phage therapy treatments is the capacity to predict the interactions between the target pathogen and its potential phage. The long term goal of the proposed research is, therefore, to develop quantitative and predictive in silico models of phage-bacteria infection networks. These models will describe the interactions between phage and bacteria and will serve to fasten the selection of effective phages to propose phage therapy in a personalized fashion.To efficiently predict successful phage-bacteria interactions suitable for phage therapy, we will develop a novel in silico methodology that will, ultimately, enable the selection of phage candidates from an existing phage library to target a given pathogenic bacteria. To achieve this, we will combine genomic information with state-of-the-art bioinformatic and machine learning techniques, taking advantage of the growing amount of interaction data already available as well as of our own data to keep uncovering new phage families. We will ensure that our methodology brings explanatory power along, thereby shedding light on the relevant genomic features underscoring the interactions. To challenge our approach, we will eventually prospectively validate our methodology using paradigmatic pathogens (Pendleton et al. 2013). For this, we will construct the phage-bacterium infection networks around those pathogens, to identify single phages and/or phage cocktails with extended bactericidal activities, as assessed in different models of infections, including a Galleria melonnella model of infection and a rat endocarditis model. We expect our methodology to drive a paradigm shift in phage therapy, by offering a time-sparing and easy-to-use way to accurately select phages for each individual patient. The methodology will be made available online and several future developments of this project are already envisioned.	Medical Research Council	Research Grant	319522.0GBP
274	Dr Devine Rory	University of Birmingham	2019-02-01	2021-01-31	Mindreading, Psychopathology and Social Adjustment in Middle Childhood	The emergence and rapid dissemination of antibiotic resistance worldwide threatens medical progress. As a consequence, medicine might face a return to the pre-antibiotic era in a very soon future. The paucity of potential new anti-infectives in the pipeline of pharmaceutical industries urges the need for alternatives to fight this public health problem. Phage therapy might represent such an alternative. This re-emerging therapy uses viruses that specifically infect and kill bacteria during their life cycle to reduce/eliminate bacterial load and cure infections. These viruses, called bacteriophages or phages, have been co-evolving with bacteria for billions years, controlling bacterial populations and epidemics, and contributing to their genetic exchanges. With the advantage of having low impact on the commensal flora, as they are highly strain specific, some phages might, nevertheless, harbor virulence factors and drive horizontal gene transfer mediating dissemination of pathogenic traits including antibiotic resistance, calling for their careful selection before their therapeutic use (see below “Phage lifestyle inside the bacteria”). The success of phage therapy mainly relies on the exact matching between both the target pathogenic bacteria and the therapeutic phage. Therefore, having access to a fully characterized phage library is necessary, although not sufficient, to start with phage therapy. An essential and obligate second step to conceive personalized phage therapy treatments is the capacity to predict the interactions between the target pathogen and its potential phage. The long term goal of the proposed research is, therefore, to develop quantitative and predictive in silico models of phage-bacteria infection networks. These models will describe the interactions between phage and bacteria and will serve to fasten the selection of effective phages to propose phage therapy in a personalized fashion.To efficiently predict successful phage-bacteria interactions suitable for phage therapy, we will develop a novel in silico methodology that will, ultimately, enable the selection of phage candidates from an existing phage library to target a given pathogenic bacteria. To achieve this, we will combine genomic information with state-of-the-art bioinformatic and machine learning techniques, taking advantage of the growing amount of interaction data already available as well as of our own data to keep uncovering new phage families. We will ensure that our methodology brings explanatory power along, thereby shedding light on the relevant genomic features underscoring the interactions. To challenge our approach, we will eventually prospectively validate our methodology using paradigmatic pathogens (Pendleton et al. 2013). For this, we will construct the phage-bacterium infection networks around those pathogens, to identify single phages and/or phage cocktails with extended bactericidal activities, as assessed in different models of infections, including a Galleria melonnella model of infection and a rat endocarditis model. We expect our methodology to drive a paradigm shift in phage therapy, by offering a time-sparing and easy-to-use way to accurately select phages for each individual patient. The methodology will be made available online and several future developments of this project are already envisioned.	Wellcome Trust	Seed Award in Science	97109.0GBP
275	Professor Counsell Serena	King's College London	2014-09-01	2019-12-31	In vivo microstructural neuroimaging in infants at risk of developing neurocognitive delay or neurobehavioural disorders	None	Medical Research Council	Research Grant	814464.0GBP
276	Dr Brown Benjamin	University of Manchester	2019-01-01	2023-01-01	Actionable Analytics Linking Patient, Practitioner and Population Primary Care	None	Wellcome Trust	Clinical Research Career Development Fellowship	858035.0GBP
277	Prof de Lusignan Simon	University of Surrey	2018-10-01	2021-03-31	Royal College of General Practitioners (RCGP), Research and Surveillance Centre (RSC) quinquagenarian (QQG) practice network. Creating a longitudinal linked sentinel database of 50 years clinical and virology data and prospective research platform.	None	Wellcome Trust	Biomedical Resources Grant	786564.0GBP
278	Dr Nellaker Christoffer	University of Oxford	2016-06-01	2019-07-30	Automated phenotyping to accurately infer functional variants in clinical genetics	None	Medical Research Council	Research Grant	319522.0GBP
279	Dr Chikowore Tinashe	University of the Witwatersrand	2019-02-01	2022-02-01	Characterisation of gene-lifestyle interactions associated with obesity-related traits in African populations	None	Wellcome Trust	International Training Fellowship - Full	224233.0GBP
280	Dr Chikowore Tinashe	University of Exeter	2019-02-01	2022-02-01	COVID-19 (Mis)Information Exposure and Messaging Effects in the United Kingdom	Fighting the COVID-19 pandemic requires understanding what information people have about the disease, which misperceptions might be prevalent, and how officials can improve public knowledge and encourage behaviours that will protect public health. First, this study will measure beliefs and attitudes about COVID-19, providing a thorough map of accurate knowledge, but also of misperceptions, particularly those driven by conspiratorial thinking. Second, the study will catalogue the online sources from which our respondents get COVID-19-related information. This will allow us to gauge which accurate information sources reach a wide audience and which sources of online misinformation are systematically distorting people's views. Third, the study will test whether public health messages that seek to correct misinformation are actually effective in changing people's beliefs. Behavioural and survey data will be collected in a multi-wave nationally representative survey that measures both prevalence of false and accurate beliefs about COVID-19 (including beliefs in misperceptions and conspiracy theories) and support for recommendations from public health authorities. To evaluate responses to information from public health officials, the second survey wave will include a randomized experiment evaluating the effects of messaging from health and medical authorities. Finally, the study will measure the quality of the information people consume online about the pandemic by analysing the behavioural data provided by respondents using a combination of human-coded and machine learning approaches. These data will make it possible to identify which groups are most frequently exposed to inaccurate or untrustworthy information about COVID-19, which should aid in the design of effective interventions.	UK Research and Innovation	International Training Fellowship - Full	308109.0GBP
281	Professor Loram Ian	Manchester Metropolitan University	2020-01-01	2022-12-31	Quantification of head and trunk control for children with neuromotor and neuromuscular disorders	Fighting the COVID-19 pandemic requires understanding what information people have about the disease, which misperceptions might be prevalent, and how officials can improve public knowledge and encourage behaviours that will protect public health. First, this study will measure beliefs and attitudes about COVID-19, providing a thorough map of accurate knowledge, but also of misperceptions, particularly those driven by conspiratorial thinking. Second, the study will catalogue the online sources from which our respondents get COVID-19-related information. This will allow us to gauge which accurate information sources reach a wide audience and which sources of online misinformation are systematically distorting people's views. Third, the study will test whether public health messages that seek to correct misinformation are actually effective in changing people's beliefs. Behavioural and survey data will be collected in a multi-wave nationally representative survey that measures both prevalence of false and accurate beliefs about COVID-19 (including beliefs in misperceptions and conspiracy theories) and support for recommendations from public health authorities. To evaluate responses to information from public health officials, the second survey wave will include a randomized experiment evaluating the effects of messaging from health and medical authorities. Finally, the study will measure the quality of the information people consume online about the pandemic by analysing the behavioural data provided by respondents using a combination of human-coded and machine learning approaches. These data will make it possible to identify which groups are most frequently exposed to inaccurate or untrustworthy information about COVID-19, which should aid in the design of effective interventions.	Medical Research Council	Research Grant	774510.0GBP
282	Professor Loram Ian	QUEEN MARY UNIVERSITY OF LONDON	2019-07-01	2020-06-30	Implementation of a 3D computational mouse atlas for detection of pancreatic tumours in transgenic mice	High impact cancer research demands the use of clinically relevant disease models. One approach is to use genetically modified mouse models (GEMM's) and this has seen an increase in the number of animals bred for scientific research, a high proportion of which are wasted as they don't have the correct phenotype. Tumour development in GEMMs is difficult to evaluate as their disease develops over long periods of time, usually deep within the body and so tumour detection and measurement of response is challenging unless the animal is sacrificed. The mouse pancreas is an ill-defined organ and in GEMMs of pancreatic cancer, the development of tumour is extremely difficult to assess non-invasively. Magnetic resonance imaging (MRI) and ultrasound (US) are both useful but their potential to reduce animal numbers by providing early detection of tumour and accurate longitudinal imaging data is limited for a number of reasons. MRI is preferred but can be expensive, less widely available and accurate determination of tumour volume via image analysis of the abdominal area is challenging. However, we have developed a 3D computational mouse atlas (3D-CAMMP) that can automatically detect pancreas and pancreatic tumour (with an accuracy of 95%) in MR images of a commonly-used pancreatic GEMM (the KPC mouse). This machine learning model uses imaging data collected on a relatively inexpensive low field MRI instrument. Automatic detection allows a non-expert user to perform MRI imaging and analysis with minimal training. In this project we will transfer use of the 3D-CAMMP mouse atlas to three cancer institutes in the UK, all with high numbers of KPC mice. We are confident that early and accurate identification of tumours will allow optimised evaluation of potential treatments to reduce variability, experiment duration and ultimately have the effect of reducing numbers of required animals used in pancreatic cancer research.	National Centre for the Replacement, Refinement and Reduction of Animals in Research	Skills and Knowledge Transfer	75593.0GBP
283	Dr Cangiani Andrea	University of Nottingham	2019-12-31	2021-12-30	LOng-Term anatomical fluid dynamics for new Univentricular heartS palliation (LOTUS)	High impact cancer research demands the use of clinically relevant disease models. One approach is to use genetically modified mouse models (GEMM's) and this has seen an increase in the number of animals bred for scientific research, a high proportion of which are wasted as they don't have the correct phenotype. Tumour development in GEMMs is difficult to evaluate as their disease develops over long periods of time, usually deep within the body and so tumour detection and measurement of response is challenging unless the animal is sacrificed. The mouse pancreas is an ill-defined organ and in GEMMs of pancreatic cancer, the development of tumour is extremely difficult to assess non-invasively. Magnetic resonance imaging (MRI) and ultrasound (US) are both useful but their potential to reduce animal numbers by providing early detection of tumour and accurate longitudinal imaging data is limited for a number of reasons. MRI is preferred but can be expensive, less widely available and accurate determination of tumour volume via image analysis of the abdominal area is challenging. However, we have developed a 3D computational mouse atlas (3D-CAMMP) that can automatically detect pancreas and pancreatic tumour (with an accuracy of 95%) in MR images of a commonly-used pancreatic GEMM (the KPC mouse). This machine learning model uses imaging data collected on a relatively inexpensive low field MRI instrument. Automatic detection allows a non-expert user to perform MRI imaging and analysis with minimal training. In this project we will transfer use of the 3D-CAMMP mouse atlas to three cancer institutes in the UK, all with high numbers of KPC mice. We are confident that early and accurate identification of tumours will allow optimised evaluation of potential treatments to reduce variability, experiment duration and ultimately have the effect of reducing numbers of required animals used in pancreatic cancer research.	Medical Research Council	Research Grant	322383.0GBP
284	Dr Laine Romain	University College London	2019-12-28	2022-12-27	How does virus shape relate to infectivity?	High impact cancer research demands the use of clinically relevant disease models. One approach is to use genetically modified mouse models (GEMM's) and this has seen an increase in the number of animals bred for scientific research, a high proportion of which are wasted as they don't have the correct phenotype. Tumour development in GEMMs is difficult to evaluate as their disease develops over long periods of time, usually deep within the body and so tumour detection and measurement of response is challenging unless the animal is sacrificed. The mouse pancreas is an ill-defined organ and in GEMMs of pancreatic cancer, the development of tumour is extremely difficult to assess non-invasively. Magnetic resonance imaging (MRI) and ultrasound (US) are both useful but their potential to reduce animal numbers by providing early detection of tumour and accurate longitudinal imaging data is limited for a number of reasons. MRI is preferred but can be expensive, less widely available and accurate determination of tumour volume via image analysis of the abdominal area is challenging. However, we have developed a 3D computational mouse atlas (3D-CAMMP) that can automatically detect pancreas and pancreatic tumour (with an accuracy of 95%) in MR images of a commonly-used pancreatic GEMM (the KPC mouse). This machine learning model uses imaging data collected on a relatively inexpensive low field MRI instrument. Automatic detection allows a non-expert user to perform MRI imaging and analysis with minimal training. In this project we will transfer use of the 3D-CAMMP mouse atlas to three cancer institutes in the UK, all with high numbers of KPC mice. We are confident that early and accurate identification of tumours will allow optimised evaluation of potential treatments to reduce variability, experiment duration and ultimately have the effect of reducing numbers of required animals used in pancreatic cancer research.	Medical Research Council	Fellowship	301830.0GBP
285	Mr Harrison Conrad	University of Oxford	2019-12-28	2022-12-27	Transforming outcome measures in plastic surgery through computer science	High impact cancer research demands the use of clinically relevant disease models. One approach is to use genetically modified mouse models (GEMM's) and this has seen an increase in the number of animals bred for scientific research, a high proportion of which are wasted as they don't have the correct phenotype. Tumour development in GEMMs is difficult to evaluate as their disease develops over long periods of time, usually deep within the body and so tumour detection and measurement of response is challenging unless the animal is sacrificed. The mouse pancreas is an ill-defined organ and in GEMMs of pancreatic cancer, the development of tumour is extremely difficult to assess non-invasively. Magnetic resonance imaging (MRI) and ultrasound (US) are both useful but their potential to reduce animal numbers by providing early detection of tumour and accurate longitudinal imaging data is limited for a number of reasons. MRI is preferred but can be expensive, less widely available and accurate determination of tumour volume via image analysis of the abdominal area is challenging. However, we have developed a 3D computational mouse atlas (3D-CAMMP) that can automatically detect pancreas and pancreatic tumour (with an accuracy of 95%) in MR images of a commonly-used pancreatic GEMM (the KPC mouse). This machine learning model uses imaging data collected on a relatively inexpensive low field MRI instrument. Automatic detection allows a non-expert user to perform MRI imaging and analysis with minimal training. In this project we will transfer use of the 3D-CAMMP mouse atlas to three cancer institutes in the UK, all with high numbers of KPC mice. We are confident that early and accurate identification of tumours will allow optimised evaluation of potential treatments to reduce variability, experiment duration and ultimately have the effect of reducing numbers of required animals used in pancreatic cancer research.	National Institute for Health Research (Department of Health)	Fellowship	384813.0GBP
286	Dr. LUGHOFER Edwin	University of Linz	2020-03-01	2023-02-28	Interactive Machine Learning with Evolving Fuzzy Systems	High impact cancer research demands the use of clinically relevant disease models. One approach is to use genetically modified mouse models (GEMM's) and this has seen an increase in the number of animals bred for scientific research, a high proportion of which are wasted as they don't have the correct phenotype. Tumour development in GEMMs is difficult to evaluate as their disease develops over long periods of time, usually deep within the body and so tumour detection and measurement of response is challenging unless the animal is sacrificed. The mouse pancreas is an ill-defined organ and in GEMMs of pancreatic cancer, the development of tumour is extremely difficult to assess non-invasively. Magnetic resonance imaging (MRI) and ultrasound (US) are both useful but their potential to reduce animal numbers by providing early detection of tumour and accurate longitudinal imaging data is limited for a number of reasons. MRI is preferred but can be expensive, less widely available and accurate determination of tumour volume via image analysis of the abdominal area is challenging. However, we have developed a 3D computational mouse atlas (3D-CAMMP) that can automatically detect pancreas and pancreatic tumour (with an accuracy of 95%) in MR images of a commonly-used pancreatic GEMM (the KPC mouse). This machine learning model uses imaging data collected on a relatively inexpensive low field MRI instrument. Automatic detection allows a non-expert user to perform MRI imaging and analysis with minimal training. In this project we will transfer use of the 3D-CAMMP mouse atlas to three cancer institutes in the UK, all with high numbers of KPC mice. We are confident that early and accurate identification of tumours will allow optimised evaluation of potential treatments to reduce variability, experiment duration and ultimately have the effect of reducing numbers of required animals used in pancreatic cancer research.	Austrian Science Fund FWF	Stand-Alone Projects	409109.4EUR
287	Dr. LUGHOFER Edwin	Arizona State University	2020-04-15	2021-03-31	Collaborative Research: RAPID: RTEM: Rapid Testing as Multi-fidelity Data Collection for Epidemic Modeling	Biological Sciences - The novel coronavirus (COVID-19) epidemic is generating significant social, economic, and health impacts and has highlighted the importance of real-time analysis of the spatio-temporal dynamics of emerging infectious diseases. COVID-19, which emerged out of the city of Wuhan in China in December 2019 is now spreading in multiple countries. It is particularly concerning that the case fatality rate appears to be higher for the novel coronavirus than for seasonal influenza, and especially so for older populations and those with prior health conditions such as cardiovascular disease and diabetes. Any plan for stopping the epidemic must be based on a quantitative understanding of the proportion of the at-risk population that needs to be protected by effective control measures in order for transmission to decline sufficiently and quickly enough for the epidemic to end. Different data collection and testing modalities and strategies available to help calibrate transmission models and predict the spread/severity of a disease, have variable costs, response times, and accuracies. In this Rapid Response Research (RAPID) project, the team will examine the problem of establishing optimal practices for rapid testing for the novel coronavirus. The result will be the Rapid Testing for Epidemic Modeling (RTEM), which will translate into science-based predictions of the COVID-19 epidemic's characteristics, including the duration and overall size, and help the global efforts to combat the disease. The RTEM will fill an important gap in data-driven decision making during the COVID-19 epidemic and, thus, will enable services with significant national economic and health impact. The educational impact of the project will be on mentoring of post-doctoral and PhD researchers and on curricula by incorporating research challenges and outcomes into existing undergraduate and graduate classes. <br/><br/>Computational models for the spatio-temporal dynamics of emerging infectious diseases and data- and model-driven computer simulations for disease spreading are increasingly critical in predicting geo-temporal evolution of epidemics as well as designing, activating, and adapting practices for controlling epidemics. In this project, the researchers tackle a Rapid Testing for Epidemic Modeling (RTEM) problem: Given a partially known target disease model and a set of testing modalities (from surveys to surveillance testing at known disease hotspots), with varying costs, accuracies, and observational delays, what is the best rapid testing strategy that would help recover the underlying disease model? Several scientific questions arise: What is the value of testing? Should only sick people be tested for virus detection? What level of resources should be devoted to the development of highly accurate tests (low false positives, low false negatives)? Is it better to use only one type of test aiming at the best cost/effectiveness trade off, or a non-homogeneous testing policy? Naturally these questions need to be investigated at the interface of epidemiology, computer science, machine learning, mathematical modeling and statistics. As part of the work, the team will develop a model of transmission dynamics and control, tailored to COVID-19 in a way that accommodates diagnostic testing with varying fidelities and delays underlying a rapid testing regimen. The investigators will further integrate the resulting RTEM-SEIR model with EpiDMS and DataStorm for executing continuous coupled simulations.<br/><br/>This project is jointly funded through the Ecology and Evolution of Infectious Diseases program (Division of Environmental Biology) and the Civil, Mechanical and Manufacturing innovation program (Engineering).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	122998.0USD
288	Dr. LUGHOFER Edwin	Florida International University	2020-05-01	2021-04-30	RAPID: #COVID-19: Understanding Community Response in the Emergence and Spread of Novel Coronavirus through Health Risk Communications in Socio-Technical Systems	Computer and Information Science and Engineering - Risk perception and risk averting behaviors of vulnerable communities in the emergence and spread of COVID-19 are spatio-temporal functions of individual or group interactions with their online social neighbors within or outside their communities and such interactions need to be captured through diverse information channels (e.g. traditional outlets such as radio, television, internet and/or non-traditional outlets such as social media). The primary goal of this Rapid Response Research (RAPID) project is to collect time-sensitive online social media and crowd-sourced data and analyze patterns of health-risk communication and community response in the emergence and spread of novel Coronavirus using data-driven methods and network science theories. The major focus will be towards understanding how individuals are socially influenced online, while communicating risk and interacting in their respective communities as the disease continues to spread. The notion of influence will be captured by quantifying the network effects on such communication behavior and characterizing how information is exchanged among people who are socially connected online and exposed to health risk in such outbreaks of disease. Given that communities responded to COVID-19 with limited or no preparation and there is uncertainty in the length of recovery for the communities already affected while new communities being threatened, the data collection effort requires rapid response for better coverage and careful monitoring. The data will include large-scale ephemeral online interactions of people in the affected communities and public officials who are involved in COVID-19 response, recovery, and mitigation efforts, followed by a data-driven network analytics and infographics of COVID-19 risk communication strategies and risk averting behaviors adopted. The proposed research will not only expand the knowledge base of spatio-temporal dynamics of risk perception and dissemination strategies in the emergence and aftermath of a major disease outbreak, but will also result in data-driven inference techniques to improve our understanding of how people express diverse concerns and how to harness and embed such information for designing intervention measures. The methodologies and findings of this rapid response research will benefit emergency management and public health agencies to define targeted information dissemination policies for public with diverse needs based on how people reacted to COVID-19 and their social network characteristics, activities, and interactions in response to similar public health hazards.<br/><br/>Public engagement in risk communication can lead to more effective decision-making and enhanced public feedback to the regulatory process. The primary goal of this RAPID project is to mine and analyze large-scale time-sensitive perishable crowd-sourced and social media data (rich spatio-temporal data) and reveal patterns of health-risk communication and community response in the emergence and spread of novel Coronavirus using data-driven methods and network science theories. The specific aims are threefold: (1) to document how public interact and communicate health risk information through their online social networks during a major disease outbreak; (2) to authenticate data from multiple sources and detect anomalies to avoid information overload and spread of misinformation; and (3) to examine how online social networks influence protective actions (e.g., social distancing, self-quarantine decisions) i.e. information cascades in health risk communication. To achieve the goal and aims, the project will utilize ephemeral time and geo-tagged social media interactions of users, agencies, news sources supplemented with crowd-sourced information on COVID-19. This study will have five theoretical and methodological contributions to the literature. It will: (1) advance our understanding of how individuals are socially influenced online, while communicating health risks and interacting in their respective communities as the disease continues to spread; (2) inform the literature on how information is exchanged among people who are socially connected online and exposed to health risk in such outbreaks of disease; (3) use novel machine-learning and network science models to quantify influence and network effects on such communication behavior; (4) capture the variability in network composition, risk communication strategies and risk averting behaviors adopted based on spatio-temporal correlations of risk and disease contagion; (5) ensure authenticity of the collected data from multiple sources and develop more accurate fully-distributed computational algorithms tailored to health risk anomaly detection in socio-technical systems. The findings from this research will be useful to public health and emergency management agencies for tailoring effective information dissemination policies for diverse user groups based on their social network characteristics, activities, and interactions in response to similar public health hazards. The methodologies, and implications of this research can be transferred in designing effective intervention policies to other natural and man-made disaster contexts in which public health risks become major concerns. The project will engage, mentor, and offer an innovative active learning environment for K-12, undergraduate, and graduate students by giving priority to disadvantaged and underrepresented communities in USA. The project will train students on computational skills required for collecting, storing, processing, analyzing and modeling large-scale data using high performance computational resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	79380.0USD
289	Dr. LUGHOFER Edwin	University of Pittsburgh	2020-05-01	2021-04-30	RAPID: Countering COVID-19 Misinformation via Situation-Aware Visually Informed Treatment	Computer and Information Science and Engineering - As the COVID-19 pandemic spreads, countries and cities around the globe have taken stringent measures including quarantine and regional lockdown. The increasing isolation, along with the panic and anxiety, creates challenges for countering misinformation--people are increasingly tapping into online information sources already familiar to them with declining chances of accessing alternative stories. This project will develop mechanisms based on text and image analysis, social psychology, and crowd-sourcing that can be used in a timely manner to counter misinformation during the ongoing COVID-19 crisis and beyond. One of the novel features of the approach is to deal with a specific instance of misinformation by crowd-sourcing authentic images that counter this misinformation. This research will contribute to the scientific understanding of misinformation and of persuasive narrative construction, to the assessment of risk for the spread of misinformation, and to the development of mechanisms to counter misinformation. <br/><br/>The technical aims of this project are divided into three thrusts. The first thrust will investigate what information content and which specific part of a multimodal social media post (e.g, a piece of text, text with an image, image with an embedded slogan) will receive stronger responses and hence increase the likelihood of the post being shared. The second thrust will create metrics to assess the likelihood of the spread of misinformation based on predictors learned from the content to which users are exposed. The third thrust will focus on the development of a system to counter misinformation based on citizen journalists? inputs of field investigations and on machine learning techniques. Finally, the system will be evaluated by survey studies and interviews to examine the system?s usability, usefulness, and effectiveness in reducing the spreading and impact of misinformation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	104491.0USD
290	Dr. LUGHOFER Edwin	George Washington University	2020-05-01	2021-04-30	RAPID: A novel platform for data integration and deep learning on COVID-19	Biological Sciences - The COVID-19 pandemic, caused by the SARS-CoV-2 virus, has fundamentally changed the world, and yet its ultimate impact is unknown. While China has experienced a slowdown in new cases, infections in the US continue to rise and are threatening to exceed our health care system?s capacity. Tests capacities are limited compared to the need, hospital services are becoming overwhelmed, and critical supplies are in shortage. There is a diversity of efforts currently ongoing to develop both new treatments as well as vaccine strategies to combat COVID-19. Yet, we know from experience, the virus will evolve solutions to both host immune systems and intervention strategies. In order to diminish both the short-term and long-term impacts of COVID-19, it is essential to develop robust, repeatable, and accessible tools to integrate and analyze the diversity of data becoming available in the face of the COVID-19 pandemic. The development of a platform to characterize the dynamic nature of mutations in the virus and testing for associations with clinical variables and biomarkers is an essential broader impact and will help in making informed predictions of health outcomes such as the stage of the severity of the disease and efficacy of treatment. Additionally, this project provides professional development opportunities for early career researchers.<br/><br/>Advances in omics technologies provide a broad and deep range of genotypic and phenotypic data to integrate with clinical phenotypes. Machine learning techniques such as clustering using phylogenetic distance and Deep Neural Networks (DNNs) are suitable techniques to link these DNA level changes to clinical metadata for human disease prediction, diagnosis, and therapeutics. This project develops tools within an open-source platform for documented, repeatable analyses that can be conducted in real-time allowing integration of data from patients with new treatments/vaccines strategies. This deep learning bioinformatics platform will allow the prioritization of genes associated with outcome predictors, including health, therapeutic, and vaccine outcomes, as well as inform improved DNA tests for predicting disease status and severity. The computational tools developed in this study will provide the research community and health professionals with comprehensive and generic approaches for characterizing the dynamics of genotype/phenotype associations in viruses. Such tools allow healthcare professionals and researchers to address specific properties of viruses such as frequency and location of mutations across the viral genome. When added to other clinical and epidemiological data, such information could help pave the way to better treatments or a vaccine. The developed platform will provide a venue for robust, open, repeatable analyses of COVID-19 as more and more data become available.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	200000.0USD
291	Dr. LUGHOFER Edwin	George Mason University	2020-05-15	2021-04-30	RAPID/Collaborative Research: Human-AI Teaming for Big Data Analytics to Enhance Response to the COVID-19 Pandemic	Engineering - Social media data can provide important clues and local knowledge that can help emergency managers and responders better comprehend and capture the evolving nature of many disasters. Yet humans alone cannot grasp the vast data generated by social media, so computers are used to assist. Very little is currently known about how to leverage the skills of humans and machines when they work together (human-machine teaming) to identify meaningful patterns in social media data. Therefore, the fundamental issues this Rapid Response Research (RAPID) project seeks to address are 1) understanding the process of real-time decisions that human digital volunteers make when they rapidly convert social media data into structured codes the machine (Artificial Intelligence algorithms) can understand, and 2) using this knowledge to improve human-machine teaming. This project advances the field by revealing the unique abilities that both humans and machines bring when working together to comprehend social media patterns during an evolving disaster. It supports education and diversity by providing research experiences to diverse students, as well as generating data useful for interdisciplinary courses teaching teamwork, social media analysis, and human-machine teaming. Finally, the findings can help emergency managers better train their volunteers who comb through social media using their understanding of the local knowledge and built environment to help machines see new patterns in data. Hence, this project supports NSF's mission to promote the progress of science and to advance the nation's health, prosperity, and welfare by articulating the unique value that both humans and computers bring that can lead to better decisions during disasters. The goal of this research is to better understand the real-time decisions that human annotators make under different environmental constraints, and how those contribute to the learning of Artificial Intelligence (AI) models. Under time constraints and information overload, human decision-making capabilities are limited; yet, humans still have a unique ability to understand the contextual references to the structures in the built environment that machines cannot recognize. For example, the meaning of the tweet, ?Memorial is overloaded,? -- which means the hospital, called Memorial, is out of beds for patients ?- can be lost on AI systems that lack the knowledge of the built environment. This example demonstrates the value that humans in the loop offer in a human-AI teaming context. <br/><br/>This research focuses on capturing the ephemeral data from a variety of social media sources and our two research thrusts include: 1) online observations of Community Emergency Response Team (CERT) volunteers and a manager (a collaborator on this project) using think-aloud and cognitive interviewing strategies to reveal the real-time mental models used to make coding decisions for annotation tasks; and 2) an empirical analysis of different sampling algorithms for active (machine) learning paradigms to develop a typology of machine errors under diverse contexts that affect the quality of human decision making for annotation. This research will generate design guidelines that bridge the gap between the mechanisms used for real-time data processing with AI models and the understanding of context contributed by a human user teaming with the AI models. Using theories of human decision-making combined with knowledge of how AI functions, this project provides a real-time, mid-disaster examination of 1) how humans understand, process, and interpret social media messages, and 2) how to refine AI algorithms to optimize active learning paradigm. This understanding will provide a theoretical framework enabling future research to develop protocols to optimize human-AI teaming by using concepts such as motivation and information theory. This work can help emergency managers conduct better training of their CERT volunteers and other annotators and provide clearer guidelines for how to communicate the unique value that humans bring to the annotation process for AI systems. Both our protocols and developed understanding of how humans interact with AI systems will be helpful for global health organizations, local and state-level disaster decision-makers, as well as provide direction for the vast CERT network in the United States.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	24266.0USD
292	Dr. LUGHOFER Edwin	Auburn University	2020-05-15	2021-04-30	RAPID/Collaborative Research: Quantifying Social Media Data for Improved Modeling of Mitigation Strategies for the COVID-19 Pandemic	Engineering - This Rapid Response Research (RAPID) grant will support research that will contribute new knowledge related to modeling social behavior and community activity during the COVID-19 pandemic, as well as future pandemics with COVID-19 characteristics. The model focuses on compliance with mitigation strategies and public health guidelines, thus enabling the selection of policies that are most effective in promoting both the progress of science and advancing national health and prosperity. Various pandemic models are currently being used to predict the spread of a virus and establish which mitigation strategies are the most effective. These models are heavily based on assumptions and may include an oversimplified reality of how populations react and behave. This research will provide needed knowledge and methods for the development of a model of how individuals in the U.S. react to certain mitigation strategies, such as social-distancing, stay-at-home orders, quarantines, and travel advisories, by mining and analyzing social media data during the COVID-19 crisis. This enhanced modeling approach and its resultant model will be of great value to disaster response managers and policy/decision makers to understand human social behavior. This work allows assessment of the effectiveness of mitigation strategies and public health guidelines during pandemics (and other crises). This project will also form the basis of a publicly available case study suitable for university level students that can be widely incorporated in courses. <br/><br/>Although individual-based and homogeneous mixing pandemic models provide useful insights and predictive capabilities within a range of possibilities, they are highly sensitive to people?s actions. This research aims to provide an enhanced approach to model social behavior and community activity during a pandemic in terms of compliance with mitigation strategies and public health guidelines. Social media data present a brief window of opportunity for research on how, and to what extent, the public does or does not comply with the recommended mitigation strategies and public health guidelines. The research team will collect real-time data from social media related to COVID19-exposed regional populations in the U.S. The data will be analyzed using machine learning techniques to identify non-mutually exclusive clusters of people based on similarity of their demographic, geographic, and time information, and establish relationships among clusters. The analyzed data will form the basis of a data-driven multi-paradigm simulation model that captures changes in public sentiment over time, quantifies the resistance/compliance with mitigation strategies and health guidelines, and gauges overall effectiveness of various mitigation strategies and advice over time.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	141527.0USD
293	Dr. LUGHOFER Edwin	University of Michigan Ann Arbor	2020-05-15	2021-04-30	RAPID: Subtyping and Identifying Shared Genomic Sequences of SARS-CoV-2 (COVID-19)	Biological Sciences - People are threatened by an unprecedented pandemic: COVID-19 (caused by SARS-CoV-2). This virus is now threatening not only physical health, but also psychology, education, economy, and every corner of the infrastructure of society. So far, there is no treatment for this disease, while vaccines and neutralizing antibodies are perceived as one of the eventual solutions to this crisis. A critical piece of knowledge supporting vaccine and antibody development is understanding the genome of this virus. What is the common sequence shared among the SARS-CoV-2 strains across the globe? What are the subtypes? Are the genome variances overlapping with important genomic regions for vaccine design? Using state of the art machine learning approaches, this research will identify the shared, representative sequence across SARS-CoV-2 strains and group them by major types. This project will connect this information to the important genomic regions identified in the literature that can be used for vaccines, and thereby continuously inform the ongoing effort of vaccine development, antibody selection, and therapeutic development. The research from this study would provide society benefits through monthly updates onto web interfaces that allow the vaccine developers, the biomedical research community as well as the general public to conveniently get access to the above information. This project will support training of a graduate student in bioinformatics and provide outreach opportunities to K-12 students and the public.<br/><br/>This work will be a continuous effort to monthly subtype SARS-CoV-2 strains and update the shared sequences of SARS-CoV-2, in order to facilitate vaccine development and antibody design. Specifically, this research will be focusing on three objectives: 1) identifying and updating the common sequences of SARS-CoV-2 by forcing the common sequence to have the minimal evolutionary distance with all strains, or covering as many sequences as possible 2) subtyping the SARS-CoV-2 strains into major groups, which will be important to inform treatment, management and prevention measures; 3) connecting the subtyped and common genomic sequences of SARS-CoV-2 to epitopes identified in the literature. To develop vaccines or neutralizing antibody treatment, it is critical that the major variations are covered and considered. The algorithms and visualization tools will overlay the curated list of potential epitopes on top of the subtypes and the shared sequence of the virus genomes, and directly support the effort of vaccine and antibody development. This RAPID award is made by the Systematics and Biodiversity Science Cluster in the Division of Environmental Biology, using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	199705.0USD
294	Dr. LUGHOFER Edwin	University of California-Los Angeles	2020-06-01	2021-05-31	RAPID: Dynamic Graph Neural Networks for Modeling and Monitoring COVID-19 Pandemic	Computer and Information Science and Engineering - The novel coronavirus, COVID-19, has become one of the biggest pandemics in human history and has generated lasting impacts on public health, society, and economy. The number of cases in the United States has passed 1 million with a total number of deaths over 50 thousand. There is an urgent need for research and development that can bring a predictive understanding of the spread of the virus, thereby enabling mitigation methods to alleviate the negative effects of COVID-19. Traditional epidemiological models usually take into consideration only a small number of features in building a prediction model, which may not be able to capture potential risk factors and effects of various intervention mechanisms of this new pandemic. In this project the investigators develop novel machine learning methods that can simultaneously model and predict the COVID-19 spread, detect and monitor risk factors, and evaluate effectiveness of interventions over time and space. <br/><br/>The new model ingests and integrates heterogeneous and rapidly accumulating data across diverse sources, such as publications, news, census, social media, and outbreak observation trackers. It employs a new contextualized language model to accurately recognize named entities and relations from vast text data and build knowledge graphs to extract potential risk factors. A dynamic graph is constructed. Each location node may have a set of static and time-dependent attributes. Events, individual behaviors, social activities, interventions are mapped to activity nodes with edges connecting to the corresponding location nodes at the time. A novel dynamic graph neural network is trained to perform joint predictions of all locations over time. Activity nodes of significant attention weights represent major risk factors or effective intervention mechanisms. The project will result in public dissemination of the prediction model and all source codes, immediately benefiting the combat against COVID-19.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	90000.0USD
295	Dr. PETUTSCHNIGG Alexander	University of Applied Sciences Salzburg	2018-02-01	2021-01-31	TreeTrace - Biometric fingerprints of trees: log tracing from forest to sawmill and early esti	With the increasing amount of imaging devices installed at sawmills, the importance of using these data for improving workflow and for increasing revenues in the wood processing industries is growing. In this context, challenging questions with respect to imaging and image processing technology arise, several of which will be tackled in this joint project. The project considers two application cases as follows: The first application case is the question of tracing tree logs from the forest harvesting site to the sawmill by using biometrics related tree log recognition techniques based on image processing of cross-section data only. This approach of course assumes the additional availability of imaging sensors in the forest. Since there is a trend for installing CT imaging devices at sawmills, which are of course not available in the forest, the challenging issue of cross modality matching arises. The second application case is the determination of wood quality from cross-section imagery, applicable already in the forest, and/or at the sawmill. Obviously, these two application cases share many aspects. (1) They can be combined at application level, i.e. wood quality may be determined already in the forest due to imaging devices available for the tracing application, and further refined using the sensors available at the sawmill. Conversely, CT data from the sawmill, acquired to analyse the wood quality, can be used for the tracing application; (2) data preprocessing and many features extracted are required for both, matching cross section images as well as automated wood quality analysis; (3) the questions which imaging sensors should be employed and how the resulting data can be combined effectively have to be answered. Thus, synergies arise between these two application cases which will be efficiently exploited in the project. A common data set for experimental validation can be used (which implies also sharing employed sensors), ground truth data established wrt. annotating images can be shared, many software components implementing preprocessing (e.g. pith detection, cross section texture segmentation, contrast optimisation) as well as feature extraction techniques (e.g. annual ring detection, spiral growth detection) can be developed jointly and shared subsequently. The project will break new grounds in the area of wood imaging and processing of corresponding data with advanced algorithms in vision and machine learning with particular focus on cross modality processing. While those techniques are being developed for two specific application cases, the developed algorithms will be applicable to a wide range of applications in wood imagery processing and analysis as well as for other domains where similar settings arise.	Austrian Science Fund FWF	Research Grant	328573.89EUR
296	Dr Deprez Maria	King's College London	2019-06-15	2021-06-14	Artificial intelligence to characterise abnormal brain development and neurocognitive disorders	With the increasing amount of imaging devices installed at sawmills, the importance of using these data for improving workflow and for increasing revenues in the wood processing industries is growing. In this context, challenging questions with respect to imaging and image processing technology arise, several of which will be tackled in this joint project. The project considers two application cases as follows: The first application case is the question of tracing tree logs from the forest harvesting site to the sawmill by using biometrics related tree log recognition techniques based on image processing of cross-section data only. This approach of course assumes the additional availability of imaging sensors in the forest. Since there is a trend for installing CT imaging devices at sawmills, which are of course not available in the forest, the challenging issue of cross modality matching arises. The second application case is the determination of wood quality from cross-section imagery, applicable already in the forest, and/or at the sawmill. Obviously, these two application cases share many aspects. (1) They can be combined at application level, i.e. wood quality may be determined already in the forest due to imaging devices available for the tracing application, and further refined using the sensors available at the sawmill. Conversely, CT data from the sawmill, acquired to analyse the wood quality, can be used for the tracing application; (2) data preprocessing and many features extracted are required for both, matching cross section images as well as automated wood quality analysis; (3) the questions which imaging sensors should be employed and how the resulting data can be combined effectively have to be answered. Thus, synergies arise between these two application cases which will be efficiently exploited in the project. A common data set for experimental validation can be used (which implies also sharing employed sensors), ground truth data established wrt. annotating images can be shared, many software components implementing preprocessing (e.g. pith detection, cross section texture segmentation, contrast optimisation) as well as feature extraction techniques (e.g. annual ring detection, spiral growth detection) can be developed jointly and shared subsequently. The project will break new grounds in the area of wood imaging and processing of corresponding data with advanced algorithms in vision and machine learning with particular focus on cross modality processing. While those techniques are being developed for two specific application cases, the developed algorithms will be applicable to a wide range of applications in wood imagery processing and analysis as well as for other domains where similar settings arise.	The Academy of Medical Sciences	Springboard Round 4	99895.85GBP
297	Dr Schweikert Gabriele	University of Dundee	2019-06-01	2021-05-31	Novel Machine Learning Techniques to Elucidate Function and Dynamics of Epigenomic Mechanisms	With the increasing amount of imaging devices installed at sawmills, the importance of using these data for improving workflow and for increasing revenues in the wood processing industries is growing. In this context, challenging questions with respect to imaging and image processing technology arise, several of which will be tackled in this joint project. The project considers two application cases as follows: The first application case is the question of tracing tree logs from the forest harvesting site to the sawmill by using biometrics related tree log recognition techniques based on image processing of cross-section data only. This approach of course assumes the additional availability of imaging sensors in the forest. Since there is a trend for installing CT imaging devices at sawmills, which are of course not available in the forest, the challenging issue of cross modality matching arises. The second application case is the determination of wood quality from cross-section imagery, applicable already in the forest, and/or at the sawmill. Obviously, these two application cases share many aspects. (1) They can be combined at application level, i.e. wood quality may be determined already in the forest due to imaging devices available for the tracing application, and further refined using the sensors available at the sawmill. Conversely, CT data from the sawmill, acquired to analyse the wood quality, can be used for the tracing application; (2) data preprocessing and many features extracted are required for both, matching cross section images as well as automated wood quality analysis; (3) the questions which imaging sensors should be employed and how the resulting data can be combined effectively have to be answered. Thus, synergies arise between these two application cases which will be efficiently exploited in the project. A common data set for experimental validation can be used (which implies also sharing employed sensors), ground truth data established wrt. annotating images can be shared, many software components implementing preprocessing (e.g. pith detection, cross section texture segmentation, contrast optimisation) as well as feature extraction techniques (e.g. annual ring detection, spiral growth detection) can be developed jointly and shared subsequently. The project will break new grounds in the area of wood imaging and processing of corresponding data with advanced algorithms in vision and machine learning with particular focus on cross modality processing. While those techniques are being developed for two specific application cases, the developed algorithms will be applicable to a wide range of applications in wood imagery processing and analysis as well as for other domains where similar settings arise.	The Academy of Medical Sciences	Springboard Round 4	99705.0GBP
298	Dr Cowley Lauren	University of Bath	2020-05-01	2022-04-30	Novel machine learning models for real time pathogen management	With the increasing amount of imaging devices installed at sawmills, the importance of using these data for improving workflow and for increasing revenues in the wood processing industries is growing. In this context, challenging questions with respect to imaging and image processing technology arise, several of which will be tackled in this joint project. The project considers two application cases as follows: The first application case is the question of tracing tree logs from the forest harvesting site to the sawmill by using biometrics related tree log recognition techniques based on image processing of cross-section data only. This approach of course assumes the additional availability of imaging sensors in the forest. Since there is a trend for installing CT imaging devices at sawmills, which are of course not available in the forest, the challenging issue of cross modality matching arises. The second application case is the determination of wood quality from cross-section imagery, applicable already in the forest, and/or at the sawmill. Obviously, these two application cases share many aspects. (1) They can be combined at application level, i.e. wood quality may be determined already in the forest due to imaging devices available for the tracing application, and further refined using the sensors available at the sawmill. Conversely, CT data from the sawmill, acquired to analyse the wood quality, can be used for the tracing application; (2) data preprocessing and many features extracted are required for both, matching cross section images as well as automated wood quality analysis; (3) the questions which imaging sensors should be employed and how the resulting data can be combined effectively have to be answered. Thus, synergies arise between these two application cases which will be efficiently exploited in the project. A common data set for experimental validation can be used (which implies also sharing employed sensors), ground truth data established wrt. annotating images can be shared, many software components implementing preprocessing (e.g. pith detection, cross section texture segmentation, contrast optimisation) as well as feature extraction techniques (e.g. annual ring detection, spiral growth detection) can be developed jointly and shared subsequently. The project will break new grounds in the area of wood imaging and processing of corresponding data with advanced algorithms in vision and machine learning with particular focus on cross modality processing. While those techniques are being developed for two specific application cases, the developed algorithms will be applicable to a wide range of applications in wood imagery processing and analysis as well as for other domains where similar settings arise.	The Academy of Medical Sciences	Springboard Round 5	84905.59GBP
299	Dr Namburete Ana	University of Oxford	2021-05-01	2023-04-30	Ultrasound-Based Assessment of Brain Folding Patterns in Early Pregnancy	With the increasing amount of imaging devices installed at sawmills, the importance of using these data for improving workflow and for increasing revenues in the wood processing industries is growing. In this context, challenging questions with respect to imaging and image processing technology arise, several of which will be tackled in this joint project. The project considers two application cases as follows: The first application case is the question of tracing tree logs from the forest harvesting site to the sawmill by using biometrics related tree log recognition techniques based on image processing of cross-section data only. This approach of course assumes the additional availability of imaging sensors in the forest. Since there is a trend for installing CT imaging devices at sawmills, which are of course not available in the forest, the challenging issue of cross modality matching arises. The second application case is the determination of wood quality from cross-section imagery, applicable already in the forest, and/or at the sawmill. Obviously, these two application cases share many aspects. (1) They can be combined at application level, i.e. wood quality may be determined already in the forest due to imaging devices available for the tracing application, and further refined using the sensors available at the sawmill. Conversely, CT data from the sawmill, acquired to analyse the wood quality, can be used for the tracing application; (2) data preprocessing and many features extracted are required for both, matching cross section images as well as automated wood quality analysis; (3) the questions which imaging sensors should be employed and how the resulting data can be combined effectively have to be answered. Thus, synergies arise between these two application cases which will be efficiently exploited in the project. A common data set for experimental validation can be used (which implies also sharing employed sensors), ground truth data established wrt. annotating images can be shared, many software components implementing preprocessing (e.g. pith detection, cross section texture segmentation, contrast optimisation) as well as feature extraction techniques (e.g. annual ring detection, spiral growth detection) can be developed jointly and shared subsequently. The project will break new grounds in the area of wood imaging and processing of corresponding data with advanced algorithms in vision and machine learning with particular focus on cross modality processing. While those techniques are being developed for two specific application cases, the developed algorithms will be applicable to a wide range of applications in wood imagery processing and analysis as well as for other domains where similar settings arise.	The Academy of Medical Sciences	Springboard Round 5	99963.0GBP
300	Dr Roney Caroline	King's College London	2018-10-01	2021-09-30	Predicting Atrial Fibrillation Mechanisms Through Deep Learning	None	Medical Research Council	Fellowship	311169.0GBP
301	Dr Spivakov Mikhail	MRC London Institute of Medical Sciences	2018-07-01	2021-03-31	Functional Gene Control	None	Medical Research Council	Unit	311169.0GBP
302	Prof. Dr. NEUBAUER Heidi	University of Veterinary Medicine, Vienna	2019-06-01	2022-05-31	Novel therapies in JAK/STAT driven T-cell malignancies (JAKSTAT-TARGET)	Mature T-cell leukemias/lymphomas (MaTCL) are hematologic malignancies of mostly incurable prospects in light of limited efficient therapies. As a heterogeneous group of tumors, single MaTCL types are rare, which impedes large-volume biomaterial and data collections and clinical studies. Hyperactivating somatic mutations in JAK/STAT genes stand out as high-incidence aberrations across MaTCL entities. Particularly the model diseases investigated here, T-cell prolymphocytic leukemia (T-PLL) and T-cell large granular lymphocyte leukemia (T-LGLL), carry those in 50-70% of cases. This interdisciplinary JAKSTAT-TARGET consortium of immunologists, hematologists, structural chemists, and systems biologists capitalizes on unique resources such as clinical registry linked sample repositories, new high-fidelity mouse models, pipelines of structure-based target design, and in-silico machine learning tools. We propose that targeting the JAK/STAT signalling network in synergistic combinations with drugs inhibiting inter-connected other key driver pathways will improve individualized anti-leukemic efficacy. We will achieve this by three objectives: O1 will address the causal T-cell-receptor and cytokine mediated impact on genome integrity and on the occurrence of JAK/STAT mutations. It interrogates the biochemical and functional consequences of the mutated and unmutated clones, and derives actionable differential vulnerabilities. O2 will study the performance of identified synergistic drug combinations using in-house developed optimized STAT-inhibitors in novel animal models and primary samples. O3 will implement drug-screen data from patient material into an ongoing clinical trial. Ultimately, with machine learning algorithms we will integrate the harmonized data of genomic profiles, drug-sensitivity patterns, and clinical outcomes from all objectives toward multi-omics guided predictions of optimal choices for trial designs and individual-patient based therapy selection.	Austrian Science Fund FWF	02 International programmes	274900.5EUR
303	Dr Ajnakina Olesya	King's College London	2019-01-01	2023-12-31	Predicting a) risk of onset and of b) psychosis outcomes based on individual genetic and environmental profiles	Mature T-cell leukemias/lymphomas (MaTCL) are hematologic malignancies of mostly incurable prospects in light of limited efficient therapies. As a heterogeneous group of tumors, single MaTCL types are rare, which impedes large-volume biomaterial and data collections and clinical studies. Hyperactivating somatic mutations in JAK/STAT genes stand out as high-incidence aberrations across MaTCL entities. Particularly the model diseases investigated here, T-cell prolymphocytic leukemia (T-PLL) and T-cell large granular lymphocyte leukemia (T-LGLL), carry those in 50-70% of cases. This interdisciplinary JAKSTAT-TARGET consortium of immunologists, hematologists, structural chemists, and systems biologists capitalizes on unique resources such as clinical registry linked sample repositories, new high-fidelity mouse models, pipelines of structure-based target design, and in-silico machine learning tools. We propose that targeting the JAK/STAT signalling network in synergistic combinations with drugs inhibiting inter-connected other key driver pathways will improve individualized anti-leukemic efficacy. We will achieve this by three objectives: O1 will address the causal T-cell-receptor and cytokine mediated impact on genome integrity and on the occurrence of JAK/STAT mutations. It interrogates the biochemical and functional consequences of the mutated and unmutated clones, and derives actionable differential vulnerabilities. O2 will study the performance of identified synergistic drug combinations using in-house developed optimized STAT-inhibitors in novel animal models and primary samples. O3 will implement drug-screen data from patient material into an ongoing clinical trial. Ultimately, with machine learning algorithms we will integrate the harmonized data of genomic profiles, drug-sensitivity patterns, and clinical outcomes from all objectives toward multi-omics guided predictions of optimal choices for trial designs and individual-patient based therapy selection.	National Institute for Health Research (Department of Health)	Full Grant	323879.0GBP
304	Dr Goehring Tobias	University of Cambridge	2020-06-12	2025-06-11	Restoring the sense of sound: deep-learning based compensation strategies for the electro-neural transmission of sound by cochlear implants	Mature T-cell leukemias/lymphomas (MaTCL) are hematologic malignancies of mostly incurable prospects in light of limited efficient therapies. As a heterogeneous group of tumors, single MaTCL types are rare, which impedes large-volume biomaterial and data collections and clinical studies. Hyperactivating somatic mutations in JAK/STAT genes stand out as high-incidence aberrations across MaTCL entities. Particularly the model diseases investigated here, T-cell prolymphocytic leukemia (T-PLL) and T-cell large granular lymphocyte leukemia (T-LGLL), carry those in 50-70% of cases. This interdisciplinary JAKSTAT-TARGET consortium of immunologists, hematologists, structural chemists, and systems biologists capitalizes on unique resources such as clinical registry linked sample repositories, new high-fidelity mouse models, pipelines of structure-based target design, and in-silico machine learning tools. We propose that targeting the JAK/STAT signalling network in synergistic combinations with drugs inhibiting inter-connected other key driver pathways will improve individualized anti-leukemic efficacy. We will achieve this by three objectives: O1 will address the causal T-cell-receptor and cytokine mediated impact on genome integrity and on the occurrence of JAK/STAT mutations. It interrogates the biochemical and functional consequences of the mutated and unmutated clones, and derives actionable differential vulnerabilities. O2 will study the performance of identified synergistic drug combinations using in-house developed optimized STAT-inhibitors in novel animal models and primary samples. O3 will implement drug-screen data from patient material into an ongoing clinical trial. Ultimately, with machine learning algorithms we will integrate the harmonized data of genomic profiles, drug-sensitivity patterns, and clinical outcomes from all objectives toward multi-omics guided predictions of optimal choices for trial designs and individual-patient based therapy selection.	Medical Research Council	Fellowship	1025336.0GBP
305	Dr Mareschal Isabelle	Queen Mary University of London	2019-07-01	2022-12-31	Understanding individual differences in facial emotion perception and their association with psychiatric risk indicators	Mature T-cell leukemias/lymphomas (MaTCL) are hematologic malignancies of mostly incurable prospects in light of limited efficient therapies. As a heterogeneous group of tumors, single MaTCL types are rare, which impedes large-volume biomaterial and data collections and clinical studies. Hyperactivating somatic mutations in JAK/STAT genes stand out as high-incidence aberrations across MaTCL entities. Particularly the model diseases investigated here, T-cell prolymphocytic leukemia (T-PLL) and T-cell large granular lymphocyte leukemia (T-LGLL), carry those in 50-70% of cases. This interdisciplinary JAKSTAT-TARGET consortium of immunologists, hematologists, structural chemists, and systems biologists capitalizes on unique resources such as clinical registry linked sample repositories, new high-fidelity mouse models, pipelines of structure-based target design, and in-silico machine learning tools. We propose that targeting the JAK/STAT signalling network in synergistic combinations with drugs inhibiting inter-connected other key driver pathways will improve individualized anti-leukemic efficacy. We will achieve this by three objectives: O1 will address the causal T-cell-receptor and cytokine mediated impact on genome integrity and on the occurrence of JAK/STAT mutations. It interrogates the biochemical and functional consequences of the mutated and unmutated clones, and derives actionable differential vulnerabilities. O2 will study the performance of identified synergistic drug combinations using in-house developed optimized STAT-inhibitors in novel animal models and primary samples. O3 will implement drug-screen data from patient material into an ongoing clinical trial. Ultimately, with machine learning algorithms we will integrate the harmonized data of genomic profiles, drug-sensitivity patterns, and clinical outcomes from all objectives toward multi-omics guided predictions of optimal choices for trial designs and individual-patient based therapy selection.	Medical Research Council	Research Grant	722846.0GBP
306	Dr Wijeratne Peter	University College London	2019-09-01	2022-08-31	Computational models for clinical trial design in Huntington's disease	Mature T-cell leukemias/lymphomas (MaTCL) are hematologic malignancies of mostly incurable prospects in light of limited efficient therapies. As a heterogeneous group of tumors, single MaTCL types are rare, which impedes large-volume biomaterial and data collections and clinical studies. Hyperactivating somatic mutations in JAK/STAT genes stand out as high-incidence aberrations across MaTCL entities. Particularly the model diseases investigated here, T-cell prolymphocytic leukemia (T-PLL) and T-cell large granular lymphocyte leukemia (T-LGLL), carry those in 50-70% of cases. This interdisciplinary JAKSTAT-TARGET consortium of immunologists, hematologists, structural chemists, and systems biologists capitalizes on unique resources such as clinical registry linked sample repositories, new high-fidelity mouse models, pipelines of structure-based target design, and in-silico machine learning tools. We propose that targeting the JAK/STAT signalling network in synergistic combinations with drugs inhibiting inter-connected other key driver pathways will improve individualized anti-leukemic efficacy. We will achieve this by three objectives: O1 will address the causal T-cell-receptor and cytokine mediated impact on genome integrity and on the occurrence of JAK/STAT mutations. It interrogates the biochemical and functional consequences of the mutated and unmutated clones, and derives actionable differential vulnerabilities. O2 will study the performance of identified synergistic drug combinations using in-house developed optimized STAT-inhibitors in novel animal models and primary samples. O3 will implement drug-screen data from patient material into an ongoing clinical trial. Ultimately, with machine learning algorithms we will integrate the harmonized data of genomic profiles, drug-sensitivity patterns, and clinical outcomes from all objectives toward multi-omics guided predictions of optimal choices for trial designs and individual-patient based therapy selection.	Medical Research Council	Fellowship	273137.0GBP
307	Dr Laine Romain	University College London	2019-12-28	2022-12-27	How does virus shape relate to infectivity?	Mature T-cell leukemias/lymphomas (MaTCL) are hematologic malignancies of mostly incurable prospects in light of limited efficient therapies. As a heterogeneous group of tumors, single MaTCL types are rare, which impedes large-volume biomaterial and data collections and clinical studies. Hyperactivating somatic mutations in JAK/STAT genes stand out as high-incidence aberrations across MaTCL entities. Particularly the model diseases investigated here, T-cell prolymphocytic leukemia (T-PLL) and T-cell large granular lymphocyte leukemia (T-LGLL), carry those in 50-70% of cases. This interdisciplinary JAKSTAT-TARGET consortium of immunologists, hematologists, structural chemists, and systems biologists capitalizes on unique resources such as clinical registry linked sample repositories, new high-fidelity mouse models, pipelines of structure-based target design, and in-silico machine learning tools. We propose that targeting the JAK/STAT signalling network in synergistic combinations with drugs inhibiting inter-connected other key driver pathways will improve individualized anti-leukemic efficacy. We will achieve this by three objectives: O1 will address the causal T-cell-receptor and cytokine mediated impact on genome integrity and on the occurrence of JAK/STAT mutations. It interrogates the biochemical and functional consequences of the mutated and unmutated clones, and derives actionable differential vulnerabilities. O2 will study the performance of identified synergistic drug combinations using in-house developed optimized STAT-inhibitors in novel animal models and primary samples. O3 will implement drug-screen data from patient material into an ongoing clinical trial. Ultimately, with machine learning algorithms we will integrate the harmonized data of genomic profiles, drug-sensitivity patterns, and clinical outcomes from all objectives toward multi-omics guided predictions of optimal choices for trial designs and individual-patient based therapy selection.	Medical Research Council	Fellowship	301830.0GBP
308	Mr Harrison Conrad	University of Oxford	2019-12-28	2022-12-27	Transforming outcome measures in plastic surgery through computer science	Mature T-cell leukemias/lymphomas (MaTCL) are hematologic malignancies of mostly incurable prospects in light of limited efficient therapies. As a heterogeneous group of tumors, single MaTCL types are rare, which impedes large-volume biomaterial and data collections and clinical studies. Hyperactivating somatic mutations in JAK/STAT genes stand out as high-incidence aberrations across MaTCL entities. Particularly the model diseases investigated here, T-cell prolymphocytic leukemia (T-PLL) and T-cell large granular lymphocyte leukemia (T-LGLL), carry those in 50-70% of cases. This interdisciplinary JAKSTAT-TARGET consortium of immunologists, hematologists, structural chemists, and systems biologists capitalizes on unique resources such as clinical registry linked sample repositories, new high-fidelity mouse models, pipelines of structure-based target design, and in-silico machine learning tools. We propose that targeting the JAK/STAT signalling network in synergistic combinations with drugs inhibiting inter-connected other key driver pathways will improve individualized anti-leukemic efficacy. We will achieve this by three objectives: O1 will address the causal T-cell-receptor and cytokine mediated impact on genome integrity and on the occurrence of JAK/STAT mutations. It interrogates the biochemical and functional consequences of the mutated and unmutated clones, and derives actionable differential vulnerabilities. O2 will study the performance of identified synergistic drug combinations using in-house developed optimized STAT-inhibitors in novel animal models and primary samples. O3 will implement drug-screen data from patient material into an ongoing clinical trial. Ultimately, with machine learning algorithms we will integrate the harmonized data of genomic profiles, drug-sensitivity patterns, and clinical outcomes from all objectives toward multi-omics guided predictions of optimal choices for trial designs and individual-patient based therapy selection.	National Institute for Health Research (Department of Health)	Fellowship	384813.0GBP
309	Dr. LUGHOFER Edwin	University of Linz	2020-03-01	2023-02-28	Interactive Machine Learning with Evolving Fuzzy Systems	Mature T-cell leukemias/lymphomas (MaTCL) are hematologic malignancies of mostly incurable prospects in light of limited efficient therapies. As a heterogeneous group of tumors, single MaTCL types are rare, which impedes large-volume biomaterial and data collections and clinical studies. Hyperactivating somatic mutations in JAK/STAT genes stand out as high-incidence aberrations across MaTCL entities. Particularly the model diseases investigated here, T-cell prolymphocytic leukemia (T-PLL) and T-cell large granular lymphocyte leukemia (T-LGLL), carry those in 50-70% of cases. This interdisciplinary JAKSTAT-TARGET consortium of immunologists, hematologists, structural chemists, and systems biologists capitalizes on unique resources such as clinical registry linked sample repositories, new high-fidelity mouse models, pipelines of structure-based target design, and in-silico machine learning tools. We propose that targeting the JAK/STAT signalling network in synergistic combinations with drugs inhibiting inter-connected other key driver pathways will improve individualized anti-leukemic efficacy. We will achieve this by three objectives: O1 will address the causal T-cell-receptor and cytokine mediated impact on genome integrity and on the occurrence of JAK/STAT mutations. It interrogates the biochemical and functional consequences of the mutated and unmutated clones, and derives actionable differential vulnerabilities. O2 will study the performance of identified synergistic drug combinations using in-house developed optimized STAT-inhibitors in novel animal models and primary samples. O3 will implement drug-screen data from patient material into an ongoing clinical trial. Ultimately, with machine learning algorithms we will integrate the harmonized data of genomic profiles, drug-sensitivity patterns, and clinical outcomes from all objectives toward multi-omics guided predictions of optimal choices for trial designs and individual-patient based therapy selection.	Austrian Science Fund FWF	Stand-Alone Projects	409109.4EUR
310	Dr. LUGHOFER Edwin	Arizona State University	2020-04-15	2021-03-31	Collaborative Research: RAPID: RTEM: Rapid Testing as Multi-fidelity Data Collection for Epidemic Modeling	Biological Sciences - The novel coronavirus (COVID-19) epidemic is generating significant social, economic, and health impacts and has highlighted the importance of real-time analysis of the spatio-temporal dynamics of emerging infectious diseases. COVID-19, which emerged out of the city of Wuhan in China in December 2019 is now spreading in multiple countries. It is particularly concerning that the case fatality rate appears to be higher for the novel coronavirus than for seasonal influenza, and especially so for older populations and those with prior health conditions such as cardiovascular disease and diabetes. Any plan for stopping the epidemic must be based on a quantitative understanding of the proportion of the at-risk population that needs to be protected by effective control measures in order for transmission to decline sufficiently and quickly enough for the epidemic to end. Different data collection and testing modalities and strategies available to help calibrate transmission models and predict the spread/severity of a disease, have variable costs, response times, and accuracies. In this Rapid Response Research (RAPID) project, the team will examine the problem of establishing optimal practices for rapid testing for the novel coronavirus. The result will be the Rapid Testing for Epidemic Modeling (RTEM), which will translate into science-based predictions of the COVID-19 epidemic's characteristics, including the duration and overall size, and help the global efforts to combat the disease. The RTEM will fill an important gap in data-driven decision making during the COVID-19 epidemic and, thus, will enable services with significant national economic and health impact. The educational impact of the project will be on mentoring of post-doctoral and PhD researchers and on curricula by incorporating research challenges and outcomes into existing undergraduate and graduate classes. <br/><br/>Computational models for the spatio-temporal dynamics of emerging infectious diseases and data- and model-driven computer simulations for disease spreading are increasingly critical in predicting geo-temporal evolution of epidemics as well as designing, activating, and adapting practices for controlling epidemics. In this project, the researchers tackle a Rapid Testing for Epidemic Modeling (RTEM) problem: Given a partially known target disease model and a set of testing modalities (from surveys to surveillance testing at known disease hotspots), with varying costs, accuracies, and observational delays, what is the best rapid testing strategy that would help recover the underlying disease model? Several scientific questions arise: What is the value of testing? Should only sick people be tested for virus detection? What level of resources should be devoted to the development of highly accurate tests (low false positives, low false negatives)? Is it better to use only one type of test aiming at the best cost/effectiveness trade off, or a non-homogeneous testing policy? Naturally these questions need to be investigated at the interface of epidemiology, computer science, machine learning, mathematical modeling and statistics. As part of the work, the team will develop a model of transmission dynamics and control, tailored to COVID-19 in a way that accommodates diagnostic testing with varying fidelities and delays underlying a rapid testing regimen. The investigators will further integrate the resulting RTEM-SEIR model with EpiDMS and DataStorm for executing continuous coupled simulations.<br/><br/>This project is jointly funded through the Ecology and Evolution of Infectious Diseases program (Division of Environmental Biology) and the Civil, Mechanical and Manufacturing innovation program (Engineering).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	122998.0USD
311	Dr. LUGHOFER Edwin	Florida International University	2020-05-01	2021-04-30	RAPID: #COVID-19: Understanding Community Response in the Emergence and Spread of Novel Coronavirus through Health Risk Communications in Socio-Technical Systems	Computer and Information Science and Engineering - Risk perception and risk averting behaviors of vulnerable communities in the emergence and spread of COVID-19 are spatio-temporal functions of individual or group interactions with their online social neighbors within or outside their communities and such interactions need to be captured through diverse information channels (e.g. traditional outlets such as radio, television, internet and/or non-traditional outlets such as social media). The primary goal of this Rapid Response Research (RAPID) project is to collect time-sensitive online social media and crowd-sourced data and analyze patterns of health-risk communication and community response in the emergence and spread of novel Coronavirus using data-driven methods and network science theories. The major focus will be towards understanding how individuals are socially influenced online, while communicating risk and interacting in their respective communities as the disease continues to spread. The notion of influence will be captured by quantifying the network effects on such communication behavior and characterizing how information is exchanged among people who are socially connected online and exposed to health risk in such outbreaks of disease. Given that communities responded to COVID-19 with limited or no preparation and there is uncertainty in the length of recovery for the communities already affected while new communities being threatened, the data collection effort requires rapid response for better coverage and careful monitoring. The data will include large-scale ephemeral online interactions of people in the affected communities and public officials who are involved in COVID-19 response, recovery, and mitigation efforts, followed by a data-driven network analytics and infographics of COVID-19 risk communication strategies and risk averting behaviors adopted. The proposed research will not only expand the knowledge base of spatio-temporal dynamics of risk perception and dissemination strategies in the emergence and aftermath of a major disease outbreak, but will also result in data-driven inference techniques to improve our understanding of how people express diverse concerns and how to harness and embed such information for designing intervention measures. The methodologies and findings of this rapid response research will benefit emergency management and public health agencies to define targeted information dissemination policies for public with diverse needs based on how people reacted to COVID-19 and their social network characteristics, activities, and interactions in response to similar public health hazards.<br/><br/>Public engagement in risk communication can lead to more effective decision-making and enhanced public feedback to the regulatory process. The primary goal of this RAPID project is to mine and analyze large-scale time-sensitive perishable crowd-sourced and social media data (rich spatio-temporal data) and reveal patterns of health-risk communication and community response in the emergence and spread of novel Coronavirus using data-driven methods and network science theories. The specific aims are threefold: (1) to document how public interact and communicate health risk information through their online social networks during a major disease outbreak; (2) to authenticate data from multiple sources and detect anomalies to avoid information overload and spread of misinformation; and (3) to examine how online social networks influence protective actions (e.g., social distancing, self-quarantine decisions) i.e. information cascades in health risk communication. To achieve the goal and aims, the project will utilize ephemeral time and geo-tagged social media interactions of users, agencies, news sources supplemented with crowd-sourced information on COVID-19. This study will have five theoretical and methodological contributions to the literature. It will: (1) advance our understanding of how individuals are socially influenced online, while communicating health risks and interacting in their respective communities as the disease continues to spread; (2) inform the literature on how information is exchanged among people who are socially connected online and exposed to health risk in such outbreaks of disease; (3) use novel machine-learning and network science models to quantify influence and network effects on such communication behavior; (4) capture the variability in network composition, risk communication strategies and risk averting behaviors adopted based on spatio-temporal correlations of risk and disease contagion; (5) ensure authenticity of the collected data from multiple sources and develop more accurate fully-distributed computational algorithms tailored to health risk anomaly detection in socio-technical systems. The findings from this research will be useful to public health and emergency management agencies for tailoring effective information dissemination policies for diverse user groups based on their social network characteristics, activities, and interactions in response to similar public health hazards. The methodologies, and implications of this research can be transferred in designing effective intervention policies to other natural and man-made disaster contexts in which public health risks become major concerns. The project will engage, mentor, and offer an innovative active learning environment for K-12, undergraduate, and graduate students by giving priority to disadvantaged and underrepresented communities in USA. The project will train students on computational skills required for collecting, storing, processing, analyzing and modeling large-scale data using high performance computational resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	79380.0USD
312	Dr. LUGHOFER Edwin	University of Pittsburgh	2020-05-01	2021-04-30	RAPID: Countering COVID-19 Misinformation via Situation-Aware Visually Informed Treatment	Computer and Information Science and Engineering - As the COVID-19 pandemic spreads, countries and cities around the globe have taken stringent measures including quarantine and regional lockdown. The increasing isolation, along with the panic and anxiety, creates challenges for countering misinformation--people are increasingly tapping into online information sources already familiar to them with declining chances of accessing alternative stories. This project will develop mechanisms based on text and image analysis, social psychology, and crowd-sourcing that can be used in a timely manner to counter misinformation during the ongoing COVID-19 crisis and beyond. One of the novel features of the approach is to deal with a specific instance of misinformation by crowd-sourcing authentic images that counter this misinformation. This research will contribute to the scientific understanding of misinformation and of persuasive narrative construction, to the assessment of risk for the spread of misinformation, and to the development of mechanisms to counter misinformation. <br/><br/>The technical aims of this project are divided into three thrusts. The first thrust will investigate what information content and which specific part of a multimodal social media post (e.g, a piece of text, text with an image, image with an embedded slogan) will receive stronger responses and hence increase the likelihood of the post being shared. The second thrust will create metrics to assess the likelihood of the spread of misinformation based on predictors learned from the content to which users are exposed. The third thrust will focus on the development of a system to counter misinformation based on citizen journalists? inputs of field investigations and on machine learning techniques. Finally, the system will be evaluated by survey studies and interviews to examine the system?s usability, usefulness, and effectiveness in reducing the spreading and impact of misinformation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	104491.0USD
313	Dr. LUGHOFER Edwin	George Washington University	2020-05-01	2021-04-30	RAPID: A novel platform for data integration and deep learning on COVID-19	Biological Sciences - The COVID-19 pandemic, caused by the SARS-CoV-2 virus, has fundamentally changed the world, and yet its ultimate impact is unknown. While China has experienced a slowdown in new cases, infections in the US continue to rise and are threatening to exceed our health care system?s capacity. Tests capacities are limited compared to the need, hospital services are becoming overwhelmed, and critical supplies are in shortage. There is a diversity of efforts currently ongoing to develop both new treatments as well as vaccine strategies to combat COVID-19. Yet, we know from experience, the virus will evolve solutions to both host immune systems and intervention strategies. In order to diminish both the short-term and long-term impacts of COVID-19, it is essential to develop robust, repeatable, and accessible tools to integrate and analyze the diversity of data becoming available in the face of the COVID-19 pandemic. The development of a platform to characterize the dynamic nature of mutations in the virus and testing for associations with clinical variables and biomarkers is an essential broader impact and will help in making informed predictions of health outcomes such as the stage of the severity of the disease and efficacy of treatment. Additionally, this project provides professional development opportunities for early career researchers.<br/><br/>Advances in omics technologies provide a broad and deep range of genotypic and phenotypic data to integrate with clinical phenotypes. Machine learning techniques such as clustering using phylogenetic distance and Deep Neural Networks (DNNs) are suitable techniques to link these DNA level changes to clinical metadata for human disease prediction, diagnosis, and therapeutics. This project develops tools within an open-source platform for documented, repeatable analyses that can be conducted in real-time allowing integration of data from patients with new treatments/vaccines strategies. This deep learning bioinformatics platform will allow the prioritization of genes associated with outcome predictors, including health, therapeutic, and vaccine outcomes, as well as inform improved DNA tests for predicting disease status and severity. The computational tools developed in this study will provide the research community and health professionals with comprehensive and generic approaches for characterizing the dynamics of genotype/phenotype associations in viruses. Such tools allow healthcare professionals and researchers to address specific properties of viruses such as frequency and location of mutations across the viral genome. When added to other clinical and epidemiological data, such information could help pave the way to better treatments or a vaccine. The developed platform will provide a venue for robust, open, repeatable analyses of COVID-19 as more and more data become available.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	200000.0USD
314	Dr. LUGHOFER Edwin	George Mason University	2020-05-15	2021-04-30	RAPID/Collaborative Research: Human-AI Teaming for Big Data Analytics to Enhance Response to the COVID-19 Pandemic	Engineering - Social media data can provide important clues and local knowledge that can help emergency managers and responders better comprehend and capture the evolving nature of many disasters. Yet humans alone cannot grasp the vast data generated by social media, so computers are used to assist. Very little is currently known about how to leverage the skills of humans and machines when they work together (human-machine teaming) to identify meaningful patterns in social media data. Therefore, the fundamental issues this Rapid Response Research (RAPID) project seeks to address are 1) understanding the process of real-time decisions that human digital volunteers make when they rapidly convert social media data into structured codes the machine (Artificial Intelligence algorithms) can understand, and 2) using this knowledge to improve human-machine teaming. This project advances the field by revealing the unique abilities that both humans and machines bring when working together to comprehend social media patterns during an evolving disaster. It supports education and diversity by providing research experiences to diverse students, as well as generating data useful for interdisciplinary courses teaching teamwork, social media analysis, and human-machine teaming. Finally, the findings can help emergency managers better train their volunteers who comb through social media using their understanding of the local knowledge and built environment to help machines see new patterns in data. Hence, this project supports NSF's mission to promote the progress of science and to advance the nation's health, prosperity, and welfare by articulating the unique value that both humans and computers bring that can lead to better decisions during disasters. The goal of this research is to better understand the real-time decisions that human annotators make under different environmental constraints, and how those contribute to the learning of Artificial Intelligence (AI) models. Under time constraints and information overload, human decision-making capabilities are limited; yet, humans still have a unique ability to understand the contextual references to the structures in the built environment that machines cannot recognize. For example, the meaning of the tweet, ?Memorial is overloaded,? -- which means the hospital, called Memorial, is out of beds for patients ?- can be lost on AI systems that lack the knowledge of the built environment. This example demonstrates the value that humans in the loop offer in a human-AI teaming context. <br/><br/>This research focuses on capturing the ephemeral data from a variety of social media sources and our two research thrusts include: 1) online observations of Community Emergency Response Team (CERT) volunteers and a manager (a collaborator on this project) using think-aloud and cognitive interviewing strategies to reveal the real-time mental models used to make coding decisions for annotation tasks; and 2) an empirical analysis of different sampling algorithms for active (machine) learning paradigms to develop a typology of machine errors under diverse contexts that affect the quality of human decision making for annotation. This research will generate design guidelines that bridge the gap between the mechanisms used for real-time data processing with AI models and the understanding of context contributed by a human user teaming with the AI models. Using theories of human decision-making combined with knowledge of how AI functions, this project provides a real-time, mid-disaster examination of 1) how humans understand, process, and interpret social media messages, and 2) how to refine AI algorithms to optimize active learning paradigm. This understanding will provide a theoretical framework enabling future research to develop protocols to optimize human-AI teaming by using concepts such as motivation and information theory. This work can help emergency managers conduct better training of their CERT volunteers and other annotators and provide clearer guidelines for how to communicate the unique value that humans bring to the annotation process for AI systems. Both our protocols and developed understanding of how humans interact with AI systems will be helpful for global health organizations, local and state-level disaster decision-makers, as well as provide direction for the vast CERT network in the United States.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	24266.0USD
315	Dr. LUGHOFER Edwin	Auburn University	2020-05-15	2021-04-30	RAPID/Collaborative Research: Quantifying Social Media Data for Improved Modeling of Mitigation Strategies for the COVID-19 Pandemic	Engineering - This Rapid Response Research (RAPID) grant will support research that will contribute new knowledge related to modeling social behavior and community activity during the COVID-19 pandemic, as well as future pandemics with COVID-19 characteristics. The model focuses on compliance with mitigation strategies and public health guidelines, thus enabling the selection of policies that are most effective in promoting both the progress of science and advancing national health and prosperity. Various pandemic models are currently being used to predict the spread of a virus and establish which mitigation strategies are the most effective. These models are heavily based on assumptions and may include an oversimplified reality of how populations react and behave. This research will provide needed knowledge and methods for the development of a model of how individuals in the U.S. react to certain mitigation strategies, such as social-distancing, stay-at-home orders, quarantines, and travel advisories, by mining and analyzing social media data during the COVID-19 crisis. This enhanced modeling approach and its resultant model will be of great value to disaster response managers and policy/decision makers to understand human social behavior. This work allows assessment of the effectiveness of mitigation strategies and public health guidelines during pandemics (and other crises). This project will also form the basis of a publicly available case study suitable for university level students that can be widely incorporated in courses. <br/><br/>Although individual-based and homogeneous mixing pandemic models provide useful insights and predictive capabilities within a range of possibilities, they are highly sensitive to people?s actions. This research aims to provide an enhanced approach to model social behavior and community activity during a pandemic in terms of compliance with mitigation strategies and public health guidelines. Social media data present a brief window of opportunity for research on how, and to what extent, the public does or does not comply with the recommended mitigation strategies and public health guidelines. The research team will collect real-time data from social media related to COVID19-exposed regional populations in the U.S. The data will be analyzed using machine learning techniques to identify non-mutually exclusive clusters of people based on similarity of their demographic, geographic, and time information, and establish relationships among clusters. The analyzed data will form the basis of a data-driven multi-paradigm simulation model that captures changes in public sentiment over time, quantifies the resistance/compliance with mitigation strategies and health guidelines, and gauges overall effectiveness of various mitigation strategies and advice over time.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	141527.0USD
316	Dr. LUGHOFER Edwin	Unity Health Toronto	2020-05-15	2021-04-30	Acute Respiratory Mortality Surveillance (ARMS) for Coronavirus Infection (COVID-19): A globally relevant technology to strengthen mortality surveillance for acute respiratory deaths in many countries lacking complete medical certification of death	The current global infectious threat, COVID-19, has not yet been widely detected in sub-Saharan Africa or other low income countries in Asia. It is almost inevitable that it will reach those places. While unusual spikes in infection-related deaths can register quickly in higher income countries and in China, they can go unrecognized for weeks or months in low-income settings where even very ill people do not go to a hospital, infecting others. Detecting a mortality signal is important and may be the first step in recognizing a serious outbreak. We propose to build on our extensive experience using verbal autopsy (VA) in the long-running Indian Million Death Study, and ongoing studies in China, Hong Kong, Ethiopia and Sierra Leone to develop an enhanced verbal autopsy module to identify deaths from COVID-19. This will serve as a model for the next novel pathogen-as near as possible to real time in settings without routine medical certification of death. We will test three hypotheses: #1 An "Acute Respiratory Mortality Surveillance" (ARMS) module can be added quickly to the WHO VA instrument and validated against hospitalized cases and deaths (paired with epidemiological information and machine learning) to distinguish COVID-19 from other causes of respiratory deaths. #2 Early deployment of ARMS in China, Hong Kong, India, Sierra Leone, and Ethiopia will help establish baseline distributions of usual acute respiratory deaths, as a comparator for COVID-19 deaths, and to inform modelling. #3 Effective knowledge translation of an open-source, widely-available ARMS module will improve the global response to COVID-19, particularly in the lowest income countries and help to improve mortality assessments for any subsequent COVID-19 waves. A successful ARMS will contribute to stopping the current outbreak and add novel surveillance tools. All materials and results will be made available globally to ensure the broadest use.	Canadian Institutes of Health Research	Research Grant	956320.0CAD
317	Dr. LUGHOFER Edwin	London School of Economics	2020-05-15	2021-04-30	COVID-19: Outreach to Domestic Abuse Victims in Times of Quarantine	The current global infectious threat, COVID-19, has not yet been widely detected in sub-Saharan Africa or other low income countries in Asia. It is almost inevitable that it will reach those places. While unusual spikes in infection-related deaths can register quickly in higher income countries and in China, they can go unrecognized for weeks or months in low-income settings where even very ill people do not go to a hospital, infecting others. Detecting a mortality signal is important and may be the first step in recognizing a serious outbreak. We propose to build on our extensive experience using verbal autopsy (VA) in the long-running Indian Million Death Study, and ongoing studies in China, Hong Kong, Ethiopia and Sierra Leone to develop an enhanced verbal autopsy module to identify deaths from COVID-19. This will serve as a model for the next novel pathogen-as near as possible to real time in settings without routine medical certification of death. We will test three hypotheses: #1 An "Acute Respiratory Mortality Surveillance" (ARMS) module can be added quickly to the WHO VA instrument and validated against hospitalized cases and deaths (paired with epidemiological information and machine learning) to distinguish COVID-19 from other causes of respiratory deaths. #2 Early deployment of ARMS in China, Hong Kong, India, Sierra Leone, and Ethiopia will help establish baseline distributions of usual acute respiratory deaths, as a comparator for COVID-19 deaths, and to inform modelling. #3 Effective knowledge translation of an open-source, widely-available ARMS module will improve the global response to COVID-19, particularly in the lowest income countries and help to improve mortality assessments for any subsequent COVID-19 waves. A successful ARMS will contribute to stopping the current outbreak and add novel surveillance tools. All materials and results will be made available globally to ensure the broadest use.	UK Research and Innovation	Research Grant	956320.0CAD
318	Dr Wang Dennis	University of Sheffield	2019-03-01	2021-08-31	Evidencing subtypes of disorders with comorbidities through a consensus of clinical and multi-omic traits	The current global infectious threat, COVID-19, has not yet been widely detected in sub-Saharan Africa or other low income countries in Asia. It is almost inevitable that it will reach those places. While unusual spikes in infection-related deaths can register quickly in higher income countries and in China, they can go unrecognized for weeks or months in low-income settings where even very ill people do not go to a hospital, infecting others. Detecting a mortality signal is important and may be the first step in recognizing a serious outbreak. We propose to build on our extensive experience using verbal autopsy (VA) in the long-running Indian Million Death Study, and ongoing studies in China, Hong Kong, Ethiopia and Sierra Leone to develop an enhanced verbal autopsy module to identify deaths from COVID-19. This will serve as a model for the next novel pathogen-as near as possible to real time in settings without routine medical certification of death. We will test three hypotheses: #1 An "Acute Respiratory Mortality Surveillance" (ARMS) module can be added quickly to the WHO VA instrument and validated against hospitalized cases and deaths (paired with epidemiological information and machine learning) to distinguish COVID-19 from other causes of respiratory deaths. #2 Early deployment of ARMS in China, Hong Kong, India, Sierra Leone, and Ethiopia will help establish baseline distributions of usual acute respiratory deaths, as a comparator for COVID-19 deaths, and to inform modelling. #3 Effective knowledge translation of an open-source, widely-available ARMS module will improve the global response to COVID-19, particularly in the lowest income countries and help to improve mortality assessments for any subsequent COVID-19 waves. A successful ARMS will contribute to stopping the current outbreak and add novel surveillance tools. All materials and results will be made available globally to ensure the broadest use.	The Academy of Medical Sciences	Springboard Round 4	99411.0GBP
319	Dr Charlton Peter	Cambridge, University of	2019-03-01	2021-08-31	Using clinical and consumer devices to enhance screening for atrial fibrillation	The current global infectious threat, COVID-19, has not yet been widely detected in sub-Saharan Africa or other low income countries in Asia. It is almost inevitable that it will reach those places. While unusual spikes in infection-related deaths can register quickly in higher income countries and in China, they can go unrecognized for weeks or months in low-income settings where even very ill people do not go to a hospital, infecting others. Detecting a mortality signal is important and may be the first step in recognizing a serious outbreak. We propose to build on our extensive experience using verbal autopsy (VA) in the long-running Indian Million Death Study, and ongoing studies in China, Hong Kong, Ethiopia and Sierra Leone to develop an enhanced verbal autopsy module to identify deaths from COVID-19. This will serve as a model for the next novel pathogen-as near as possible to real time in settings without routine medical certification of death. We will test three hypotheses: #1 An "Acute Respiratory Mortality Surveillance" (ARMS) module can be added quickly to the WHO VA instrument and validated against hospitalized cases and deaths (paired with epidemiological information and machine learning) to distinguish COVID-19 from other causes of respiratory deaths. #2 Early deployment of ARMS in China, Hong Kong, India, Sierra Leone, and Ethiopia will help establish baseline distributions of usual acute respiratory deaths, as a comparator for COVID-19 deaths, and to inform modelling. #3 Effective knowledge translation of an open-source, widely-available ARMS module will improve the global response to COVID-19, particularly in the lowest income countries and help to improve mortality assessments for any subsequent COVID-19 waves. A successful ARMS will contribute to stopping the current outbreak and add novel surveillance tools. All materials and results will be made available globally to ensure the broadest use.	British Heart Foundation	Springboard Round 4	251475.0GBP
320	Mag. DI Dr.phil. Dr.techn. HAUSER Andreas	Graz University of Technology	2016-12-01	2020-05-31	Heterogeneous catalysis on metallic nanoparticles	The usage of machine learning techniques is a promising approach towards the modeling of materials with applications ranging from surface science to molecular systems. Among its various methods, the field of neural networks has seen a recent revival, triggered by the introduction of high-dimensional neural network schemes by Behler and Parrinello. The combination of many ‘standard’-type multilayered feed-forward neural networks, with each one describing the local chemical environment of a single atom of the system, allows the construction of economic, but also reliable interatomic potentials for a large-scale simulation of complex materials. It is the aim of this project to develop such a type of neural network structure for the simulation of bimetallic nanoparticles in the context of heterogeneous catalysis. Our target systems are pure and mixed clusters of gold and silver, in the range from 10 to 1000 metal atoms. The neural network will be fed with data obtained from density functional theory calculations on finite and periodic systems, i.e. on small mixed-metallic clusters as well as on bulk metals and alloys. Complementary to current research on the interplay between highly dispersed metal clusters and a given support, this project focuses on internal properties of the mixed-metallic particles, such as cluster structure, shape and stability, as a function of particle size and metallic ratio. We can take advantage of the fact that for structures with a well-defined separation of metallic phases, e.g. ‘Janus’-type clusters or core-shell structures, the number of different local atomic environments is comparably small. Later steps of the project will also comprise first interactions with gas phase molecules such as molecular hydrogen and carbon monoxide. While a full study of reaction pathways lies beyond the scope of this project, it should be possible to simulate and compare the physisorption step, a crucial step in heterogenous catalysis, on metal particles of various size and composition. Understanding how size, shape, structure and metallic ratio affects the catalytic properties of a particle is of fundamental interest to the catalysis community. The project will be carried out at the TU Graz Institute of Experimental Physics and supervised by Ass.-Prof. Andreas Hauser. It will be conducted in a tight collaboration with the experimental group of Prof. Wolfgang Ernst, head of the institute, whose expertise lies in the synthesis and the imaging of smallest mixed-metallic nanoparticles, and the experimental group of Prof. Martin Sterrer, who is currently working on scanning probe techniques for catalytic reactions on nanoparticles. Dr. Philip Marquetand from the Institute of Theoretical Chemistry at the University of Vienna offered support in the early phase of program code development.	Austrian Science Fund FWF	Stand-Alone Project	249477.91EUR
321	Mag. DI Dr.phil. Dr.techn. HAUSER Andreas	University of Michigan Ann Arbor	2020-05-15	2021-04-30	RAPID: Subtyping and Identifying Shared Genomic Sequences of SARS-CoV-2 (COVID-19)	Biological Sciences - People are threatened by an unprecedented pandemic: COVID-19 (caused by SARS-CoV-2). This virus is now threatening not only physical health, but also psychology, education, economy, and every corner of the infrastructure of society. So far, there is no treatment for this disease, while vaccines and neutralizing antibodies are perceived as one of the eventual solutions to this crisis. A critical piece of knowledge supporting vaccine and antibody development is understanding the genome of this virus. What is the common sequence shared among the SARS-CoV-2 strains across the globe? What are the subtypes? Are the genome variances overlapping with important genomic regions for vaccine design? Using state of the art machine learning approaches, this research will identify the shared, representative sequence across SARS-CoV-2 strains and group them by major types. This project will connect this information to the important genomic regions identified in the literature that can be used for vaccines, and thereby continuously inform the ongoing effort of vaccine development, antibody selection, and therapeutic development. The research from this study would provide society benefits through monthly updates onto web interfaces that allow the vaccine developers, the biomedical research community as well as the general public to conveniently get access to the above information. This project will support training of a graduate student in bioinformatics and provide outreach opportunities to K-12 students and the public.<br/><br/>This work will be a continuous effort to monthly subtype SARS-CoV-2 strains and update the shared sequences of SARS-CoV-2, in order to facilitate vaccine development and antibody design. Specifically, this research will be focusing on three objectives: 1) identifying and updating the common sequences of SARS-CoV-2 by forcing the common sequence to have the minimal evolutionary distance with all strains, or covering as many sequences as possible 2) subtyping the SARS-CoV-2 strains into major groups, which will be important to inform treatment, management and prevention measures; 3) connecting the subtyped and common genomic sequences of SARS-CoV-2 to epitopes identified in the literature. To develop vaccines or neutralizing antibody treatment, it is critical that the major variations are covered and considered. The algorithms and visualization tools will overlay the curated list of potential epitopes on top of the subtypes and the shared sequence of the virus genomes, and directly support the effort of vaccine and antibody development. This RAPID award is made by the Systematics and Biodiversity Science Cluster in the Division of Environmental Biology, using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	199705.0USD
322	Mag. DI Dr.phil. Dr.techn. HAUSER Andreas	University of California-Los Angeles	2020-06-01	2021-05-31	RAPID: Dynamic Graph Neural Networks for Modeling and Monitoring COVID-19 Pandemic	Computer and Information Science and Engineering - The novel coronavirus, COVID-19, has become one of the biggest pandemics in human history and has generated lasting impacts on public health, society, and economy. The number of cases in the United States has passed 1 million with a total number of deaths over 50 thousand. There is an urgent need for research and development that can bring a predictive understanding of the spread of the virus, thereby enabling mitigation methods to alleviate the negative effects of COVID-19. Traditional epidemiological models usually take into consideration only a small number of features in building a prediction model, which may not be able to capture potential risk factors and effects of various intervention mechanisms of this new pandemic. In this project the investigators develop novel machine learning methods that can simultaneously model and predict the COVID-19 spread, detect and monitor risk factors, and evaluate effectiveness of interventions over time and space. <br/><br/>The new model ingests and integrates heterogeneous and rapidly accumulating data across diverse sources, such as publications, news, census, social media, and outbreak observation trackers. It employs a new contextualized language model to accurately recognize named entities and relations from vast text data and build knowledge graphs to extract potential risk factors. A dynamic graph is constructed. Each location node may have a set of static and time-dependent attributes. Events, individual behaviors, social activities, interventions are mapped to activity nodes with edges connecting to the corresponding location nodes at the time. A novel dynamic graph neural network is trained to perform joint predictions of all locations over time. Activity nodes of significant attention weights represent major risk factors or effective intervention mechanisms. The project will result in public dissemination of the prediction model and all source codes, immediately benefiting the combat against COVID-19.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	90000.0USD
323	Dr Deprez Maria	King's College London	2019-06-15	2021-06-14	Artificial intelligence to characterise abnormal brain development and neurocognitive disorders	Computer and Information Science and Engineering - The novel coronavirus, COVID-19, has become one of the biggest pandemics in human history and has generated lasting impacts on public health, society, and economy. The number of cases in the United States has passed 1 million with a total number of deaths over 50 thousand. There is an urgent need for research and development that can bring a predictive understanding of the spread of the virus, thereby enabling mitigation methods to alleviate the negative effects of COVID-19. Traditional epidemiological models usually take into consideration only a small number of features in building a prediction model, which may not be able to capture potential risk factors and effects of various intervention mechanisms of this new pandemic. In this project the investigators develop novel machine learning methods that can simultaneously model and predict the COVID-19 spread, detect and monitor risk factors, and evaluate effectiveness of interventions over time and space. <br/><br/>The new model ingests and integrates heterogeneous and rapidly accumulating data across diverse sources, such as publications, news, census, social media, and outbreak observation trackers. It employs a new contextualized language model to accurately recognize named entities and relations from vast text data and build knowledge graphs to extract potential risk factors. A dynamic graph is constructed. Each location node may have a set of static and time-dependent attributes. Events, individual behaviors, social activities, interventions are mapped to activity nodes with edges connecting to the corresponding location nodes at the time. A novel dynamic graph neural network is trained to perform joint predictions of all locations over time. Activity nodes of significant attention weights represent major risk factors or effective intervention mechanisms. The project will result in public dissemination of the prediction model and all source codes, immediately benefiting the combat against COVID-19.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	The Academy of Medical Sciences	Springboard Round 4	99895.85GBP
324	Dr Schweikert Gabriele	University of Dundee	2019-06-01	2021-05-31	Novel Machine Learning Techniques to Elucidate Function and Dynamics of Epigenomic Mechanisms	Computer and Information Science and Engineering - The novel coronavirus, COVID-19, has become one of the biggest pandemics in human history and has generated lasting impacts on public health, society, and economy. The number of cases in the United States has passed 1 million with a total number of deaths over 50 thousand. There is an urgent need for research and development that can bring a predictive understanding of the spread of the virus, thereby enabling mitigation methods to alleviate the negative effects of COVID-19. Traditional epidemiological models usually take into consideration only a small number of features in building a prediction model, which may not be able to capture potential risk factors and effects of various intervention mechanisms of this new pandemic. In this project the investigators develop novel machine learning methods that can simultaneously model and predict the COVID-19 spread, detect and monitor risk factors, and evaluate effectiveness of interventions over time and space. <br/><br/>The new model ingests and integrates heterogeneous and rapidly accumulating data across diverse sources, such as publications, news, census, social media, and outbreak observation trackers. It employs a new contextualized language model to accurately recognize named entities and relations from vast text data and build knowledge graphs to extract potential risk factors. A dynamic graph is constructed. Each location node may have a set of static and time-dependent attributes. Events, individual behaviors, social activities, interventions are mapped to activity nodes with edges connecting to the corresponding location nodes at the time. A novel dynamic graph neural network is trained to perform joint predictions of all locations over time. Activity nodes of significant attention weights represent major risk factors or effective intervention mechanisms. The project will result in public dissemination of the prediction model and all source codes, immediately benefiting the combat against COVID-19.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	The Academy of Medical Sciences	Springboard Round 4	99705.0GBP
325	Dr Uhlhaas Peter	University of Glasgow	2014-07-31	2019-02-28	Using Magnetoencephalography to Investigate Aberrant Neural Synchrony in Prodromal Schizophrenia: A Translational Biomarker Approach	None	Medical Research Council	Research Grant	816353.0GBP
326	Dr Cawthorn William	University of Edinburgh	2019-06-30	2022-06-29	Population-level imaging, genomic and phenotypic analyses to determine how bone marrow adiposity impacts human health	None	Medical Research Council	Research Grant	550452.0GBP
327	Dr. CSETNEK Ernö Robert	University of Vienna	2017-03-01	2021-02-28	Nonsmooth optimization problems: splitting and dynamics	Many real-life problems can be modeled as structured optimization problems involving both nonsmooth and smooth data. The aim of this project with principal investigator Ernö Robert Csetnek is to deal with nonsmooth convex and nonconvex optimization problems having complex structures, by making use of splitting algorithms. The main feature of these algorithms is that they fully decompose all the objects from the model, which is a very useful aspect from the point of view of providing numerical implementations. The research project follows three objectives. The main focus of the first objective is to investigate the impact of inertial and memory effects on the convergence performances of the primal-dual algorithms designed for solving highly structured convex optimization problems. We intend to analyze the speed of convergence for both, the sequence of iterates generated by the iterative schemes, and the objective function values. In this context the parallelism with monotone inclusions and especially the Attouch-Théra duality will play a significant role. Further, we aim to make use of acceleration techniques in the sense of Nesterov, with the main focus on optimization problems involving compositions with linear operators, going beyond the classical FISTA method designed for minimizing the sum of a (simple) nonsmooth function with a smooth one. The second objective considers nonconvex structured optimization problems for which we aim to formulate suitable splitting methods. In this context, the classical proximal-point and proximal-gradient methods are known to have good convergence properties for minimization problems involving functions with analytic features, namely those which satisfy the Kurdyka-Lojasiewicz inequality. Our main focus will be on nonconvex optimization problems involving composition with linear operators. Further, we aim to make use of inertial and memory effects and to investigate the speed of convergence of the resulting numerical schemes. The third main objective is to investigate optimization problems and monotone inclusions from the continuous perspective. The main focus will be on dynamical systems where time dependent operators/functions are involved. This class of differential inclusions/equations plays a fundamental role, for example, when dealing with sweeping processes. Beyond that, time discretizations of continuous systems serve as a model for numerical schemes. We mention here the generalized Nash equilibrium problem, which can be formulated as a quasi-variational inequality problem and solved by projected-gradient-type algorithms, in the formulation of which moving sets are involved in order to handle the problem properly. The applications we expect range from image processing, with a particular emphasis on the solving of real-life problems dealing with image denoising, image deblurring, image inpainting and image segmentation, to machine learning in connection with support vector classification and support vector regression. Other fields that we have in mind for numerical experiments are optimal portfolio selection, decomposition of video streams, clustering and network communication.	Austrian Science Fund FWF	Stand-Alone Project	313362.0EUR
328	Ms. Elo-Uhlgren Laura Linnea Maria	University of Turku	2016-06-01	2021-05-31	From longitudinal proteomics to dynamic individualized diagnostics	Longitudinal omics data hold great promise to improve biomarker detection and enable dynamic individualized predictions. Recent technological advances have made proteomics an increasingly attractive option but clinical longitudinal proteomic datasets are still rare and computational tools for their analysis underdeveloped. The objective of this proposal is to create a roadmap to detect clinically feasible protein markers using longitudinal data and effective computational tools. A biomedical focus is on early detection of Type 1 diabetes (T1D). Specific objectives are: 1) Novel biomarker detector using longitudinal data. DynaOmics introduces novel types of multi-level dynamic markers that are undetectable in conventional single-time cross-sectional studies (e.g. within-individual changes in abundance or associations), develops optimization methods for their robust and reproducible detection within and across individuals, and validates their utility in well-defined samples. 2) Individualized disease risk prediction dynamically. DynaOmics develops dynamic individualized predictive models using the multi-level longitudinal proteome features and novel statistical and machine learning methods that have previously not been used in this context, including joint models of longitudinal and time-to-event data, and one-class classification type techniques. 3) Dynamic prediction of T1D. DynaOmics builds a predictive model of dynamic T1D risk to assist early detection of the disease, which is crucial for developing future therapeutic and preventive strategies. T1D typically involves a relatively long symptom-free period before clinical diagnosis but current tools to predict early T1D risk have restricted power. The objectives involve innovative and unconventional approaches and address major unmet challenges in the field, having high potential to open new avenues for diagnosis and treatment of complex diseases and fundamentally novel insights towards precision medicine.	European Research Council	Starting Grant	1499869.0EUR
329	Ao. Prof. Dr. LACKNER Peter	University of Salzburg	2017-07-01	2020-06-30	Structure based prediction of MHC II binding peptides	A key event in immune reactions is the binding of a pathogen derived antigen peptide to a MHC class II molecule. Only the resulting complex binds to the T-Cell receptor and triggers responses against the pathogen. These processes also play a role in allergies and they are associated with different autoimmune diseases such as diabetes or multiple sclerosis. In this respect, the identification of MHC-II binding peptides is a central task in biomedical research. Potential wet lab experiments are time consuming and costly. This constraint has driven the development of computational prediction methods for MHC-II binding peptides. However, to date their accuracy and/or range of applicability is still limited. Sequence-based approaches normally apply machine learning, utilizing sequence data of known binders and non-binders for certain MHC-II allotypes. The accuracy can be rather good, but in general these approaches can only be reliably applied to MHC-II allotypes where sufficient experimental data are available. In contrast, structure based methods operate on known 3D structures of MHC-II molecules and usually employ physical force fields. In general, they do not build on experimental binding data. However, they are yet limited by the number experimentally determined MHC-II structures and lack in prediction accuracy. We here propose a new structure based approach. We hypothesize that the application of statistical scoring functions (SSFs) in combination with information about intrinsic features of the binding peptides such as flexibility or preference for a certain local conformation can considerably improves the prediction accuracy. SSFs have been successfully used in protein structure bioinformatics for many years. Several parameters such as the compilation data set composition, the reference state, and the method for significance estimate calculation restrains the utilization of SSFs for a certain problem. We will investigate the influence of the different parameters on the prediction accuracy. In order to achieve a practical solution, two further points are important. First, the calculation of high-quality structural models for all presently known MHC-II allotypes applying comparative modeling accompanied by exhaustive model assessment. And second, the subsequent reduction of the molecular representation to the main chain, such that a fast threading approach can be applied for binding prediction. This simplified model has the prime advantage that the potential errors in side chain modeling will not obstruct the binder prediction. The final software will be released as standalone program for high throughput predictions and as a web-service for small scale experiments. We further will provide the set of MHC-II models annotated with quality scores and allotype information as a separate database. Finally we intend to use the method in In-house collaborations to design hypo-allergens.	Austrian Science Fund FWF	Stand-Alone Project	203710.5EUR
330	Univ.Prof. Dr. BOMZE Immanuel	University of Vienna	2014-05-01	2019-04-30	Network Optimization in Bioinformatics and Systems Biology	Mathematical models and algorithmic approaches for solving combinatorial optimization problems from the field of network optimization are known to be essential in telecommunications and the design of transportation and supply chain networks. More recently, it has been discovered that network optimization algorithms are also crucial in the context of bioinformatics and systems biology. Numerous publications in systems biology point out that studying functions, structures and interactions of proteins in combination with networks can provide new insights regarding robust biomarkers, can allow new discoveries regarding protein functions, or testing of new hypothesis regarding their interactions. Network optimization algorithms have also been applied in the analysis of functional modules in protein-protein interaction networks, the discovery of regulatory subnetworks, in revealing hidden components in biological processes, or in detecting transcription factor modules. Motivated by these recent developments, we aim to study several network optimization problems that are among the most challenging ones in these fields and that were not sufficiently studied or understood so far. One of them is essential for the analysis of protein-protein interaction networks, and the other one is an innovative way of combining machine learning and network optimization techniques for extracting signaling pathways or building network-driven classifiers. Existing approaches for these benchmark problems in particular are not able to tackle the supernetworks of very large-scale arising in real world. Thus, in this project we aim at developing the first supernetwork-driven approach in combinatorial optimization that will seamlessly integrate various methodologies from operations research (exact and metaheuristic approaches for network optimization) and computer science (machine learning) into a single mathematical framework. To capture important real world characteristics of the problems to be studied, we will thereby combine techniques of combinatorial, robust optimization and bi-objective optimization. In order to ensure that our algorithms will be able to cope with the enormous size of supernetworks, the tools we will develop will rely on decomposition approaches for mixed integer programming, metaheuristics and parallelization techniques. The obtained results will be important contributions to the fields of combinatorial optimization and bioinformatics.	Austrian Science Fund FWF	Stand-Alone Project	347760.0EUR
331	Dr Lorenz Romy	University of Cambridge	2018-09-01	2022-09-01	Fractionating the human frontoparietal cortex: combining meta-analytic and real-time optimization approaches	Mathematical models and algorithmic approaches for solving combinatorial optimization problems from the field of network optimization are known to be essential in telecommunications and the design of transportation and supply chain networks. More recently, it has been discovered that network optimization algorithms are also crucial in the context of bioinformatics and systems biology. Numerous publications in systems biology point out that studying functions, structures and interactions of proteins in combination with networks can provide new insights regarding robust biomarkers, can allow new discoveries regarding protein functions, or testing of new hypothesis regarding their interactions. Network optimization algorithms have also been applied in the analysis of functional modules in protein-protein interaction networks, the discovery of regulatory subnetworks, in revealing hidden components in biological processes, or in detecting transcription factor modules. Motivated by these recent developments, we aim to study several network optimization problems that are among the most challenging ones in these fields and that were not sufficiently studied or understood so far. One of them is essential for the analysis of protein-protein interaction networks, and the other one is an innovative way of combining machine learning and network optimization techniques for extracting signaling pathways or building network-driven classifiers. Existing approaches for these benchmark problems in particular are not able to tackle the supernetworks of very large-scale arising in real world. Thus, in this project we aim at developing the first supernetwork-driven approach in combinatorial optimization that will seamlessly integrate various methodologies from operations research (exact and metaheuristic approaches for network optimization) and computer science (machine learning) into a single mathematical framework. To capture important real world characteristics of the problems to be studied, we will thereby combine techniques of combinatorial, robust optimization and bi-objective optimization. In order to ensure that our algorithms will be able to cope with the enormous size of supernetworks, the tools we will develop will rely on decomposition approaches for mixed integer programming, metaheuristics and parallelization techniques. The obtained results will be important contributions to the fields of combinatorial optimization and bioinformatics.	Wellcome Trust	Sir Henry Wellcome Postdoctoral Fellowship	250000.0GBP
332	Dr Lorenz Romy	University of Cambridge	2018-10-01	2021-09-30	Moving the Adverse Outcome Pathways Framework towards Practical Utility by Integrating Compound Profiling Data and Using Deep Learning	Despite a significant investment in in vitro and in vivo screening, clinical safety concerns are still a major cause of compounds being withdrawn from the development process. A lack of mechanistic understanding of safety findings, combined with sparse data sets, restricts the development of approaches capable of predicting earlier potential safety concerns for new chemical entities. It is often unclear whether these are truly surprise events or, with hindsight, could actually have been predicted based on the available data at the time. This failure to detect early signals of safety issues may arise due to problems at the interface between three main components of predictive analysis: observer, data and technology. In this work we propose to employ novel mathematical approaches to data representation and modelling, such as deep learning, of the processes that lead to an adverse outcome in order to reduce or eliminate events of this type. Data will be compiled from within AZ and GSK and shared via Lhasa Limited Ltd, with the student having access to both data sources for model generation. Depending on the adverse event considered, we will either compile AOP frameworks for adverse events, and/or use deep learning approaches for cases where only observational data is available, and hence extend current AOP frameworks where indicated. In particular, nodes in deep networks can be interpreted with respect to their contribution to output toxicity, and hence information for AOPs can be derived from them. The current project will involve GSK and AstraZeneca as industrial partners, with the charity Lhasa Ltd. as an 'honest broker', to exchange and pool compound profiling data for model generation. This framework is currently already being established in the context of the Cambridge Alliance on Medicines Safety (CAMS), and it hence improves significantly upon previous analyses which were based either on public data, or data from only a single source. A pilot study on Structural Cardiotoxicity has already been completed and it is currently being written up for publication, proving the viability and benefit of exchanging data for safety prediction in this framework. We now would like to broaden the approach to other adverse events where data is available. This proposal directly puts into practice recommendations published recently, and it represents an advancement over previous approaches (such as eTox) in being able to have broader access to internal safety data from two major pharmaceutical companies in the UK, AstraZeneca and GSK, shared under confidentiality agreements via a Trusted Broker, Lhasa. This means that all biological data will be made available in the context of the project, including experimental protocols, quantitative data on exposure of compounds, and additional target-based and biological profiling information of compounds can be used in this project. Hence, we expect that the combination of access to data, and the utilization of state-of-the-art machine learning methods, will enhance our ability to predict drug safety and toxicity events considerably. Increasing our understanding of the molecular mechanisms by which compounds can cause adverse events will lead to the implementation of much improved in silico and in vitro screens to detect safety risks will avoid unnecessary investments in preclinical and clinical studies.	National Centre for the Replacement, Refinement and Reduction of Animals in Research	Studentship	90000.0GBP
333	Professor Gleeson Fergus	Oxford University Hospitals NHS Foundation Trust	2017-10-01	2021-09-30	IDEAL: Artificial Intelligence and Big Data for Early Lung Cancer Diagnosis	Despite a significant investment in in vitro and in vivo screening, clinical safety concerns are still a major cause of compounds being withdrawn from the development process. A lack of mechanistic understanding of safety findings, combined with sparse data sets, restricts the development of approaches capable of predicting earlier potential safety concerns for new chemical entities. It is often unclear whether these are truly surprise events or, with hindsight, could actually have been predicted based on the available data at the time. This failure to detect early signals of safety issues may arise due to problems at the interface between three main components of predictive analysis: observer, data and technology. In this work we propose to employ novel mathematical approaches to data representation and modelling, such as deep learning, of the processes that lead to an adverse outcome in order to reduce or eliminate events of this type. Data will be compiled from within AZ and GSK and shared via Lhasa Limited Ltd, with the student having access to both data sources for model generation. Depending on the adverse event considered, we will either compile AOP frameworks for adverse events, and/or use deep learning approaches for cases where only observational data is available, and hence extend current AOP frameworks where indicated. In particular, nodes in deep networks can be interpreted with respect to their contribution to output toxicity, and hence information for AOPs can be derived from them. The current project will involve GSK and AstraZeneca as industrial partners, with the charity Lhasa Ltd. as an 'honest broker', to exchange and pool compound profiling data for model generation. This framework is currently already being established in the context of the Cambridge Alliance on Medicines Safety (CAMS), and it hence improves significantly upon previous analyses which were based either on public data, or data from only a single source. A pilot study on Structural Cardiotoxicity has already been completed and it is currently being written up for publication, proving the viability and benefit of exchanging data for safety prediction in this framework. We now would like to broaden the approach to other adverse events where data is available. This proposal directly puts into practice recommendations published recently, and it represents an advancement over previous approaches (such as eTox) in being able to have broader access to internal safety data from two major pharmaceutical companies in the UK, AstraZeneca and GSK, shared under confidentiality agreements via a Trusted Broker, Lhasa. This means that all biological data will be made available in the context of the project, including experimental protocols, quantitative data on exposure of compounds, and additional target-based and biological profiling information of compounds can be used in this project. Hence, we expect that the combination of access to data, and the utilization of state-of-the-art machine learning methods, will enhance our ability to predict drug safety and toxicity events considerably. Increasing our understanding of the molecular mechanisms by which compounds can cause adverse events will lead to the implementation of much improved in silico and in vitro screens to detect safety risks will avoid unnecessary investments in preclinical and clinical studies.	National Institute for Health Research (Department of Health)	Full Grant	1425634.0GBP
334	Dr. WASSERMANN Demian	National Institute for Research in Computer Science and Automatic Control (INRIA)	2018-03-01	2023-02-28	Accelerating Neuroscience Research by Unifying Knowledge Representation and Analysis Through a Domain Specific Language	Neuroscience is at an inflection point. The 150-year old cortical specialization paradigm, in which cortical brain areas have a distinct set of functions, is experiencing an unprecedented momentum with over 1000 articles being published every year. However, this paradigm is reaching its limits. Recent studies show that current approaches to atlas brain areas, like relative location, cellular population type, or connectivity, are not enough on their own to characterize a cortical area and its function unequivocally. This hinders the reproducibility and advancement of neuroscience. Neuroscience is thus in dire need of a universal standard to specify neuroanatomy and function: a novel formal language allowing neuroscientists to simultaneously specify tissue characteristics, relative location, known function and connectional topology for the unequivocal identification of a given brain region. The vision of NeuroLang is that a unified formal language for neuroanatomy will boost our understanding of the brain. By defining brain regions, networks, and cognitive tasks through a set of formal criteria, researchers will be able to synthesize and integrate data within and across diverse studies. NeuroLang will accelerate the development of neuroscience by providing a way to evaluate anatomical specificity, test current theories, and develop new hypotheses. NeuroLang will lead to a new generation of computational tools for neuroscience research. In doing so, we will be shedding a novel light onto neurological research and possibly disease treatment and palliative care. Our project complements current developments in large multimodal studies across different databases. This project will bring the power of Domain Specific Languages to neuroscience research, driving the field towards a new paradigm articulating classical neuroanatomy with current statistical and machine learning-based approaches.	European Research Council	Starting Grant	1497045.0EUR
335	Dr. WASSERMANN Demian	University of Zurich	2017-11-01	2019-01-31	EEG based microsleep episode detection in the maintenance of wakefulness test and the driving simulator using a machine learning approach	Road traffic injuries are the leading cause of death for those aged 15-29 years. Excessive daytime sleepiness (EDS) is estimated to be the underlying cause in up to 15-20% of motor vehicle accidents (MVA), and is most often caused by socially induced sleep deprivation or poor sleep hygiene in otherwise healthy individuals, followed by medical disorders, or the intake of drugs. Methods for reliably objectifying sleepiness are urgently sought, primarily for sleepiness detection while driving but also for predicting the risk of sleepiness induced accidents during laboratory assessments of patients. The EEG is widely recognized as the gold standard for determining sleep-wake stages and their sudden, as well as gradual, changes. In the clinical and scientific context, standard EEG scoring criteria are generally applied, in which sleep is scored based on 30-s epochs. These sleep criteria consider neither the occurrence of microsleep episodes (MSE) nor the local aspects of sleep demonstrated in both animals and humans. Falling asleep is a gradual, not a sudden, process and shows fluctuations between waking and sleep. Particularly when assessing objective sleepiness in the context of driving ability, MSE of short duration originating in any brain area become an important criterion.We aim to characterize and identify MSE by defining visual scoring rules for MSE as short as 1 s, extracting relevant features based on quantitative EEG analyses, and by developing machine learning algorithms to detect MSE. First, we will only consider occipital EEG leads but will extend our analyses to central EEG leads in a second step. Our algorithms will be trained and verified on MWT data of patients and subsequently applied to MWT data of sleep deprived healthy individuals, and simulated driving conditions. In the driving simulator, we will investigate the association of MSE with impaired driving performance and spontaneously perceived sleepiness (SPS). In the context of fitness to drive assessments, one generally assumes that the perception of sleepiness precedes the occurrence of MSE. Our approach should allow to validate or disprove such an assumption. Previous studies mainly assessed relationships based on mean values or pooled data. Our aim is to take inter-individual variations into account and relate single events (i.e. MSE) with quantitative EEG measures. Therefore, we intend to track the sleep-wake transition zone (e.g. occurrence of MSE, vigilance fluctuations) with high temporal resolution on a second-by-second basis. In summary, we intend to develop and formulate the first standardized MSE scoring rules worldwide and to establish reliable automatic detection algorithms. This will have a major impact in sleep medicine and research and open new areas of research and diagnostic procedures. Since sleepiness is among the most frequent causes of car accidents, and an important risk factor for train and truck drivers and in many surveillance tasks such as air traffic or nuclear power plant control, reliable diagnostic tools to judge fitness to drive or fitness on the job become essential for reducing car and work accidents, catastrophic incidents, and the immense costs related to excessive daytime sleepiness.	Swiss National Science Foundation	Project funding (Div. I-III)	217848.0CHF
336	Dr. WASSERMANN Demian	ETH Zurich	2010-08-01	2013-07-31	The evolution of proteins with tandem repeats: a large-scale study of rates, selective forces, complex patterns and associations with human disease	Protein repeats are predominantly found in muscle, brain, synaptic cell adhesion proteins, but underrepresented in very basic cellular functions. Over the last years, the important and versatile roles of tandem repeats have been documented by an increasing number of studies. Repeat lengths vary from homorepeats (eg. in the Huntington disease gene) to long repeats with multiple domains (eg. the cytoskeletal protein titin). Functional classification of proteins with tandem repeats suggests that they are often involved in multiple binding, and so facilitate protein-protein interactions and are required for the formation of multi-protein complexes. Protein repeats tend to have structural roles in proteins with fundamental biological functions, including survival facilitation. Antigenic and other virulence related proteins such as toxins and allergens may also be encoded by sequences with repeats. Proteins with tandem repeats are frequently associated with infectious, neurodegenerative diseases. Along this line, the discovery of repeat-containing proteins and their structure-function study promise to be a fertile direction for research leading to the identification of targets for new medicaments and vaccines. The systematic bioinformatics analysis of protein repeats in genomes can provide a global view on these motifs, their structures, functions and evolution. This should facilitate a significant improvement of our understanding of structural and functional changes during the evolution of proteins with repeats and their protein-protein interaction networks.Despite several studies of protein repeats, the fundamental questions about the evolution and roles of repeats are far from being answered. Hardly any previous studies went beyond the classification-type summaries of single repeats and proteins containing them. This project aims to make a major contribution to the understanding of repeat-containing proteins, their structure, function and the dynamics of protein-protein interactions. • The project will assemble the most complete set of protein repeats using a novel algorithm for the identification of repeats that have diverged via substitution and indel events, based on the K-clustering-based approach. Data will be made publicly available through our Protein Repeat DataBase (PRDB). • We will study biological forces shaping the mutational landscape of proteins with repeats. Several purpose-built novel codon substitution models will be developed for this task. For example, using these models we will assess selective forces modulating the number and length of repeats. We will obtain estimates of rates of repeat duplications (and mutations) relative to point mutation. The project will seek to compare and characterize the selective forces acting on the globular and the repetitive parts of the proteins as well as the composition bias, recombination and co-evolutionary forces. PI’s expertise in codon models will be important for the proposed development of novel tailor-made Markov models, which will consequently be used to assess selective forces modulating the number and length of repeats, and the rates of evolution of the globular part vs the periodic part. The developed methods will be implemented in a user-friendly software package and made available to the users.• Complementary information (expression, disease associations, biochemical pathways, etc.) will be assembled for each repeat protein. Together with evolutionary rates and selection estimates, these data will be analyzed using machine learning and pattern discovery techniques. A particular emphasis will be on the trends observed for disease-associated genes. Do the disease genes exhibit unusual evolutionary patterns, selective pressures, expression profiles, etc. compared to genes lacking such associations? Prediction of genes with roles in disease will have invaluable medical applications. • Important examples of disease-related genes will be analyzed in more detail, applying most suitable evolutionary models to the expanded datasets. In particular, codon models that we will develop will be used to evaluate the selective pressure acting on the length of homorepeats during their evolution. These models will assess protein fitness changes as a result of altered numbers of repeats or their lengths. While the conservation is observed in protein coding homorepeats, their length and variability in population is not conserved, suggesting differential selection. The codon models will provide powerful means to evaluate various selection scenarios. Structural modeling techniques will be applied to a set of important proteins (collaboration with Dr Kajava, CRBM-CNRS, Montpellier, France), which will also be studied further using experimental techniques. For the empirical studies we will work in collaboration with biophysicists, experimental biologists and biochemists (Dr Padilla, CBS-CNRS, Montpellier, France).• We will study repeat-containing proteins in pathogenic organisms. Due to their binding properties, proteins with repeats are potential candidate genes influencing the pathogeneicity and disease progression. Indeed, many repeat-containing proteins are expressed on the surface of rapidly evolving pathogens, such as bacteria, viruses that are serious human pathogens or have agricultural or environmental importance. Their antigenic proteins are under strong selective pressure to escape host immune response. A combination of adaptive mutations in such genes may be at the origin of emerging infectious diseases. We will aim to identify the genes and residues that may be used as drug/vaccine targets, essential for controlling and preventing epidemics. Studies of protein with repeats in plant or animal pathogens, are of agricultural and environmental importance. • All assembled data on repeat-containing proteins will be available via mirror web-server. Our database PRDB will be regularly updated, and will incorporate basic tools for categorized searches of repeats, their basic analysis and organization. The goal is to create an integrated data resource for proteins with repeats. Such approach will enable researchers to combine the multitude of available resources and consider them in their integrity. This will facilitate a more efficient and rapid progress in studying structure-function relationship of proteins with tandem repeats and subsequent medical, agricultural and environmental applications. The project will lay a sound foundation for further work on structure of proteins with repeats. The discovery process has high potential and may open new aspects of theoretical biology, molecular evolution, protein structure, and medical related research.	Swiss National Science Foundation	Project funding (Div. I-III)	260278.0CHF
337	Dr. WASSERMANN Demian	University of Fribourg	2012-10-01	2013-09-30	Coestimating selection and demography	Detecting signatures of past selective events provides insights into the evolutionary history of a species by evidencing adaptive events. The identification of molecular targets of selection in humans, for instance, pinpoints biologically relevant differences between us and other apes. In addition, inferences regarding selection provide important functional information by elucidating the interaction between genotype and phenotype. Since positions in the genome that are under selection are functionally important by definition, detecting signatures of selection has also been used extensively to identify functional regions or protein residues. Finally, inferring the molecular locations at which selection is acting may help us to predict responses to selective pressures in organisms such as viruses, which would revolutionize the management of pandemics and the development of drugs. Unfortunately, the demographic history of a population or species is a major confounding factor when inferring past selective events. Indeed, neutrality tests are very sensitive to violations of the underlying demographic assumption of constant population size - and it is often impossible to distinguish between adaptive or demographic processes in putatively identified regions. After a population bottleneck, for instance, false positive rates of up to 90\% are not uncommon. Current approaches to deal with this problem rely on the assumption that selection is acting on a few loci only, while demography affects all loci equally. A two step procedure has been proposed in which a set of selectively neutral loci are used to calibrate a demographic model against which putatively selected loci are compared. However, recent evidence suggests that selection may be common in the genome of many organisms and a priori knowledge on the neutrality of markers is often difficult to obtain. As a result, this approach suffers from high false negative rates - not to mention relying on very strong assumptions regarding the pervasiveness of adaptive mutations.There is currently no flexible approach to estimate demography and selection jointly. However, recent advances in computational approaches offer new hopes to tackle such an inference. A particularly promising approach is Approximate Bayesian Computation (ABC), a technique to sidestep analytical likelihood calculations with simulations. To this end, ABC has been used to infer a wide range of evolutionary scenarios such as population bottlenecks, population splits and migration, but also to distinguish between a classic selective sweep and recurrent selective events.Here I propose to develop an ABC framework to estimate demography and selection jointly, and to apply it to a variety of organisms with very different evolutionary histories. Major new developments are needed to reach this goal. Firstly, the application of ABC to large scale data sets is tenuous without novel techniques to reduce the computational effort required. I will address this through various innovations including new ABC-MCMC algorithms with increased performance, an efficient recycling of simulations, and extending recent approaches to hybridize ABC with traditional full-likelihood methods. Such a hybridization will enable us to profit from the rich literature on full-likelihood solutions to simpler problems and to take linkage properly into account. Secondly, the joint inference of demography and selection calls for new models and new sets of informative summary statistics. I will approach this challenge by integrating models established to infer either selection or demography alone and through current techniques to explore a large space of summary statistics, such as PLS or machine learning algorithms. The proposed innovations will allow me to work towards answering some of the most controversial questions in evolutionary biology, namely the importance of adaptation in shaping genomic variation. I will approach these questions by inferring genome-wide selection coefficients from unique data sets of four organisms representing various selective and demographic histories: Deer mice (Peromyscus maniculatus), HCM viruses, Drosophila melanogaster and humans. These estimates will not only have broad implications for the management of pandemics or the development of new drugs, but will also greatly improve our understanding of the mode and tempo of the process of adaptation.	Swiss National Science Foundation	Ambizione	214250.0CHF
338	Dr. WASSERMANN Demian	Langer lab Department of Chemical Engineering Massachusetts Institute of Technology	2016-07-01	2017-12-31	Active learning for late-stage drug design	Challenges in solubility, stability, and absorption require formulation development that can significantly delay the introduction of promising drugs to patients. Active machine learning allows for rapid model development by implementing a feedback-driven artificial intelligence that puts the machine in charge of requesting additional data for iteratively improving predictive accuracy. Coupled to novel organ-on-a-chip assay systems, active learning workflows have the potential to transform formulation development and generate in silico models that help streamlining translational drug design. This proposal aims to establish active learning workflows for rapidly generating models for various objectives relevant to formulation development using intestine-on-a-chip systems. In addition to predictive models, such workflows will generate vast amounts of biological data to deepen our understanding of organ-on-a-chip systems as well as provide theoretical insights into the learning patterns of artificial intelligence.	Swiss National Science Foundation	Early Postdoc.Mobility	214250.0CHF
339	Dr. WASSERMANN Demian	University of Zurich	2017-02-01	2019-01-31	ICU-Cockpit: IT platform for multimodal patient monitoring and therapy support in intensive care and emergency medicine	In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.	Swiss National Science Foundation	NRP 75 Big Data	573853.0CHF
340	Dr. WASSERMANN Demian	University of Basel	2009-10-01	2013-03-31	Inference of post-transcriptional regulatory codes involving miRNAs and RNA binding proteins	Although for many years transcription factors held the center stage inthe regulation of gene expression, a very complex post-transcriptionalregulatory layer implemented by miRNAs and RNA-binding proteins andfrequently acting precisely on the mRNAs encoding transcriptionfactors, has been uncovered. Far from passively carrying the geneticinformation to the ribosomal translation machines, messenger RNAs havea life of their own and this can be extended or shortened depending oninteractions with various protein and ribonucleoprotein complexes inthe cell. Although the miRNAs have initially been described asregulators of the mRNA translation rate, it is now clear that animportant mechanism behind the miRNA-based regulation is target mRNAdegradation. The activity of miRNA-containing ribonucleoproteincomplexes can itself be modulated for example by RNA-binding proteins(RBPs) that recognize sites in the vicinity of miRNA-binding sites andact as competitors for miRNA binding.Much effort in the past years has been devoted to identifying newplayers in post-transcriptional control. Many groups, including ourown, contributed to the catalog of miRNA genes in species ranging fromviruses to human. Similarly, many groups, ours included, contributedthrough computational or experimental approaches to the list ofputative miRNA-target interactions. Yet although it is clear that onaverage a miRNA targets hundreds of genes, much remains to be done tounderstand the determinants of miRNA targeting specificity. Based onthe observation that miRNAs destabilize their mRNA targets manyexperimental studies aimed to identify miRNA targets by miRNAtransfection and microarray profiling. In spite of the fact thathundreds to thousands of transcripts respond in a typical experiment,only a fraction of the predicted miRNA targets do. Moreover, forreasons that are so far unclear, the magnitude of the response varieswidely. In a recent analysis of over seventy experimental data sets ofmRNA or protein level changes that occurred upon perturbation of miRNAexpression, we found that functional miRNA target sites reside inaccessible regions, and that the degree of mRNA degradation is relatedto the U (and to some extent A) nucleotide content of the miRNA targetsite environment. This suggests that the fate of miRNA targets dependsnot only on their interaction with the RNA-induced silencing complex(RISC), but also on the presence of modulatory protein co-factors. Incollaboration with the group of Tom Tuschl (The RockefellerUniversity) we have recently identified a number of RBPs that interactwith the RISC complex and we characterized their cognate mRNAs byimmunoprecipitation and microarray expression profiling. We havefurther developed a novel cross-linking and immunoprecipitation methodto identify RBP targets at the resolution of individual bindingsites. Building on this work, we here propose to carry out the following projects:1. Computational modeling of miRNA-target interactions.The starting point of this study will be the set of sequences that we found, with the CLIP method mentioned above, to be bound by Argonaute protein-containing complexes. We will attempt to use machine learning approaches to identify the location of miRNA binding sites in these data and to characterize the mode of binding of miRNAs to their targets. We will also thoroughly characterize the properties of these binding sites and we will use the results to develop improved methods for miRNA target prediction.2. Experimental identification of the targets of U-rich element binding proteins.This project will build on one hand on the experimental method that we developed in collaboration with the group of Tom Tuschl (The Rockefeller University), and on the other hand on our computational results that indicate that U-rich elements are associated with the degradation of miRNA targets. We have selected a set of U-rich element binding proteins whose targets we would like to identify by CLIP, and then study in relationship with the miRNA targets that we already obtained.3. Development of computational models of RBP-mRNA interactions.This projects aims to again use the CLIP data that we have and will obtain in the near future to start developing models to specifically describe RBP-mRNA interactions, taking into account the particularities of this class of proteins. These projects will make an important contribution to a quantitativeunderstanding of miRNA-based post-transcriptional regulation and tothe inference of the elusive 'RNA code'.	Swiss National Science Foundation	Project funding (Div. I-III)	600000.0CHF
341	Dr Nachev Parashkev	University College London	2010-07-01	2013-12-31	Automated high-dimensional outcome prediction in stroke.	Stroke is the leading cause of adult disability in the UK at an estimated overall annual cost of ?7 billion. Alarmingly, clinical outcomes in stroke are not improving as fast as conditions of similar aetiology. A major cause is the difficulty in providing targeted care in a patient group with such hugely diverse requirements: indeed, the government predicts that better organisation of care can both save lives and optimize resource allocation. We therefore propose to develop a system for predicting clinical outcomes so as to provide advance information on the optimal clinical management of each patient. Our solution automated brain image analysis with high-dimensional machine-learning inference exploits the fine scale functional specialization of the brain, capturing the relation between the pattern of brain damage and clinical outcome at very high resolution. As our pilot data demonstrate, this approach permits much greater predictive power than is offered by current techniques which all rely on crude, global features such as the volume of damaged brain or its rough location and yet adds minimal resource overhead because it uses routinely acquired clinical data. Our proposal seeks to develop this technology to the point of direct clinical application.	Wellcome Trust	Translation Award	392444.0GBP
342	Dr Nachev Parashkev	University College London	2014-09-15	2018-02-01	Automatic anomaly detection for brain imaging triage and classification	Modern brain imaging generates many thousands of data observations per patient, yet its clinical impact remains determined only by the summary points in a radiologist's verbal report. This information gap shows that we are not fully exploiting all potentially relevant clinical information from this expensively acquired data. Our technology will seek dramatically to reduce this gap within current clinical pathways by applying novel machine-learning algorithms to routine brain imaging data, with the aim of automatically extracting rich, high-dimensional information of clinical utility. The system will deliver automatic quantification of the anomaly of each data point of an image to assist radiological reporting and automated outcome prediction based on disease patterns and an anomaly score for the whole image to aid radiological triage and resource/performance management. Our goal is to demonstrate the feasibility, robustness, clinical, and managerial value of the approach using a large dataset of routine clinical brain imaging, delivering a pilot system translatable into a full clinical product. Merely by adding an inexpensive layer of computation to existing pathways, the system should improve report fidelity and optimise radiological triage and management, while creating a scalable new platform for facilitating big data' approaches to major neurological disorders.	Wellcome Trust	Health Innovation Challenge Fund Award	1088636.0GBP
343	Dr Cardinal Rudolf	University of Cambridge	2018-03-31	2021-03-31	MICA: Mental Health Data Pathfinder: University of Cambridge, Cambridgeshire & Peterborough NHS Foundation Trust, and Microsoft	With strong NHS partnerships and recent contributions to national mental health (MH) informatics, we shall add novel methods, epidemiology and phenotyping to the MH Platform. We envisage a modular pipeline that de-identifies MH data; supports flexible consent for sharing/contact; and links MH, cognitive, physical, psychosocial and biomarker data. Project (P) 1. Our open-source tools de-identify clinical records to create CPFT’s Research Database, supporting research and participation. We shall extend them to generate anonymised subsets and link data from consenting patients across MH/community services, acute care, and research organizations, including from existing deeply phenotyped longitudinal cohorts. We emphasize rigorous interface standards and NHS governance over identifiable/pseudonymised data. We shall collaborate on a national natural language processing framework, allowing NHS/research organizations to generate structured data from free text. P2. We have created novel open-source neuropsychiatric assessment software. We shall extend it for broad and integrated NHS and research use. This will take automated cognitive testing into routine clinical practice. As a bold but tractable exemplar with research and clinical applications, we shall use it to apply electronic diagnostic algorithms and neuropsychiatric phenotyping, and link these detailed data to clinical records and biomarkers that include immunophenotyping. P3. We shall apply P1 tools to a public health crisis: the premature death of those with serious mental illness. We shall link MH, national and acute Trust data and use machine learning to develop early predictors of mortality. P4. We shall democratize MH research though broad consultation on generic tiered consent models for data-sharing and participation, by giving the research database direct clinical interfaces, and by enhancing data visualization to help clinicians and service users develop research and the NHS improve local services.	Medical Research Council	P&Cs	1496194.0GBP
344	Mag. DI Dr.phil. Dr.techn. HAUSER Andreas	Graz University of Technology	2016-12-01	2020-05-31	Heterogeneous catalysis on metallic nanoparticles	The usage of machine learning techniques is a promising approach towards the modeling of materials with applications ranging from surface science to molecular systems. Among its various methods, the field of neural networks has seen a recent revival, triggered by the introduction of high-dimensional neural network schemes by Behler and Parrinello. The combination of many ‘standard’-type multilayered feed-forward neural networks, with each one describing the local chemical environment of a single atom of the system, allows the construction of economic, but also reliable interatomic potentials for a large-scale simulation of complex materials. It is the aim of this project to develop such a type of neural network structure for the simulation of bimetallic nanoparticles in the context of heterogeneous catalysis. Our target systems are pure and mixed clusters of gold and silver, in the range from 10 to 1000 metal atoms. The neural network will be fed with data obtained from density functional theory calculations on finite and periodic systems, i.e. on small mixed-metallic clusters as well as on bulk metals and alloys. Complementary to current research on the interplay between highly dispersed metal clusters and a given support, this project focuses on internal properties of the mixed-metallic particles, such as cluster structure, shape and stability, as a function of particle size and metallic ratio. We can take advantage of the fact that for structures with a well-defined separation of metallic phases, e.g. ‘Janus’-type clusters or core-shell structures, the number of different local atomic environments is comparably small. Later steps of the project will also comprise first interactions with gas phase molecules such as molecular hydrogen and carbon monoxide. While a full study of reaction pathways lies beyond the scope of this project, it should be possible to simulate and compare the physisorption step, a crucial step in heterogenous catalysis, on metal particles of various size and composition. Understanding how size, shape, structure and metallic ratio affects the catalytic properties of a particle is of fundamental interest to the catalysis community. The project will be carried out at the TU Graz Institute of Experimental Physics and supervised by Ass.-Prof. Andreas Hauser. It will be conducted in a tight collaboration with the experimental group of Prof. Wolfgang Ernst, head of the institute, whose expertise lies in the synthesis and the imaging of smallest mixed-metallic nanoparticles, and the experimental group of Prof. Martin Sterrer, who is currently working on scanning probe techniques for catalytic reactions on nanoparticles. Dr. Philip Marquetand from the Institute of Theoretical Chemistry at the University of Vienna offered support in the early phase of program code development.	Austrian Science Fund FWF	Stand-Alone Project	249477.91EUR
345	Prof. BAAYEN Rolf Harald	University Of Tuebingen	2017-09-01	2022-08-31	Wide Incremental learning with Discrimination nEtworks	Although homo sapiens has been endowed with language for over 50,000 years, the invention of alphabet-like scripts 3,000 years ago dominates Western linguistic thinking. Training in literacy starts in early childhood, and because of this, words and letter-like sound units can naturally seem to be the building blocks of language. The Chinese writing system highlights the cultural-specificity of this approach: characters are juxtaposed without intervening spaces, and their interpretation is highly context-dependent. Words are not singled out. And although more frequent characters contain parts indicating pronunciation, it is syllables that are referred to, not letter-like sound units. The research proposed here seeks to break the hold that the alphabet-centric approach has on our understanding of language by exploring the idea that instead of being phone and word-based, languages use low-level properties of the acoustic signal to directly reduce uncertainty about the messages encoded in the speech signal. My work with wide learning networks (two-layer networks with many thousands of units, using the simplest possible error-driven learning rule) provides remarkable support for this suggestion: For reading and speech comprehension, their performance closely matches both the strengths and the weaknesses of human processing. Especially at a time when machine learning and artificial intelligence are moving beyond human capacity, it is a methodological imperative to study and work with algorithms reflecting both the advantages and disadvantages of human learning. I am requesting funding to take this radically novel research program to the next level by further developing our account of auditory comprehension, by modeling more typologically diverse languages, by extending this approach to speech production, and by developing a discrimination-based language theory.	European Research Council	Advanced Grant	2496875.0EUR
346	Dr Taylor Peter	Newcastle University	2018-08-13	2020-08-13	EpiChange: Quantifying longitudinal changes after epilepsy surgery	Resective surgery for epilepsy, where the part of the brain thought to cause seizures is removed, leads to seizure freedom in around 70% of patients 1 year post-surgery. This falls to around 50% at 5 years post-surgery. It is not fully understood why surgery only works initially for some patients, and why this falls over time post-operatively. Surgery has a substantial immediate impact on brain structure, however, the long-term impact of surgery on brain dynamics is poorly understood. In order to make progress in this area we will perform a retrospective analysis of longitudinally acquired electroencephalographic (EEG) data. EEG recordings were made pre-operatively, and post-operatively in 76 patients for up to 5 years. Using univariate and multivariate data analysis, in conjunction with machine learning, we will learn how brain dynamics change after surgery, and if this change relates to outcome. Crucially, we will attempt to identify which factors in brain dynamics correlate with seizure relapse, even years after surgery. If successful, this will pave the way to a larger project where changes can be reverse engineered to give predictions of post-operative decline using pre-operative data. Long-term, this research has implications for other disorders involving longitudinal decline following structural brain damage.	Wellcome Trust	Seed Award in Science	99978.0GBP
347	Dr Taylor Peter	Department of Computing Imperial College London	2015-09-01	2016-02-29	Multimodal patient-specific eye model for delineation and treatment planning of ocular tumors	In the past few years, retinoblastoma and uveal melanoma, the most common cancers of the eye, have witnessed an important advance in the field of medical imaging and its applications in ophthalmology. MRI, CT, Fundus Photography and Ultrasound are among the image modalities of reference for treatment planning and diagnosis confirmation of the disease. However, existing methods for modeling the eye are still imprecise and they do not benefit from state of the art techniques developed in other medical image processing applications. In addition, there is a disconnection between the different image modalities due to the lack of common anatomical landmarks and to the difficult method standardization in ophthalmic radiation oncology. This situation mainly affects the treatment planning (radiation dose optimization) step involved in External Beam Radiotherapy (EBRT), Cryotherapy and Brachytherapy, thus precluding optimal preservation of healthy tissue in patients. Furthermore, some patients need to undergo eye surgery prior to treatment planning, with the objective of offering a better tracking of the tumor location during therapy, therefore they require the implant of tantalum fiducial markers in the posterior part of the eye. Hence, techniques for preventing eye surgery while providing similar tracking results would offer an unprecedented opportunity for improving the patient’s quality of life during and after treatment. During the last two years we have developed a method to fully automatically segment the eye in the 3D MRI, and collaborated in the preliminary fusion of MRI with Fundus photography. These contributions follow the motivations of the presented project: i) create a method for the automatic delineation of pathological areas inside the eye in the MRI by combining multiple MRI sequences and machine learning techniques, ii) propose a framework for the fusion of MRI and Fundus photography for robust and accurate eye tumor delineation towards preventing eye surgery and iii) to set the basis for techniques to predict the tumor growth based on the shape and location of the tumor, and the technique applied for treatment. These ideas and the extended motivation can be read in detail in the research plan.	Swiss National Science Foundation	Doc.Mobility	99978.0GBP
348	Dr de Goede Christan	Lancashire Teaching Hospitals NHS Foundation Trust	2017-12-01	2021-01-31	MyPad – Intelligent Bladder Pre-void Alerting System	In the past few years, retinoblastoma and uveal melanoma, the most common cancers of the eye, have witnessed an important advance in the field of medical imaging and its applications in ophthalmology. MRI, CT, Fundus Photography and Ultrasound are among the image modalities of reference for treatment planning and diagnosis confirmation of the disease. However, existing methods for modeling the eye are still imprecise and they do not benefit from state of the art techniques developed in other medical image processing applications. In addition, there is a disconnection between the different image modalities due to the lack of common anatomical landmarks and to the difficult method standardization in ophthalmic radiation oncology. This situation mainly affects the treatment planning (radiation dose optimization) step involved in External Beam Radiotherapy (EBRT), Cryotherapy and Brachytherapy, thus precluding optimal preservation of healthy tissue in patients. Furthermore, some patients need to undergo eye surgery prior to treatment planning, with the objective of offering a better tracking of the tumor location during therapy, therefore they require the implant of tantalum fiducial markers in the posterior part of the eye. Hence, techniques for preventing eye surgery while providing similar tracking results would offer an unprecedented opportunity for improving the patient’s quality of life during and after treatment. During the last two years we have developed a method to fully automatically segment the eye in the 3D MRI, and collaborated in the preliminary fusion of MRI with Fundus photography. These contributions follow the motivations of the presented project: i) create a method for the automatic delineation of pathological areas inside the eye in the MRI by combining multiple MRI sequences and machine learning techniques, ii) propose a framework for the fusion of MRI and Fundus photography for robust and accurate eye tumor delineation towards preventing eye surgery and iii) to set the basis for techniques to predict the tumor growth based on the shape and location of the tumor, and the technique applied for treatment. These ideas and the extended motivation can be read in detail in the research plan.	National Institute for Health Research (Department of Health)	Full Grant	477200.0GBP
349	Dr Caminati Marco	University of St Andrews	2018-02-14	2021-02-13	Stochastic models to enable tailoring of medications to patients with multiple morbidities	In the past few years, retinoblastoma and uveal melanoma, the most common cancers of the eye, have witnessed an important advance in the field of medical imaging and its applications in ophthalmology. MRI, CT, Fundus Photography and Ultrasound are among the image modalities of reference for treatment planning and diagnosis confirmation of the disease. However, existing methods for modeling the eye are still imprecise and they do not benefit from state of the art techniques developed in other medical image processing applications. In addition, there is a disconnection between the different image modalities due to the lack of common anatomical landmarks and to the difficult method standardization in ophthalmic radiation oncology. This situation mainly affects the treatment planning (radiation dose optimization) step involved in External Beam Radiotherapy (EBRT), Cryotherapy and Brachytherapy, thus precluding optimal preservation of healthy tissue in patients. Furthermore, some patients need to undergo eye surgery prior to treatment planning, with the objective of offering a better tracking of the tumor location during therapy, therefore they require the implant of tantalum fiducial markers in the posterior part of the eye. Hence, techniques for preventing eye surgery while providing similar tracking results would offer an unprecedented opportunity for improving the patient’s quality of life during and after treatment. During the last two years we have developed a method to fully automatically segment the eye in the 3D MRI, and collaborated in the preliminary fusion of MRI with Fundus photography. These contributions follow the motivations of the presented project: i) create a method for the automatic delineation of pathological areas inside the eye in the MRI by combining multiple MRI sequences and machine learning techniques, ii) propose a framework for the fusion of MRI and Fundus photography for robust and accurate eye tumor delineation towards preventing eye surgery and iii) to set the basis for techniques to predict the tumor growth based on the shape and location of the tumor, and the technique applied for treatment. These ideas and the extended motivation can be read in detail in the research plan.	Medical Research Council	Fellowship	289274.0GBP
350	Jost Kerstin	Other Hospitals	2017-06-01	2017-07-31	NEWS - Newborn Early Warning Signs	Sepsis and meningitis are major causes of morbidity and mortality in theneonatal population. Despite vigilant clinical assessment of infants in the neonatalintensive care unit (NICU), diagnosis of sepsis and necrotizing enterocolitis (NEC) oftendoes not occur until an infant has significant hemodynamic compromise. Analysis of heart rae variability becomes more and more relevant in predicting outcomes over short and long time. Thus decription of apnea, bradycardia and desaturation (ABD-events) are of major importance to predict future potentially life-threatening events. Combining biomarker screening with predictive monitoring of physiologic markers for assessing risk of sepsis or NEC and for detection of early-stage illness could prevent progression to severe illness and shock.The neonatal intensive care unit's database for surveillance and storage ofcardiorespiratory parameters (Clinisoft®), allows us to prospectively characterizedevelopment of ABD-events over time.To examine the contribution of infection, acute and chronic inflammation, indevelopment of cardiorespiratory dysfunction there are ongoing prospective studies ofterm and premature infants at the NICU and paediatric intensive care units at the Karolinska University Hospital Solna. There cardiorespiratory recordings (Clinisoft®) of ABD- events are correlated with routine inflammatory biomarkers & medical records. We then further examine how physiomarkers (ABD-events) & biomarkers may act as Newborn Early Warning Signs (NEWS) for infection, inflammation and need for increased therapeutic interventions. In this project, our objective is to develop machine learning based methods for using low frequency labeled data (Clinisoft) in conjunction with the high frequency unlabeled data. Using pattern recognition we develop better physiomarkers (cardiorespiratory indexes) as NEWS that will help to predict short-term outcome and enable rapid therapeutic interventions.	Swiss National Science Foundation	International short research visits	6300.0CHF
351	Bucher Philipp	EPF Lausanne	2009-04-01	2013-06-30	Chromatin structure-driven computational analysis of gene regulatory regions	The genetic code for gene regulatory information and the molecular mechanisms of transcriptional control are still poorly understood. The advent of high-throughput technologies to map in vivo DNA-protein complexes, in particular chromatin immunoprecipitation combined with ultrahigh-throughput sequencing (ChIP-Seq) raises new hope for a breakthrough. For the first time it is possible to obtain a comprehensive and detailed view of the chromatin structure of an entire genome in a particular cell type. In addition, we are now able to identify the complete repertoire of in vivo binding sites for a given transcription factor under given conditions. In other words, the molecular events that control gene expression in proximity to the DNA suddenly have become visible. The genome-wide ChIP-Seq datasets that have become available over the last year, already led to numerous new insights about gene regulation. We anticipate that very large volumes of similar data will be released during the course of the proposed project. Computational analysis of ChIP-Seq data is in its infancy. For computational biology, the next few years will probably be remembered as a learning phase during which the community has acquired experience how to make best use of this new type of molecular data. Standard protocols for quality-control and low level data processing will arise. The type of biological questions that can be answered will be better defined and stated in terms amenable to computational approaches. Appropriate data structures for higher level analysis will arise and methodological principles and algorithm will consolidate over time. This proposal is meant to be a contribution to this learning process. The focus is on higher level analysis and interpretation, not on primary data processing. Specifically we are interested in questions concerning the relationship between DNA sequence, chromatin structure, transcriptional activation and repression. A key question in gene regulation, especially with regard to large higher eukaryotic genomes, is why a given transcription factor binds only to a subset of potential target sites in the genome. Much of the proposed work is devoted directly or indirectly to this problem. Histone modification maps based on ChIP-Seq provide clues about DNA accessibility, transcriptional status, and other physiological processes acting on a gene regulatory region. Classification and categorization will help to define subsets of genomic regions that offer similar conditions for transcription factor binding. Information on evolutionary conservation may further direct our analyses to functionally relevant chromatin regions. Comprehensive maps of in vivo transcription factor binding sites provide ideal training and test sets for machine learning approaches that may help to answer the key question of what directs gene regulatory proteins to their physiological target sites. Our research will be mostly biological question-driven in this sense. In addition to methodological advances, we hope that the proposed work will lead to a number of specific and experimentally testable hypotheses regarding chromatin-modulated gene regulatory mechanisms.	Swiss National Science Foundation	Project funding (Div. I-III)	199116.1CHF
352	Hahnloser Richard	University of Zurich	2010-10-01	2014-09-30	The roles of social context and sleep replay for vocal learning in a songbird	Many complex learning behaviors such as speech learning are strongly influenced by factors including social context and sleep. Although many influences are known today, they have mostly been studied in artificial rather than natural settings and are currently supported only by correlative but not by causal evidence. Our work aims at bridging this gap by studying vocal development in the songbird and its dependence on social interactions with conspecific birds and on neural replay of behavioral sequences during sleep. The songbird is a vocal learner in which the brain mechanisms involved in sleep replay are well known. During sleep, premotor neurons of the vocal apparatus in zebra finches engage in bursting patterns that are reminiscent of their patterns generated during song production. In the past, we have performed extensive studies on the generation of such sleep-burst sequences in large song-control networks. We now want to make use of his knowledge and test for a causal relationship between sleep sequences and vocal development. Such a causal relationship has been widely hypothesized but has never been tested experimentally. We plan to implant stimu-lation electrodes into the brain area that generates sleep sequences and perturb downstream sleep bursts to study their effects on vocal changes. Comparisons will be made with birds that are similarly stimulated, but at daytime rather than during sleep. We hope this research will provide one of the first demonstrations that off-line neural activity during sleep has key roles for procedural learning.The social interactions during vocal learning and their influences have not been studied exten-sively yet. In a different set of experiments we will study the social factors that are beneficial or detrimental to vocal learning. It is known that birds with siblings produce less accurate copies of tutor song than birds without siblings, an effect known as a fraternal inhibition. However, the precise factors of this inhibition and the role of the parents in mediating this inhibition are currently not known. To study such effects of social context and many more, we will design and build a bird monitoring system in which several microphones and a video camera jointly operate to record all songs, the locations of the singers, and the locations of the other birds of a family living inside the same cage. This bird-monitoring system is a difficult instrumentation task because jointly tutored birds sing similar songs and often they sing at the same time. Hence, we will apply sophisticated machine-learning techniques to solve this blind-source separation problem. Our bird monitoring system will be fully automated and minimally rely on human input, thus facilitating behavioral experiments and high-throughput data acquisition. We are convinced that this tool will be useful to a large community of birdsong and neuroethology researchers worldwide, because one of the most successful strategies for understanding brain mechanisms is to study them in natural settings rather than in artificial ones. In future experiments beyond this grant application, we will make use of this bird-monitoring system also in electrophysiological experiments, to study brain mechanisms in a natural social context.The broader relevance of our work extends from songbirds to human social sciences and physi-ology, because effects of sibling number and parental interactions influence speech learning also in children. And, the functions of sleep and dreams remain deeply mysterious and any new insights even in animal models will be highly valuable.	Swiss National Science Foundation	Project funding (Div. I-III)	463572.0CHF
353	Guckenberger Matthias	University of Zurich	2017-08-01	2020-07-31	Radiomics as biomarker in multi-modality treatment of locally advanced non-small cell lung cancer	Radiomics is defined as the use of quantitative image analysis algorithms for calculation of a comprehensive set of image phenotypes also called image biomarkers. In contrast to conventional radiological image analysis, this methodology is a) objective and observer-independent and b) allows for a comprehensive description of all available information within medical images. Radiomics is therefore considered as a potential addition to the efforts currently undertaken in the field of precision medicine: to achieve a comprehensive analysis of the cancer phenotype in order to adjust the treatment to the patient-individual cancer characteristics.The aim of this project is to establish and to use comprehensive radiomics analyses in CT and FDG-PET images for outcome modelling. Prior to all image analyses, the radiomics algorithms will be evaluated regarding their robustness against non-standardized image acquisition and image reconstruction parameters. In addition, a model for quantification of loco-regional tumor spread will be established and its prognostic and predictive value will be compared to the conventional staging system. Machine learning algorithms will be established to correlate the large amount of radiomics biomarkers with clinical outcome parameters. The radiomics methodology will be tested based on images acquired within the randomized SAKK 16-00 study of multi-modality treatment for locally advanced non-small cell lung cancer (NSCLC). Radiomics biomarkers of pre-treatment CT and FDG-PET images, or post induction therapy images and changes of radiomics parameters between these two time points will be analysed. Radiomics parameters will be correlated with available histo-pathological tumor characteristics, primary (event-free survival) and secondary (overall survival, response to induction therapy, failure pattern, pulmonary toxicity) study endpoints.It is the hypothesis, that radiomics analyses will enable us to build better and more accurate prognostic models than the ones currently based on the TNM system as well as predictive models for identification of patients, who might benefit from neoadjuvant radiochemotherapy instead of neoadjuvant chemotherapy, only.The radiomics methodology and the machine learning algorithms will be evaluated in cancer patients suffering from NSCLC, a disease with currently very poor prognosis. It is planned to evaluate radiomics and the machine learning algorithms in other cancer sites as well. Additionally, the application of radiomics and the machine learning algorithms are not restricted to Oncology but can maybe successfully used in other medical questions.	Swiss National Science Foundation	Project funding (Div. I-III)	427191.0CHF
354	Dr. PETUTSCHNIGG Alexander	University of Applied Sciences Salzburg	2018-02-01	2021-01-31	TreeTrace - Biometric fingerprints of trees: log tracing from forest to sawmill and early esti	With the increasing amount of imaging devices installed at sawmills, the importance of using these data for improving workflow and for increasing revenues in the wood processing industries is growing. In this context, challenging questions with respect to imaging and image processing technology arise, several of which will be tackled in this joint project. The project considers two application cases as follows: The first application case is the question of tracing tree logs from the forest harvesting site to the sawmill by using biometrics related tree log recognition techniques based on image processing of cross-section data only. This approach of course assumes the additional availability of imaging sensors in the forest. Since there is a trend for installing CT imaging devices at sawmills, which are of course not available in the forest, the challenging issue of cross modality matching arises. The second application case is the determination of wood quality from cross-section imagery, applicable already in the forest, and/or at the sawmill. Obviously, these two application cases share many aspects. (1) They can be combined at application level, i.e. wood quality may be determined already in the forest due to imaging devices available for the tracing application, and further refined using the sensors available at the sawmill. Conversely, CT data from the sawmill, acquired to analyse the wood quality, can be used for the tracing application; (2) data preprocessing and many features extracted are required for both, matching cross section images as well as automated wood quality analysis; (3) the questions which imaging sensors should be employed and how the resulting data can be combined effectively have to be answered. Thus, synergies arise between these two application cases which will be efficiently exploited in the project. A common data set for experimental validation can be used (which implies also sharing employed sensors), ground truth data established wrt. annotating images can be shared, many software components implementing preprocessing (e.g. pith detection, cross section texture segmentation, contrast optimisation) as well as feature extraction techniques (e.g. annual ring detection, spiral growth detection) can be developed jointly and shared subsequently. The project will break new grounds in the area of wood imaging and processing of corresponding data with advanced algorithms in vision and machine learning with particular focus on cross modality processing. While those techniques are being developed for two specific application cases, the developed algorithms will be applicable to a wide range of applications in wood imagery processing and analysis as well as for other domains where similar settings arise.	Austrian Science Fund FWF	Project funding (Div. I-III)	328573.89EUR
355	Professor Chakrabarti Bhismadev	University of Reading	2019-08-01	2024-07-31	Scalable TRansdiagnostic Early Assessment of Mental Health (STREAM)	With the increasing amount of imaging devices installed at sawmills, the importance of using these data for improving workflow and for increasing revenues in the wood processing industries is growing. In this context, challenging questions with respect to imaging and image processing technology arise, several of which will be tackled in this joint project. The project considers two application cases as follows: The first application case is the question of tracing tree logs from the forest harvesting site to the sawmill by using biometrics related tree log recognition techniques based on image processing of cross-section data only. This approach of course assumes the additional availability of imaging sensors in the forest. Since there is a trend for installing CT imaging devices at sawmills, which are of course not available in the forest, the challenging issue of cross modality matching arises. The second application case is the determination of wood quality from cross-section imagery, applicable already in the forest, and/or at the sawmill. Obviously, these two application cases share many aspects. (1) They can be combined at application level, i.e. wood quality may be determined already in the forest due to imaging devices available for the tracing application, and further refined using the sensors available at the sawmill. Conversely, CT data from the sawmill, acquired to analyse the wood quality, can be used for the tracing application; (2) data preprocessing and many features extracted are required for both, matching cross section images as well as automated wood quality analysis; (3) the questions which imaging sensors should be employed and how the resulting data can be combined effectively have to be answered. Thus, synergies arise between these two application cases which will be efficiently exploited in the project. A common data set for experimental validation can be used (which implies also sharing employed sensors), ground truth data established wrt. annotating images can be shared, many software components implementing preprocessing (e.g. pith detection, cross section texture segmentation, contrast optimisation) as well as feature extraction techniques (e.g. annual ring detection, spiral growth detection) can be developed jointly and shared subsequently. The project will break new grounds in the area of wood imaging and processing of corresponding data with advanced algorithms in vision and machine learning with particular focus on cross modality processing. While those techniques are being developed for two specific application cases, the developed algorithms will be applicable to a wide range of applications in wood imagery processing and analysis as well as for other domains where similar settings arise.	Medical Research Council	Research Grant	3743775.0GBP
356	Dr Patel Rashmi	King's College London	2016-10-03	2018-10-02	Symptom dimensions in first episode psychosis: predicting|clinical outcomes using natural language processing	Psychotic disorders, including schizophrenia and bipolar disorder, can cause considerable distress to affected individuals and their families. One of the key challenges faced by clinicians is that it is not currently possible to predict clinical outcomes following a first episode of psychosis at an individual patient level. This may reflect the fact that existing diagnostic classification systems do not account for the variation in the symptoms experienced by different individuals and also because of variations in genetic and environmental risk factors for psychosis between different people. Many healthcare providers now record clinical information in the form of electronic health records (EHRs). Natural language processing (NLP) is an automated information extraction technique which allows clinical data to be quickly extracted from the EHRs of large numbers of patients. In this study, I will use NLP techniques to identify the presence of psychotic symptoms in over 2,000 people with FEP and use sophisticated machine learning techniques to develop clinical prediction algorithms to determine risk of future hospitalisation and likelihood of antipsychotic treatment failure. I will evaluate the accuracy of these algorithms to make predictions with the aim of subsequently translating them into clinical practice to support clinical decision making at an individual patient level. At present, there is no way to predict which treatments will work best for individual patients with psychotic disorders. I hope that this research study may make it possible to better tailor individual treatment strategies in order to improve clinical outcomes of people with psychotic disorders.	The Academy of Medical Sciences	Starter Grant for Clinical Lecturers	30000.0GBP
357	Dr Patel Rashmi	King's College London	2018-02-14	2021-02-13	Linking electronic health records with passive smartphone activity data to predict outcomes in psychotic disorders	Psychotic disorders, including schizophrenia and bipolar disorder, can cause considerable distress to affected individuals and their families. One of the key challenges faced by clinicians is that it is not currently possible to predict clinical outcomes following a first episode of psychosis at an individual patient level. This may reflect the fact that existing diagnostic classification systems do not account for the variation in the symptoms experienced by different individuals and also because of variations in genetic and environmental risk factors for psychosis between different people. Many healthcare providers now record clinical information in the form of electronic health records (EHRs). Natural language processing (NLP) is an automated information extraction technique which allows clinical data to be quickly extracted from the EHRs of large numbers of patients. In this study, I will use NLP techniques to identify the presence of psychotic symptoms in over 2,000 people with FEP and use sophisticated machine learning techniques to develop clinical prediction algorithms to determine risk of future hospitalisation and likelihood of antipsychotic treatment failure. I will evaluate the accuracy of these algorithms to make predictions with the aim of subsequently translating them into clinical practice to support clinical decision making at an individual patient level. At present, there is no way to predict which treatments will work best for individual patients with psychotic disorders. I hope that this research study may make it possible to better tailor individual treatment strategies in order to improve clinical outcomes of people with psychotic disorders.	Medical Research Council	Fellowship	326857.0GBP
358	Dr Patel Rashmi	Association Pour La Recherche Et Le Développement Des Méthodes Et Processus Industriels (Armines)	2012-02-01	2017-02-01	Statistical machine learning for complex biological data	This interdisciplinary project aims to develop new statistical and machine learning approaches to analyze high-dimensional, structured and heterogeneous biological data. We focus on the cases where a relatively small number of samples are characterized by huge quantities of quantitative features, a common situation in large-scale genomic projects, but particularly challenging for statistical inference. In order to overcome the curse of dimension we propose to exploit the particular structures of the data, and encode prior biological knowledge in a unified, mathematically sound, and computationally efficient framework. These methodological development, both theoretical and practical, will be guided by and applied to the inference of predictive models and the detection of predictive factors for prognosis and drug response prediction in cancer.	European Research Council	Starting Grant	1496004.0EUR
359	Dr Patel Rashmi	ETH Zurich	2012-10-01	2016-03-31	Ribosome synthesis in mammalian cells	Ribosomes are large molecular machines that drive biosynthesis of proteins in all kingdoms of life. The biogenesis of ribosomes is an extremely complex pathway that involves many accessory non-ribosomal proteins and small RNAs. These factors collectively promote rRNA maturation and ribosomal protein deposition. Ribosome biogenesis (RB) is a highly energy demanding process that is stimulated in response to nutrients and growth factors, and diminished by cellular stress. Deregulation of ribosome synthesis is of medical importance, as it leads to various syndromes and is linked to tumorigenesis. Most knowledge on eukaryotic RB comes from studies in yeast, while comparatively little is known in mammalian cells. Here, we propose a comprehensive, interdisciplinary approach at the interfaces of biochemistry, cell biology, systems biology and bioinformatics to fill these gaps. To identify the repertoire of factors involved in human ribosome biogenesis, we will perform genome-wide RNAi-screens on 40S and 60S biogenesis. Our RNAi screening assay is imaging-based and relies on a panel of cell lines carrying fluorescent reporter proteins. Evaluation of the high content screening data will involve assisted machine learning techniques for image analysis as well as bioinformatics tools for prediction of off-target effects and identification of protein networks. In parallel, we will characterize assembly intermediates of human ribosomal subunits on the proteomic and RNA level. We will exploit the screening and proteomics data to study the molecular function of selected trans-acting factors in ribosome assembly, export and maturation. Further, we will investigate how signaling pathways and transcription factors cooperate in the control the synthesis of ribosomal proteins and rRNA. Our goal is to expand the understanding of the mechanisms that govern the formation of a fundamental macromolecular complex in human cells.	Swiss National Science Foundation	Project funding (Div. I-III)	772500.0CHF
360	Dr Patel Rashmi	Brain Imaging and EEG Laboratory San Francisco VA Medical Center University of California, San Francisco	2016-03-01	2017-08-31	Neural Oscillations as Predictors of Psychosis	Aims:This study will attempt to predict the transition to psychosis on a landmark sample whose size is unmatched by any other study in the world. The study design of the North American Prodrome Longitudinal Study 2 (NAPLS-2) allows to investigate brain activity in clinical high risk (CHR) individuals longitudinally (repeatedly over time) and observe whether the anomalies identified at baseline worsen over time thereby leading to psychosis. Owing to the rich and varied set of experimental paradigms assessed in NAPLS 2, the proposed project will allow identifying neurophysiological paradigms that are most sensitive to conversion to psychosis. These paradigms could subsequently be selectively implemented in early detection clinics in Switzerland.Methodology:We will assess 242 Healthy Controls (HC), 199 CHR individuals who did not convert to psychosis (CHR-NP) and 72 CHR individuals who converted to psychosis (CHR-P) on three different EEG paradigms collected as part of NAPLS-2. NAPLS-2 is a consortium of eight universities (including prestigious universities such as Harvard, Yale and UCLA) with the largest CHR sample in the field and allows for advanced analyses and statistical power that cannot be performed in single-site studies. In particular, we will assess neural oscillations during the (1) visual and (2) auditory oddball paradigms, along with the (3) mismatch negativity response. Analyses will include: assessing neural oscillations using time-frequency analyses, phase locking value, and lagged phase synchronization across frontal and temporal regions. We will also make use of advanced machine learning algorithms (artificial intelligence) to identify multivariate patterns of brain activity predictive of transition to psychosis.Hypotheses:•Compared to both CHR-NP and HC, CHR-P individuals will demonstrate altered theta activity during both the P3a and P3b response during context-updating processes elicited by oddball target and novel stimuli.•During the P3a and P3b response, CHR-P individuals will have reduced frontal-temporal phase synchronization of theta neural oscillations compared to both CHR-NP and HC due to lower grey matter volume in both of these brain regions.•The auditory oddball paradigm will yield markers more predictive of conversion to psychosis than the visual oddball paradigm.•Compared to both CHR-NP and HC, CHR-P individuals will demonstrate lower early theta activity during the MMN response that will be associated with lower MMN amplitude.•Following the deviant tone, CHR-P individuals will have reduced frontal-temporal phase synchronization of theta neural oscillations compared to both CHR-P and HC due to lower grey matter volume in both of these brain regions.•In CHR-P and CHR-NP individuals, aberrant frontal-temporal phase-synchronization and lower theta oscillations following deviant stimuli in the MMN paradigm will be associated with psychotic symptoms and neuropsychological deficits.•Variation in brain structural (acquired via MRI) and functional data (acquired via EEG) among CHR individuals will be predictive of the individualized transition to psychosis.Expected value of the project:Currently in Switzerland, about 32,000 people are affected by schizophrenia and the average cost per patient has been estimated to be about EUR 39,000 in the year 2012 alone. Therefore, the early detection of psychosis potentially leading to enhanced treatment approaches will not only benefit the national economy in the long run but also help the patients and their families to get ready for a possible transition to psychosis. An early detection could allow for an early intervention, that is, the patients could undergo mild treatments such as a psychological intervention or administration of low-dosage antipsychotic medication. This study is unique as it is the first to investigate, repeatedly over time, a rich and varied set of experimental paradigms on the largest sample of CHR individuals worldwide. The main findings could help clarify whether brain abnormalities identified at baseline worsen overtime and allow for the individualized prediction of transition to psychosis.	Swiss National Science Foundation	Early Postdoc.Mobility	772500.0CHF
361	Dr Patel Rashmi	University of Zurich	2012-01-01	2015-07-31	Developing Bayesian Networks as a tool for Zoonotic Systems Epidemiology	A primary objective of many zoonotic epidemiological studies is to investigate hypothesized relationships between covariates of interest, and one or more outcome variables, through analyses of appropriate data. Typically, the biological, epidemiology and behavioural processes which generated this data are highly complex, resulting in multiple correlations/dependencies between covariates and also between outcome variables. Standard epidemiological and statistical approaches cannot adequately describe such inter-dependent multi-factorial relationships. Bayesian Network (BN) modelling is a generic and well established data mining/machine learning methodology, which has been demonstrated in other fields of study to be ideally suited to such analyses. The accessibility, however, of this methodology to epidemiologists is severely limited due to the sheer breadth and diversity of zoonotic epidemiological data, which is outside the established application areas of BN modelling. Two key challenges exist, one technical and one epidemiological. Firstly, no appropriate software exists for fitting the types of BN models necessary for analysing zoonotic epidemiological data, where complexities such as grouped/overdispersed/correlated observations are ubiquitous. This project will develop easy-to-use software to allow ready access to BN modelling to epidemiological practitioners, which is essential in order to make the crucial transition from merely a technically attractive methodology, to an approach which is actually used in practice. Secondly, to demonstrate and promote the use of this methodology in zoonotic epidemiology, relevant and high quality exemplar case studies will be developed showing objectively, situations in which BN models can offer the most added value, relative to existing standard statistical and epidemiological methods. Through the project's collaborators a diverse range of zoonotic data are available for analyses, including cross-sectional and longitudinal studies of antimicrobial resistance in Escherichia coli in farmed pigs in Canada, along with a wide range of different parasite field studies including Echinococcus multilocularis, Taenia, Mesocestoides, Uncinaria and Toxocara collected across eastern Europe and central Asia. This project will deliver a range of peer-reviewed publications comprising both methods orientated papers, and pathogen focused studies. The final deliverable of the project will be to provide a valuable addition to the quantitative skills base within veterinary science, by providing PhD level training in applied computational epidemiology of direct relevance to both zoonotic and animal disease research.	Swiss National Science Foundation	Project funding (Div. I-III)	193569.0CHF
362	Dr Patel Rashmi	University of Zurich	2017-09-01	2021-08-31	Predicting outcome after stroke: take a look at the other side	Despite improvements in primary prophylaxis and acute recanalization treatments, stroke remains one of the leading causes of death and disability worldwide. In order to achieve the best outcome possible for the individual patient, therapies have to be administered rapidly. However, the longer the time from symptom onset, the lower the efficacy and the higher the risk of treatment side effects. Although brain imaging is the mainstay of acute stroke diagnostics, current imaging strategies that aim to predict therapeutic success or failure in acute stroke patients remain insufficient. Here, we propose a novel prediction approach that is based on immediate and long-term vascular adaptations affecting the contralateral side of stroke. From data obtained through animal models of stroke and imaging in patients with proximal vessel occlusions, contralateral cerebral blood flow (CBF) appears to be particularly suited to predict clinical benefit from recanalization therapies. Contralateral CBF and related perfusion parameters may indicate the ability of the individual to withstand longer durations of ischemia. In the experimental part of the project, we will use a thrombin-injection stroke model that does not artificially impact collateral supply. Reperfusion will be achieved through intravenous injection of recombinant tissue plasminogen activator (rtPA). To assess CBF and ischemic tissue damage, repeated magnetic resonance imaging (MRI) combined with positron emission tomography (PET) will be performed during ischemia, after reperfusion and in the chronic phase of stroke along with behavioral assessments. In the same stroke model, processes influencing CBF on the microvascular level will be directly observed using advanced optical imaging methods. Structural adaptations of the microvascular bed as well as gene and protein expression profiles contralateral to stroke will be analyzed in brain samples. In the clinical part of the project, we will analyze stroke patient imaging data in relation to clinical outcome. In addition to a traditional region-of interest (ROI) based analysis we will apply a novel, “unbiased” machine learning algorithm to extract relevant outcome predictors from patient imaging data.Using a translational approach we aim for a mechanistic characterization of the concept of contralateral flow changes in stroke, and will generate and apply imaging predictors to clinical patient data. With the multidisciplinary study proposed here, I envision i) to deepen the understanding of the basic mechanisms regulating brain perfusion after ischemic stroke, and ii) to improve therapeutic decision-making in acute stroke patients.	Swiss National Science Foundation	SNSF Professorships	1509881.0CHF
363	Dr Clarke Emily	University of Leeds	2019-11-01	2023-07-31	Enhanced phenotyping of melanoma whole slide images with artificial intelligence	Despite improvements in primary prophylaxis and acute recanalization treatments, stroke remains one of the leading causes of death and disability worldwide. In order to achieve the best outcome possible for the individual patient, therapies have to be administered rapidly. However, the longer the time from symptom onset, the lower the efficacy and the higher the risk of treatment side effects. Although brain imaging is the mainstay of acute stroke diagnostics, current imaging strategies that aim to predict therapeutic success or failure in acute stroke patients remain insufficient. Here, we propose a novel prediction approach that is based on immediate and long-term vascular adaptations affecting the contralateral side of stroke. From data obtained through animal models of stroke and imaging in patients with proximal vessel occlusions, contralateral cerebral blood flow (CBF) appears to be particularly suited to predict clinical benefit from recanalization therapies. Contralateral CBF and related perfusion parameters may indicate the ability of the individual to withstand longer durations of ischemia. In the experimental part of the project, we will use a thrombin-injection stroke model that does not artificially impact collateral supply. Reperfusion will be achieved through intravenous injection of recombinant tissue plasminogen activator (rtPA). To assess CBF and ischemic tissue damage, repeated magnetic resonance imaging (MRI) combined with positron emission tomography (PET) will be performed during ischemia, after reperfusion and in the chronic phase of stroke along with behavioral assessments. In the same stroke model, processes influencing CBF on the microvascular level will be directly observed using advanced optical imaging methods. Structural adaptations of the microvascular bed as well as gene and protein expression profiles contralateral to stroke will be analyzed in brain samples. In the clinical part of the project, we will analyze stroke patient imaging data in relation to clinical outcome. In addition to a traditional region-of interest (ROI) based analysis we will apply a novel, “unbiased” machine learning algorithm to extract relevant outcome predictors from patient imaging data.Using a translational approach we aim for a mechanistic characterization of the concept of contralateral flow changes in stroke, and will generate and apply imaging predictors to clinical patient data. With the multidisciplinary study proposed here, I envision i) to deepen the understanding of the basic mechanisms regulating brain perfusion after ischemic stroke, and ii) to improve therapeutic decision-making in acute stroke patients.	Medical Research Council	Fellowship	245899.0GBP
364	Dr Zeldin Oliver	Diamond Light Source Ltd	2015-02-01	2019-01-31	Serial crystallographic studies of radiation sensitive macromolecules.	This proposal outlines a graph-based approach to dealing with the heterogeneous datasets present in serial femtosecond crystallography (SFX). By describing the hundreds to tens of thousands of images in an SFX dataset as the vertices of a graph, and the agreement between pairs of images that share reflections as the edge weights (with no edge between images that have few or zero Miller indices in common), it is possible to refine partialities and merge observations without first averaging all th e images. This opens up exciting new opportunities for sub-population clustering, outlier rejection, and time-resolved methods. Preliminary work where a graph has been created, and the individual image parameters optimized through a graph synchronization procedure has demonstrated the promise of this approach. Many powerful algorithms and machine learning techniques exist for analyzing graphs, and these will be leveraged through close collaboration with members of the Stanford Computer Science D epartment. Biological relevance of the methodology will be kept as a priority throughout by being initially hosted by Dr. Axel Brunger (Molecular and Cellular Physiology), and through collaboration with biology groups and my sponsor, Dr. Dave Stuart.	Wellcome Trust	Sir Henry Wellcome Postdoctoral Fellowship	250000.0GBP
365	Dr Zeldin Oliver	Institute for Prediction Technology (UCIPT) Department of Family Medicine University of California	2017-04-01	2017-09-30	Predictive Modeling of cART Medication Adherence and Immuno-virologic outcomes among HIV infected Adults in Lausanne, Switzerland	Advancements in human immunodeficiency tritherapy have allowed patients on treatment to achieve a life expectancy similar to those without the disease. To achieve therapeutic goals, it is important to adhere to treatment regimen. The monitoring of adherence can be done using several methods such as patient reports, pill count and electronic monitoring. Electronic monitoring allows daily monitoring of medication taking through the use of electronic pill boxes (Medication event monitoring systems, MEMS) that store the time and day of opening. This method is used routinely in care at the pharmacy of the Policlinique Medicale Universitaire since 2004. Currently there is medication adherence data for about 500 patients followed at the adherence programme. Few studies have described the erosion of medication adherence over time, but more research is needed to identify the early critical indicators of initial nonadherence signs in HIV. To be able to investigate the effect of medication nonadherence on immune-virologic outcomes, we would like to conduct a predictive modeling analysis of the adherence and clinical data collected retrospectively at our institute in Lausanne. The medication adherence data in Lausanne is very unique as it comprises daily medication intake and interview transcripts with patients about the barriers and facilitators to adherence collected routinely in care of a long period of time (12 years). This predictive modeling will be done using state of the art prediction algorithms combining Big Data Science, statistical modeling, behavioral psychology, clinical and adherence sciences. The the University of California Institute for Prediction Technology (UCIPT) has developed a lot of novel methods for big data science analysis and this joint project will combine unique patient data from the adherence clinic in Lausanne with state of the art predictive machine learning methodology at UCIPT.The analysis of this data can provide unprecedented insights into patient behavior regarding dealing with chronic medication in general, and their struggle to deal with HIV. Consequently, tailored interventions can be developed to prevent nonadherence that can lead to viral failure, and mortality.	Swiss National Science Foundation	Doc.Mobility	250000.0GBP
366	Dr Oldroyd Alexander	University of Manchester	2018-08-20	2020-08-19	The Development of a Continuous and Remote Disease Activity Monitoring System in Inflammatory Myopathies to Improve Management and Treatment	Purpose The idiopathic inflammatory myopathies (IIMs) are a group of diseases characterised by muscle inflammation. Clinical decision making is limited by the inability to remotely and continuously measure disease activity. Previous studies have developed smartphone-based apps for patient reported outcome measurement (PROM) entry ("PROM apps"). The muscles of the thighs that allow hip flexion are particularly affected in the IIMs, resulting in predictable changes of gait. Accelerometers incorporated into user-friendly devices can remotely measure gait patterns. Changes of PROMs and gait patterns follow IIM disease activity. Aim This study aims to develop, test and evaluate a novel system that can continuously and remotely measure a patient’s IIM disease activity through daily submission of PROMs and continuous assessment of gait pattern. Study design Stage 1: An IIM patient participant group will inform changes to the initial design and capability of an already designed PROM app template and the proposed use of an accelerometer "patch" system. Stage 2: The system will be trialled in 30-40 participants with an IIM over a 90 day period. Each day participants will be requested to complete a panel of PROMs through the developed PROM app. Participants will also continuously wear an accelerometer patch on the lateral aspect of their thigh for continuous collection of gait data. Stage 3: Analysis will employ machine learning techniques to identify PROM and gait variables associated with changes in disease activity. A novel “disease activity score” algorithm will be developed, which will allow IIM disease activity to be measured (through daily PROM submission and gait pattern analysis) for clinical purposes. Stage 4: The clinical utility of the novel system will be explored through focus groups with IIM-specialist clinicians. Application Patient care will be improved through the ability to remotely identify worsening IIM disease activity, thus preventing irreversible muscle damage and disability.	Versus Arthritis	Clinical Research Fellowship	134818.2GBP
367	Dr Howard David	University of Edinburgh	2019-01-03	2023-01-02	Using multiple data sources to stratify depression and identify more targeted drug treatments	Purpose The idiopathic inflammatory myopathies (IIMs) are a group of diseases characterised by muscle inflammation. Clinical decision making is limited by the inability to remotely and continuously measure disease activity. Previous studies have developed smartphone-based apps for patient reported outcome measurement (PROM) entry ("PROM apps"). The muscles of the thighs that allow hip flexion are particularly affected in the IIMs, resulting in predictable changes of gait. Accelerometers incorporated into user-friendly devices can remotely measure gait patterns. Changes of PROMs and gait patterns follow IIM disease activity. Aim This study aims to develop, test and evaluate a novel system that can continuously and remotely measure a patient’s IIM disease activity through daily submission of PROMs and continuous assessment of gait pattern. Study design Stage 1: An IIM patient participant group will inform changes to the initial design and capability of an already designed PROM app template and the proposed use of an accelerometer "patch" system. Stage 2: The system will be trialled in 30-40 participants with an IIM over a 90 day period. Each day participants will be requested to complete a panel of PROMs through the developed PROM app. Participants will also continuously wear an accelerometer patch on the lateral aspect of their thigh for continuous collection of gait data. Stage 3: Analysis will employ machine learning techniques to identify PROM and gait variables associated with changes in disease activity. A novel “disease activity score” algorithm will be developed, which will allow IIM disease activity to be measured (through daily PROM submission and gait pattern analysis) for clinical purposes. Stage 4: The clinical utility of the novel system will be explored through focus groups with IIM-specialist clinicians. Application Patient care will be improved through the ability to remotely identify worsening IIM disease activity, thus preventing irreversible muscle damage and disability.	Wellcome Trust	Sir Henry Wellcome Postdoctoral Fellowship	300000.0GBP
368	Prof Miho Enkelejda	FHNW University of Applied Sciences and Arts Northwestern	2019-10-01	2022-01-01	Discovery of anti-DENV Antibodies Using Artificial Intelligence	Purpose The idiopathic inflammatory myopathies (IIMs) are a group of diseases characterised by muscle inflammation. Clinical decision making is limited by the inability to remotely and continuously measure disease activity. Previous studies have developed smartphone-based apps for patient reported outcome measurement (PROM) entry ("PROM apps"). The muscles of the thighs that allow hip flexion are particularly affected in the IIMs, resulting in predictable changes of gait. Accelerometers incorporated into user-friendly devices can remotely measure gait patterns. Changes of PROMs and gait patterns follow IIM disease activity. Aim This study aims to develop, test and evaluate a novel system that can continuously and remotely measure a patient’s IIM disease activity through daily submission of PROMs and continuous assessment of gait pattern. Study design Stage 1: An IIM patient participant group will inform changes to the initial design and capability of an already designed PROM app template and the proposed use of an accelerometer "patch" system. Stage 2: The system will be trialled in 30-40 participants with an IIM over a 90 day period. Each day participants will be requested to complete a panel of PROMs through the developed PROM app. Participants will also continuously wear an accelerometer patch on the lateral aspect of their thigh for continuous collection of gait data. Stage 3: Analysis will employ machine learning techniques to identify PROM and gait variables associated with changes in disease activity. A novel “disease activity score” algorithm will be developed, which will allow IIM disease activity to be measured (through daily PROM submission and gait pattern analysis) for clinical purposes. Stage 4: The clinical utility of the novel system will be explored through focus groups with IIM-specialist clinicians. Application Patient care will be improved through the ability to remotely identify worsening IIM disease activity, thus preventing irreversible muscle damage and disability.	Wellcome Trust	Innovator Award: Digital Technologies	658569.8GBP
369	Dr Clarke Jonathan	Imperial College London	2019-10-01	2023-09-30	Care as a Complex System: Understanding the Network Dynamics of Healthcare Delivery	Purpose The idiopathic inflammatory myopathies (IIMs) are a group of diseases characterised by muscle inflammation. Clinical decision making is limited by the inability to remotely and continuously measure disease activity. Previous studies have developed smartphone-based apps for patient reported outcome measurement (PROM) entry ("PROM apps"). The muscles of the thighs that allow hip flexion are particularly affected in the IIMs, resulting in predictable changes of gait. Accelerometers incorporated into user-friendly devices can remotely measure gait patterns. Changes of PROMs and gait patterns follow IIM disease activity. Aim This study aims to develop, test and evaluate a novel system that can continuously and remotely measure a patient’s IIM disease activity through daily submission of PROMs and continuous assessment of gait pattern. Study design Stage 1: An IIM patient participant group will inform changes to the initial design and capability of an already designed PROM app template and the proposed use of an accelerometer "patch" system. Stage 2: The system will be trialled in 30-40 participants with an IIM over a 90 day period. Each day participants will be requested to complete a panel of PROMs through the developed PROM app. Participants will also continuously wear an accelerometer patch on the lateral aspect of their thigh for continuous collection of gait data. Stage 3: Analysis will employ machine learning techniques to identify PROM and gait variables associated with changes in disease activity. A novel “disease activity score” algorithm will be developed, which will allow IIM disease activity to be measured (through daily PROM submission and gait pattern analysis) for clinical purposes. Stage 4: The clinical utility of the novel system will be explored through focus groups with IIM-specialist clinicians. Application Patient care will be improved through the ability to remotely identify worsening IIM disease activity, thus preventing irreversible muscle damage and disability.	Wellcome Trust	Sir Henry Wellcome Postdoctoral Fellowship	300000.0GBP
370	Ao. Prof. Dr. HOLZINGER Andreas	Medical University of Graz	2019-11-04	2022-11-03	A Reference Model of Explainable AI for the Medical Domain	Towards A Reference Model of Explainable AI for the Medical Domain Andreas Holzinger December, 27, 2018 The progress of statistical machine learning methods has made AI increasingly successful. Deep learning exceeds human performance even in the medical domain. However, their full potential is limited by the difficulty to generate the underlying explanatory structures. The central problem is that they are regarded as "black-boxes" and even if we understand the mathematical principles, they lack an explicit declarative knowledge representation. A motivation for this project are rising legal and privacy issues. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make results re-traceable on demand - understandable by a human domain expert. We learned of a variety of technical solutions which are currently in development, which could help explain AI/ML systems and their decisions. Transparent algorithms could appropriately enhance trust of medical professionals, thereby raising acceptance of machine learning. Recently, the Google AI team emphasized the significance of research in explainable AI, the importance of Human-Computer Interaction (HCI) research for Knowledge Discovery from Data (KDD), and the urgent need for a research framework around the field of interpretability. In our ongoing research collaboration "Machine learning for diagnosis of colorectal cancer" with the Google AI group and the Institute of Pathology (Ethics Vote Medical University Graz 30-184 ex17/18) a training data set for AI/ML in digital pathology has been generated. As agreed with our colleagues at the Google AI group, we have now the chance to use machine learning algorithms developed in this context for our test environment; in return we will research towards making their algorithms explainable for our domain experts, who have the benefits of using the results for their medical teaching. This project will focus on but is not limited to digital pathological data and context. The advantage of this project is working with real-world data (under full ethics/data protection regulations) together with medical experts. This project will provide important contributions to the international research community in the following ways: evidence in various methods of explainability and novel methods and urgently needed tools for benchmarking and evaluation; Moreover, the contributions gained in this project can be used generally for reverse-engineering human learning and cognitive development and specifically to engineer more human like machine learning systems. All outcomes of this project will be made openly available to the international research community.	Austrian Science Fund FWF	Stand-Alone Projects	392773.5EUR
371	Dr Howard James	Imperial College London	2017-10-04	2020-10-03	Applications of machine learning in clinical cardiovascular imaging	Echocardiography (echo) remains the primary tool through which we assess the function of patients’ hearts. It is cheap, quick and safe. However, interpretation of the pictures can be very difficult and requires years of experience for doctors to accurately identify whether a scan is normal or abnormal. Recently, people have used computer programs to try and help in analysing medical images, and they have shown promise in the field of MRI scanning the heart. In echocardiography, however, training a computer to look at scans has been significantly more difficult. I believe I can overcome these issues by (1) developing a system which can identify exactly what heart structures are being scanned at any moment, and (2) building up a new library of echo scans from which to train a computer so it can analyse these structures. This technique of using neural networks to analyse medical images has many other applications and I am working with co-supervisors to employ my skills in different areas. These include creating a smart phone application which can identify a pacemaker’s model, speeding up the treatment of patients in emergency situations.	Wellcome Trust	PhD Training Fellowship for Clinicians	392773.5EUR
372	Dr Uhlhaas Peter	University of Glasgow	2014-07-31	2019-02-28	Using Magnetoencephalography to Investigate Aberrant Neural Synchrony in Prodromal Schizophrenia: A Translational Biomarker Approach	Echocardiography (echo) remains the primary tool through which we assess the function of patients’ hearts. It is cheap, quick and safe. However, interpretation of the pictures can be very difficult and requires years of experience for doctors to accurately identify whether a scan is normal or abnormal. Recently, people have used computer programs to try and help in analysing medical images, and they have shown promise in the field of MRI scanning the heart. In echocardiography, however, training a computer to look at scans has been significantly more difficult. I believe I can overcome these issues by (1) developing a system which can identify exactly what heart structures are being scanned at any moment, and (2) building up a new library of echo scans from which to train a computer so it can analyse these structures. This technique of using neural networks to analyse medical images has many other applications and I am working with co-supervisors to employ my skills in different areas. These include creating a smart phone application which can identify a pacemaker’s model, speeding up the treatment of patients in emergency situations.	Medical Research Council	Research Grant	816353.0GBP
373	Dr Cawthorn William	University of Edinburgh	2019-06-30	2022-06-29	Population-level imaging, genomic and phenotypic analyses to determine how bone marrow adiposity impacts human health	Echocardiography (echo) remains the primary tool through which we assess the function of patients’ hearts. It is cheap, quick and safe. However, interpretation of the pictures can be very difficult and requires years of experience for doctors to accurately identify whether a scan is normal or abnormal. Recently, people have used computer programs to try and help in analysing medical images, and they have shown promise in the field of MRI scanning the heart. In echocardiography, however, training a computer to look at scans has been significantly more difficult. I believe I can overcome these issues by (1) developing a system which can identify exactly what heart structures are being scanned at any moment, and (2) building up a new library of echo scans from which to train a computer so it can analyse these structures. This technique of using neural networks to analyse medical images has many other applications and I am working with co-supervisors to employ my skills in different areas. These include creating a smart phone application which can identify a pacemaker’s model, speeding up the treatment of patients in emergency situations.	Medical Research Council	Research Grant	550452.0GBP
374	Mr Mwanga Emmanuel	Ifakara Health Institute	2019-07-01	2022-01-01	Using machine-learning and mid-infrared spectroscopy for rapid assessment of blood-feeding histories and parasite infection rates in field-collected malaria mosquitoes	Echocardiography (echo) remains the primary tool through which we assess the function of patients’ hearts. It is cheap, quick and safe. However, interpretation of the pictures can be very difficult and requires years of experience for doctors to accurately identify whether a scan is normal or abnormal. Recently, people have used computer programs to try and help in analysing medical images, and they have shown promise in the field of MRI scanning the heart. In echocardiography, however, training a computer to look at scans has been significantly more difficult. I believe I can overcome these issues by (1) developing a system which can identify exactly what heart structures are being scanned at any moment, and (2) building up a new library of echo scans from which to train a computer so it can analyse these structures. This technique of using neural networks to analyse medical images has many other applications and I am working with co-supervisors to employ my skills in different areas. These include creating a smart phone application which can identify a pacemaker’s model, speeding up the treatment of patients in emergency situations.	Wellcome Trust	International Masters Fellowship	120000.0GBP
375	Dr Secrier Maria	University College London	2019-06-17	2021-06-17	Quantitative evaluation of gene dosage effects in the Ras/ERK and PI3K/mTOR pathways on metastatic transformation of oesophageal cancer	None	Wellcome Trust	Seed Award in Science	99983.0GBP
376	Dr Secrier Maria	U-FLOOR TECHNOLOGIES LTD	2019-06-17	2021-06-17	OptimisAir - Air quality control combined with behaviourial science	Occupants' daily activities make up 35% of the factors leading to indoor air-pollution \[UCL,2018\]. In the face of COVID-19 we are expecting people to spend much more time in their homes which has two serious knock-on effects: 1) occupants experience longer exposure to indoor pollutants, increasing the risk of respiratory illnesses; 2) Poor IAQ is linked to COPD, asthma, stroke & heart diseases -- all of which are underlying health conditions, increasing the risk of severe illness from covid-19, putting further pressure on the NHS. To address this challenge we are now developing OptimisAir: an integrated 'Indoor air quality management system' that helps Registered Social Landlords reducing their maintenance costs through automated airflow control combined with AI-based activity-recognition and nudge-techniques to reduce the root cause of poor indoor air quality. It uses IoT-enabled sensors to monitor indoor pollutants and utilises game-changing technologies (machine-learning, activity-recognition, 'nudge'/behavioural sciences) to tackle an age-old problem: poor indoor air quality, dampness and draughts in homes. The outcome of the project creates a novel, patented system at an extremely competitive cost.	UK Research and Innovation	Seed Award in Science	49703.0GBP
377	Dr Secrier Maria	ELECTRONIC MEDIA SERVICES LIMITED	2019-06-17	2021-06-17	Conquering COVID in Construction – a safe, managed return to site for construction workers	The IHS Markit purchasing managers' index for UK construction dropped to 39.3 last month from 52.6 the previous month, the lowest reading in more than 10 years. The UK construction sector employs 2.4m workers many of whom are self-employed and are unable to benefit from the Treasury's Furlough Scheme. The sector is a significant contributor to the UK's economic activity producing about 6 per cent of the country's total economic output. The vision for the project is an easy to use health check and tracking app that will give the worker and their employer a simple red/green check of their ability to work. This would enable the industry to end the lockdown, re-engage the predominately self-employed workforce and restart economic activity in a significant sector. The red/green advice from the app would be based on: 1\. Daily self-declared monitoring of general health and especially of any symptoms specific to COVID-19\. These declarations will be performed even when the user is self-isolating. 2\. A tracking feature that would record the user's location while at work and who else they have been in contact with (e.g. < 5 metres separation). 3\. Alerting when there are too many people too close together. 4\. Occupancy of any welfare units, so they can self-time their breaks to minimise unnecessary contacts. 5\. Alerting of the user when a co-worker who they have been in contact with is developing symptoms or who has tested positive for COVID-19\. 6\. Machine Learning-based algorithm to track the development of symptoms in the workforce. The app would have a dashboard for the employer showing who is currently on-site, real-time hot spots showing where workers are congregating so they can be instructed to disperse, a list of staff who have reported symptoms or have tested positive, a list of staff who should be self-isolating because they may have come into contact with another infected member of staff, potential return dates for self-isolating staff plus a list of staff who have tested positive for antibodies and may be immune to re-infection. Both China and South Korea have demonstrated the benefit of using tracking and contact tracing to reduce the spread of COVID-19\. Therefore, the proposed system can be part of an industry-led approach that will help the global construction sector to safely return to work.	UK Research and Innovation	Seed Award in Science	49882.0GBP
378	Dr Secrier Maria	DEEP RENDER LTD	2019-06-17	2021-06-17	AI-based Image-As-Video Streaming	Deep Render Ltd is a London based deep-tech start-up developing the next generation of AI-based media compression algorithms. Our proprietary and patented technology is at the forefront of machine learning research. Deep Render is combining the fields of artificial intelligence, statistics and information theory to unlock the fundamental limits of image and video compression: The human eye is the best data compressor known to humanity - with compression ratios at least 2,000 times better than everything developed to date. Our Biological Compression approach approximates the neurological processes of the human eye through a non-linear, learning-based approach, thereby creating a novel class of highly efficient compression algorithms. We are world leaders in this domain, and our unique, AI-based image compression technology is already providing a 75% efficiency gain over the best previous compression standards. As global data consumption is growing exponentially with more than 80% of traffic being Image/Video, Deep Render's AI-based compression technology is vital to bypass broadband constraints. The outbreak of COVID-19 has now accelerated this growth, as a result of the crisis, internet usage has increased significantly. In particular, the demand for live-streaming and video-chat services. Therefore we want to apply our already working image compression codec to live-video streaming. The outcome of the project is to extend our image compression codec to an image-as-video live-streaming codec, at least 75% more efficient than the current state-of-the-art. Our target customers are the live video chat services (Zoom, Skype, Microsoft Teams), as well as the entertainment industry, including live streaming platforms (Twitch, Facebook, Instagram, YouTube). Our value proposition is easy to understand. By making file sizes 75% smaller, we directly increase the bandwidth supply of the internet for live-streaming by a factor of 4\. Increasing the bandwidth supply by making file sizes smaller, is magnitudes more value- and time-efficient than increasing the bandwidth supply through rewiring the globe with progressively more fibre-cables. Deep Render is going to help create a new age in which bandwidth constraints are a problem of the past. As a result of COVID 19 solving this problem has gained more importance and Deep Render is determined to create a fast solution.	UK Research and Innovation	Seed Award in Science	50000.0GBP
379	Dr Secrier Maria	V2G EVSE LIMITED	2019-06-17	2021-06-17	Covid-19 eHealth Data Acquisition Unit (COVeHealth)	The UK is currently in "lock down" due to the novel coronavirus pandemic. Before putting the currently locked down country back to work we need to: 1) Reduce the incidence of cases to reduce the burden on the NHS to more normal levels 2) Once that is achieved, we need to prevent the virus from catching fire a second time To reduce the risk of a second wave, we need to develop tools to enable us to: a) Preemptively identify early stage Covid-19 sufferers b) Use these tools to guard the borders of spaces (hospitals, homes, shops, workplaces, shopping malls, schools, universities etc.) The World Health Organization's message is that we must **"Find, isolate, test and treat every case to break the chains of Covid transmission."** Covid-19 screening tests are currently performed at a modest distance by a health worker using a "no touch" infrared thermometer to detect the tell tale fever. The other common symptom indicative of Covid-19 is the persistent dry cough, detected by the characteristic sound. What is desperately needed, both in the UK and around the World, is a way of performing screening for early stage Covid-19 symptoms remotely. With DfT seed funding V2G EVSE have developed a low cost "smart" 0G to 5G enabled communications controller with a wide range of input/output, currently configured to monitor and control an electric vehicle charging station and securely communicate with a cloud based management system. We will repurpose our existing technology by connecting the controller to a microphone and infra-red camera, then use novel machine learning algorithms to detect the characteristic cough and fever of Covid-19 sufferers. This will allow us to create an installed or hand held device that can be used to identify those with potential Covid-19 symptoms. Since our existing hardware is effectively the internals of a smartphone with no touch screen but more versatile communications, including Bluetooth, we are also ideally placed to participate in the recently announced Apple/Google track/trace initiative. The COVeHealth project will develop proof of concept prototypes of a "commercial" version suitable for use at the entrance to public spaces and an alternative low-cost "domestic" version for use hand held or in confined spaces.	UK Research and Innovation	Seed Award in Science	50000.0GBP
380	Dr Secrier Maria	OKEO LIMITED	2019-06-17	2021-06-17	OKEO - COVID-19 financial modelling	OKEO is a London-based challenger credit card company that uses Open Banking data and machine learning to provide young adults with access to affordable credit. OKEO proposes to develop innovative affordability and credit risk models using current "crisis data" to help OKEO lend money to financially excluded young adults (aged between 20 to 30 years old), self-employed contractors and gig economy workers at low rates during the pandemic. By utilising innovative data modelling and lending to critical sectors of the economy, OKEO's vision is to help meet the needs of financially excluded individuals and help stimulate the UK economy. OKEO is innovating now to respond to the financial needs of individuals brought on by COVID-19 and a weakened economy.	UK Research and Innovation	Seed Award in Science	49815.0GBP
381	Dr Thomas T Hannah Mary	Christian Medical College, Vellore	2020-01-01	2024-12-31	Radiomics based tumor phenotypes to predict individual risk and treatment response in head and neck cancer	OKEO is a London-based challenger credit card company that uses Open Banking data and machine learning to provide young adults with access to affordable credit. OKEO proposes to develop innovative affordability and credit risk models using current "crisis data" to help OKEO lend money to financially excluded young adults (aged between 20 to 30 years old), self-employed contractors and gig economy workers at low rates during the pandemic. By utilising innovative data modelling and lending to critical sectors of the economy, OKEO's vision is to help meet the needs of financially excluded individuals and help stimulate the UK economy. OKEO is innovating now to respond to the financial needs of individuals brought on by COVID-19 and a weakened economy.	DBT/Wellcome Trust India Alliance	Early Career Fellowship	13521386INR
382	Dr Thomas T Hannah Mary	QUALIS FLOW LIMITED	2020-01-01	2024-12-31	Automating compliance for remote construction working	Construction logistics monitoring and compliance is typically paper-based and requires multiple individuals and teams to process, in order to ensure that material movements are handled in a safe and environmentally responsible manner. Qflow is a software tool, using a combination of Internet of Things and machine learning techniques, to digitise the information recovered from paper tickets at a construction site entrance. This ensures that this information is fed directly to the managers who need it most, and without the need for any on-site presence aside from the existing traffic marshals. Information is provided on the material types and waste removals, descriptions, relevant certifications of the supply chain, and listed quantities. This supports several teams to work remotely, including logistics, environmental, QS and data administration. The innovation that Qflow is exploring now is automated compliance screening and anomaly detection, to notify users when there are discrepancies in their data that require investigation, or where there is an illegal waste transfer. The software provides the eyes and ears on site in real-time, and provides access to that information on a cloud-based platform, meaning that those who need the data can access it wherever they are, without having to double handle paperwork and ensuring that the site operations continues smoothly without the need for intervention.	UK Research and Innovation	Early Career Fellowship	27374.0GBP
383	Dr. Clemens Jan	UNIVERSITAETSMEDIZIN GOETTINGEN - GEORG-AUGUST-UNIVERSITAET GOETTINGEN - STIFTUNG OEFFENTLICHEN RECHTS	2020-02-01	2025-01-31	Neural Computations Underlying Social Behavior in Complex Sensory Environments	Animals often interact in groups. Animal groups constitute complex sensory environments which challenge the brain and engage complex neural computations. This behavioral context is therefore fruitful for understanding how sophisticated neural computations give rise to behavior. However, it is also technically difficult since many of the relevant sensory cues arise from the members of the group and are therefore hard to quantify or control. Consequently, we only incompletely understand how the brain drives complex social behaviors in naturalistic contexts. To uncover the neural computations underlying social behavior in groups, we are using Drosophila, which provides unprecedented experimental access to the nervous system via genetic tools. Drosophila gathers on rotten fruit to feed and mate. Courtship and aggression dominate social interactions and rely on the recognition of sex-specific chemical cues and the production of context-specific acoustic signals. How are these multi-modal cues integrated to control and switch between courtship and aggression? How is unstable and conflicting sensory information resolved to promote stable behavioral strategies? How does sensory processing adapt to socially crowded environments in order to efficiently target behavior at individual members of the group? These issues will be addressed by combining computational modeling and genetic tools. Using machine learning, we will quantify and model the fine structure of social interactions to identify the social cues that drive behavior. Closed-loop optogenetics and calcium imaging in behaving animals will allow us to test the models and to ultimately reveal how the brain integrates, selects and combines social cues to drive social interactions. This multi-disciplinary approach will uncover the computational principles and mechanisms by which sensory information is processed to drive behavior in the complex sensory environment of animal groups.	European Research Council	Starting Grant	1476920.0EUR
384	Dr. Ruigrok Ynte	UNIVERSITAIR MEDISCH CENTRUM UTRECHT	2020-02-01	2025-01-31	Early recognition of intracranial aneurysms to PRevent aneurYSMal subarachnoid hemorrhage	Intracranial aneurysms usually go undetected until rupture occurs leading to aneurysmal subarachnoid hemorrhage (ASAH), a type of stroke with devastating effects. Early detection and preventive treatment of aneurysms fall short as we do not know who is it at risk and why, as we have insufficient insight in the contribution and interplay of genetic, environmental and intermediate phenotypic risk factors. Given the rarity of the disease, there is a paucity of large and rich cohorts to study risk factors separately with sufficient power. To add to the problem, my preliminary findings suggest disease heterogeneity with subgroup specific risk factors for aneurysms. The sex-related heterogeneity is most eminent in the disease with 2/3 of patients being women. I aim to advance disease understanding to allow early recognition of intracranial aneurysms to prevent ASAH. I have established a new conceptual approach that integrates genetic and environmental risk factors with imaging markers as intermediate phenotypes for genetic factors. With data reduction and machine-learning approaches I will for the first time address disease heterogeneity and aneurysm risk with adequate power. I will develop and validate a tool to automatically detect new imaging markers predicting aneurysm development applying feature-learning models. Next I will elucidate the genetic basis underlying differential imaging risk patterns (imaging genetic factors). I will apply a new hypothesis-free strategy to detect and validate yet unknown environmental risk factors predicting aneurysm presence. I will assess the contribution to disease of all factors detected according to sex. All risk factors will be combined in an aneurysm prediction risk model to understand relative contribution of different risk factors in different subgroups. It will advance disease understanding and individualized risk prediction of aneurysms leading to precision medicine in early aneurysm detection to reduce the burden of ASAH.	European Research Council	Starting Grant	1499108.0EUR
385	Dr. Philiastides Marios	University of Glasgow	2020-09-01	2025-08-31	Dynamic Network Reconstruction of Human Perceptual and Reward Learning via Multimodal Data Fusion	Training and experience can lead to long-lasting improvements in our ability to make decisions based on either ambiguous sensory or probabilistic information (e.g. learning to diagnose a noisy x-ray image or betting on the stock market). These two processes are referred to as perceptual and probabilistic/reward learning, respectively. Despite considerable efforts to uncover the neural systems involved in these processes, perceptual and reward learning have largely been studied in separate lines of research using divergent learning mechanisms. The primary aim of this proposal is to develop a unified framework for integrating these lines of research and understand the extent to which they share a common computational and neurobiological basis. Specifically, we will test the proposition that both the perceptual and reward systems could be understood in a common framework of “reward maximization”, whereby a domain-general reinforcement-guided learning mechanism – based on separate prediction error representations – facilitates future actions and adaptive behavior. To offer a comprehensive spatiotemporal characterization of the relevant networks and their computational principles we will adopt a state-of-the-art multimodal neuroimaging approach to fuse simultaneously-acquired EEG and fMRI data, via machine-learning-inspired multivariate single-trial analysis techniques and computational modelling. The project’s ultimate goal is to empower a level of neuronal and mechanistic understanding that extends beyond what could be inferred with each of these modalities in isolation. We will achieve this goal by exploiting endogenous trial-by-trial electrophysiological variability to build parametric fMRI predictors that can offer additional explanatory power than what can already be achieved by stimulus- or behaviorally-derived predictors, allowing us to go over and beyond what has been reported previously in the literature.	European Research Council	Consolidator Grant	1996043.0EUR
386	Dr. EEFTENS Marloes	SCHWEIZERISCHES TROPEN- UND PUBLIC HEALTH-INSTITUT	2020-05-01	2025-04-30	Beyond seasonal suffering: Effects of Pollen on Cardiorespiratory Health and Allergies	As climate change increases the duration and intensity of the pollen season, allergies to airborne pollen are increasingly common in Europe. Yet, it is not well recognized that high pollen concentrations may increase respiratory and cardiovascular events, leading to mortality and excess hospitalizations. I aim to investigate how short-term exposure to pollen is related to mortality, hospitalization and allergic symptoms, both on its own and synergistically with air pollution and weather. I will develop spatiotemporal exposure models of pollen for the years 2003-2022 based on a network of 14 pollen measurements stations in Switzerland. Taking advantage of large, real-world datasets without selection bias (Swiss National Cohort) and the efficient case-crossover study design, I will investigate the population effects of pollen on daily respiratory and cardiovascular mortality and hospitalization, also accounting for variation in air pollution and weather conditions. To explore individual sensitivity, I will conduct repeated measurements of lung function and airway inflammation in a dedicated panel of 400 allergic patients complemented with opportunistic repeated accounts of self-reported symptoms from the “e-symptoms” app by Swiss Allergy Centre. To provide personalized prevention recommendations and enhance quality of life for the allergic population, I will derive exposure-response relationships based on prevalent pollen, air pollution and weather triggers and individual symptom reports, allowing me to ultimately forecast symptom severity using machine learning techniques. This highly innovative project utilizes available nationwide health datasets and systematic novel data collection methods (in the in-depth panel study), to better understand the role of pollen in respiratory and cardiovascular diseases at both personalized and population levels. The project will prevent and reduce health effects due to pollen, which constitute a large burden on health and economy.	European Research Council	Starting Grant	1381932.0EUR
387	Dr. Zlatic Marta	THE CHANCELLOR MASTERS AND SCHOLARS OF THE UNIVERSITY OF CAMBRIDGE	2019-09-01	2024-08-31	Principles of Learning in a Recurrent Neural Network	Forming memories, generating predictions based on memories, and updating memories when predictions no longer match actual experience are fundamental brain functions. Dopaminergic neurons provide a so-called “teaching signal” that drives the formation and updates of associative memories across the animal kingdom. Many theoretical models propose how neural circuits could compute the teaching signals, but the actual implementation of this computation in real nervous systems is unknown. This project will discover the basic principles by which neural circuits compute the teaching signals that drive memory formation and updates using a tractable insect model system, the Drosophila larva. We will generate, for the first time in any animal, the following essential datasets for a distributed, multilayered, recurrent learning circuit, the mushroom body-related circuitry in the larval brain. First, building on our preliminary work that provides the synaptic-resolution connectome of the circuit, including all feedforward and feedback pathways upstream of all dopaminergic neurons, we will generate a map of functional monosynaptic connections. Second, we will obtain cellular-resolution whole-nervous system activity maps in intact living animals, as they form, extinguish, or consolidate memories to discover the features represented in each layer of the circuit (e.g. predictions, actual reinforcement, and prediction errors), the learning algorithms, and the candidate circuit motifs that implement them. Finally, we will develop a model of the circuit constrained by these datasets and test the predictions about the necessity and sufficiency of uniquely identified circuit elements for implementing learning algorithms by selectively manipulating their activity. Understanding the basic functional principles of an entire multilayered recurrent learning circuit in an animal has the potential to revolutionize, not only neuroscience and medicine, but also machine-learning and robotics.	European Research Council	Consolidator Grant	2350000.0EUR
388	Dr Guo Ya	MRC London Institute of Medical Sciences	2018-07-01	2020-06-01	NPIF Rutherford Fellowship	Gene regulation is of fundamental biological interest and has immediate medical relevance, as it is essential for normal development, and often disrupted in cancer. Gene regulation takes place in the context of chromatin states and 3D chromatin contacts. This field of study has recently been enriched by the application of machine learning/AI approaches to classify chromatin states and chromatin contacts, and by experimental evidence indicating that concepts of self-organised criticality and phase transitions can be productively applied to transitions between chromatin states and the formation of 3D chromatin contacts. This proposal builds on our recent discovery that cohesin and CTCF are selectively required for the regulation of genes that are dynamically expressed during cellular activation and differentiation. This includes genes programmed for up- or downregulated during mouse T cell differentiation, and genes that respond to inflammatory signals in mouse macrophages. Cohesin mutations are recurrent in human acute myeloid leukaemia (AML), and primary AML cells with cohesin mutations show reduced expression of inducible pro-inflammatory genes. As inflammatory signals promote the differentiation of myeloid cells, our data suggest a mechanism for the selection of cohesin mutations whereby AML cells with reduced expression of inflammatory genes evade differentiation in favour of self-renewal.Focus of this project is to understand the mechanisms that underlie the dependence of developmentally regulated and stimulus-responsive genes on genome organiser proteins. We will use molecular biology and genome informatics tools to assemble comprehensive maps of chromatin state (histone modifications, including numerous ChIP-seq data sets from ENCODE, chromatin accessibility, transcription factor binding, transcription) and chromatin contacts (Hi-C). We imagine that the dynamic regulation of inducible and developmental genes may rely on a spectrum of chromatin states that represent intermediates between full activation and maximal repression. These in-between states may be maintained by specific chromatin features (such as bivalency) and by active formation of chromatin contacts by cohesin and CTCF, which counteracts the stable segregation of active and silent chromatin regions. We will apply machine learning/AI approaches to classify these states and to identify features that predict dependency on genome organiser proteins. The results we will uncover new principles of genome organisation and transcriptional regulation and improve our understanding of cohesin mutations in cancer.	Medical Research Council	Fellowship	132676.0GBP
389	Dr Keevil Victoria	University of Cambridge	2019-10-07	2022-10-06	Big data analysis of electronic hospital records: inpatient trajectories and pharmacological exposures associated with mortality in older adults.	Gene regulation is of fundamental biological interest and has immediate medical relevance, as it is essential for normal development, and often disrupted in cancer. Gene regulation takes place in the context of chromatin states and 3D chromatin contacts. This field of study has recently been enriched by the application of machine learning/AI approaches to classify chromatin states and chromatin contacts, and by experimental evidence indicating that concepts of self-organised criticality and phase transitions can be productively applied to transitions between chromatin states and the formation of 3D chromatin contacts. This proposal builds on our recent discovery that cohesin and CTCF are selectively required for the regulation of genes that are dynamically expressed during cellular activation and differentiation. This includes genes programmed for up- or downregulated during mouse T cell differentiation, and genes that respond to inflammatory signals in mouse macrophages. Cohesin mutations are recurrent in human acute myeloid leukaemia (AML), and primary AML cells with cohesin mutations show reduced expression of inducible pro-inflammatory genes. As inflammatory signals promote the differentiation of myeloid cells, our data suggest a mechanism for the selection of cohesin mutations whereby AML cells with reduced expression of inflammatory genes evade differentiation in favour of self-renewal.Focus of this project is to understand the mechanisms that underlie the dependence of developmentally regulated and stimulus-responsive genes on genome organiser proteins. We will use molecular biology and genome informatics tools to assemble comprehensive maps of chromatin state (histone modifications, including numerous ChIP-seq data sets from ENCODE, chromatin accessibility, transcription factor binding, transcription) and chromatin contacts (Hi-C). We imagine that the dynamic regulation of inducible and developmental genes may rely on a spectrum of chromatin states that represent intermediates between full activation and maximal repression. These in-between states may be maintained by specific chromatin features (such as bivalency) and by active formation of chromatin contacts by cohesin and CTCF, which counteracts the stable segregation of active and silent chromatin regions. We will apply machine learning/AI approaches to classify these states and to identify features that predict dependency on genome organiser proteins. The results we will uncover new principles of genome organisation and transcriptional regulation and improve our understanding of cohesin mutations in cancer.	Medical Research Council	Research Grant	302805.0GBP
390	Ms Townson Julia	Cardiff University	2019-12-01	2021-03-01	Developing a predictive tool to promote earlier diagnosis of Type 1 diabetes in childhood for use in primary care	Aims: To develop and validate a predictive tool to help primary care healthcare professionals identify children at risk of Type 1 Diabetes (T1D), aiding early diagnosis and preventing the morbidity and mortality associated with late presentation. Background: Early diagnosis of childhood T1D is critical to avoid diabetic ketoacidosis (DKA). Delayed diagnosis and misdiagnosis are significant risk factors. Identifying children developing T1D amongst many others with similar symptoms who do not have diabetes, is challenging for GPs. Methods: We will use machine learning to develop a predictive model using a contemporary data set of linked primary care records (SAIL databank) and a secondary care diagnostic database (Brecon Cohort) in Wales. The predictive model will be tested in an independent data set, by linking English primary and secondary care datasets. The performance of the model to promote earlier diagnoses, reduce DKA and false positive diagnoses will be assessed. We will then hold a workshop of key stakeholders to determine the utility of the model and how we may test its effectiveness in future. Conclusion: Potentially, the predictive tool could be used within primary care computer systems to prompt GPs to conduct a finger prick test for T1D.	Diabetes UK	Project Grant	105447.0GBP
391	Dr Leyrat Clemence	London Sch of Hygiene and Trop Medicine	2020-04-01	2023-03-31	Enhancing the design and analysis of cluster randomised trials using machine learning	Aims: To develop and validate a predictive tool to help primary care healthcare professionals identify children at risk of Type 1 Diabetes (T1D), aiding early diagnosis and preventing the morbidity and mortality associated with late presentation. Background: Early diagnosis of childhood T1D is critical to avoid diabetic ketoacidosis (DKA). Delayed diagnosis and misdiagnosis are significant risk factors. Identifying children developing T1D amongst many others with similar symptoms who do not have diabetes, is challenging for GPs. Methods: We will use machine learning to develop a predictive model using a contemporary data set of linked primary care records (SAIL databank) and a secondary care diagnostic database (Brecon Cohort) in Wales. The predictive model will be tested in an independent data set, by linking English primary and secondary care datasets. The performance of the model to promote earlier diagnoses, reduce DKA and false positive diagnoses will be assessed. We will then hold a workshop of key stakeholders to determine the utility of the model and how we may test its effectiveness in future. Conclusion: Potentially, the predictive tool could be used within primary care computer systems to prompt GPs to conduct a finger prick test for T1D.	Medical Research Council	Fellowship	301682.0GBP
392	Dr Reynard Charles	The University of Manchester	2020-04-01	2023-03-31	Optimising diagnostic efficiency in the Emergency Department by using advanced machine learning methods to update and personalise a contemporary clinical prediction model for early identification and exclusion of acute coronary syndromes	Aims: To develop and validate a predictive tool to help primary care healthcare professionals identify children at risk of Type 1 Diabetes (T1D), aiding early diagnosis and preventing the morbidity and mortality associated with late presentation. Background: Early diagnosis of childhood T1D is critical to avoid diabetic ketoacidosis (DKA). Delayed diagnosis and misdiagnosis are significant risk factors. Identifying children developing T1D amongst many others with similar symptoms who do not have diabetes, is challenging for GPs. Methods: We will use machine learning to develop a predictive model using a contemporary data set of linked primary care records (SAIL databank) and a secondary care diagnostic database (Brecon Cohort) in Wales. The predictive model will be tested in an independent data set, by linking English primary and secondary care datasets. The performance of the model to promote earlier diagnoses, reduce DKA and false positive diagnoses will be assessed. We will then hold a workshop of key stakeholders to determine the utility of the model and how we may test its effectiveness in future. Conclusion: Potentially, the predictive tool could be used within primary care computer systems to prompt GPs to conduct a finger prick test for T1D.	National Institute for Health Research (Department of Health)	Fellowship	230920.0GBP
393	Dr Reynard Charles	Georgia State University Research Foundation, Inc.	2020-04-15	2021-03-31	Collaborative Research: RAPID: RTEM: Rapid Testing as Multi-fidelity Data Collection for Epidemic Modeling	Biological Sciences - The novel coronavirus (COVID-19) epidemic is generating significant social, economic, and health impacts and has highlighted the importance of real-time analysis of the spatio-temporal dynamics of emerging infectious diseases. COVID-19, which emerged out of the city of Wuhan in China in December 2019 is now spreading in multiple countries. It is particularly concerning that the case fatality rate appears to be higher for the novel coronavirus than for seasonal influenza, and especially so for older populations and those with prior health conditions such as cardiovascular disease and diabetes. Any plan for stopping the epidemic must be based on a quantitative understanding of the proportion of the at-risk population that needs to be protected by effective control measures in order for transmission to decline sufficiently and quickly enough for the epidemic to end. Different data collection and testing modalities and strategies available to help calibrate transmission models and predict the spread/severity of a disease, have variable costs, response times, and accuracies. In this Rapid Response Research (RAPID) project, the team will examine the problem of establishing optimal practices for rapid testing for the novel coronavirus. The result will be the Rapid Testing for Epidemic Modeling (RTEM), which will translate into science-based predictions of the COVID-19 epidemic's characteristics, including the duration and overall size, and help the global efforts to combat the disease. The RTEM will fill an important gap in data-driven decision making during the COVID-19 epidemic and, thus, will enable services with significant national economic and health impact. The educational impact of the project will be on mentoring of post-doctoral and PhD researchers and on curricula by incorporating research challenges and outcomes into existing undergraduate and graduate classes. <br/><br/>Computational models for the spatio-temporal dynamics of emerging infectious diseases and data- and model-driven computer simulations for disease spreading are increasingly critical in predicting geo-temporal evolution of epidemics as well as designing, activating, and adapting practices for controlling epidemics. In this project, the researchers tackle a Rapid Testing for Epidemic Modeling (RTEM) problem: Given a partially known target disease model and a set of testing modalities (from surveys to surveillance testing at known disease hotspots), with varying costs, accuracies, and observational delays, what is the best rapid testing strategy that would help recover the underlying disease model? Several scientific questions arise: What is the value of testing? Should only sick people be tested for virus detection? What level of resources should be devoted to the development of highly accurate tests (low false positives, low false negatives)? Is it better to use only one type of test aiming at the best cost/effectiveness trade off, or a non-homogeneous testing policy? Naturally these questions need to be investigated at the interface of epidemiology, computer science, machine learning, mathematical modeling and statistics. As part of the work, the team will develop a model of transmission dynamics and control, tailored to COVID-19 in a way that accommodates diagnostic testing with varying fidelities and delays underlying a rapid testing regimen. The investigators will further integrate the resulting RTEM-SEIR model with EpiDMS and DataStorm for executing continuous coupled simulations. <br/><br/>This project is jointly funded through the Ecology and Evolution of Infectious Diseases program (Division of Environmental Biology) and the Civil, Mechanical and Manufacturing innovation program (Engineering).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	67000.0USD
394	Dr Reynard Charles	University of California-San Diego	2020-05-01	2021-04-30	RAPID: Explainable Machine Learning for Analysis of COVID-19 Chest CT	Computer and Information Science and Engineering - In December 2019, it was discovered that a widely contagious pneumonia was caused by a new coronavirus infection now named COVID-19. The primary test for detection of the virus is real-time polymerase chain reaction (RT-PCR) with sensitivity of approximately 71% in some studies. However, this test may require several days to provide a result. Perhaps more importantly, imaging with x-ray or computed tomography (CT) are required to confirm pneumonia, which is the principal cause of death, as it leads to acute respiratory distress syndrome (ARDS). Recent studies have shown sensitivity of chest CT for approximately 98% for COVID-19 pneumonia and could provide immediate results but currently require human interpretation. Given the need for rapid, more accurate diagnosis, this project will use, adapt, and evaluate explainable machine learning techniques to diagnosis of COVID-19 pneumonia. This project will improve the understanding of mechanisms of COVID-19 and will help mitigate its impacts.<br/><br/>Viral nucleic acid detection using real-time polymerase chain reaction (RT-PCR) is the primary method for diagnosis of COVID-19 infection, which has rapidly spread worldwide as a global pandemic. Sensitivity of this test for COVID-19 infection has been estimated at approximately 71% in some studies and may require several days for a result. X-ray and CT imaging are complementary technologies that allow diagnosis of COVID-19 pneumonia, which can evolve to acute respiratory distress syndrome (ARDS) -- the principal cause of death in patients with COVID-19 infection. Especially early in the course of the disease, chest CT has multiple advantages over RT-PCR yielding results more quickly and is already widely deployed, but requires expert radiologist interpretation. The number of chest CTs may rapidly exceed the speed and capacity of already strained radiologists. An explainable machine learning algorithm may address this disadvantage to expedite the interpretation of chest CT and assist rapid triage of patients to the ICU, inpatient ward, monitoring unit, or home self-quarantine. Machine learning algorithms, specifically those leveraging deep convolutional neural networks (deep learning), have the potential for facilitating even more rapid diagnosis within minutes. This project seeks to validate the use of explainable deep learning methods to adjust diagnostic operating points for multiple applications, including (a) disease screening, (b) disease staging and prognostication, and (c) evaluation of treatment response.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	101643.0USD
395	Dr. FLEISHMAN Sarel-Jacob	WEIZMANN INSTITUTE OF SCIENCE LTD	2019-01-01	2023-12-31	Automated computational design of site-targeted repertoires of camelid antibodies	We propose to develop the first high-throughput strategy to design, synthesize, and screen repertoires comprising millions of single-domain camelid antibodies (VHH) that target desired protein surfaces. Each VHH will be individually designed for high stability and target-site affinity. We will leverage recent methods developed by our lab for designing stable, specific, and accurate backbones at interfaces, the advent of massive and affordable custom-DNA oligo synthesis, and machine learning methods to accomplish the following aims: Aim 1: Establish a completely automated computational pipeline that uses Rosetta to design millions of VHHs targeting desired protein surfaces. The variable regions in each design will be encoded in DNA oligo pools, which will be assembled to generate the entire site-targeted repertoire. We will then use high-throughput binding screens followed by deep sequencing to characterize the designs’ target-site affinity and isolate high-affinity binders. Aim 2: Develop an epitope-focusing strategy that designs several variants of a target antigen, each of which encodes dozens of radical surface mutations outside the target site to disrupt potential off-target site binding. The designs will be used to isolate site-targeting binders from repertoires of Aim 1. Each high-throughput screen will provide unprecedented experimental data on target-site affinity in millions of individually designed VHHs. Aim 3: Use machine learning methods to infer combinations of molecular features that distinguish high-affinity binders from non binders. These will be encoded in subsequent designed repertoires, leading to a continuous “learning loop” of methods for high-affinity, site-targeted binding. AutoCAb’s interdisciplinary strategy will thus lead to deeper understanding of and new general methods for designing stable, high-affinity, site-targeted antibodies, potentially revolutionizing binder and inhibitor discovery in basic and applied biomedical research.	European Research Council	Consolidator Grant	2337500.0EUR
396	Professor Murray Alison	University of Aberdeen	2018-03-01	2019-02-28	Early-life origins of brain resilience to mental illness and cognitive impairment across the life-course	We propose to develop the first high-throughput strategy to design, synthesize, and screen repertoires comprising millions of single-domain camelid antibodies (VHH) that target desired protein surfaces. Each VHH will be individually designed for high stability and target-site affinity. We will leverage recent methods developed by our lab for designing stable, specific, and accurate backbones at interfaces, the advent of massive and affordable custom-DNA oligo synthesis, and machine learning methods to accomplish the following aims: Aim 1: Establish a completely automated computational pipeline that uses Rosetta to design millions of VHHs targeting desired protein surfaces. The variable regions in each design will be encoded in DNA oligo pools, which will be assembled to generate the entire site-targeted repertoire. We will then use high-throughput binding screens followed by deep sequencing to characterize the designs’ target-site affinity and isolate high-affinity binders. Aim 2: Develop an epitope-focusing strategy that designs several variants of a target antigen, each of which encodes dozens of radical surface mutations outside the target site to disrupt potential off-target site binding. The designs will be used to isolate site-targeting binders from repertoires of Aim 1. Each high-throughput screen will provide unprecedented experimental data on target-site affinity in millions of individually designed VHHs. Aim 3: Use machine learning methods to infer combinations of molecular features that distinguish high-affinity binders from non binders. These will be encoded in subsequent designed repertoires, leading to a continuous “learning loop” of methods for high-affinity, site-targeted binding. AutoCAb’s interdisciplinary strategy will thus lead to deeper understanding of and new general methods for designing stable, high-affinity, site-targeted antibodies, potentially revolutionizing binder and inhibitor discovery in basic and applied biomedical research.	Medical Research Council	P&Cs	192240.0GBP
397	Professor Murray Alison	University of Chicago	2020-05-01	2021-04-30	RAPID: Data-driven Multiscale Integrative Model of the Coronavirus Virion	Mathematical and Physical Sciences - Gregory Voth of the University of Chicago is supported by this RAPID award to develop and deploy multiscale models of the entire SARS-CoV-2 virus, the virus that causes the novel coronavirus infectious disease 2019 (COVID-19). Such multiscale models, at both the atomistic and coarse grain levels, contribute greatly to our understanding of how this virus replicates. Molecular simulations of viral processes in COVID-19 are useful to identify possible weaknesses in the viral life cycle. This research focuses on the dynamics of coronavirus processes, including the conformational transitions that are required for the virus to function. The project has three main foci: 1) all-atom simulations of individual viral proteins that are essential to the viral life cycle; 2) a coarse-grain models to a holistic understanding of entire virion (the virus outside the host cell) and its large scale processes, such as fusion of virions with host cells; and 3) machine-learning-based approaches to link the all-atom and coarse grain models and further refine their accuracy. As part of a larger, international community working on COVID-19, all data, models and analysis code will be made publicly available as soon as they are developed, including through the NSF-funded Molecular Science Software Institute (MolSSI). The complete multiscale picture of virus structure and dynamics will be used to identify potential target sites for drug development and other therapeutic strategies.<br/><br/><br/>The research in this RAPID project is for the development and application of multiscale computer simulation methods to characterize key elements of large-scale viral processes in SARS-CoV-2 replication. To achieve this goal there are three main objectives: (1) to characterize the dynamical behavior of essential viral proteins involved using all-atom molecular dynamics simulations and understand the conformational transitions necessary for their function; (2) to develop and model the complete SARS-CoV-2 virion using coarse-grained simulation methods; and (3) to develop machine learning based approaches that systematically link atomic-level and coarse-grained simulation scales, and facilitate the generation of even more accurate and descriptive coarse-grained models. This research focuses on several biomolecular systems that are urgently needed to understand and characterize the transmission and propagation of the SARS-CoV-2 virus, including the spike protein that mediates entry of the viral particles into host cells, the host cell receptor, angiotensin-converting-enzyme 2, which binds the spike protein, coronavirus protease which catalyzes viral processes, and other viral protein components, especially as structural data and biochemical information are released in the next few months. Coarse-grained simulations will focus on the urgent need to develop a holistic model of the entire SARS-CoV-2 virion as well as its large-scale processes such as the fusion of virions with host cells.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	190595.0USD
398	Professor Murray Alison	Virginia Polytechnic Institute and State University	2020-05-01	2021-04-30	RAPID: MolSSI COVID-19 Biomolecular Simulation Data and Algorithm Consortium	Computer and Information Science and Engineering - In response to the growing COVID-19 pandemic, the Molecular Sciences Software Institute (MolSSI) will leverage its position as a neutral commodity resource to help the global computational molecular sciences community quickly provide their scientific data and expertise to address the COVID-19 crisis. The MolSSI is jointly supported by the Office of Advanced Cyberinfrastructure and the Divisions of Chemistry and Materials Research. The centerpieces of this engagement will be (1) a centralized repository for simulation-related data targeting the virus and host proteins and potential pharmaceuticals, and (2) a select set of MolSSI Software Seed Fellowships for Ph.D. students and postdocs targeting COVID-19 related software tools that operate on the data developed in the repository. These two components will enable the biomolecular simulation community to share and utilize key data and other resources to help identify the structural and dynamic characteristics of the host-virus complex to generate potential leads for therapeutics. Although this project is intended to address the acute COVID-19 crisis, in the near term, it also will impact research communities and the next generation of computational molecular scientists in the confrontation and proactive resolution of future world problems.<br/><br/>The MolSSI will create and curate a large-scale repository containing: simulation input files (structures, configurations, scripts, Jupyter notebooks) in an organized structure; MD trajectories, analysis tools, and ready models for drug discovery; pointers to preprint servers such as arXiv, bioRxiv, and ChemRxiv on biomolecular simulation research in regards SARS-CoV-2; and DOI services that create citable data. In addition, it will engage the molecular sciences community through a set of Software Fellowships for graduate student and postdocs to carry out software development, such as large-scale MD simulations, design of drug discovery tools such as docking, machine learning for small molecule toxicity predictions, and methods for determining whether new drugs are bioavailable or can be synthesized. Collectively, these resources will speed the identification and development of leads for antiviral drugs, analyzing structural effects of genetic variation in the SARS-CoV-2 virus, and inhibitors that can disrupt protein-protein interactions to viral entry into cells and adherence to surfaces that cause disease spread.<br/><br/>This award is being funded by the CARES Act supplemental funds allocated to CISE and MPS.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	200000.0USD
399	Professor Murray Alison	University of Wisconsin-Milwaukee	2020-05-01	2022-04-30	EAGER: Functionally Relevant Structural Heterogeneity in Coronavirus SARS-CoV2 Proteins	Biological Sciences - In order to prevent infections by specific viruses, it is important to understand the molecular details that drive the virus into host cells where they replicate, making more viral particles that spread to other cells in the infected individual. This award will help understand the mechanisms of the SARS-CoV2 infectivity, the virus responsible for the current COVID-19 pandemic, by employing machine learning algorithms to make movies of key proteins involved in driving its infection. There is mounting evidence that viral proteins exist in a range of structures, known as conformations, and that these can play a critical role in their function. In this project, recently developed machine-learning techniques will be used to determine the conformational landscape of key SARS-CoV2 proteins at near-atomic level, with and without antibody involvement. Atomistic insight into the conformational changes in SAR-CoV2 proteins is expected to help clarify the structural basis of virulence in this virus and its successors, ultimately providing a foundation for the development of suitable therapeutic strategies against coronaviruses. <br/><br/>Using experimental cryo-EM snapshots, this project will map the functionally relevant conformational heterogeneities of key SARS-CoV2 proteins to gain a deeper understanding of the role of conformational heterogeneity in this pandemic virus. The specific goals of this project are as follows: (1) Apply advanced machine-learning algorithms to experimental cryo-EM single-particle snapshots in order to determine the energy landscapes of key SARS-CoV2 proteins with and without antibody involvement; (2) Identify the functionally important conformational paths on the relevant energy landscapes; (3) Compare and contrast motions along functional paths with those inferred by discrete clustering methods; (4) Determine the biological implications of conformational motions associated with function; and (5) Make the results widely accessible in order to help facilitate the development of therapeutic strategies.<br/><br/>This RAPID award is made by the Division of Biological Infrastructure (DBI) using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	299723.0USD
400	Dr Wijeratne Peter	University College London	2019-09-01	2022-08-31	Computational models for clinical trial design in Huntington's disease	None	Medical Research Council	Fellowship	273137.0GBP
401	Professor Holmans Peter	Cardiff University	2006-09-01	2009-08-31	MSc in Bioinformatics	The Biostatistics and Bioinformatics Unit (BBU), in close collaboration with five internationally recognised cancer research groups with substantial Cancer Research UK support, requests funding to enable cancer researchers (or future cancer researchers) to study on an MSc course in bioinformatics. We seek this funding to enable highly motivated individuals to train in bioinformatics in order to boost the use of higher level bioinformatics in cancer research. The course will develop the appropriate complementary skills to provide students with the multidisciplinary bioinformatic skill set required to work effectively in the post genomic era in cancer research. The course contains core modules in computer science, statistics and the use of bioinformatics in the postgenomic era and specialist modules in databasing, machine learning and data mining, algorithmic aspects of sequence analysis and molecular modelling. Students complete a mini-research project as well as a full-time 3 month research project and the latter two components are embedded within the collaborating groups. We envisage individuals already in cancer research will obtain these bursaries in order to develop new skills but in some circumstances highly motivated individuals with no previous cancer research experience but with clear aspirations to move into cancer research may be accepted.	Cancer Research UK	Bursary	273137.0GBP
402	Dr Chan Laureen Lui Yan	Cardiff University	2010-09-01	2011-08-31	MSc in Bioinformatics and Genetic Epidemiology & Bioinformatics	The aim of the MSc Bioinformatics programme is to gain skills in computational power to investigate genetic epidemiology and post-genomics biology. The exponential growth of epigenetics and genomic data facilitates innovative approaches to investigate risk of having cancer and underlying mechanisms. As some genes play a dual role in cancer and obesity, association between cancer and obesity can be assessed. I have reported that Bitter melon possesses anti-diabetic and anti-obesity effects. Recent studies revealed its anti-tumour property. It was shown that Bitter melon decreased O6-methylguanine DNA adduct in colonic mucosa. It is hypothesized that localization of CpG island in genes and hence mechanisms could substantiate effects of Bitter melon in obesity-related cancer. Besides, maternal nutrition remodels underlying mechanism of obesity which may exaggerate the growth of tumours and then hinder treatment outcome. Genetics and epigenetics may be clues to delineate mechanisms of obesity-related cancer. The effects of diet on the obesity-cancer association could be investigated in network pathway analysis. Genetic epidemiology and artificial biological models can be established to study time-dependent effects. Having been trained in machine learning, pattern recognition, genetic epidemiology, likelihood and quantitative trait analysis, cutting edge methods could be used to further investigate these polygenic diseases.	Cancer Research UK	Bursary	273137.0GBP
403	Dr Nangalia Vishal	University College London	2013-06-03	2016-08-02	Machine Learning - Early Warning System (ML-EWS)	The aim of the MSc Bioinformatics programme is to gain skills in computational power to investigate genetic epidemiology and post-genomics biology. The exponential growth of epigenetics and genomic data facilitates innovative approaches to investigate risk of having cancer and underlying mechanisms. As some genes play a dual role in cancer and obesity, association between cancer and obesity can be assessed. I have reported that Bitter melon possesses anti-diabetic and anti-obesity effects. Recent studies revealed its anti-tumour property. It was shown that Bitter melon decreased O6-methylguanine DNA adduct in colonic mucosa. It is hypothesized that localization of CpG island in genes and hence mechanisms could substantiate effects of Bitter melon in obesity-related cancer. Besides, maternal nutrition remodels underlying mechanism of obesity which may exaggerate the growth of tumours and then hinder treatment outcome. Genetics and epigenetics may be clues to delineate mechanisms of obesity-related cancer. The effects of diet on the obesity-cancer association could be investigated in network pathway analysis. Genetic epidemiology and artificial biological models can be established to study time-dependent effects. Having been trained in machine learning, pattern recognition, genetic epidemiology, likelihood and quantitative trait analysis, cutting edge methods could be used to further investigate these polygenic diseases.	Medical Research Council	Fellowship	260560.0GBP
404	Dr Hayes Joseph	University College London	2019-01-01	2022-01-01	Personalising the pharmacological treatment of bipolar disorder	The aim of the MSc Bioinformatics programme is to gain skills in computational power to investigate genetic epidemiology and post-genomics biology. The exponential growth of epigenetics and genomic data facilitates innovative approaches to investigate risk of having cancer and underlying mechanisms. As some genes play a dual role in cancer and obesity, association between cancer and obesity can be assessed. I have reported that Bitter melon possesses anti-diabetic and anti-obesity effects. Recent studies revealed its anti-tumour property. It was shown that Bitter melon decreased O6-methylguanine DNA adduct in colonic mucosa. It is hypothesized that localization of CpG island in genes and hence mechanisms could substantiate effects of Bitter melon in obesity-related cancer. Besides, maternal nutrition remodels underlying mechanism of obesity which may exaggerate the growth of tumours and then hinder treatment outcome. Genetics and epigenetics may be clues to delineate mechanisms of obesity-related cancer. The effects of diet on the obesity-cancer association could be investigated in network pathway analysis. Genetic epidemiology and artificial biological models can be established to study time-dependent effects. Having been trained in machine learning, pattern recognition, genetic epidemiology, likelihood and quantitative trait analysis, cutting edge methods could be used to further investigate these polygenic diseases.	Wellcome Trust	Clinical Research Career Development Fellowship	460704.0GBP
405	Dr Hayes Joseph	University of Tennessee Knoxville	2020-05-01	2021-04-30	RAPID: Impacts of Design and Operation Attributes of Mass-Gathering Civil Infrastructure Systems on Pathogen Transmission and Exposure	Engineering - This Rapid Response Research (RAPID) grant will support fundamental research to reveal how the design attributes and operation strategies will influence the transmission of and exposure to infectious pathogens within mass-gathering civil infrastructure systems. During the pandemic of 2019 novel coronavirus, the mass-gathering civil infrastructure systems, such as schools, airports, and public transit systems, can become hot spots for spreading the infectious disease. There remains a striking knowledge gap in understanding the impacts of infrastructure design and operation on the occurrence, distribution, transport, and viability of pathogens. It is imperative to address this knowledge gap. Results from this project will lead to bio-informed guidelines for managing critical civil infrastructure systems to prevent exposure of facility users to pathogenic microorganisms, and reduce risks of spreading infectious diseases, and thus alleviating burdens on healthcare systems and citizens. This project will provide much needed insights for infrastructure design reconfigurations and operation practices during the pandemic, in the recovery, and beyond to prevent further disease outbreaks and support healthy, resilient, and smart communities. As a result, this project will help promote public health, national security, and economic prosperity. In addition, this project will raise public science literacy and awareness of infectious diseases, and improve student education and training, as well as K-12 outreach and engagement activities. <br/><br/>The specific objective of this research is to parameterize relevant design attributes and operation strategies of infrastructure systems, and subsequently evaluate their impacts on pathogen transmission and exposure from spatiotemporal microbiome profiles. Three aims will be pursued: 1) identify and quantify the design attributes and operation strategies that may impact pathogen dynamics; 2) audit the types, abundance, and co-occurrence patterns, as well as spatiotemporal dynamics of microorganisms, particularly pathogens, associated with spatially and functionally distributed system components; and 3) characterize the impacts of design and operation on the transmission and exposure pathways of microorganisms in infrastructure systems. The spatial and functional interdependence of system components will be considered to parameterize design attributes based on building information modeling and syntactic analysis. The operation strategies will be modeled using integrated data sensing and simulation techniques. A model-informed sampling approach will be developed with molecular and metagenomics techniques to characterize spatiotemporal microbiome dynamics. The impacts of design and operation on microbial transmission and exposure pathways will be assessed using integrated source tracking and machine learning methods. At the nexus of infrastructure system engineering and environmental microbiology, this convergence research will provide unique insights into design attributes and operation practices impacting pathogen transmission and exposure in mass-gathering civil infrastructure systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	199809.0USD
406	Dr Hayes Joseph	Massachusetts Institute of Technology	2020-04-01	2021-03-31	RAPID: Immunogenicity of SARS-CoV2 to Human T Cells	Mathematical and Physical Sciences - Pandemics caused by infectious pathogens have plagued humanity since antiquity. The Coronavirus Disease 2019 (COVID-19) caused by the SARS-CoV-2 virus is currently spreading across the world rapidly, including in the United States, with major adverse impact on health and the economy. The SARSCoV-2 outbreak has led to several urgent efforts to develop vaccines that may offer protection against this virus. It is unknown as to whether the current approaches being pursued will elicit protective immune responses in humans. While vaccines have been very effective against many pathogens, the empirical methods for vaccine development pioneered by Pasteur and Jenner over two centuries ago have failed to produce effective vaccines against Human Immune Deficiency Virus, Malaria, Tuberculosis, and many other pathogens. Therefore, rational design of vaccines based on a mechanistic understanding of the pertinent virology and immunology is being pursued, and these efforts include work that is rooted in statistical physics. SARSCoV-2 is phylogenetically most similar to SARS-CoV. This project will use a machine learning approach to understand how the SARS-CoV-2 virus interacts with the immune T cells. This work will directly impact the design of SARS-CoV-2 vaccines and vaccines against future endemic-causing pathogens.<br/><br/>Analyses of patients who have recovered from SARS-CoV shows that antibody responses are not prevalent a few years later, but memory T cell responses are durable and may offer long-term protection. The main questions addressed by this project are 1. Will the SARS-CoV peptides targeted by human T cells that are mutated in SARS-CoV-2 still elicit human T cell responses - i.e. are they immunogenic? 2: Are the 102 peptides identified by host major histocompatibility molecules binding assays alone that are common between SARS-CoV and SARS-CoV-2 immunogenic in humans? If not, they are irrelevant from vaccine design perspective. The goal of the work proposed here is to take a physics-based machine learning approach to determine the immunogenicity of SARS-CoV-2 proteins to human T cell responses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	124472.0USD
407	Dr Hayes Joseph	University of Southern California	2020-05-01	2021-04-30	RAPID: ReCOVER: Accurate Predictions and Resource Allocation for COVID-19 Epidemic Response	Computer and Information Science and Engineering - The recent outbreak of COVID-19 and its world-wide impact calls for urgent measures to contain the epidemic. Predicting the speed and severity of infectious diseases like COVID-19 and allocating medical resources appropriately is central to dealing with epidemics. Epidemics like COVID-19 not only affect world-wide health, but also have profound economic and social impact. Containing the epidemic, providing informed predictions and preventing future epidemics is essential for the global population to resume their day-to-day work and travel without fear. Shortage of resources puts undue stress on healthcare system further risking health of the community. Preparedness and better management of available resources would require specific predictions at the level of cities and counties around the world rather than solely at the level of countries. The project will provide a predictive understanding of the spread of the virus by developing machine learning based computational models to study the transmission of the virus and evaluate the impact of various interventions on disease spread. The project will learn infection prediction models for COVID-19 considering the following. (i) Predicting at state/county/city-level rather than country-level as finer granularity is essential in planning and managing resources. (ii) How infectious a person is changes over time. Learning the model through observed data will help in understanding of the temporal nature of the virality. (iii) At such granularity travel is a significant reason for the spread and needs to be accounted for. (iv) Available data needs to be ?corrected? by finding the number of underlying unreported cases that are not observed and yet influence the epidemic dynamics. The project will also solve the resource allocation problem based on the prediction ? for instance if a certain number of masks will be available next week in a certain state, how should they be distributed across different hospitals in the state (which hospitals and how many in each state)?<br/><br/>Proposed project ReCOVER will use a novel fine-grained, heterogeneous infection rate model to perform predictions at various granularities (hospital/airports, city, state, country) while accounting for human mobility. ReCOVER will integrate data from various sources to build highly accurate models for prediction of the epidemic across the world at various granularity. Due to the ability to capture temporal heterogeneity in infection rate, the approach has the potential to provide insights into infectious nature of COVID-19 which are not fully understood yet. The project will address the issue of unreported cases through temporal analysis of historical infections and correct the data. The right granularities of modeling will be automatically identified, e.g., when to model a state over its cities to trade-off precision for higher reliability in predictions. The proposed project also formulates and solves a resource allocation problem that can guide the response to contain the epidemic and prevent future outbreaks. This is provided by optimal solutions to resource allocation over a network where each node (representing a region) has a function that captures probabilistic response. While the project obtains data with COVID-19 in consideration, the model and algorithms developed under the project are applicable to a wide class of contagious diseases. The project will culminate into an interactive customizable tool that can be used to perform predictions and resource management by a qualified user such as a government entity tasked with managing the epidemic response. The data and code will also be shared with research community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	158592.0USD
408	Dr Hayes Joseph	Colorado State University	2020-05-01	2021-04-30	RAPID: ENSURING INTEGRITY OF COVID-19 DATA AND NEWS ACROSS REGIONS	Computer and Information Science and Engineering - Large amounts of epidemiological data are being generated and collected from a variety of sources to understand the impact and propagation of COVID-19. Similarly, huge amounts of news articles are generated and disseminated about the pandemic to keep the population informed. The appropriateness of the actions taken by individuals, corporations, and governments are often based on the quality of data and news. Thus, ensuring the quality of data and news is important. However, malicious actors can alter the attributes of data records, insert spurious records, or suppress records causing any analysis to be inadequate and misinformation to be propagated. This project addresses the critical problem of defining and identifying spurious data and news concerning COVID-19, and tracking the source of misinformation. The project novelty lies in the development of an approach and associated toolset that adapts and combines Machine Learning technologies to detect spurious data and misinformation and presents the results in a manner that is easy for end users to understand and interpret. The approach detects discrepancies in COVID-19 data and traces the flagged discrepancies back to the data sources. The results obtained from the news sources and those obtained from the medical data analysis are compared to determine correlations between the quality of news and the degree and type of data manipulation performed at any region. The project?s impacts are on significantly enhancing the ability to perform accurate scientific analysis, and detecting and explaining news manipulation with respect to COVID-19. The scientific principles developed in the project are expected to be useful outside the medical domain. The PI and the students identified for this project are minorities. The project will be carried out in the Computer Science Department at Colorado State University which is a BRAID affiliate.<br/><br/>COVID-19 data discrepancies are related to (1) single records, where some field is modified, (2) sequence of records over time forming a temporal dimension, where spurious records have been inserted or records have been suppressed, and (3) sequences of records across regions forming a spatial dimension, where there is a pattern of manipulation or information disclosure across regions. The approach determines the appropriate combination of autoencoders, Long Short-Term Memory (LSTM), Temporal Convolution Network (TCNs), and Convolution Neural Networks (CNNs) that can work with data obtained from medical sources and news containing both spatial and temporal dimensions. The tools help the investigators? collaborators at the University of Colorado Anschutz Medical Center and Center for Disease Control and Prevention to perform data integrity checking of medical records and to provide explanations of integrity violations. The tools also handle different types of data and news alterations pertaining to COVID-19.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	199748.0USD
409	Dr Hayes Joseph	University of California-Irvine	2020-05-01	2021-04-30	RAPID: The Role of Emerging Virtual Cultures in the Prevention of COVID-19 Transmission	Social, Behavioral and Economic Sciences - The COVID-19 pandemic has transformed our relationship to the physical world. Social distancing guidelines have led many people to avoid all forms of public life, from concerts and restaurants to everyday interaction in parks, neighborhoods, and the homes of family and friends. In response there has been a massive increase in online interaction: the internet has suddenly become the primary way that many Americans socialize, labor, and learn. It is crucial to gain a better understanding of how the emergence of these changes is related to the pandemic. Even if a vaccine is discovered, preventing catastrophic levels of COVID-19 transmission into the next few years will depend on social distancing that can be sustained and integrated with work, education, and community. This means going online. The starting point for addressing this global challenge is thus the fact that what we call ?social distancing? is really physical distancing. Successful physical distancing will rely on new forms of social closeness online. Yet there is not just one ?online.? A rapid and effective response requires clarifying the impact of virtual worlds as part of different forms of online interaction that comprise a virtual culture: social network sites, streaming websites, and multiplayer platforms. The project will also train graduate student researchers in methodological approaches for studying online cultures. <br/><br/>This research will be conducted in a densely trafficked virtual world. Virtual worlds are places where individuals interact with avatars in online environments. The investigators have conducted research in a virtual world context for over a decade, and thus have detailed baseline data with which to examine what is happening as a large number of individuals enter that virtual world due to the COVID-19 pandemic. What is the sudden move to virtual worlds doing in terms of social closeness and interaction? How does co-presence in virtual place transform intimacy and collaboration? How might this provide innovative strategies for preventing viral transmission, by forging new forms of social closeness in the context of physical distancing? To investigate these questions, the researchers will conduct participant observation, individual interviews, and group interviews. The study will compare individuals who have spent time in the virtual world for years with individuals who have entered the virtual world after COVID-19. Findings from this research will provide insight into the specific possibilities virtual worlds are providing in the circumstances of societies reshaped by COVID-19. In these new circumstances, virtual worlds will be one element of an online ecosystem linking drones, robots, and autonomous vehicles to mobile devices, social network sites, online games and streaming, augmented reality, artificial intelligence, machine learning, and data analytics. The research will thus provide a better understanding of the place of virtual worlds in this emerging online ecosystem.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	195619.0USD
410	Dr Hayes Joseph	WASHINGTON UNIVERSITY	2020-05-15	2021-04-30	RAPID: A multiscale approach to dissect SARS-CoV-2 attachment to host cells and detect viruses on surfaces	Biological Sciences - The 2019 novel coronavirus, identified as the cause for the pneumonia pathology reported in Wuhan, spread quickly and became a global pandemic. The project will employ experimental methods to develop sensors for the detection of SARSCoV-2 from environmental samples and develop predictive models for virus attachment to cells by applying computational machine learning methods. The outcome of this project will contribute to the development of proactive measures to identify viruses with pandemic potential before they are able to transmit and spread broadly among humans. The graduate students involved in this research will gain experience in protein biochemistry, fluorescence microscopy, and computational simulations and experience utilizing those skills to problems of societal importance.<br/><br/>This NSF Rapid response Research (RAPID) project will support a project that is aimed to characterize receptor interactions mediated by the Spike protein (S) of SARS-CoV-2. Development of fluorescence-based assays to characterize SARSCoV-2 attachment to Angiotensin converting enzyme (ACE2)-functionalized surfaces with controlled density and mobility, identifying peptide mimics of the ACE2 ectodomain for the development of sensors to detect SARSCoV-2 from environmental samples, and develop and validate predictive models of CoV attachment from primary sequence using machine learning constitute the specific goals of this project.<br/><br/>This RAPID award is made by the Molecular Biophysics Program in the Division of Molecular and Cellular Biosciences, using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	200000.0USD
411	Dr Hayes Joseph	Columbus State University	2020-05-15	2021-04-30	RAPID/Collaborative Research: Quantifying Social Media Data for Improved Modeling of Mitigation Strategies for the COVID-19 Pandemic	Engineering - This Rapid Response Research (RAPID) grant will support research that will contribute new knowledge related to modeling social behavior and community activity during the COVID-19 pandemic, as well as future pandemics with COVID-19 characteristics. The model focuses on compliance with mitigation strategies and public health guidelines, thus enabling the selection of policies that are most effective in promoting both the progress of science and advancing national health and prosperity. Various pandemic models are currently being used to predict the spread of a virus and establish which mitigation strategies are the most effective. These models are heavily based on assumptions and may include an oversimplified reality of how populations react and behave. This research will provide needed knowledge and methods for the development of a model of how individuals in the U.S. react to certain mitigation strategies, such as social-distancing, stay-at-home orders, quarantines, and travel advisories, by mining and analyzing social media data during the COVID-19 crisis. This enhanced modeling approach and its resultant model will be of great value to disaster response managers and policy/decision makers to understand human social behavior. This work allows assessment of the effectiveness of mitigation strategies and public health guidelines during pandemics (and other crises). This project will also form the basis of a publicly available case study suitable for university level students that can be widely incorporated in courses. <br/><br/>Although individual-based and homogeneous mixing pandemic models provide useful insights and predictive capabilities within a range of possibilities, they are highly sensitive to people?s actions. This research aims to provide an enhanced approach to model social behavior and community activity during a pandemic in terms of compliance with mitigation strategies and public health guidelines. Social media data present a brief window of opportunity for research on how, and to what extent, the public does or does not comply with the recommended mitigation strategies and public health guidelines. The research team will collect real-time data from social media related to COVID19-exposed regional populations in the U.S. The data will be analyzed using machine learning techniques to identify non-mutually exclusive clusters of people based on similarity of their demographic, geographic, and time information, and establish relationships among clusters. The analyzed data will form the basis of a data-driven multi-paradigm simulation model that captures changes in public sentiment over time, quantifies the resistance/compliance with mitigation strategies and health guidelines, and gauges overall effectiveness of various mitigation strategies and advice over time.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	13406.0USD
412	Dr Hayes Joseph	Northeastern University	2020-05-15	2021-04-30	RAPID: D3SC: Identification of Chemical Probes and Inhibitors Targeting Novel Sites on SARS-CoV-2 Proteins for COVID-19 Intervention	Mathematical and Physical Sciences - The life cycle of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) involves a number of viral proteins and enzymes required for infectivity and replication. Inhibitors that target these enzymes serve as potential therapeutic interventions against coronavirus disease 2019 (COVID-19). With this award, the Chemistry of Life Processes program in the Chemistry Division is supporting the research of Drs. Mary Jo Ondrechen and Penny J. Beuning from Northeastern University to apply computational methods to identify sites in SARS-CoV-2 proteins that would be good targets for binding inhibitors. The project uses artificial intelligence methods developed at Northeastern University to identify pockets and crevices in the structures of viral proteins that may serve as new targets for the development of antiviral agents. Large datasets of natural and synthetic compounds are computationally searched for molecules that fit into these alternative sites, and any compounds that fit will be experimentally tested for their ability to inhibit the functions of these viral enzymes. The project provides training in computational chemistry and biochemical analysis to graduate students and postdoctoral associates.<br/><br/>This project uses the unique Partial Order Optimum Likelihood (POOL) machine learning (ML) method developed by Dr. Ondrechen?s group to predict multiple types of binding sites in SARS-CoV-2 proteins, including catalytic sites, allosteric sites, and other interaction sites. The goals of this project are to apply the POOL-ML method to identify the binding sites on viral pathogen SARS-CoV-2 proteins using the three-dimensional protein structures as input. Molecular dynamics simulations are used to generate conformations for ensemble docking. Compounds from the large molecular databases are computationally docked into the predicted sites to identify potentially strong binding ligands. Candidate ligands to selected SARS-CoV-2 proteins, including the main protease and 2?-O-ribose RNA methyltransferase, are experimentally tested in vitro for binding affinity and the effect of the best predicted inhibitors on catalytic activities determined by direct biochemical assays. All the SARS-CoV-2 protein structures in the Protein Data Bank (PDB) are studied. Compound libraries for the study include: a) selected 2600+ compounds from the ZINC and Enamine databases that are already being manufactured; b) a library of 20,000+ compounds found in foods that the team recently gained access to; these potentially hold some special advantages, including ready availability in the public domain and low cost; and c) the March 2020 open access CAS (American Chemical Society) database of 50,000 compounds with known or potential anti-viral activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	165808.0USD
413	Dr Hayes Joseph	Harvard University	2020-05-01	2021-04-30	RAPID:Collaborative Research: Computational Drug Repurposing for COVID-19	Computer and Information Science and Engineering - With the disruptive nature of the COVID-19 pandemic, effective treatments could save the lives of severely ill patients, protect individuals with a high risk of infection, and reduce the time patients spend in hospital beds. However, there are currently no effective treatments for COVID-19. Traditional methodologies take years to develop and test compounds from scratch. Machine learning provides promising new approaches to repurpose drugs that are safe and already approved for other diseases. This project will develop a machine learning toolset to expedite the development of safe and effective medicines for COVID-19. The toolset will rapidly identify safe repurposing opportunities for approved and experimental drugs. It will predict whether treatments may have therapeutic effects in COVID-19 patients, allowing the identification of drugs and drug cocktails that are safe and plentiful enough to treat a substantial number of patients. By putting tools in the hand of practitioners, the activities in this project will have an immediate impact. They will result in actionable predictions that are accurate and interpretable. <br/><br/>Recently, the principal investigators have developed a series of machine learning tools to identify drug repurposing opportunities. Building on foundational previous work, in this project, the principal investigators will first build a large COVID-19 focused knowledge graph that will capture fundamental and COVID-19-specific biological knowledge. The graph learning methods will be adapted to identify safe drugs and drug cocktails for COVID-19. To predict the safety of cocktails with two or more drugs, the methods will generalize to an exponentially large space of high-order drug combinations. In addition to drug safety, efficacy is a crucial endpoint for drug development. The project will develop a novel graph neural network (GNN) method to identify efficacious drug repurposing opportunities, even for diseases, such as COVID-19, that do not yet have any drug treatments and thereby, no label, supervised information. The method will predict what drugs and drug combinations may have a therapeutic effect on COVID-19. Finally, the principal investigators will integrate the developed tools into a complete, explainable framework that will generate predictions, provide explanations, and incorporate human feedback into the machine learning loop. This project will provide new, open tools for rapid drug repurposing that will be relevant for COVID-19 and other emerging pathogens. Additionally, the project will provide unique opportunities for multi-disciplinary curriculum development, training and advising, and professional activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	99863.0USD
414	Dr Hayes Joseph	University of Minnesota-Twin Cities	2020-05-15	2021-04-30	RAPID: The effect of contact network structure on the spread of COVID-19: balancing disease mitigation and socioeconomic well-being	Biological Sciences - What makes COVID-19 spread rapidly in some places, yet slowly in others? How should society lessen social distancing while limiting an increase in infections? To answer these questions, this Rapid Response Research (RAPID) project seeks to understand how patterns of interpersonal interaction (?structure?) in social contact networks affect disease spread in a population. The researchers will simulate a disease spreading through a variety of social contact networks, and use machine learning to relate each network?s structure to the number and timing of new infections. By limiting structures related to increased disease, societies may be able to reopen other parts of their economies while still curbing overall disease spread. The researchers will produce an interactive web application for the public and decision-makers to visualize trade-offs between reducing disease and maintaining social cohesion. This research will support the professional development of an early career scientist.<br/><br/>This research aims to determine the inherent risk of SARS-CoV-2 spread based on contact network structure. The researchers will use machine learning to 1) identify network structures that influence disease spread and 2) predict disease spread on empirical contact networks. Important network structures will serve as targets for simulated disease mitigation interventions (e.g. reducing structures that increase levels of disease or increasing structures that reduce disease levels). Finally, the researchers will investigate whether future outbreaks of COVID-19 or other diseases could be alleviated through optimizing social contact networks ahead of time. The outcomes of this research will inform and facilitate quick, efficient interventions to reduce the social and economic costs of COVID-19. This research will develop a general framework for relating disease to network structure. Thus, results can be generalized beyond the current pandemic, serving to further our understanding of potential future waves of COVID-19, as well as other directly-transmitted diseases in humans, livestock, and wildlife.<br/><br/>This RAPID award is made by the Ecology and Evolution of Infectious Diseases Program in the Division of Environmental Biology, using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	199136.0USD
415	Dr Hayes Joseph	New Mexico Highlands University	2020-05-15	2021-04-30	RAPID: Machine Learning Methods to Understand, Predict and Reduce the Spread of COVID-19 in Small Communities	Mathematical and Physical Sciences - The ongoing COVID-19 outbreak has recently reached pandemic status spreading all around the world. The severity of the pandemic, along with an enormous impact on world?s economy and society, has forced governments to introduce emergency measures. It is essential to utilize the available statistical data from trusted sources in order to model and evaluate the dynamics of the pandemic spread, to not only better understand such complex systems, but to learn and develop possible solutions to prevent further spread of current and/or similar future outbreaks. Thus, this research, devoted to the development of mathematical models of COVID-19 pandemic spread, addresses an urgent national need. Faculty and students in computer science, anthropology, and computational chemistry at New Mexico Highlands University have formed a diverse group for finding a solution to the complicated problems of the description and prediction of COVID-19 spread. This multidisciplinary project is expected to yield a better understanding of the interconnections among many factors that contribute to the spread of COVID-19. Statistical data will be collected in regions of Northern New Mexico, including San Juan and McKinley Counties in the Navajo Nation and Los Alamos county outside of the Navajo Nation. Analysis of the collected statistical data along with socio-cultural assessment from this project will be presented to New Mexico (NM) tribal and health authorities. The project will aim to provide a scientific basis for the prediction of disease spread and will consider scenarios associated with the possibility of another wave of the pandemic. Students from this minority-serving institution involved in the project will obtain valuable experience in the application of advanced machine learning models and methods in providing fast robust reaction to a national health, economic, and societal crisis.<br/><br/>In this study, machine learning methods will be used to analyze pandemic spread scenarios in different regions and to glean the most important features of the data characterizing the spread. The research team will use both traditional machine learning techniques and advanced methods, such as artificial neural networks, allowing development of virus incidence model capturing dependencies in both linear and nonlinear domains. The work will concentrate on understanding disease spread with regard to multiple socioeconomic factors. The problem can be treated as a sequence modeling one; so, recurrent neural networks and more complex models based on their recurrent cells might be one promising direction. The next step will be to assemble datasets for small isolated communities with different socioeconomic backgrounds and ethnicities ? comparing Navajo Indians living on the Navajo reservation to Los Alamos County (NM) ? and to test the applicability of the developed model to these regions. The spatiotemporal data available on the spread is heterogeneous in character. An important goal of this research is to classify the collected data with respect to the similarity in the epidemic curve behavior and then build separate models for different regions according to this classification. The proposed model will be used for prediction of future incidents and to produce the most effective non-medical recommendations for suppression and prevention of future viral outbreaks.<br/><br/>This research is supported by the Partnerships for Research and Education in Materials (PREM) program and the Condensed Matter and Materials Theory (CMMT) program in the Division of Materials Research in the Directorate for Mathematical and Physical Science using supplemental funds made available by the Coronavirus Aid, Relief, and Economic Security (CARES) Act.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	185747.0USD
416	Dr Hayes Joseph	University of British Columbia	2020-05-15	2021-04-30	Digital Virtual Support of Cases and Contacts to Novel Coronavirus (COVID-19): Readiness and Knowledge Sharing for Global Outbreaks (WelTel PHM)	The global outbreak of COVID-19 is the latest example of a rapidly spreading infectious outbreak with global impact. Infected patients with mild symptoms and asymptomatic contacts need to be isolated, ideally without overwhelming health facilities. WelTel, an integrated virtual care and patient engagement solution, emerged as an innovation initially to support the global HIV pandemic through a Canadian-Kenyan partnership over a decade ago. Co-founded by the lead investigator and registered in British Columbia, WelTel has continued to integrate research into a richly featured virtual care platform that can be used on the frontlines of healthcare delivery. The study aims to: 1-Deploy and co-optimize WelTel to assist in home monitoring and support of COVID-19 cases and contacts; 2- Determine essential linkages and technical demands of the digital health ecosystem for data security purposes and integration into other electronic health records (EHR) & health information management systems (HIMS); 3-Evaluate communication and other metadata captured by the system for public health quality improvement to better understand and reduce barriers (such as stigma); 4-Use novel computing approaches such as natural language processing (NLP) and machine learning to harness artificial intelligence (AI) capabilities to model, predict, and provide insights into future precision public health approaches. Collaborators have necessary expert skills in quantitative and qualitative research methods for rigorous assessment, and come from the countries targeted for the research deployment (Canada, UK, US, Kenya, and Rwanda). A rapid digital landscape analysis will also be done as a part of this research. Virtual care may be an efficient, cost-effective way to provide the necessary public health monitoring and support for patients and contacts of COVID-19 and future emerging communicable pathogens, as well as can inform public health quality improvement and precision care.	Canadian Institutes of Health Research	Research Grant	500000.0CAD
417	Prof. Cotton Matthew	MRC/UVRI & LSHTM Uganda Research Unit	2020-05-15	2021-04-30	African COVID-19 Preparedness (AFRICO19)	Our project, AFRICO19, will enhance capacity to understand SARS-CoV-2/hCoV-19 infection in three regions of Africa and globally. Building on existing infrastructures and collaborations we will create a network to share knowledge on next generation sequencing (NGS), including Oxford Nanopore Technology (MinION), coronavirus biology and COVID-19 disease control. Our consortium links three African sites combined with genomics and informatics support from the University of Glasgow to achieve the following key goals: 1. Support East and West African capacities for rapid diagnosis and sequencing of SARS-CoV-2 to help with contact tracing and quarantine measures. Novel diagnostic tools optimized for this virus will be deployed. An African COVID-19 case definition will be refined using machine learning for identification of SARS-CoV-2 infections. 2. Surveillance of SARS-CoV-2 will be performed in one cohort at each African site. This will use established cohorts to ensure that sampling begins quickly. A sampling plan optimized to detect initial moderate and severe cases followed by household contact tracing will be employed to obtain both mild to severe COVID-19 cases. 3. Provide improved understanding of SARS-CoV-2 biology/evolution using machine learning and novel bioinformatics analyses. Our results will be shared via a real-time analysis platform using the newly developed CoV-GLUE resource.	Wellcome/Department for International Development	Research Grant	2001990.0GBP
418	Prof. Cotton Matthew	Foundation For Research And Technology - Hellas	2012-10-01	2017-10-01	Dissecting the Role of Dendrites in Memory	Understanding the rules and mechanisms underlying memory formation, storage and retrieval is a grand challenge in neuroscience. In light of cumulating evidence regarding non-linear dendritic events (dendritic-spikes, branch strength potentiation, temporal sequence detection etc) together with activity-dependent rewiring of the connection matrix, the classical notion of information storage via Hebbian-like changes in synaptic connections is inadequate. While more recent plasticity theories consider non-linear dendritic properties, a unifying theory of how dendrites are utilized to achieve memory coding, storing and/or retrieval is cruelly missing. Using computational models, we will simulate memory processes in three key brain regions: the hippocampus, the amygdala and the prefrontal cortex. Models will incorporate biologically constrained dendrites and state-of-the-art plasticity rules and will span different levels of abstraction, ranging from detailed biophysical single neurons and circuits to integrate-and-fire networks and abstract theoretical models. Our main goal is to dissect the role of dendrites in information processing and storage across the three different regions by systematically altering their anatomical, biophysical and plasticity properties. Findings will further our understanding of the fundamental computations supported by these structures and how these computations, reinforced by plasticity mechanisms, sub-serve memory formation and associated dysfunctions, thus opening new avenues for hypothesis driven experimentation and development of novel treatments for memory-related diseases. Identification of dendrites as the key processing units across brain regions and complexity levels will lay the foundations for a new era in computational and experimental neuroscience and serve as the basis for groundbreaking advances in the robotics and artificial intelligence fields while also having a large impact on the machine learning community.	European Research Council	Starting Grant	1398000.0EUR
419	Dr Bottle Alex	Imperial College London	2010-09-01	2014-02-28	Can valid and practical risk-prediction or casemix adjustment models, including adjustment for co-morbidity, be generated from English hospital administrative data (Hospital Episode Statistics)?	Understanding the rules and mechanisms underlying memory formation, storage and retrieval is a grand challenge in neuroscience. In light of cumulating evidence regarding non-linear dendritic events (dendritic-spikes, branch strength potentiation, temporal sequence detection etc) together with activity-dependent rewiring of the connection matrix, the classical notion of information storage via Hebbian-like changes in synaptic connections is inadequate. While more recent plasticity theories consider non-linear dendritic properties, a unifying theory of how dendrites are utilized to achieve memory coding, storing and/or retrieval is cruelly missing. Using computational models, we will simulate memory processes in three key brain regions: the hippocampus, the amygdala and the prefrontal cortex. Models will incorporate biologically constrained dendrites and state-of-the-art plasticity rules and will span different levels of abstraction, ranging from detailed biophysical single neurons and circuits to integrate-and-fire networks and abstract theoretical models. Our main goal is to dissect the role of dendrites in information processing and storage across the three different regions by systematically altering their anatomical, biophysical and plasticity properties. Findings will further our understanding of the fundamental computations supported by these structures and how these computations, reinforced by plasticity mechanisms, sub-serve memory formation and associated dysfunctions, thus opening new avenues for hypothesis driven experimentation and development of novel treatments for memory-related diseases. Identification of dendrites as the key processing units across brain regions and complexity levels will lay the foundations for a new era in computational and experimental neuroscience and serve as the basis for groundbreaking advances in the robotics and artificial intelligence fields while also having a large impact on the machine learning community.	National Institute for Health Research (Department of Health)	Full Grant	400921.33GBP
420	Professor Herten Dirk-Peter	University of Birmingham	2020-08-01	2023-08-01	AMS Professorship Award for Professor Dirk-Peter Herten, University of Birmingham	I want to establish a world-leading research group in receptor signalling by providing tools and support for quantitative studies with molecular sensitivity for biomedical research. The prestigious award of the AMS fellowship will enable the purchase of cutting-edge microscopy tools and allow further automation of our microscopes to increase throughput and provide the capacity for super-resolved 3D reconstructions of whole cells. My position in COMPARE aligned to the Institute of Cardiovascular Sciences (ICVS) and the School of Chemistry at the University of Birmingham (UoB) adds expertise in methods development to the COMPARE advanced microscopy facility at UoB and will allow me to work on my quantitative microscopy methods. My appointment creates a unique opportunity to support researchers and clinicians in their search for solutions to urgent biomedical questions, like HIV infection or thrombosis, by use of existing techniques and novel approaches. I will involve Jeremy Pike (UoB, Data Analysis Officer) and Iain Styles (Computer Sciences, Turing Institute Fellow and Deputy Director of COMPARE, UoB) in machine learning approaches for the fast processing of 3D microscopy data. Additionally, I will initiate a planned industry collaboration with IRIS Biotech (Germany) to elaborate on the potential commercialisation of fluorescent probes for super-resolution microscopy, chemical multiplexing and metal cation sensing based on my patent (DE 10 2016 012 162.9). A challenge in methods development is to identify suitable targets and questions for the novel approach. The fellowship will enable collaborations with colleagues on campus and within COMPARE to increase the number of biomedical targets and facilitate translational application of my techniques which focus on T cell receptor signalling and more recently on GPCRs. Examples of new collaborations include the platelet immunoglobulin C-type lectin-like receptors GPVI and CLEC-2 (with Steve Watson, Steve Thomas and Natalie Poulter, ICVS) which are novel targets for a variety of thrombosis and thrombo-inflammatory disorders, and in the chemokine GPCR family (with Dimity Veprintsev, University of Nottingham - UoN) that play an important role in diseases like hypertension, hypoxia, or hypoglycaemia. In this context, I envision a key training centre for advanced microscopy at UoB also addressing problems like fluorescence labelling and probe development. This will involve colleagues in the School of Chemistry as well as COMPARE researchers working on fluorophores (Jon Preece, UoB) and bioactive molecules (Liam Cox, UoB; Barrie Kellam, UoN). COMPARE offered me a generous starting budget COMPARE (£ 450K) and the institute is in the process of refurbishing the labs to my needs, to move 3 co-workers together with my equipment by February 2020. Since I am unable to bring any overseas funding, the award will allow me to get accommodated with the UK grant application system and plan projects to strengthen my translational research (MRC) and the development of novel probes and techniques (BBSRC and EPSCR) without losing any momentum in my ongoing research activities. I have already applied for a translational research grant (BBSCR TRDF) in collaboration with Robert Henderson (University of Edinburgh), which involves quantitative imaging microscopy through use of his 256x256 APD camera.	The Academy of Medical Sciences	AMS Professorship Scheme Round 2	470621.92GBP
421	Professor Huang Xiaolin	Tongji Hospital, Tongji Medical College, Huazhong University of Science and Technology	2018-02-01	2019-06-30	Low cost robotic orthosis for stroke treatment in rural China	China is the worst affected developing country with 2.5M new stroke cases each year and 11.1M stroke survivors at any given time. In the past decades, due to lower socioeconomic status, less stroke awareness, and inequitable distribution of medical resources to rural areas, the incidence rate and burden of stroke in China has increased disproportionately in rural areas of the northeast and central regions. Rehabilitation programmes have been shown to be extremely effective in reducing the disability and restoring walking ability through early training. However, in most rural areas of China no such medical rehabilitation centres or hospitals exist leaving the rural population without the therapies which will allow them to regain mobility functions after stroke. This leads to disability and has broader negative socioeconomic impacts. This project seeks to gather evidence directly from the key stakeholders in the beneficiary ODA countries (China and Kazakhstan) about what their problems and requirements are. The networking activities will allow us to build a full proposal based around actual rather than assumed need, and therefore maximise the impact achieved. The project aims to establish a multi-disciplinary consortium including experts from rehabilitation robotics, sensing, machine learning, physiotherapy and rehabilitation medicine. We aim to provide low-cost robotic solutions to stroke survivors in remote home and community environments. This would reduce therapists’/stroke carers' demand in rural areas of ODA countries whilst maximizing the stroke survivor’s sense of intention, involvement, interaction and achievement of limb movement, with greater likelihood of successful rehabilitation and improved quality of life.	The Academy of Medical Sciences	AMS Professorship Scheme Round 2	22000.0GBP
422	Professor Huang Xiaolin	TWO WORLDS CONSULTING LIMITED	2018-02-01	2019-06-30	udu: AI Platform for Pandemic Intelligence	The UK, amongst others, lacks a coherent infrastructure to support effective direct and timely collection and analysis of pandemic data, about both the progressIon of Covid-19 itself and the population response to public policy aimed at mitigating and profiling its progress. Refining policy and informing the judgement calls required to navigate the balance between lockdown and economic damage requires both accurate data and the ability to rapidly model multiple, 'What if?' scenarios. Current data intelligence systems are partial, fragmented, incomplete, lag reality and, in most cases can only surface what they have specifically been asked to look for. AI systems used to look for patterns are often constrained by the quality and range of data available to them. Existing models tend to look at single factors in isolation, e.g. not taking into account multiple sources of mortality data or failing to take account factors such as population mobility and behaviour, the impact of events such as Cheltenham races, sunny bank holiday weather or other regional and seasonal variations. This can only be addressed through a more holistic approach to data collection and integration. This project therefore uses an advanced data intelligence platform, udu, which is capable of integrating a wide range of data from multiple sources and of multiple types and of actively discovering new data online. It then uses software that can self-organise itself around a task to discover relationships between and patterns in the collected data to provide an inferential view of pandemic impact, policy effectiveness and population behaviour. udu has been established for several years in niche markets. Here, we are building on previous experience by Two Worlds in using udu to create systems for the predictive analysis of environmental change to public health for the first time. The resulting system is intended to be capable of supporting direct exploration by human users, providing an interface (API) to allow other teams to test their own analytic models against the datascape created by udu and supporting local and external machine learning systems with a wider range of high quality data and analysis.	UK Research and Innovation	AMS Professorship Scheme Round 2	49984.0GBP
423	Dr Courtin Emilie	London Sch of Hygiene and Trop Medicine	2020-05-01	2023-04-30	Heterogeneous and long-term effects of social interventions on mortality and psychological health	The UK, amongst others, lacks a coherent infrastructure to support effective direct and timely collection and analysis of pandemic data, about both the progressIon of Covid-19 itself and the population response to public policy aimed at mitigating and profiling its progress. Refining policy and informing the judgement calls required to navigate the balance between lockdown and economic damage requires both accurate data and the ability to rapidly model multiple, 'What if?' scenarios. Current data intelligence systems are partial, fragmented, incomplete, lag reality and, in most cases can only surface what they have specifically been asked to look for. AI systems used to look for patterns are often constrained by the quality and range of data available to them. Existing models tend to look at single factors in isolation, e.g. not taking into account multiple sources of mortality data or failing to take account factors such as population mobility and behaviour, the impact of events such as Cheltenham races, sunny bank holiday weather or other regional and seasonal variations. This can only be addressed through a more holistic approach to data collection and integration. This project therefore uses an advanced data intelligence platform, udu, which is capable of integrating a wide range of data from multiple sources and of multiple types and of actively discovering new data online. It then uses software that can self-organise itself around a task to discover relationships between and patterns in the collected data to provide an inferential view of pandemic impact, policy effectiveness and population behaviour. udu has been established for several years in niche markets. Here, we are building on previous experience by Two Worlds in using udu to create systems for the predictive analysis of environmental change to public health for the first time. The resulting system is intended to be capable of supporting direct exploration by human users, providing an interface (API) to allow other teams to test their own analytic models against the datascape created by udu and supporting local and external machine learning systems with a wider range of high quality data and analysis.	Medical Research Council	Fellowship	306714.0GBP
424	Dr Courtin Emilie	UNIVERSITAIR MEDISCH CENTRUM UTRECHT	2013-08-01	2018-08-01	Intracranial COnnection with Neural Networks for Enabling Communication in Total paralysis	iCONNECT aims to give severely paralyzed people the means to communicate by merely imagining to talk or make hand gestures. Imagining specific movements generates spatiotemporal patterns of neuronal activity in the brain which I intend to record and decode with an intracranial Brain-Computer Interface (BCI) system. Many people suffer from partial or full loss of control over their body due to stroke, disease or trauma, and this will increase with population ageing. With both duration and quality of life beyond 60 increasing in the western world, more and more people will suffer from the consequences of function loss (mostly stroke) with the prospect of living for decades with the handicap, and will stand to benefit from restorative technology that has yet to be developed. I believe that functionality can be restored with brain implants. My goal is to develop a BCI that can interpret activity patterns on the surface of the brain in real-time. For this we need to discover how the brain codes for (imagined) actions, how codes can be captured and decoded and how an intracranial BCI system impacts on a user. I will use state of the art techniques (7 Tesla MRI and electrocorticography, ECoG) to explore brain codes and develop decoding strategies. Interactions between user and implanted device will be studied in paralyzed people. I will directly link decoded movements to animated visual feedback of the same body part, expecting to induce a feeling of ownership of the animation, and thereby a sense of actual movement. This research is only possible because of the latest developments in imaging of human brain activity, machine learning techniques, and micro systems technology. My lab is unique in bringing together all these techniques. Success of the project will lead to deeper understanding of how sensorimotor functions are represented in the human brain. The ability to ?read' the brain will add a new dimension to the field of neural prosthetics.	European Research Council	Advanced Grant	2498829.0EUR
425	Professor Gleeson Fergus	Oxford University Hospitals NHS Foundation Trust	2017-10-01	2021-09-30	IDEAL: Artificial Intelligence and Big Data for Early Lung Cancer Diagnosis	None	National Institute for Health Research (Department of Health)	Full Grant	1425634.0GBP
426	Dr. WASSERMANN Demian	National Institute for Research in Computer Science and Automatic Control (INRIA)	2018-03-01	2023-02-28	Accelerating Neuroscience Research by Unifying Knowledge Representation and Analysis Through a Domain Specific Language	Neuroscience is at an inflection point. The 150-year old cortical specialization paradigm, in which cortical brain areas have a distinct set of functions, is experiencing an unprecedented momentum with over 1000 articles being published every year. However, this paradigm is reaching its limits. Recent studies show that current approaches to atlas brain areas, like relative location, cellular population type, or connectivity, are not enough on their own to characterize a cortical area and its function unequivocally. This hinders the reproducibility and advancement of neuroscience. Neuroscience is thus in dire need of a universal standard to specify neuroanatomy and function: a novel formal language allowing neuroscientists to simultaneously specify tissue characteristics, relative location, known function and connectional topology for the unequivocal identification of a given brain region. The vision of NeuroLang is that a unified formal language for neuroanatomy will boost our understanding of the brain. By defining brain regions, networks, and cognitive tasks through a set of formal criteria, researchers will be able to synthesize and integrate data within and across diverse studies. NeuroLang will accelerate the development of neuroscience by providing a way to evaluate anatomical specificity, test current theories, and develop new hypotheses. NeuroLang will lead to a new generation of computational tools for neuroscience research. In doing so, we will be shedding a novel light onto neurological research and possibly disease treatment and palliative care. Our project complements current developments in large multimodal studies across different databases. This project will bring the power of Domain Specific Languages to neuroscience research, driving the field towards a new paradigm articulating classical neuroanatomy with current statistical and machine learning-based approaches.	European Research Council	Starting Grant	1497045.0EUR
427	Dr. WASSERMANN Demian	University of Zurich	2017-11-01	2019-01-31	EEG based microsleep episode detection in the maintenance of wakefulness test and the driving simulator using a machine learning approach	Road traffic injuries are the leading cause of death for those aged 15-29 years. Excessive daytime sleepiness (EDS) is estimated to be the underlying cause in up to 15-20% of motor vehicle accidents (MVA), and is most often caused by socially induced sleep deprivation or poor sleep hygiene in otherwise healthy individuals, followed by medical disorders, or the intake of drugs. Methods for reliably objectifying sleepiness are urgently sought, primarily for sleepiness detection while driving but also for predicting the risk of sleepiness induced accidents during laboratory assessments of patients. The EEG is widely recognized as the gold standard for determining sleep-wake stages and their sudden, as well as gradual, changes. In the clinical and scientific context, standard EEG scoring criteria are generally applied, in which sleep is scored based on 30-s epochs. These sleep criteria consider neither the occurrence of microsleep episodes (MSE) nor the local aspects of sleep demonstrated in both animals and humans. Falling asleep is a gradual, not a sudden, process and shows fluctuations between waking and sleep. Particularly when assessing objective sleepiness in the context of driving ability, MSE of short duration originating in any brain area become an important criterion.We aim to characterize and identify MSE by defining visual scoring rules for MSE as short as 1 s, extracting relevant features based on quantitative EEG analyses, and by developing machine learning algorithms to detect MSE. First, we will only consider occipital EEG leads but will extend our analyses to central EEG leads in a second step. Our algorithms will be trained and verified on MWT data of patients and subsequently applied to MWT data of sleep deprived healthy individuals, and simulated driving conditions. In the driving simulator, we will investigate the association of MSE with impaired driving performance and spontaneously perceived sleepiness (SPS). In the context of fitness to drive assessments, one generally assumes that the perception of sleepiness precedes the occurrence of MSE. Our approach should allow to validate or disprove such an assumption. Previous studies mainly assessed relationships based on mean values or pooled data. Our aim is to take inter-individual variations into account and relate single events (i.e. MSE) with quantitative EEG measures. Therefore, we intend to track the sleep-wake transition zone (e.g. occurrence of MSE, vigilance fluctuations) with high temporal resolution on a second-by-second basis. In summary, we intend to develop and formulate the first standardized MSE scoring rules worldwide and to establish reliable automatic detection algorithms. This will have a major impact in sleep medicine and research and open new areas of research and diagnostic procedures. Since sleepiness is among the most frequent causes of car accidents, and an important risk factor for train and truck drivers and in many surveillance tasks such as air traffic or nuclear power plant control, reliable diagnostic tools to judge fitness to drive or fitness on the job become essential for reducing car and work accidents, catastrophic incidents, and the immense costs related to excessive daytime sleepiness.	Swiss National Science Foundation	Project funding (Div. I-III)	217848.0CHF
428	Dr. WASSERMANN Demian	ETH Zurich	2010-08-01	2013-07-31	The evolution of proteins with tandem repeats: a large-scale study of rates, selective forces, complex patterns and associations with human disease	Protein repeats are predominantly found in muscle, brain, synaptic cell adhesion proteins, but underrepresented in very basic cellular functions. Over the last years, the important and versatile roles of tandem repeats have been documented by an increasing number of studies. Repeat lengths vary from homorepeats (eg. in the Huntington disease gene) to long repeats with multiple domains (eg. the cytoskeletal protein titin). Functional classification of proteins with tandem repeats suggests that they are often involved in multiple binding, and so facilitate protein-protein interactions and are required for the formation of multi-protein complexes. Protein repeats tend to have structural roles in proteins with fundamental biological functions, including survival facilitation. Antigenic and other virulence related proteins such as toxins and allergens may also be encoded by sequences with repeats. Proteins with tandem repeats are frequently associated with infectious, neurodegenerative diseases. Along this line, the discovery of repeat-containing proteins and their structure-function study promise to be a fertile direction for research leading to the identification of targets for new medicaments and vaccines. The systematic bioinformatics analysis of protein repeats in genomes can provide a global view on these motifs, their structures, functions and evolution. This should facilitate a significant improvement of our understanding of structural and functional changes during the evolution of proteins with repeats and their protein-protein interaction networks.Despite several studies of protein repeats, the fundamental questions about the evolution and roles of repeats are far from being answered. Hardly any previous studies went beyond the classification-type summaries of single repeats and proteins containing them. This project aims to make a major contribution to the understanding of repeat-containing proteins, their structure, function and the dynamics of protein-protein interactions. • The project will assemble the most complete set of protein repeats using a novel algorithm for the identification of repeats that have diverged via substitution and indel events, based on the K-clustering-based approach. Data will be made publicly available through our Protein Repeat DataBase (PRDB). • We will study biological forces shaping the mutational landscape of proteins with repeats. Several purpose-built novel codon substitution models will be developed for this task. For example, using these models we will assess selective forces modulating the number and length of repeats. We will obtain estimates of rates of repeat duplications (and mutations) relative to point mutation. The project will seek to compare and characterize the selective forces acting on the globular and the repetitive parts of the proteins as well as the composition bias, recombination and co-evolutionary forces. PI’s expertise in codon models will be important for the proposed development of novel tailor-made Markov models, which will consequently be used to assess selective forces modulating the number and length of repeats, and the rates of evolution of the globular part vs the periodic part. The developed methods will be implemented in a user-friendly software package and made available to the users.• Complementary information (expression, disease associations, biochemical pathways, etc.) will be assembled for each repeat protein. Together with evolutionary rates and selection estimates, these data will be analyzed using machine learning and pattern discovery techniques. A particular emphasis will be on the trends observed for disease-associated genes. Do the disease genes exhibit unusual evolutionary patterns, selective pressures, expression profiles, etc. compared to genes lacking such associations? Prediction of genes with roles in disease will have invaluable medical applications. • Important examples of disease-related genes will be analyzed in more detail, applying most suitable evolutionary models to the expanded datasets. In particular, codon models that we will develop will be used to evaluate the selective pressure acting on the length of homorepeats during their evolution. These models will assess protein fitness changes as a result of altered numbers of repeats or their lengths. While the conservation is observed in protein coding homorepeats, their length and variability in population is not conserved, suggesting differential selection. The codon models will provide powerful means to evaluate various selection scenarios. Structural modeling techniques will be applied to a set of important proteins (collaboration with Dr Kajava, CRBM-CNRS, Montpellier, France), which will also be studied further using experimental techniques. For the empirical studies we will work in collaboration with biophysicists, experimental biologists and biochemists (Dr Padilla, CBS-CNRS, Montpellier, France).• We will study repeat-containing proteins in pathogenic organisms. Due to their binding properties, proteins with repeats are potential candidate genes influencing the pathogeneicity and disease progression. Indeed, many repeat-containing proteins are expressed on the surface of rapidly evolving pathogens, such as bacteria, viruses that are serious human pathogens or have agricultural or environmental importance. Their antigenic proteins are under strong selective pressure to escape host immune response. A combination of adaptive mutations in such genes may be at the origin of emerging infectious diseases. We will aim to identify the genes and residues that may be used as drug/vaccine targets, essential for controlling and preventing epidemics. Studies of protein with repeats in plant or animal pathogens, are of agricultural and environmental importance. • All assembled data on repeat-containing proteins will be available via mirror web-server. Our database PRDB will be regularly updated, and will incorporate basic tools for categorized searches of repeats, their basic analysis and organization. The goal is to create an integrated data resource for proteins with repeats. Such approach will enable researchers to combine the multitude of available resources and consider them in their integrity. This will facilitate a more efficient and rapid progress in studying structure-function relationship of proteins with tandem repeats and subsequent medical, agricultural and environmental applications. The project will lay a sound foundation for further work on structure of proteins with repeats. The discovery process has high potential and may open new aspects of theoretical biology, molecular evolution, protein structure, and medical related research.	Swiss National Science Foundation	Project funding (Div. I-III)	260278.0CHF
429	Dr. WASSERMANN Demian	University of Fribourg	2012-10-01	2013-09-30	Coestimating selection and demography	Detecting signatures of past selective events provides insights into the evolutionary history of a species by evidencing adaptive events. The identification of molecular targets of selection in humans, for instance, pinpoints biologically relevant differences between us and other apes. In addition, inferences regarding selection provide important functional information by elucidating the interaction between genotype and phenotype. Since positions in the genome that are under selection are functionally important by definition, detecting signatures of selection has also been used extensively to identify functional regions or protein residues. Finally, inferring the molecular locations at which selection is acting may help us to predict responses to selective pressures in organisms such as viruses, which would revolutionize the management of pandemics and the development of drugs. Unfortunately, the demographic history of a population or species is a major confounding factor when inferring past selective events. Indeed, neutrality tests are very sensitive to violations of the underlying demographic assumption of constant population size - and it is often impossible to distinguish between adaptive or demographic processes in putatively identified regions. After a population bottleneck, for instance, false positive rates of up to 90\% are not uncommon. Current approaches to deal with this problem rely on the assumption that selection is acting on a few loci only, while demography affects all loci equally. A two step procedure has been proposed in which a set of selectively neutral loci are used to calibrate a demographic model against which putatively selected loci are compared. However, recent evidence suggests that selection may be common in the genome of many organisms and a priori knowledge on the neutrality of markers is often difficult to obtain. As a result, this approach suffers from high false negative rates - not to mention relying on very strong assumptions regarding the pervasiveness of adaptive mutations.There is currently no flexible approach to estimate demography and selection jointly. However, recent advances in computational approaches offer new hopes to tackle such an inference. A particularly promising approach is Approximate Bayesian Computation (ABC), a technique to sidestep analytical likelihood calculations with simulations. To this end, ABC has been used to infer a wide range of evolutionary scenarios such as population bottlenecks, population splits and migration, but also to distinguish between a classic selective sweep and recurrent selective events.Here I propose to develop an ABC framework to estimate demography and selection jointly, and to apply it to a variety of organisms with very different evolutionary histories. Major new developments are needed to reach this goal. Firstly, the application of ABC to large scale data sets is tenuous without novel techniques to reduce the computational effort required. I will address this through various innovations including new ABC-MCMC algorithms with increased performance, an efficient recycling of simulations, and extending recent approaches to hybridize ABC with traditional full-likelihood methods. Such a hybridization will enable us to profit from the rich literature on full-likelihood solutions to simpler problems and to take linkage properly into account. Secondly, the joint inference of demography and selection calls for new models and new sets of informative summary statistics. I will approach this challenge by integrating models established to infer either selection or demography alone and through current techniques to explore a large space of summary statistics, such as PLS or machine learning algorithms. The proposed innovations will allow me to work towards answering some of the most controversial questions in evolutionary biology, namely the importance of adaptation in shaping genomic variation. I will approach these questions by inferring genome-wide selection coefficients from unique data sets of four organisms representing various selective and demographic histories: Deer mice (Peromyscus maniculatus), HCM viruses, Drosophila melanogaster and humans. These estimates will not only have broad implications for the management of pandemics or the development of new drugs, but will also greatly improve our understanding of the mode and tempo of the process of adaptation.	Swiss National Science Foundation	Ambizione	214250.0CHF
430	Dr. WASSERMANN Demian	Langer lab Department of Chemical Engineering Massachusetts Institute of Technology	2016-07-01	2017-12-31	Active learning for late-stage drug design	Challenges in solubility, stability, and absorption require formulation development that can significantly delay the introduction of promising drugs to patients. Active machine learning allows for rapid model development by implementing a feedback-driven artificial intelligence that puts the machine in charge of requesting additional data for iteratively improving predictive accuracy. Coupled to novel organ-on-a-chip assay systems, active learning workflows have the potential to transform formulation development and generate in silico models that help streamlining translational drug design. This proposal aims to establish active learning workflows for rapidly generating models for various objectives relevant to formulation development using intestine-on-a-chip systems. In addition to predictive models, such workflows will generate vast amounts of biological data to deepen our understanding of organ-on-a-chip systems as well as provide theoretical insights into the learning patterns of artificial intelligence.	Swiss National Science Foundation	Early Postdoc.Mobility	214250.0CHF
431	Dr. WASSERMANN Demian	University of Zurich	2017-02-01	2019-01-31	ICU-Cockpit: IT platform for multimodal patient monitoring and therapy support in intensive care and emergency medicine	In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.	Swiss National Science Foundation	NRP 75 Big Data	573853.0CHF
432	Mr Schultesz Ferenc	City, University of London	2016-07-04	2016-09-03	Pattern Classification of attention deficit hyperactivity disorder: Integrating functional magnetic resonance imaging and genetics data	In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.	Wellcome Trust	Vacation Scholarships	2000.0GBP
433	Dr de Goede Christan	Lancashire Teaching Hospitals NHS Foundation Trust	2017-12-01	2021-01-31	MyPad – Intelligent Bladder Pre-void Alerting System	In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.	National Institute for Health Research (Department of Health)	Full Grant	477200.0GBP
434	Dr Caminati Marco	University of St Andrews	2018-02-14	2021-02-13	Stochastic models to enable tailoring of medications to patients with multiple morbidities	In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.	Medical Research Council	Fellowship	289274.0GBP
435	Dr Desrivieres Sylvane	King's College London	2017-09-01	2019-08-31	Neurobiological underpinning of eating disorders: integrative biopsychosocial longitudinal analyses in adolescents	In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.	Medical Research Council	Research Grant	230792.0GBP
436	Professor Kadioglu Aras	University of Liverpool	2017-07-10	2020-12-31	Mechanisms for acquisition and transmission of successful antibiotic resistant pneumococcal clones pre- and post-vaccination	In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.	Medical Research Council	Research Grant	273163.0GBP
437	Dr Meyer Nicholas	King's College London	2016-10-01	2020-10-31	Detecting early signs of relapse in psychosis using remote monitoring technology	In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.	Medical Research Council	Fellowship	243384.0GBP
438	Dr Mukherjee Sach	MRC Biostatistics Unit	2014-03-01	2016-11-30	Statistics and machine learning for precision medicine	In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.	Medical Research Council	Unit	243384.0GBP
439	Dr Waithe Dominic	University of Oxford	2018-04-01	2021-03-31	Quantitative and Real-Time Image Analysis for Advanced Light Microscopy.	In neurocritical care, patient monitoring is based not only on standard intensive care monitoring, but also on numerous data obtained from very complex pathophysiological changes in the human brain. The medical staff often cannot integrate the huge amount of clinical data continuously generated by different devices. The amount of available data per patient is enormous (petabyte), and the data are multimodal. Examples are recordings from electrocardiography, artificial ventilation, electroencephalography, hemodynamics, metabolism and video monitoring. The lack of data integration and usability is one of the major reasons why only a small part of the knowledge that physicians use in this field is evidence based. The growth of knowledge in medicine, as well as personalized medicine including genotype data is no longer manageable for physicians alone in daily clinical practice.Machine learning and artificial intelligence, which enable for semi-automatic analysis of these complex and big data, are fundamental tools. Started in 2014, ICU-Cockpit is a joint research project between the University, ETH and IBM Research Zurich, in order to create an integrated platform for patient monitoring and therapy support. A framework based on data analysis, machine learning and modeling will allow for therapy support resulting in a better outcome for patients. ICU Cockpit can trigger a fundamental change in safety culture and standard operating procedures in daily emergency and intensive care medicine as well as telemedicine, and opens up enormous potential for clinical studies and scientific evidence.	Medical Research Council	Fellowship	345783.0GBP
440	Dr Hirst Yasemin	University College London	2016-05-01	2017-12-31	CRUK Guardian Angels	Background: Most patients with cancer present with symptoms / via symptomatic routes (including emergency, urgent two week GP referrals or other – non-urgent- referrals). One year survival following the cancer diagnosis relies on early detection.Helping doctors to suspect cancer after patients have presented with symptoms is obviously crucial – but at the same time it is also crucial to support patients to seek help soon after symptoms have developed. Delays in patients' symptom appraisal and help-seeking have usually been studied retrospectively in surveys and interviews. However, there is vast amount of existing data available in the digital and commercial platforms to be used in prospective and complex study designs. To some extent, individual (e.g. google searches, symptom checkers), consumer (e.g. loyalty card) and clinical data (GP visits, referral etc) have previously been a focus of research but have not yet been investigated collectively to understand delays in symptom appraisal and help-seeking in cancer alarm symptoms. Aims: The proposed research aims to investigate delays in symptom appraisal and subsequent help-seeking behaviour using 'Big Data'. The future aim is to link individual, consumer and clinical datasets via a secure mobile phone application, and assess whether it is possible to identify signs of cancer looking at behaviour change and self-assessment using machine learning algorithms. Methods: The project includes three multidisciplinary (Health Psychology, Computing, Advanced Statistics and Cancer Epidemiology) studies that will (a) explore the lay language that is being used to articulate cancer symptoms in order to generate a dictionary of possible online search terms, (b) investigate the associations between symptom perception and behaviour change using consumer data, e.g. continuous purchase of cough medicine etc., and (c) explore public perceptions on data linkage to monitor cancer risk/warning signs. The first two studies will be using data-mining techniques, and the last study will conduct focus groups using inductive thematic analysis to assess the acceptability of using data linkage to monitor indicators of cancer symptoms. How the results of this research will be used; Beyond our future ambition, this project aims to produce a glossary based on lay expressions of cancer symptoms in order to better understand online information-seeking behaviours. We also aim to provide a detailed report on the availability of data from different resources, and evaluate the feasibility of the prospective studies. The results of this project will initiate further cancer research into understanding the ever increasing volumes of data across disciplines.	Cancer Research UK	PRC - Early Diagnosis - Innovation Grant	345783.0GBP
441	Dr Baranowski Elizabeth	University of Birmingham	2018-09-11	2020-12-10	Steroid Metabolomics for Diagnosis and Monitoring of Inborn Steroidogenesis Disorders	Background: Most patients with cancer present with symptoms / via symptomatic routes (including emergency, urgent two week GP referrals or other – non-urgent- referrals). One year survival following the cancer diagnosis relies on early detection.Helping doctors to suspect cancer after patients have presented with symptoms is obviously crucial – but at the same time it is also crucial to support patients to seek help soon after symptoms have developed. Delays in patients' symptom appraisal and help-seeking have usually been studied retrospectively in surveys and interviews. However, there is vast amount of existing data available in the digital and commercial platforms to be used in prospective and complex study designs. To some extent, individual (e.g. google searches, symptom checkers), consumer (e.g. loyalty card) and clinical data (GP visits, referral etc) have previously been a focus of research but have not yet been investigated collectively to understand delays in symptom appraisal and help-seeking in cancer alarm symptoms. Aims: The proposed research aims to investigate delays in symptom appraisal and subsequent help-seeking behaviour using 'Big Data'. The future aim is to link individual, consumer and clinical datasets via a secure mobile phone application, and assess whether it is possible to identify signs of cancer looking at behaviour change and self-assessment using machine learning algorithms. Methods: The project includes three multidisciplinary (Health Psychology, Computing, Advanced Statistics and Cancer Epidemiology) studies that will (a) explore the lay language that is being used to articulate cancer symptoms in order to generate a dictionary of possible online search terms, (b) investigate the associations between symptom perception and behaviour change using consumer data, e.g. continuous purchase of cough medicine etc., and (c) explore public perceptions on data linkage to monitor cancer risk/warning signs. The first two studies will be using data-mining techniques, and the last study will conduct focus groups using inductive thematic analysis to assess the acceptability of using data linkage to monitor indicators of cancer symptoms. How the results of this research will be used; Beyond our future ambition, this project aims to produce a glossary based on lay expressions of cancer symptoms in order to better understand online information-seeking behaviours. We also aim to provide a detailed report on the availability of data from different resources, and evaluate the feasibility of the prospective studies. The results of this project will initiate further cancer research into understanding the ever increasing volumes of data across disciplines.	Medical Research Council	Fellowship	167889.0GBP
442	Dr Fujita Andre	University of Sao Paulo	2018-03-31	2020-03-20	A model-based graph clustering approach for autism stratification	Autism spectrum disorder (ASD) is usually diagnosed by behavioral analyses and currently there is no objective test. Even the latest computational methods (e.g. support vector machine and deep learning) based on analyses of bloodoxygen-level dependent (BOLD) signal yield classification accuracies of about 60 to 70%, which are unsatisfactory for clinical use. However, if we focus on a group of individuals sharing a specific phenotype, we obtain better results, probably because there is more than one “type” of ASD. Thus, our main goal is to stratify the ASD into subgroups, not by the direct analysis of the BOLD signal as it is usually done, but by the functional brain network (FBN) based on the BOLD signal. The rationale is that ASD can be explained by the differences in how neurons interact. However, there are at least three technical drawbacks with this approach: (i) to the best of our knowledge, there is no method to cluster FBNs (note that this problem is different of the clustering of the vertices of the network as done by the spectral clustering algorithm); (ii) even individuals belonging to the same group present different FBNs (intrinsic fluctuation), which makes the analysis using standard computational methods unfruitful; and (iii) confounders such as age, gender, and other clinical parameters may affect the clustering structure. To overcome these problems, we will represent the FBNs as random graphs and assume that probabilistic models (e.g. Watts-Strogatz and Barabási-Albert models) generate the FBNs. Then, we will define that FBNs generated by the same model belong to the same group while FBNs generated by different models belong to different groups (similar to the Gaussian mixture model). Confounders’ effects will be removed by covarying the probabilistic distributions. We hope this stratification contribute for a better understanding of the mechanisms underlying ASD. To develop this method, Dr. Fujita of USP contacted Dr. Mourao-Miranda of UCL to complement his expertise in classification methods. Dr. Fujita works on statistical methods on graphs while Dr. Mourao-Miranda is specialist in machine learning with applications in psychiatric disorders. To train Dr. Fujita’s group in the field of machine learning, the partners will organize two courses, one at UCL and another at USP. Moreover, two Ph.D. candidates from USP will be sent for six months each one to UCL to study cutting-edge classification techniques. This project is essential to obtain novel insights, solidify this cooperation, and improve the quality of this research.	The Academy of Medical Sciences	Newton Advanced Fellowship	61814.0GBP
443	Professor Zaikin Alexey	University College London	2019-01-15	2022-01-14	HSM: Construction of graph-based network longitudinal algorithms to identify screening and prognostic biomarkers and therapeutic targets (GBNLA)	Autism spectrum disorder (ASD) is usually diagnosed by behavioral analyses and currently there is no objective test. Even the latest computational methods (e.g. support vector machine and deep learning) based on analyses of bloodoxygen-level dependent (BOLD) signal yield classification accuracies of about 60 to 70%, which are unsatisfactory for clinical use. However, if we focus on a group of individuals sharing a specific phenotype, we obtain better results, probably because there is more than one “type” of ASD. Thus, our main goal is to stratify the ASD into subgroups, not by the direct analysis of the BOLD signal as it is usually done, but by the functional brain network (FBN) based on the BOLD signal. The rationale is that ASD can be explained by the differences in how neurons interact. However, there are at least three technical drawbacks with this approach: (i) to the best of our knowledge, there is no method to cluster FBNs (note that this problem is different of the clustering of the vertices of the network as done by the spectral clustering algorithm); (ii) even individuals belonging to the same group present different FBNs (intrinsic fluctuation), which makes the analysis using standard computational methods unfruitful; and (iii) confounders such as age, gender, and other clinical parameters may affect the clustering structure. To overcome these problems, we will represent the FBNs as random graphs and assume that probabilistic models (e.g. Watts-Strogatz and Barabási-Albert models) generate the FBNs. Then, we will define that FBNs generated by the same model belong to the same group while FBNs generated by different models belong to different groups (similar to the Gaussian mixture model). Confounders’ effects will be removed by covarying the probabilistic distributions. We hope this stratification contribute for a better understanding of the mechanisms underlying ASD. To develop this method, Dr. Fujita of USP contacted Dr. Mourao-Miranda of UCL to complement his expertise in classification methods. Dr. Fujita works on statistical methods on graphs while Dr. Mourao-Miranda is specialist in machine learning with applications in psychiatric disorders. To train Dr. Fujita’s group in the field of machine learning, the partners will organize two courses, one at UCL and another at USP. Moreover, two Ph.D. candidates from USP will be sent for six months each one to UCL to study cutting-edge classification techniques. This project is essential to obtain novel insights, solidify this cooperation, and improve the quality of this research.	Medical Research Council	Research Grant	469754.0GBP
444	Ao. Univ. Prof.Dr. KAUTZKY-WILLER Alexandra	Medical University of Vienna	2019-01-21	2022-01-20	Gender Outcomes and Well-being Development Group (GOING-FWD GNP78)	Gender Outcomes INternational Group: to Further Well-being Development (GOING-FWD) Background: Beyond biological sex, gender is increasingly recognized as a pivotal determinant of health. However, there are no standardized gender measurements. We hypothesize that gender-related factors and their effect will vary substantially between countries and diseases. Aims: The overarching aims of this large Consortium are to integrate sex and gender dimensions in applied health research, to evaluate their impact on clinical cost-sensitive outcomes and patients reported outcomes related to quality of life in noncommunicable diseases including cardiovascular disease, metabolic disease, chronic kidney disease and neurological disease. We also aim to construct innovative ways to disseminate the application of gender measurement towards personalized approaches to chronic disease prevention, diagnosis and treatment. Methods: With a five-country transatlantic network comprised of 30 investigators, we will benchmark innovative solutions to measure gender in retrospective cohorts. Based on consensus, we will develop a framework to identify gender-related factors, as well as cost-sensitive and patients reported outcomes and measure their associations in 32 accessible cohorts of patients affected by cardiovascular, chronic kidney and neurological diseases and metabolic syndrome. Large database analysis and when appropriate machine learning approaches will allow the derivation of pan and within country disease specific gender scores which will be validated through e-Health and m-Health applications in prospective disease groups. Educational modules will be developed to promote awareness, implementation and dissemination. Innovation: As a five-country multidisciplinary Consortium with access to granular large databases, we are uniquely positioned to harness an innovative methodology that will provide a framework to close gender gaps in chronic disease management and promote knowledge transfer in the scientific community and clinical practice.	Austrian Science Fund FWF	02 International programmes	298552.8EUR
445	Dr Li Ke	University of Exeter	2019-05-01	2023-04-30	Transfer Optimisation System for Adaptive Automated Nature-Inspired Optimisation	Hard optimisation problems are ubiquitous across the breadth of science, engineering and economics. For example, in water system planning and management, water companies are often interested in optimising several system performance measures of their infrastructures in order to provide sustainable and resilient water/wastewater services that are able to cope with and recover from disruption, as well as wider challenges brought by climate change and population increase. As a classic discipline, significant advances in both theory and algorithms have been achieved in optimisation. However, almost all traditional optimisation solvers, ranging from classic methods to nature-inspired computational intelligence techniques, ignore some important facts: (i) real-world optimisation problems seldom exist in isolation; and (ii) artificial systems are designed to tackle a large number of problems over their lifetime, many of which are repetitive or inherently related. Instead, optimisation is run as a 'one-off' process, i.e. it is started from scratch by assuming zero prior knowledge each time. Therefore, knowledge/experience from solving different (but possibly related) optimisation exercises (either previously completed or currently underway), which can be useful for enhancing the target optimisation task at hand, will be wasted. Although the Bayesian optimisation considers incorporating some decision maker's knowledge as a prior, the gathered experience during the optimisation process is discarded afterwards. In this case, we cannot expect any automatic growth of their capability with experience. This practice is counter-intuitive from the cognitive perspective where humans routinely grow from a novice to domain experts by gradually accumulating problem-solving experience and making use of existing knowledge to tackle new unseen tasks. In machine learning, leveraging knowledge gained from related source tasks to improve the learning of the new task is known as transfer learning, an emerging field that considerable success has been witnessed in a wide range of application domains. There have been some attempts on applying transfer learning in evolutionary computation, but they do not consider the optimisation as a closed-loop system. Moreover, the recurrent patterns within problem-solving exercises have been discarded after optimisation, thus experience cannot be accumulated over time. The proposed research will develop a revolutionary general-purpose optimiser (as known as transfer optimisation system) that will be able to learn knowledge/experience from previous optimisation process and then autonomously and selectively transfer such knowledge to new unseen optimisation tasks. The transfer optimisation system places adaptive automation at the heart of the development process and explores novel synergies at the crossroads of several disciplines including nature-inspired computation, machine learning, human-computer interaction and high-performance parallel computing. The outputs will bring automation in industry, including an optimised/shortened production cycle, reduced resource consumption and more balanced and innovative products, which have great potentials to result in economic savings and an increase of turnover. The proposed methods will be rigorously evaluated by the industrial partners, first in water industry and will be expanded to a boarder range of sectors which put the optimisation at the heart of their regular production/management process (e.g. renewable energy, healthcare, automotive, appliance and medicine manufacturers).	Medical Research Council	Fellowship	1097642.0GBP
446	Professor Hogg Claire	Royal Brompton and Harefield NHS Foundation Trust	2019-03-01	2021-02-28	Improving Primary Ciliary Dyskinesia diagnosis using artificial intelligence.	Hard optimisation problems are ubiquitous across the breadth of science, engineering and economics. For example, in water system planning and management, water companies are often interested in optimising several system performance measures of their infrastructures in order to provide sustainable and resilient water/wastewater services that are able to cope with and recover from disruption, as well as wider challenges brought by climate change and population increase. As a classic discipline, significant advances in both theory and algorithms have been achieved in optimisation. However, almost all traditional optimisation solvers, ranging from classic methods to nature-inspired computational intelligence techniques, ignore some important facts: (i) real-world optimisation problems seldom exist in isolation; and (ii) artificial systems are designed to tackle a large number of problems over their lifetime, many of which are repetitive or inherently related. Instead, optimisation is run as a 'one-off' process, i.e. it is started from scratch by assuming zero prior knowledge each time. Therefore, knowledge/experience from solving different (but possibly related) optimisation exercises (either previously completed or currently underway), which can be useful for enhancing the target optimisation task at hand, will be wasted. Although the Bayesian optimisation considers incorporating some decision maker's knowledge as a prior, the gathered experience during the optimisation process is discarded afterwards. In this case, we cannot expect any automatic growth of their capability with experience. This practice is counter-intuitive from the cognitive perspective where humans routinely grow from a novice to domain experts by gradually accumulating problem-solving experience and making use of existing knowledge to tackle new unseen tasks. In machine learning, leveraging knowledge gained from related source tasks to improve the learning of the new task is known as transfer learning, an emerging field that considerable success has been witnessed in a wide range of application domains. There have been some attempts on applying transfer learning in evolutionary computation, but they do not consider the optimisation as a closed-loop system. Moreover, the recurrent patterns within problem-solving exercises have been discarded after optimisation, thus experience cannot be accumulated over time. The proposed research will develop a revolutionary general-purpose optimiser (as known as transfer optimisation system) that will be able to learn knowledge/experience from previous optimisation process and then autonomously and selectively transfer such knowledge to new unseen optimisation tasks. The transfer optimisation system places adaptive automation at the heart of the development process and explores novel synergies at the crossroads of several disciplines including nature-inspired computation, machine learning, human-computer interaction and high-performance parallel computing. The outputs will bring automation in industry, including an optimised/shortened production cycle, reduced resource consumption and more balanced and innovative products, which have great potentials to result in economic savings and an increase of turnover. The proposed methods will be rigorously evaluated by the industrial partners, first in water industry and will be expanded to a boarder range of sectors which put the optimisation at the heart of their regular production/management process (e.g. renewable energy, healthcare, automotive, appliance and medicine manufacturers).	National Institute for Health Research (Department of Health)	Full Grant	341942.0GBP
447	Professor Jefferson Emily	University of Dundee	2019-08-01	2024-07-31	MICA: InterdisciPlInary Collaboration for efficienT and effective Use of clinical images in big data health care RESearch: PICTURES	Hard optimisation problems are ubiquitous across the breadth of science, engineering and economics. For example, in water system planning and management, water companies are often interested in optimising several system performance measures of their infrastructures in order to provide sustainable and resilient water/wastewater services that are able to cope with and recover from disruption, as well as wider challenges brought by climate change and population increase. As a classic discipline, significant advances in both theory and algorithms have been achieved in optimisation. However, almost all traditional optimisation solvers, ranging from classic methods to nature-inspired computational intelligence techniques, ignore some important facts: (i) real-world optimisation problems seldom exist in isolation; and (ii) artificial systems are designed to tackle a large number of problems over their lifetime, many of which are repetitive or inherently related. Instead, optimisation is run as a 'one-off' process, i.e. it is started from scratch by assuming zero prior knowledge each time. Therefore, knowledge/experience from solving different (but possibly related) optimisation exercises (either previously completed or currently underway), which can be useful for enhancing the target optimisation task at hand, will be wasted. Although the Bayesian optimisation considers incorporating some decision maker's knowledge as a prior, the gathered experience during the optimisation process is discarded afterwards. In this case, we cannot expect any automatic growth of their capability with experience. This practice is counter-intuitive from the cognitive perspective where humans routinely grow from a novice to domain experts by gradually accumulating problem-solving experience and making use of existing knowledge to tackle new unseen tasks. In machine learning, leveraging knowledge gained from related source tasks to improve the learning of the new task is known as transfer learning, an emerging field that considerable success has been witnessed in a wide range of application domains. There have been some attempts on applying transfer learning in evolutionary computation, but they do not consider the optimisation as a closed-loop system. Moreover, the recurrent patterns within problem-solving exercises have been discarded after optimisation, thus experience cannot be accumulated over time. The proposed research will develop a revolutionary general-purpose optimiser (as known as transfer optimisation system) that will be able to learn knowledge/experience from previous optimisation process and then autonomously and selectively transfer such knowledge to new unseen optimisation tasks. The transfer optimisation system places adaptive automation at the heart of the development process and explores novel synergies at the crossroads of several disciplines including nature-inspired computation, machine learning, human-computer interaction and high-performance parallel computing. The outputs will bring automation in industry, including an optimised/shortened production cycle, reduced resource consumption and more balanced and innovative products, which have great potentials to result in economic savings and an increase of turnover. The proposed methods will be rigorously evaluated by the industrial partners, first in water industry and will be expanded to a boarder range of sectors which put the optimisation at the heart of their regular production/management process (e.g. renewable energy, healthcare, automotive, appliance and medicine manufacturers).	Medical Research Council	Research Grant	2851878.0GBP
448	Dr Walsh Simon	Imperial College of Science, Technology and Medicine	2019-02-01	2024-01-31	Fibrotic lung disease on high-resolution computed tomography: predicting disease behaviour using computer algorithms.	Hard optimisation problems are ubiquitous across the breadth of science, engineering and economics. For example, in water system planning and management, water companies are often interested in optimising several system performance measures of their infrastructures in order to provide sustainable and resilient water/wastewater services that are able to cope with and recover from disruption, as well as wider challenges brought by climate change and population increase. As a classic discipline, significant advances in both theory and algorithms have been achieved in optimisation. However, almost all traditional optimisation solvers, ranging from classic methods to nature-inspired computational intelligence techniques, ignore some important facts: (i) real-world optimisation problems seldom exist in isolation; and (ii) artificial systems are designed to tackle a large number of problems over their lifetime, many of which are repetitive or inherently related. Instead, optimisation is run as a 'one-off' process, i.e. it is started from scratch by assuming zero prior knowledge each time. Therefore, knowledge/experience from solving different (but possibly related) optimisation exercises (either previously completed or currently underway), which can be useful for enhancing the target optimisation task at hand, will be wasted. Although the Bayesian optimisation considers incorporating some decision maker's knowledge as a prior, the gathered experience during the optimisation process is discarded afterwards. In this case, we cannot expect any automatic growth of their capability with experience. This practice is counter-intuitive from the cognitive perspective where humans routinely grow from a novice to domain experts by gradually accumulating problem-solving experience and making use of existing knowledge to tackle new unseen tasks. In machine learning, leveraging knowledge gained from related source tasks to improve the learning of the new task is known as transfer learning, an emerging field that considerable success has been witnessed in a wide range of application domains. There have been some attempts on applying transfer learning in evolutionary computation, but they do not consider the optimisation as a closed-loop system. Moreover, the recurrent patterns within problem-solving exercises have been discarded after optimisation, thus experience cannot be accumulated over time. The proposed research will develop a revolutionary general-purpose optimiser (as known as transfer optimisation system) that will be able to learn knowledge/experience from previous optimisation process and then autonomously and selectively transfer such knowledge to new unseen optimisation tasks. The transfer optimisation system places adaptive automation at the heart of the development process and explores novel synergies at the crossroads of several disciplines including nature-inspired computation, machine learning, human-computer interaction and high-performance parallel computing. The outputs will bring automation in industry, including an optimised/shortened production cycle, reduced resource consumption and more balanced and innovative products, which have great potentials to result in economic savings and an increase of turnover. The proposed methods will be rigorously evaluated by the industrial partners, first in water industry and will be expanded to a boarder range of sectors which put the optimisation at the heart of their regular production/management process (e.g. renewable energy, healthcare, automotive, appliance and medicine manufacturers).	National Institute for Health Research (Department of Health)	Full Grant	1053348.0GBP
449	Dr Saxe Andrew	University of Oxford	2019-09-01	2024-08-31	Principles of Learning in Distributed Brain Networks	Hard optimisation problems are ubiquitous across the breadth of science, engineering and economics. For example, in water system planning and management, water companies are often interested in optimising several system performance measures of their infrastructures in order to provide sustainable and resilient water/wastewater services that are able to cope with and recover from disruption, as well as wider challenges brought by climate change and population increase. As a classic discipline, significant advances in both theory and algorithms have been achieved in optimisation. However, almost all traditional optimisation solvers, ranging from classic methods to nature-inspired computational intelligence techniques, ignore some important facts: (i) real-world optimisation problems seldom exist in isolation; and (ii) artificial systems are designed to tackle a large number of problems over their lifetime, many of which are repetitive or inherently related. Instead, optimisation is run as a 'one-off' process, i.e. it is started from scratch by assuming zero prior knowledge each time. Therefore, knowledge/experience from solving different (but possibly related) optimisation exercises (either previously completed or currently underway), which can be useful for enhancing the target optimisation task at hand, will be wasted. Although the Bayesian optimisation considers incorporating some decision maker's knowledge as a prior, the gathered experience during the optimisation process is discarded afterwards. In this case, we cannot expect any automatic growth of their capability with experience. This practice is counter-intuitive from the cognitive perspective where humans routinely grow from a novice to domain experts by gradually accumulating problem-solving experience and making use of existing knowledge to tackle new unseen tasks. In machine learning, leveraging knowledge gained from related source tasks to improve the learning of the new task is known as transfer learning, an emerging field that considerable success has been witnessed in a wide range of application domains. There have been some attempts on applying transfer learning in evolutionary computation, but they do not consider the optimisation as a closed-loop system. Moreover, the recurrent patterns within problem-solving exercises have been discarded after optimisation, thus experience cannot be accumulated over time. The proposed research will develop a revolutionary general-purpose optimiser (as known as transfer optimisation system) that will be able to learn knowledge/experience from previous optimisation process and then autonomously and selectively transfer such knowledge to new unseen optimisation tasks. The transfer optimisation system places adaptive automation at the heart of the development process and explores novel synergies at the crossroads of several disciplines including nature-inspired computation, machine learning, human-computer interaction and high-performance parallel computing. The outputs will bring automation in industry, including an optimised/shortened production cycle, reduced resource consumption and more balanced and innovative products, which have great potentials to result in economic savings and an increase of turnover. The proposed methods will be rigorously evaluated by the industrial partners, first in water industry and will be expanded to a boarder range of sectors which put the optimisation at the heart of their regular production/management process (e.g. renewable energy, healthcare, automotive, appliance and medicine manufacturers).	Wellcome Trust	Sir Henry Dale Fellowship	771226.0GBP
450	Dr Nellaker Christoffer	University of Oxford	2015-07-01	2019-07-30	Developing diagnostic methods for clinical genetics - phenotyping from faces in photos.	None	Medical Research Council	Fellowship	457586.0GBP
451	Dr Kiddle Steven	King's College London	2014-04-01	2017-03-31	Early identification of Alzheimer's disease: dynamic biomarkers for enrichment of trials	None	Medical Research Council	Fellowship	253519.0GBP
452	Dr Khastgir Siddartha	University of Warwick	2020-01-30	2024-01-29	Enabling a Novel Evaluation Continuum for Connected & Autonomous Vehicles (CAV)	The global Connected & Autonomous Vehicles (CAV) industry is estimated to be worth over £50billion (by 2035), with the UK CAV industry being projected over £3billion. Additionally, the UK Government's Industrial Strategy aims to bring fully autonomous cars without a human operator on the UK roads by 2021, one of the first countries in the world to achieve this. However, in order to realise this vision and the market potential, safe introduction of CAV is necessary, requiring significant research to overcome diverse barriers (technological, legislative and societal) associated with public deployment of CAV. While prototype CAV technologies have existed for some time now, ensuring the safety level of these technologies has been proving to be a hindrance to the commercialization of CAV technologies. The vision for CAV is coupled with the challenge of testing and safety analysis as it needs complex solutions to include interactions between a large number of variables and the environment. It is suggested that in order to prove that CAV are safer than human drivers, they will need to be driven for more than 11 billion miles. The vision for this fellowship is to support positioning the UK as the world leader in CAV research and innovation for a long lasting societal and economic benefit. This fellowship will develop pioneering testing methodologies and standards to enable robust and safe use of CAV with a focus on creating both fundamental knowledge and applied research methods and tools. At WMG, University of Warwick, UK, we have created a concept of the "evaluation continuum" for CAV, which involves using various environment like digital world, simulated environment, test track testing and real-world for testing. There are two aspects which are common to each of the evaluation continuum environments and also the focus areas of the fellowship research 1) Test Scenarios (input to the environment) 2) Safety Evidence (output of the environment). On the scenarios theme, ile the 11-billion-miles requirement has garnered a lot of publicity, the focus needs to be on what happens in those miles (i.e., smart miles which expose failures in CAV) and not on the number of miles themselves. As a part of this fellowship, three approaches will be explored to identify these smart miles. These include 1) using Machine Learning (ML) based methods including Bayesian Optimisation to create test cases for test scenarios, 2) Safety Of The Intended Functionality (SOTIF) (Innovative safety analysis of CAV) based test scenarios and 3) translating real-world data into executable test scenarios for a simulation tool. All these approaches will together contribute to the creation of a UK's National CAV Test Scenario Database, which will help coordinate the research work in various CAV projects part-funded by the UK Government and will prevent "reinventing of the wheel" in each of the projects with respect to test scenario identification. Industry trends in CAV suggest the widespread adoption of machine learning (ML) in the autonomous control systems. ML-systems by their structure are non-deterministic in nature, making the CAV system highly opaque in nature. Therefore, it is difficult to identify the reason of a failure in such ML-based systems and take the corrective measures. Thus, on the safety strand, fundamental research will be conducted as a part of this fellowship to explore how to make ML-based systems interpretable enabling us to explain the results. This is an essential requirement for safety of CAV due to the critical nature of their deployment and the mitigation of risk. In addition, the fellowship will also benefit from the fellow's first-hand experience as the UK's technical representative on the ISO standards committees, providing further insight and a clear route to deliver impact from the proposed research through the development of international standards, while also ensuring that the UK becomes a world leader in this area.	Medical Research Council	Fellowship	1110158.0GBP
453	Dr Oxtoby Neil	University College London	2020-01-01	2023-12-31	I-AIM: Individualised Artificial Intelligence for Medicine	Management and treatment of complex, chronic diseases such as Alzheimer's disease is one of the biggest challenges facing modern medicine. All clinical trials of investigational treatments for slowing or stopping the progression of Alzheimer's disease since 2003 have failed. This is likely due to the complexity and duration (decades) of Alzheimer's disease, coupled with the highly individual nature of the disease and its progression. Combined, this works against clinical trials by making it extremely difficult to identify and recruit a large group of individuals who are at the same stage of the same trajectory, and so who might benefit from a potential treatment. In principle, this challenge can be met by a set of modern computational approaches called data-driven disease progression modelling (D3PM), but some technological development is required first. D3PM aims to combine statistics with the latest developments in AI and data science to estimate disease signatures that describe how a progressive disease plays out from beginning to end. This active research field grew from basic supervised machine learning (pattern learning/recognition) to a range of phenomenological (top-down) models, and mechanistic (bottom-up) models that incorporate a range of AI tools including unsupervised machine learning (pattern discovery). D3PM signatures have shown promise for estimating severity and predicting progression in neurodegenerative diseases such as Alzheimer's disease, but they currently lack in individual level precision, and mechanistic rigour. This research and innovation project is a unique combination of technology development and translational product development: a series of novel technological developments for individualising D3PM and expanding mechanistic modelling; and translational efforts to develop drug-development tools based on this next-generation technology. In combination, this work will speed up drug-development by increasing the efficiency of clinical trials: recruiting smaller cohorts of suitable individuals will reduce costs and lead to fewer false-negative results - where a drug works on a fraction of the population, but the trial cannot detect it because the majority did not respond to treatment. The chosen application is Alzheimer's disease, but the ideas are fit-for-purpose for similarly complex, progressive diseases. This fellowship is a significant launchpad for my career. My ambition is to benefit patients and society by providing robust computational solutions to complex healthcare challenges. My vision for achieving this ambition starts by targeting the global epidemic of dementia, where I have identified an unmet need (improving clinical trials) and proposed a viable solution in the form of this research and innovation project. The fellowship provides essential resources to capitalise on my recent progress in the field and to personally develop into a UK-based future leader in using AI for medicine and health.	Medical Research Council	Fellowship	838376.0GBP
454	Dr Steeden Jennifer	University College London	2020-02-01	2024-01-31	Towards 10-minute Magnetic Resonance Scanning in Children - Developing Accelerated Imaging Using Machine Learning	Magnetic Resonance Imaging (MRI) scans play a vital role in helping many ill children, by finding out what the problem is and helping plan their treatment. MRI is safe because it does not use radiation. MRI scans produce good-quality pictures or images of many parts of the body, including the brain, heart, spine, joints and other organs. The main problem is they take a long time - often over an hour. During the scan, the child has to keep very still and may even need to hold their breath many times. This is especially hard for children and unwell patients. Hence, younger children under 8 years old need a general anaesthetic, to put them to sleep during the scan. In many childhood diseases, for example in cancer, children may need many MRI scans to follow up disease progression and treatment. Being put to sleep for all of these scans is not pleasant for the child and may occasionally cause problems. It also puts a lot of pressure on hospitals who need to find the doctors, beds, equipment and funds for this. One way of overcoming these problems would be to speed up the MRI scans so the children do not have to keep still or hold their breath. The simplest way of doing this is to collect less data for each image, but this causes so much distortion in the images that they cannot be used. There are some ways of converting these into useful images, but these are complicated and take too long to use in a hospital. Machine Learning is an upcoming way of teaching computers to find complicated patterns in large amounts of information. Recent advances mean that computers are now so powerful that they can learn effectively. Machine Learning has been successfully used for analysing many types of images, for example to perform de-noising, interpolation, image classification and border identification. Despite its popularity, only a few recent studies have shown its potential for reconstruction of MRI images. This is partly due to the greater complexity of the problem and importantly, the large amounts of data required to 'learn' the solution. At Great Ormond Street Hospital, we have MRI images from over 100,000 children and scan an additional 10,000 children each year, all of which we could use to help train and test Machine Learning technologies. I have already shown that basic Machine Learning techniques can remove distortions from MRI scans of the heart, so I am well placed to develop Machine Learning techniques to reconstruct MRI images from other children's diseases, as well as developing more advanced Machine Learning techniques. I showed Machine Learning to be faster than existing reconstruction methods and the images were of better quality than more conventional state-of-the-art techniques. However, much more work is needed to get Machine Learning working reliably in children's scans and to make the most of the possible benefits. If we can use fast scanning with Machine Learning we could shorten scan times from 1 hour to about 10 minutes for children having MRI scans. They would not have to keep completely still for the scan and would not have to hold their breath, therefore reducing the need to put patients to sleep. This would make MRI scanning far less difficult and daunting for children, and would eliminate the cost and side effects from the anaesthetic. Quicker scans would help reduce waiting lists and costs for the NHS. It would also mean that MRI scanning would be used far more often, so it could help many more children. Additionally, these techniques could enable MRI scans to become affordable in some countries for the first time.	Medical Research Council	Fellowship	989996.0GBP
455	Dr Steeden Jennifer	University of Tennessee Knoxville	2020-05-01	2021-04-30	RAPID: Impacts of Design and Operation Attributes of Mass-Gathering Civil Infrastructure Systems on Pathogen Transmission and Exposure	Engineering - This Rapid Response Research (RAPID) grant will support fundamental research to reveal how the design attributes and operation strategies will influence the transmission of and exposure to infectious pathogens within mass-gathering civil infrastructure systems. During the pandemic of 2019 novel coronavirus, the mass-gathering civil infrastructure systems, such as schools, airports, and public transit systems, can become hot spots for spreading the infectious disease. There remains a striking knowledge gap in understanding the impacts of infrastructure design and operation on the occurrence, distribution, transport, and viability of pathogens. It is imperative to address this knowledge gap. Results from this project will lead to bio-informed guidelines for managing critical civil infrastructure systems to prevent exposure of facility users to pathogenic microorganisms, and reduce risks of spreading infectious diseases, and thus alleviating burdens on healthcare systems and citizens. This project will provide much needed insights for infrastructure design reconfigurations and operation practices during the pandemic, in the recovery, and beyond to prevent further disease outbreaks and support healthy, resilient, and smart communities. As a result, this project will help promote public health, national security, and economic prosperity. In addition, this project will raise public science literacy and awareness of infectious diseases, and improve student education and training, as well as K-12 outreach and engagement activities. <br/><br/>The specific objective of this research is to parameterize relevant design attributes and operation strategies of infrastructure systems, and subsequently evaluate their impacts on pathogen transmission and exposure from spatiotemporal microbiome profiles. Three aims will be pursued: 1) identify and quantify the design attributes and operation strategies that may impact pathogen dynamics; 2) audit the types, abundance, and co-occurrence patterns, as well as spatiotemporal dynamics of microorganisms, particularly pathogens, associated with spatially and functionally distributed system components; and 3) characterize the impacts of design and operation on the transmission and exposure pathways of microorganisms in infrastructure systems. The spatial and functional interdependence of system components will be considered to parameterize design attributes based on building information modeling and syntactic analysis. The operation strategies will be modeled using integrated data sensing and simulation techniques. A model-informed sampling approach will be developed with molecular and metagenomics techniques to characterize spatiotemporal microbiome dynamics. The impacts of design and operation on microbial transmission and exposure pathways will be assessed using integrated source tracking and machine learning methods. At the nexus of infrastructure system engineering and environmental microbiology, this convergence research will provide unique insights into design attributes and operation practices impacting pathogen transmission and exposure in mass-gathering civil infrastructure systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	199809.0USD
456	Dr Steeden Jennifer	Massachusetts Institute of Technology	2020-04-01	2021-03-31	RAPID: Immunogenicity of SARS-CoV2 to Human T Cells	Mathematical and Physical Sciences - Pandemics caused by infectious pathogens have plagued humanity since antiquity. The Coronavirus Disease 2019 (COVID-19) caused by the SARS-CoV-2 virus is currently spreading across the world rapidly, including in the United States, with major adverse impact on health and the economy. The SARSCoV-2 outbreak has led to several urgent efforts to develop vaccines that may offer protection against this virus. It is unknown as to whether the current approaches being pursued will elicit protective immune responses in humans. While vaccines have been very effective against many pathogens, the empirical methods for vaccine development pioneered by Pasteur and Jenner over two centuries ago have failed to produce effective vaccines against Human Immune Deficiency Virus, Malaria, Tuberculosis, and many other pathogens. Therefore, rational design of vaccines based on a mechanistic understanding of the pertinent virology and immunology is being pursued, and these efforts include work that is rooted in statistical physics. SARSCoV-2 is phylogenetically most similar to SARS-CoV. This project will use a machine learning approach to understand how the SARS-CoV-2 virus interacts with the immune T cells. This work will directly impact the design of SARS-CoV-2 vaccines and vaccines against future endemic-causing pathogens.<br/><br/>Analyses of patients who have recovered from SARS-CoV shows that antibody responses are not prevalent a few years later, but memory T cell responses are durable and may offer long-term protection. The main questions addressed by this project are 1. Will the SARS-CoV peptides targeted by human T cells that are mutated in SARS-CoV-2 still elicit human T cell responses - i.e. are they immunogenic? 2: Are the 102 peptides identified by host major histocompatibility molecules binding assays alone that are common between SARS-CoV and SARS-CoV-2 immunogenic in humans? If not, they are irrelevant from vaccine design perspective. The goal of the work proposed here is to take a physics-based machine learning approach to determine the immunogenicity of SARS-CoV-2 proteins to human T cell responses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	124472.0USD
457	Dr Steeden Jennifer	University of Southern California	2020-05-01	2021-04-30	RAPID: ReCOVER: Accurate Predictions and Resource Allocation for COVID-19 Epidemic Response	Computer and Information Science and Engineering - The recent outbreak of COVID-19 and its world-wide impact calls for urgent measures to contain the epidemic. Predicting the speed and severity of infectious diseases like COVID-19 and allocating medical resources appropriately is central to dealing with epidemics. Epidemics like COVID-19 not only affect world-wide health, but also have profound economic and social impact. Containing the epidemic, providing informed predictions and preventing future epidemics is essential for the global population to resume their day-to-day work and travel without fear. Shortage of resources puts undue stress on healthcare system further risking health of the community. Preparedness and better management of available resources would require specific predictions at the level of cities and counties around the world rather than solely at the level of countries. The project will provide a predictive understanding of the spread of the virus by developing machine learning based computational models to study the transmission of the virus and evaluate the impact of various interventions on disease spread. The project will learn infection prediction models for COVID-19 considering the following. (i) Predicting at state/county/city-level rather than country-level as finer granularity is essential in planning and managing resources. (ii) How infectious a person is changes over time. Learning the model through observed data will help in understanding of the temporal nature of the virality. (iii) At such granularity travel is a significant reason for the spread and needs to be accounted for. (iv) Available data needs to be ?corrected? by finding the number of underlying unreported cases that are not observed and yet influence the epidemic dynamics. The project will also solve the resource allocation problem based on the prediction ? for instance if a certain number of masks will be available next week in a certain state, how should they be distributed across different hospitals in the state (which hospitals and how many in each state)?<br/><br/>Proposed project ReCOVER will use a novel fine-grained, heterogeneous infection rate model to perform predictions at various granularities (hospital/airports, city, state, country) while accounting for human mobility. ReCOVER will integrate data from various sources to build highly accurate models for prediction of the epidemic across the world at various granularity. Due to the ability to capture temporal heterogeneity in infection rate, the approach has the potential to provide insights into infectious nature of COVID-19 which are not fully understood yet. The project will address the issue of unreported cases through temporal analysis of historical infections and correct the data. The right granularities of modeling will be automatically identified, e.g., when to model a state over its cities to trade-off precision for higher reliability in predictions. The proposed project also formulates and solves a resource allocation problem that can guide the response to contain the epidemic and prevent future outbreaks. This is provided by optimal solutions to resource allocation over a network where each node (representing a region) has a function that captures probabilistic response. While the project obtains data with COVID-19 in consideration, the model and algorithms developed under the project are applicable to a wide class of contagious diseases. The project will culminate into an interactive customizable tool that can be used to perform predictions and resource management by a qualified user such as a government entity tasked with managing the epidemic response. The data and code will also be shared with research community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	158592.0USD
458	Dr Steeden Jennifer	Colorado State University	2020-05-01	2021-04-30	RAPID: ENSURING INTEGRITY OF COVID-19 DATA AND NEWS ACROSS REGIONS	Computer and Information Science and Engineering - Large amounts of epidemiological data are being generated and collected from a variety of sources to understand the impact and propagation of COVID-19. Similarly, huge amounts of news articles are generated and disseminated about the pandemic to keep the population informed. The appropriateness of the actions taken by individuals, corporations, and governments are often based on the quality of data and news. Thus, ensuring the quality of data and news is important. However, malicious actors can alter the attributes of data records, insert spurious records, or suppress records causing any analysis to be inadequate and misinformation to be propagated. This project addresses the critical problem of defining and identifying spurious data and news concerning COVID-19, and tracking the source of misinformation. The project novelty lies in the development of an approach and associated toolset that adapts and combines Machine Learning technologies to detect spurious data and misinformation and presents the results in a manner that is easy for end users to understand and interpret. The approach detects discrepancies in COVID-19 data and traces the flagged discrepancies back to the data sources. The results obtained from the news sources and those obtained from the medical data analysis are compared to determine correlations between the quality of news and the degree and type of data manipulation performed at any region. The project?s impacts are on significantly enhancing the ability to perform accurate scientific analysis, and detecting and explaining news manipulation with respect to COVID-19. The scientific principles developed in the project are expected to be useful outside the medical domain. The PI and the students identified for this project are minorities. The project will be carried out in the Computer Science Department at Colorado State University which is a BRAID affiliate.<br/><br/>COVID-19 data discrepancies are related to (1) single records, where some field is modified, (2) sequence of records over time forming a temporal dimension, where spurious records have been inserted or records have been suppressed, and (3) sequences of records across regions forming a spatial dimension, where there is a pattern of manipulation or information disclosure across regions. The approach determines the appropriate combination of autoencoders, Long Short-Term Memory (LSTM), Temporal Convolution Network (TCNs), and Convolution Neural Networks (CNNs) that can work with data obtained from medical sources and news containing both spatial and temporal dimensions. The tools help the investigators? collaborators at the University of Colorado Anschutz Medical Center and Center for Disease Control and Prevention to perform data integrity checking of medical records and to provide explanations of integrity violations. The tools also handle different types of data and news alterations pertaining to COVID-19.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	199748.0USD
459	Dr Steeden Jennifer	University of California-Irvine	2020-05-01	2021-04-30	RAPID: The Role of Emerging Virtual Cultures in the Prevention of COVID-19 Transmission	Social, Behavioral and Economic Sciences - The COVID-19 pandemic has transformed our relationship to the physical world. Social distancing guidelines have led many people to avoid all forms of public life, from concerts and restaurants to everyday interaction in parks, neighborhoods, and the homes of family and friends. In response there has been a massive increase in online interaction: the internet has suddenly become the primary way that many Americans socialize, labor, and learn. It is crucial to gain a better understanding of how the emergence of these changes is related to the pandemic. Even if a vaccine is discovered, preventing catastrophic levels of COVID-19 transmission into the next few years will depend on social distancing that can be sustained and integrated with work, education, and community. This means going online. The starting point for addressing this global challenge is thus the fact that what we call ?social distancing? is really physical distancing. Successful physical distancing will rely on new forms of social closeness online. Yet there is not just one ?online.? A rapid and effective response requires clarifying the impact of virtual worlds as part of different forms of online interaction that comprise a virtual culture: social network sites, streaming websites, and multiplayer platforms. The project will also train graduate student researchers in methodological approaches for studying online cultures. <br/><br/>This research will be conducted in a densely trafficked virtual world. Virtual worlds are places where individuals interact with avatars in online environments. The investigators have conducted research in a virtual world context for over a decade, and thus have detailed baseline data with which to examine what is happening as a large number of individuals enter that virtual world due to the COVID-19 pandemic. What is the sudden move to virtual worlds doing in terms of social closeness and interaction? How does co-presence in virtual place transform intimacy and collaboration? How might this provide innovative strategies for preventing viral transmission, by forging new forms of social closeness in the context of physical distancing? To investigate these questions, the researchers will conduct participant observation, individual interviews, and group interviews. The study will compare individuals who have spent time in the virtual world for years with individuals who have entered the virtual world after COVID-19. Findings from this research will provide insight into the specific possibilities virtual worlds are providing in the circumstances of societies reshaped by COVID-19. In these new circumstances, virtual worlds will be one element of an online ecosystem linking drones, robots, and autonomous vehicles to mobile devices, social network sites, online games and streaming, augmented reality, artificial intelligence, machine learning, and data analytics. The research will thus provide a better understanding of the place of virtual worlds in this emerging online ecosystem.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	195619.0USD
460	Dr Steeden Jennifer	WASHINGTON UNIVERSITY	2020-05-15	2021-04-30	RAPID: A multiscale approach to dissect SARS-CoV-2 attachment to host cells and detect viruses on surfaces	Biological Sciences - The 2019 novel coronavirus, identified as the cause for the pneumonia pathology reported in Wuhan, spread quickly and became a global pandemic. The project will employ experimental methods to develop sensors for the detection of SARSCoV-2 from environmental samples and develop predictive models for virus attachment to cells by applying computational machine learning methods. The outcome of this project will contribute to the development of proactive measures to identify viruses with pandemic potential before they are able to transmit and spread broadly among humans. The graduate students involved in this research will gain experience in protein biochemistry, fluorescence microscopy, and computational simulations and experience utilizing those skills to problems of societal importance.<br/><br/>This NSF Rapid response Research (RAPID) project will support a project that is aimed to characterize receptor interactions mediated by the Spike protein (S) of SARS-CoV-2. Development of fluorescence-based assays to characterize SARSCoV-2 attachment to Angiotensin converting enzyme (ACE2)-functionalized surfaces with controlled density and mobility, identifying peptide mimics of the ACE2 ectodomain for the development of sensors to detect SARSCoV-2 from environmental samples, and develop and validate predictive models of CoV attachment from primary sequence using machine learning constitute the specific goals of this project.<br/><br/>This RAPID award is made by the Molecular Biophysics Program in the Division of Molecular and Cellular Biosciences, using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	200000.0USD
461	Dr Steeden Jennifer	Columbus State University	2020-05-15	2021-04-30	RAPID/Collaborative Research: Quantifying Social Media Data for Improved Modeling of Mitigation Strategies for the COVID-19 Pandemic	Engineering - This Rapid Response Research (RAPID) grant will support research that will contribute new knowledge related to modeling social behavior and community activity during the COVID-19 pandemic, as well as future pandemics with COVID-19 characteristics. The model focuses on compliance with mitigation strategies and public health guidelines, thus enabling the selection of policies that are most effective in promoting both the progress of science and advancing national health and prosperity. Various pandemic models are currently being used to predict the spread of a virus and establish which mitigation strategies are the most effective. These models are heavily based on assumptions and may include an oversimplified reality of how populations react and behave. This research will provide needed knowledge and methods for the development of a model of how individuals in the U.S. react to certain mitigation strategies, such as social-distancing, stay-at-home orders, quarantines, and travel advisories, by mining and analyzing social media data during the COVID-19 crisis. This enhanced modeling approach and its resultant model will be of great value to disaster response managers and policy/decision makers to understand human social behavior. This work allows assessment of the effectiveness of mitigation strategies and public health guidelines during pandemics (and other crises). This project will also form the basis of a publicly available case study suitable for university level students that can be widely incorporated in courses. <br/><br/>Although individual-based and homogeneous mixing pandemic models provide useful insights and predictive capabilities within a range of possibilities, they are highly sensitive to people?s actions. This research aims to provide an enhanced approach to model social behavior and community activity during a pandemic in terms of compliance with mitigation strategies and public health guidelines. Social media data present a brief window of opportunity for research on how, and to what extent, the public does or does not comply with the recommended mitigation strategies and public health guidelines. The research team will collect real-time data from social media related to COVID19-exposed regional populations in the U.S. The data will be analyzed using machine learning techniques to identify non-mutually exclusive clusters of people based on similarity of their demographic, geographic, and time information, and establish relationships among clusters. The analyzed data will form the basis of a data-driven multi-paradigm simulation model that captures changes in public sentiment over time, quantifies the resistance/compliance with mitigation strategies and health guidelines, and gauges overall effectiveness of various mitigation strategies and advice over time.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	13406.0USD
462	Dr Steeden Jennifer	Northeastern University	2020-05-15	2021-04-30	RAPID: D3SC: Identification of Chemical Probes and Inhibitors Targeting Novel Sites on SARS-CoV-2 Proteins for COVID-19 Intervention	Mathematical and Physical Sciences - The life cycle of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) involves a number of viral proteins and enzymes required for infectivity and replication. Inhibitors that target these enzymes serve as potential therapeutic interventions against coronavirus disease 2019 (COVID-19). With this award, the Chemistry of Life Processes program in the Chemistry Division is supporting the research of Drs. Mary Jo Ondrechen and Penny J. Beuning from Northeastern University to apply computational methods to identify sites in SARS-CoV-2 proteins that would be good targets for binding inhibitors. The project uses artificial intelligence methods developed at Northeastern University to identify pockets and crevices in the structures of viral proteins that may serve as new targets for the development of antiviral agents. Large datasets of natural and synthetic compounds are computationally searched for molecules that fit into these alternative sites, and any compounds that fit will be experimentally tested for their ability to inhibit the functions of these viral enzymes. The project provides training in computational chemistry and biochemical analysis to graduate students and postdoctoral associates.<br/><br/>This project uses the unique Partial Order Optimum Likelihood (POOL) machine learning (ML) method developed by Dr. Ondrechen?s group to predict multiple types of binding sites in SARS-CoV-2 proteins, including catalytic sites, allosteric sites, and other interaction sites. The goals of this project are to apply the POOL-ML method to identify the binding sites on viral pathogen SARS-CoV-2 proteins using the three-dimensional protein structures as input. Molecular dynamics simulations are used to generate conformations for ensemble docking. Compounds from the large molecular databases are computationally docked into the predicted sites to identify potentially strong binding ligands. Candidate ligands to selected SARS-CoV-2 proteins, including the main protease and 2?-O-ribose RNA methyltransferase, are experimentally tested in vitro for binding affinity and the effect of the best predicted inhibitors on catalytic activities determined by direct biochemical assays. All the SARS-CoV-2 protein structures in the Protein Data Bank (PDB) are studied. Compound libraries for the study include: a) selected 2600+ compounds from the ZINC and Enamine databases that are already being manufactured; b) a library of 20,000+ compounds found in foods that the team recently gained access to; these potentially hold some special advantages, including ready availability in the public domain and low cost; and c) the March 2020 open access CAS (American Chemical Society) database of 50,000 compounds with known or potential anti-viral activity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	165808.0USD
463	Dr Steeden Jennifer	Harvard University	2020-05-01	2021-04-30	RAPID:Collaborative Research: Computational Drug Repurposing for COVID-19	Computer and Information Science and Engineering - With the disruptive nature of the COVID-19 pandemic, effective treatments could save the lives of severely ill patients, protect individuals with a high risk of infection, and reduce the time patients spend in hospital beds. However, there are currently no effective treatments for COVID-19. Traditional methodologies take years to develop and test compounds from scratch. Machine learning provides promising new approaches to repurpose drugs that are safe and already approved for other diseases. This project will develop a machine learning toolset to expedite the development of safe and effective medicines for COVID-19. The toolset will rapidly identify safe repurposing opportunities for approved and experimental drugs. It will predict whether treatments may have therapeutic effects in COVID-19 patients, allowing the identification of drugs and drug cocktails that are safe and plentiful enough to treat a substantial number of patients. By putting tools in the hand of practitioners, the activities in this project will have an immediate impact. They will result in actionable predictions that are accurate and interpretable. <br/><br/>Recently, the principal investigators have developed a series of machine learning tools to identify drug repurposing opportunities. Building on foundational previous work, in this project, the principal investigators will first build a large COVID-19 focused knowledge graph that will capture fundamental and COVID-19-specific biological knowledge. The graph learning methods will be adapted to identify safe drugs and drug cocktails for COVID-19. To predict the safety of cocktails with two or more drugs, the methods will generalize to an exponentially large space of high-order drug combinations. In addition to drug safety, efficacy is a crucial endpoint for drug development. The project will develop a novel graph neural network (GNN) method to identify efficacious drug repurposing opportunities, even for diseases, such as COVID-19, that do not yet have any drug treatments and thereby, no label, supervised information. The method will predict what drugs and drug combinations may have a therapeutic effect on COVID-19. Finally, the principal investigators will integrate the developed tools into a complete, explainable framework that will generate predictions, provide explanations, and incorporate human feedback into the machine learning loop. This project will provide new, open tools for rapid drug repurposing that will be relevant for COVID-19 and other emerging pathogens. Additionally, the project will provide unique opportunities for multi-disciplinary curriculum development, training and advising, and professional activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	99863.0USD
464	Dr Steeden Jennifer	University of Minnesota-Twin Cities	2020-05-15	2021-04-30	RAPID: The effect of contact network structure on the spread of COVID-19: balancing disease mitigation and socioeconomic well-being	Biological Sciences - What makes COVID-19 spread rapidly in some places, yet slowly in others? How should society lessen social distancing while limiting an increase in infections? To answer these questions, this Rapid Response Research (RAPID) project seeks to understand how patterns of interpersonal interaction (?structure?) in social contact networks affect disease spread in a population. The researchers will simulate a disease spreading through a variety of social contact networks, and use machine learning to relate each network?s structure to the number and timing of new infections. By limiting structures related to increased disease, societies may be able to reopen other parts of their economies while still curbing overall disease spread. The researchers will produce an interactive web application for the public and decision-makers to visualize trade-offs between reducing disease and maintaining social cohesion. This research will support the professional development of an early career scientist.<br/><br/>This research aims to determine the inherent risk of SARS-CoV-2 spread based on contact network structure. The researchers will use machine learning to 1) identify network structures that influence disease spread and 2) predict disease spread on empirical contact networks. Important network structures will serve as targets for simulated disease mitigation interventions (e.g. reducing structures that increase levels of disease or increasing structures that reduce disease levels). Finally, the researchers will investigate whether future outbreaks of COVID-19 or other diseases could be alleviated through optimizing social contact networks ahead of time. The outcomes of this research will inform and facilitate quick, efficient interventions to reduce the social and economic costs of COVID-19. This research will develop a general framework for relating disease to network structure. Thus, results can be generalized beyond the current pandemic, serving to further our understanding of potential future waves of COVID-19, as well as other directly-transmitted diseases in humans, livestock, and wildlife.<br/><br/>This RAPID award is made by the Ecology and Evolution of Infectious Diseases Program in the Division of Environmental Biology, using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	199136.0USD
465	Dr Steeden Jennifer	New Mexico Highlands University	2020-05-15	2021-04-30	RAPID: Machine Learning Methods to Understand, Predict and Reduce the Spread of COVID-19 in Small Communities	Mathematical and Physical Sciences - The ongoing COVID-19 outbreak has recently reached pandemic status spreading all around the world. The severity of the pandemic, along with an enormous impact on world?s economy and society, has forced governments to introduce emergency measures. It is essential to utilize the available statistical data from trusted sources in order to model and evaluate the dynamics of the pandemic spread, to not only better understand such complex systems, but to learn and develop possible solutions to prevent further spread of current and/or similar future outbreaks. Thus, this research, devoted to the development of mathematical models of COVID-19 pandemic spread, addresses an urgent national need. Faculty and students in computer science, anthropology, and computational chemistry at New Mexico Highlands University have formed a diverse group for finding a solution to the complicated problems of the description and prediction of COVID-19 spread. This multidisciplinary project is expected to yield a better understanding of the interconnections among many factors that contribute to the spread of COVID-19. Statistical data will be collected in regions of Northern New Mexico, including San Juan and McKinley Counties in the Navajo Nation and Los Alamos county outside of the Navajo Nation. Analysis of the collected statistical data along with socio-cultural assessment from this project will be presented to New Mexico (NM) tribal and health authorities. The project will aim to provide a scientific basis for the prediction of disease spread and will consider scenarios associated with the possibility of another wave of the pandemic. Students from this minority-serving institution involved in the project will obtain valuable experience in the application of advanced machine learning models and methods in providing fast robust reaction to a national health, economic, and societal crisis.<br/><br/>In this study, machine learning methods will be used to analyze pandemic spread scenarios in different regions and to glean the most important features of the data characterizing the spread. The research team will use both traditional machine learning techniques and advanced methods, such as artificial neural networks, allowing development of virus incidence model capturing dependencies in both linear and nonlinear domains. The work will concentrate on understanding disease spread with regard to multiple socioeconomic factors. The problem can be treated as a sequence modeling one; so, recurrent neural networks and more complex models based on their recurrent cells might be one promising direction. The next step will be to assemble datasets for small isolated communities with different socioeconomic backgrounds and ethnicities ? comparing Navajo Indians living on the Navajo reservation to Los Alamos County (NM) ? and to test the applicability of the developed model to these regions. The spatiotemporal data available on the spread is heterogeneous in character. An important goal of this research is to classify the collected data with respect to the similarity in the epidemic curve behavior and then build separate models for different regions according to this classification. The proposed model will be used for prediction of future incidents and to produce the most effective non-medical recommendations for suppression and prevention of future viral outbreaks.<br/><br/>This research is supported by the Partnerships for Research and Education in Materials (PREM) program and the Condensed Matter and Materials Theory (CMMT) program in the Division of Materials Research in the Directorate for Mathematical and Physical Science using supplemental funds made available by the Coronavirus Aid, Relief, and Economic Security (CARES) Act.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	185747.0USD
466	Dr Steeden Jennifer	University of British Columbia	2020-05-15	2021-04-30	Digital Virtual Support of Cases and Contacts to Novel Coronavirus (COVID-19): Readiness and Knowledge Sharing for Global Outbreaks (WelTel PHM)	The global outbreak of COVID-19 is the latest example of a rapidly spreading infectious outbreak with global impact. Infected patients with mild symptoms and asymptomatic contacts need to be isolated, ideally without overwhelming health facilities. WelTel, an integrated virtual care and patient engagement solution, emerged as an innovation initially to support the global HIV pandemic through a Canadian-Kenyan partnership over a decade ago. Co-founded by the lead investigator and registered in British Columbia, WelTel has continued to integrate research into a richly featured virtual care platform that can be used on the frontlines of healthcare delivery. The study aims to: 1-Deploy and co-optimize WelTel to assist in home monitoring and support of COVID-19 cases and contacts; 2- Determine essential linkages and technical demands of the digital health ecosystem for data security purposes and integration into other electronic health records (EHR) & health information management systems (HIMS); 3-Evaluate communication and other metadata captured by the system for public health quality improvement to better understand and reduce barriers (such as stigma); 4-Use novel computing approaches such as natural language processing (NLP) and machine learning to harness artificial intelligence (AI) capabilities to model, predict, and provide insights into future precision public health approaches. Collaborators have necessary expert skills in quantitative and qualitative research methods for rigorous assessment, and come from the countries targeted for the research deployment (Canada, UK, US, Kenya, and Rwanda). A rapid digital landscape analysis will also be done as a part of this research. Virtual care may be an efficient, cost-effective way to provide the necessary public health monitoring and support for patients and contacts of COVID-19 and future emerging communicable pathogens, as well as can inform public health quality improvement and precision care.	Canadian Institutes of Health Research	Research Grant	500000.0CAD
467	Prof. Cotton Matthew	MRC/UVRI & LSHTM Uganda Research Unit	2020-05-15	2021-04-30	African COVID-19 Preparedness (AFRICO19)	Our project, AFRICO19, will enhance capacity to understand SARS-CoV-2/hCoV-19 infection in three regions of Africa and globally. Building on existing infrastructures and collaborations we will create a network to share knowledge on next generation sequencing (NGS), including Oxford Nanopore Technology (MinION), coronavirus biology and COVID-19 disease control. Our consortium links three African sites combined with genomics and informatics support from the University of Glasgow to achieve the following key goals: 1. Support East and West African capacities for rapid diagnosis and sequencing of SARS-CoV-2 to help with contact tracing and quarantine measures. Novel diagnostic tools optimized for this virus will be deployed. An African COVID-19 case definition will be refined using machine learning for identification of SARS-CoV-2 infections. 2. Surveillance of SARS-CoV-2 will be performed in one cohort at each African site. This will use established cohorts to ensure that sampling begins quickly. A sampling plan optimized to detect initial moderate and severe cases followed by household contact tracing will be employed to obtain both mild to severe COVID-19 cases. 3. Provide improved understanding of SARS-CoV-2 biology/evolution using machine learning and novel bioinformatics analyses. Our results will be shared via a real-time analysis platform using the newly developed CoV-GLUE resource.	Wellcome/Department for International Development	Research Grant	2001990.0GBP
468	Professor Herten Dirk-Peter	University of Birmingham	2020-08-01	2023-08-01	AMS Professorship Award for Professor Dirk-Peter Herten, University of Birmingham	I want to establish a world-leading research group in receptor signalling by providing tools and support for quantitative studies with molecular sensitivity for biomedical research. The prestigious award of the AMS fellowship will enable the purchase of cutting-edge microscopy tools and allow further automation of our microscopes to increase throughput and provide the capacity for super-resolved 3D reconstructions of whole cells. My position in COMPARE aligned to the Institute of Cardiovascular Sciences (ICVS) and the School of Chemistry at the University of Birmingham (UoB) adds expertise in methods development to the COMPARE advanced microscopy facility at UoB and will allow me to work on my quantitative microscopy methods. My appointment creates a unique opportunity to support researchers and clinicians in their search for solutions to urgent biomedical questions, like HIV infection or thrombosis, by use of existing techniques and novel approaches. I will involve Jeremy Pike (UoB, Data Analysis Officer) and Iain Styles (Computer Sciences, Turing Institute Fellow and Deputy Director of COMPARE, UoB) in machine learning approaches for the fast processing of 3D microscopy data. Additionally, I will initiate a planned industry collaboration with IRIS Biotech (Germany) to elaborate on the potential commercialisation of fluorescent probes for super-resolution microscopy, chemical multiplexing and metal cation sensing based on my patent (DE 10 2016 012 162.9). A challenge in methods development is to identify suitable targets and questions for the novel approach. The fellowship will enable collaborations with colleagues on campus and within COMPARE to increase the number of biomedical targets and facilitate translational application of my techniques which focus on T cell receptor signalling and more recently on GPCRs. Examples of new collaborations include the platelet immunoglobulin C-type lectin-like receptors GPVI and CLEC-2 (with Steve Watson, Steve Thomas and Natalie Poulter, ICVS) which are novel targets for a variety of thrombosis and thrombo-inflammatory disorders, and in the chemokine GPCR family (with Dimity Veprintsev, University of Nottingham - UoN) that play an important role in diseases like hypertension, hypoxia, or hypoglycaemia. In this context, I envision a key training centre for advanced microscopy at UoB also addressing problems like fluorescence labelling and probe development. This will involve colleagues in the School of Chemistry as well as COMPARE researchers working on fluorophores (Jon Preece, UoB) and bioactive molecules (Liam Cox, UoB; Barrie Kellam, UoN). COMPARE offered me a generous starting budget COMPARE (£ 450K) and the institute is in the process of refurbishing the labs to my needs, to move 3 co-workers together with my equipment by February 2020. Since I am unable to bring any overseas funding, the award will allow me to get accommodated with the UK grant application system and plan projects to strengthen my translational research (MRC) and the development of novel probes and techniques (BBSRC and EPSCR) without losing any momentum in my ongoing research activities. I have already applied for a translational research grant (BBSCR TRDF) in collaboration with Robert Henderson (University of Edinburgh), which involves quantitative imaging microscopy through use of his 256x256 APD camera.	The Academy of Medical Sciences	AMS Professorship Scheme Round 2	470621.92GBP
469	Professor Huang Xiaolin	Tongji Hospital, Tongji Medical College, Huazhong University of Science and Technology	2018-02-01	2019-06-30	Low cost robotic orthosis for stroke treatment in rural China	China is the worst affected developing country with 2.5M new stroke cases each year and 11.1M stroke survivors at any given time. In the past decades, due to lower socioeconomic status, less stroke awareness, and inequitable distribution of medical resources to rural areas, the incidence rate and burden of stroke in China has increased disproportionately in rural areas of the northeast and central regions. Rehabilitation programmes have been shown to be extremely effective in reducing the disability and restoring walking ability through early training. However, in most rural areas of China no such medical rehabilitation centres or hospitals exist leaving the rural population without the therapies which will allow them to regain mobility functions after stroke. This leads to disability and has broader negative socioeconomic impacts. This project seeks to gather evidence directly from the key stakeholders in the beneficiary ODA countries (China and Kazakhstan) about what their problems and requirements are. The networking activities will allow us to build a full proposal based around actual rather than assumed need, and therefore maximise the impact achieved. The project aims to establish a multi-disciplinary consortium including experts from rehabilitation robotics, sensing, machine learning, physiotherapy and rehabilitation medicine. We aim to provide low-cost robotic solutions to stroke survivors in remote home and community environments. This would reduce therapists’/stroke carers' demand in rural areas of ODA countries whilst maximizing the stroke survivor’s sense of intention, involvement, interaction and achievement of limb movement, with greater likelihood of successful rehabilitation and improved quality of life.	The Academy of Medical Sciences	AMS Professorship Scheme Round 2	22000.0GBP
470	Professor Huang Xiaolin	TWO WORLDS CONSULTING LIMITED	2018-02-01	2019-06-30	udu: AI Platform for Pandemic Intelligence	The UK, amongst others, lacks a coherent infrastructure to support effective direct and timely collection and analysis of pandemic data, about both the progressIon of Covid-19 itself and the population response to public policy aimed at mitigating and profiling its progress. Refining policy and informing the judgement calls required to navigate the balance between lockdown and economic damage requires both accurate data and the ability to rapidly model multiple, 'What if?' scenarios. Current data intelligence systems are partial, fragmented, incomplete, lag reality and, in most cases can only surface what they have specifically been asked to look for. AI systems used to look for patterns are often constrained by the quality and range of data available to them. Existing models tend to look at single factors in isolation, e.g. not taking into account multiple sources of mortality data or failing to take account factors such as population mobility and behaviour, the impact of events such as Cheltenham races, sunny bank holiday weather or other regional and seasonal variations. This can only be addressed through a more holistic approach to data collection and integration. This project therefore uses an advanced data intelligence platform, udu, which is capable of integrating a wide range of data from multiple sources and of multiple types and of actively discovering new data online. It then uses software that can self-organise itself around a task to discover relationships between and patterns in the collected data to provide an inferential view of pandemic impact, policy effectiveness and population behaviour. udu has been established for several years in niche markets. Here, we are building on previous experience by Two Worlds in using udu to create systems for the predictive analysis of environmental change to public health for the first time. The resulting system is intended to be capable of supporting direct exploration by human users, providing an interface (API) to allow other teams to test their own analytic models against the datascape created by udu and supporting local and external machine learning systems with a wider range of high quality data and analysis.	UK Research and Innovation	AMS Professorship Scheme Round 2	49984.0GBP
471	Professor Huang Xiaolin	University of Basel	2009-10-01	2013-03-31	Inference of post-transcriptional regulatory codes involving miRNAs and RNA binding proteins	Although for many years transcription factors held the center stage inthe regulation of gene expression, a very complex post-transcriptionalregulatory layer implemented by miRNAs and RNA-binding proteins andfrequently acting precisely on the mRNAs encoding transcriptionfactors, has been uncovered. Far from passively carrying the geneticinformation to the ribosomal translation machines, messenger RNAs havea life of their own and this can be extended or shortened depending oninteractions with various protein and ribonucleoprotein complexes inthe cell. Although the miRNAs have initially been described asregulators of the mRNA translation rate, it is now clear that animportant mechanism behind the miRNA-based regulation is target mRNAdegradation. The activity of miRNA-containing ribonucleoproteincomplexes can itself be modulated for example by RNA-binding proteins(RBPs) that recognize sites in the vicinity of miRNA-binding sites andact as competitors for miRNA binding.Much effort in the past years has been devoted to identifying newplayers in post-transcriptional control. Many groups, including ourown, contributed to the catalog of miRNA genes in species ranging fromviruses to human. Similarly, many groups, ours included, contributedthrough computational or experimental approaches to the list ofputative miRNA-target interactions. Yet although it is clear that onaverage a miRNA targets hundreds of genes, much remains to be done tounderstand the determinants of miRNA targeting specificity. Based onthe observation that miRNAs destabilize their mRNA targets manyexperimental studies aimed to identify miRNA targets by miRNAtransfection and microarray profiling. In spite of the fact thathundreds to thousands of transcripts respond in a typical experiment,only a fraction of the predicted miRNA targets do. Moreover, forreasons that are so far unclear, the magnitude of the response varieswidely. In a recent analysis of over seventy experimental data sets ofmRNA or protein level changes that occurred upon perturbation of miRNAexpression, we found that functional miRNA target sites reside inaccessible regions, and that the degree of mRNA degradation is relatedto the U (and to some extent A) nucleotide content of the miRNA targetsite environment. This suggests that the fate of miRNA targets dependsnot only on their interaction with the RNA-induced silencing complex(RISC), but also on the presence of modulatory protein co-factors. Incollaboration with the group of Tom Tuschl (The RockefellerUniversity) we have recently identified a number of RBPs that interactwith the RISC complex and we characterized their cognate mRNAs byimmunoprecipitation and microarray expression profiling. We havefurther developed a novel cross-linking and immunoprecipitation methodto identify RBP targets at the resolution of individual bindingsites. Building on this work, we here propose to carry out the following projects:1. Computational modeling of miRNA-target interactions.The starting point of this study will be the set of sequences that we found, with the CLIP method mentioned above, to be bound by Argonaute protein-containing complexes. We will attempt to use machine learning approaches to identify the location of miRNA binding sites in these data and to characterize the mode of binding of miRNAs to their targets. We will also thoroughly characterize the properties of these binding sites and we will use the results to develop improved methods for miRNA target prediction.2. Experimental identification of the targets of U-rich element binding proteins.This project will build on one hand on the experimental method that we developed in collaboration with the group of Tom Tuschl (The Rockefeller University), and on the other hand on our computational results that indicate that U-rich elements are associated with the degradation of miRNA targets. We have selected a set of U-rich element binding proteins whose targets we would like to identify by CLIP, and then study in relationship with the miRNA targets that we already obtained.3. Development of computational models of RBP-mRNA interactions.This projects aims to again use the CLIP data that we have and will obtain in the near future to start developing models to specifically describe RBP-mRNA interactions, taking into account the particularities of this class of proteins. These projects will make an important contribution to a quantitativeunderstanding of miRNA-based post-transcriptional regulation and tothe inference of the elusive 'RNA code'.	Swiss National Science Foundation	Project funding (Div. I-III)	600000.0CHF
472	Dr Sampson Stephen	University of Oxford	2020-01-01	2023-06-30	Modelling the cellular causes of HLA-B*27 associated spondyloarthritis with single-cell genomics	Although for many years transcription factors held the center stage inthe regulation of gene expression, a very complex post-transcriptionalregulatory layer implemented by miRNAs and RNA-binding proteins andfrequently acting precisely on the mRNAs encoding transcriptionfactors, has been uncovered. Far from passively carrying the geneticinformation to the ribosomal translation machines, messenger RNAs havea life of their own and this can be extended or shortened depending oninteractions with various protein and ribonucleoprotein complexes inthe cell. Although the miRNAs have initially been described asregulators of the mRNA translation rate, it is now clear that animportant mechanism behind the miRNA-based regulation is target mRNAdegradation. The activity of miRNA-containing ribonucleoproteincomplexes can itself be modulated for example by RNA-binding proteins(RBPs) that recognize sites in the vicinity of miRNA-binding sites andact as competitors for miRNA binding.Much effort in the past years has been devoted to identifying newplayers in post-transcriptional control. Many groups, including ourown, contributed to the catalog of miRNA genes in species ranging fromviruses to human. Similarly, many groups, ours included, contributedthrough computational or experimental approaches to the list ofputative miRNA-target interactions. Yet although it is clear that onaverage a miRNA targets hundreds of genes, much remains to be done tounderstand the determinants of miRNA targeting specificity. Based onthe observation that miRNAs destabilize their mRNA targets manyexperimental studies aimed to identify miRNA targets by miRNAtransfection and microarray profiling. In spite of the fact thathundreds to thousands of transcripts respond in a typical experiment,only a fraction of the predicted miRNA targets do. Moreover, forreasons that are so far unclear, the magnitude of the response varieswidely. In a recent analysis of over seventy experimental data sets ofmRNA or protein level changes that occurred upon perturbation of miRNAexpression, we found that functional miRNA target sites reside inaccessible regions, and that the degree of mRNA degradation is relatedto the U (and to some extent A) nucleotide content of the miRNA targetsite environment. This suggests that the fate of miRNA targets dependsnot only on their interaction with the RNA-induced silencing complex(RISC), but also on the presence of modulatory protein co-factors. Incollaboration with the group of Tom Tuschl (The RockefellerUniversity) we have recently identified a number of RBPs that interactwith the RISC complex and we characterized their cognate mRNAs byimmunoprecipitation and microarray expression profiling. We havefurther developed a novel cross-linking and immunoprecipitation methodto identify RBP targets at the resolution of individual bindingsites. Building on this work, we here propose to carry out the following projects:1. Computational modeling of miRNA-target interactions.The starting point of this study will be the set of sequences that we found, with the CLIP method mentioned above, to be bound by Argonaute protein-containing complexes. We will attempt to use machine learning approaches to identify the location of miRNA binding sites in these data and to characterize the mode of binding of miRNAs to their targets. We will also thoroughly characterize the properties of these binding sites and we will use the results to develop improved methods for miRNA target prediction.2. Experimental identification of the targets of U-rich element binding proteins.This project will build on one hand on the experimental method that we developed in collaboration with the group of Tom Tuschl (The Rockefeller University), and on the other hand on our computational results that indicate that U-rich elements are associated with the degradation of miRNA targets. We have selected a set of U-rich element binding proteins whose targets we would like to identify by CLIP, and then study in relationship with the miRNA targets that we already obtained.3. Development of computational models of RBP-mRNA interactions.This projects aims to again use the CLIP data that we have and will obtain in the near future to start developing models to specifically describe RBP-mRNA interactions, taking into account the particularities of this class of proteins. These projects will make an important contribution to a quantitativeunderstanding of miRNA-based post-transcriptional regulation and tothe inference of the elusive 'RNA code'.	Versus Arthritis	Full Application Disease	716522.74GBP
473	Professor Rubinsztein David	University of Cambridge	2011-10-12	2013-10-11	IDENTIFICATION OF GENERIC SUPRESSORS OF PROTEINOPATHIES	Although for many years transcription factors held the center stage inthe regulation of gene expression, a very complex post-transcriptionalregulatory layer implemented by miRNAs and RNA-binding proteins andfrequently acting precisely on the mRNAs encoding transcriptionfactors, has been uncovered. Far from passively carrying the geneticinformation to the ribosomal translation machines, messenger RNAs havea life of their own and this can be extended or shortened depending oninteractions with various protein and ribonucleoprotein complexes inthe cell. Although the miRNAs have initially been described asregulators of the mRNA translation rate, it is now clear that animportant mechanism behind the miRNA-based regulation is target mRNAdegradation. The activity of miRNA-containing ribonucleoproteincomplexes can itself be modulated for example by RNA-binding proteins(RBPs) that recognize sites in the vicinity of miRNA-binding sites andact as competitors for miRNA binding.Much effort in the past years has been devoted to identifying newplayers in post-transcriptional control. Many groups, including ourown, contributed to the catalog of miRNA genes in species ranging fromviruses to human. Similarly, many groups, ours included, contributedthrough computational or experimental approaches to the list ofputative miRNA-target interactions. Yet although it is clear that onaverage a miRNA targets hundreds of genes, much remains to be done tounderstand the determinants of miRNA targeting specificity. Based onthe observation that miRNAs destabilize their mRNA targets manyexperimental studies aimed to identify miRNA targets by miRNAtransfection and microarray profiling. In spite of the fact thathundreds to thousands of transcripts respond in a typical experiment,only a fraction of the predicted miRNA targets do. Moreover, forreasons that are so far unclear, the magnitude of the response varieswidely. In a recent analysis of over seventy experimental data sets ofmRNA or protein level changes that occurred upon perturbation of miRNAexpression, we found that functional miRNA target sites reside inaccessible regions, and that the degree of mRNA degradation is relatedto the U (and to some extent A) nucleotide content of the miRNA targetsite environment. This suggests that the fate of miRNA targets dependsnot only on their interaction with the RNA-induced silencing complex(RISC), but also on the presence of modulatory protein co-factors. Incollaboration with the group of Tom Tuschl (The RockefellerUniversity) we have recently identified a number of RBPs that interactwith the RISC complex and we characterized their cognate mRNAs byimmunoprecipitation and microarray expression profiling. We havefurther developed a novel cross-linking and immunoprecipitation methodto identify RBP targets at the resolution of individual bindingsites. Building on this work, we here propose to carry out the following projects:1. Computational modeling of miRNA-target interactions.The starting point of this study will be the set of sequences that we found, with the CLIP method mentioned above, to be bound by Argonaute protein-containing complexes. We will attempt to use machine learning approaches to identify the location of miRNA binding sites in these data and to characterize the mode of binding of miRNAs to their targets. We will also thoroughly characterize the properties of these binding sites and we will use the results to develop improved methods for miRNA target prediction.2. Experimental identification of the targets of U-rich element binding proteins.This project will build on one hand on the experimental method that we developed in collaboration with the group of Tom Tuschl (The Rockefeller University), and on the other hand on our computational results that indicate that U-rich elements are associated with the degradation of miRNA targets. We have selected a set of U-rich element binding proteins whose targets we would like to identify by CLIP, and then study in relationship with the miRNA targets that we already obtained.3. Development of computational models of RBP-mRNA interactions.This projects aims to again use the CLIP data that we have and will obtain in the near future to start developing models to specifically describe RBP-mRNA interactions, taking into account the particularities of this class of proteins. These projects will make an important contribution to a quantitativeunderstanding of miRNA-based post-transcriptional regulation and tothe inference of the elusive 'RNA code'.	Medical Research Council	Research Grant	407427.0GBP
474	Dr Courtin Emilie	London Sch of Hygiene and Trop Medicine	2020-05-01	2023-04-30	Heterogeneous and long-term effects of social interventions on mortality and psychological health	Although for many years transcription factors held the center stage inthe regulation of gene expression, a very complex post-transcriptionalregulatory layer implemented by miRNAs and RNA-binding proteins andfrequently acting precisely on the mRNAs encoding transcriptionfactors, has been uncovered. Far from passively carrying the geneticinformation to the ribosomal translation machines, messenger RNAs havea life of their own and this can be extended or shortened depending oninteractions with various protein and ribonucleoprotein complexes inthe cell. Although the miRNAs have initially been described asregulators of the mRNA translation rate, it is now clear that animportant mechanism behind the miRNA-based regulation is target mRNAdegradation. The activity of miRNA-containing ribonucleoproteincomplexes can itself be modulated for example by RNA-binding proteins(RBPs) that recognize sites in the vicinity of miRNA-binding sites andact as competitors for miRNA binding.Much effort in the past years has been devoted to identifying newplayers in post-transcriptional control. Many groups, including ourown, contributed to the catalog of miRNA genes in species ranging fromviruses to human. Similarly, many groups, ours included, contributedthrough computational or experimental approaches to the list ofputative miRNA-target interactions. Yet although it is clear that onaverage a miRNA targets hundreds of genes, much remains to be done tounderstand the determinants of miRNA targeting specificity. Based onthe observation that miRNAs destabilize their mRNA targets manyexperimental studies aimed to identify miRNA targets by miRNAtransfection and microarray profiling. In spite of the fact thathundreds to thousands of transcripts respond in a typical experiment,only a fraction of the predicted miRNA targets do. Moreover, forreasons that are so far unclear, the magnitude of the response varieswidely. In a recent analysis of over seventy experimental data sets ofmRNA or protein level changes that occurred upon perturbation of miRNAexpression, we found that functional miRNA target sites reside inaccessible regions, and that the degree of mRNA degradation is relatedto the U (and to some extent A) nucleotide content of the miRNA targetsite environment. This suggests that the fate of miRNA targets dependsnot only on their interaction with the RNA-induced silencing complex(RISC), but also on the presence of modulatory protein co-factors. Incollaboration with the group of Tom Tuschl (The RockefellerUniversity) we have recently identified a number of RBPs that interactwith the RISC complex and we characterized their cognate mRNAs byimmunoprecipitation and microarray expression profiling. We havefurther developed a novel cross-linking and immunoprecipitation methodto identify RBP targets at the resolution of individual bindingsites. Building on this work, we here propose to carry out the following projects:1. Computational modeling of miRNA-target interactions.The starting point of this study will be the set of sequences that we found, with the CLIP method mentioned above, to be bound by Argonaute protein-containing complexes. We will attempt to use machine learning approaches to identify the location of miRNA binding sites in these data and to characterize the mode of binding of miRNAs to their targets. We will also thoroughly characterize the properties of these binding sites and we will use the results to develop improved methods for miRNA target prediction.2. Experimental identification of the targets of U-rich element binding proteins.This project will build on one hand on the experimental method that we developed in collaboration with the group of Tom Tuschl (The Rockefeller University), and on the other hand on our computational results that indicate that U-rich elements are associated with the degradation of miRNA targets. We have selected a set of U-rich element binding proteins whose targets we would like to identify by CLIP, and then study in relationship with the miRNA targets that we already obtained.3. Development of computational models of RBP-mRNA interactions.This projects aims to again use the CLIP data that we have and will obtain in the near future to start developing models to specifically describe RBP-mRNA interactions, taking into account the particularities of this class of proteins. These projects will make an important contribution to a quantitativeunderstanding of miRNA-based post-transcriptional regulation and tothe inference of the elusive 'RNA code'.	Medical Research Council	Fellowship	306714.0GBP
475	Prof. MELLER Amit	TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY	2019-08-01	2024-07-31	Proteome profiling using plasmonic nanopore sensors	To date, antibody-free protein identification methods have not reached single-molecule precision. Instead, they rely on averaging from many cells, obscuring the details of important biological processes. The ability to identify each individual protein from within a single cell would transform proteomics research and biomedicine. However, single protein identification (ID) presents a major challenge, necessitating a breakthrough in single-molecule sensing technologies. We propose to develop a method for proteome-level analysis, with single protein resolution. Bioinformatics studies show that >99% of human proteins can be uniquely identified by the order in which only three amino-acids, Lysine, Cysteine, and Methionine (K, C and M, respectively), appear along the proteins’ chain. By specifically labelling K, C and M residues with three distinct fluorophores, and threading them, one by one, through solid-state nanopores equipped with custom plasmonic amplifiers, we hypothesize that we can obtain multi-color fluorescence time-trace fingerprints uniquely representing most proteins in the human proteome. The feasibility of our method will be established by attaining 4 main aims: i) in vitro K,C,M protein labelling, ii) development of a machine learning classifier to uniquely ID proteins based on their optical fingerprints, iii) fabrication of state-of-the-art plasmonic nanopores for high-resolution optical sensing of proteins, and iv) devising methods for regulating the translocation speed to enhance the signal to noise ratio. Next, we will scale up our platform to enable the analysis of thousands of different proteins in minutes, and apply it to sense blood-secreted proteins, as well as whole proteomes in pre- and post-metastatic cancer cells. NanoProt-ID constitutes the first and most challenging step towards the proteomic analysis of individual cells, opening vast research directions and applications in biomedicine and systems biology.	European Research Council	Advanced Grant	2498869.0EUR
476	Prof. MELLER Amit	MASSACHUSETTS GENERAL HOSPITAL	2020-01-09	2022-12-31	Demystifying the antiviral activity of the IgG3+ antibody response	Since 2002, several coronaviruses have emerged able to cause severe respiratory disease, however no vaccineis available to prevent these rapidly spreading pathogens. Vaccine design has specifically lagged due to our lackof understanding of the correlates of immunity against these pathogens. Both cellular and humoral immuneresponses have been implicated in resolution of disease, but to date only the passive transfer of antibodies hasbeen shown to confer complete protection in mice. Interestingly, the transfer of both “neutralizing” and non-neutralizing antibodies have shown protective efficacy, highlighting the role of multiple humoral mechanisms inlimiting viral infection/spread. The precise mechanism of action of these antibodies that have the most profoundimpact on limiting disease is currently unclear, but if elucidated could provide critical insights for the developmentof effective vaccines against COVID-19 and other coronaviruses. Thus, here we aim to take a systematicapproach to dissect and define both the polyclonal and monoclonal mechanisms by which antibodies conferprotection against COVID-19. Specifically, samples from DNA- and adenovirus 26 (Ad26)- COVID-19 Spikeprotein (S) immunized animals, that will be challenged with COVID-19, will be comprehensively profiled usingSystems Serology, to define the functional humoral immune responses linked to protection from infection/diseasein mice, ferrets, and macaques. Machine learning modeling will be employed to discern key immune responsefeatures that translate usefully across these diverse animal contexts. Coupled to a novel systems-Fc-engineeringapproach, the COVID-19 CR3022 monoclonal antibody will be engineered to specifically define the Fc-effectorfunctions that provide the greatest impact on limiting disease. Collectively, these studies will not only definecorrelates of immunity across vaccines and species, but also provide mechanistic insights into the precisemechanisms by which antibodies may confer protection in the context of future vaccines.	National Institutes of Health	Research Grant	391727.0USD
477	Prof. MELLER Amit	UNIVERSITY OF MASSACHUSETTS AMHERST	2016-09-01	2021-08-31	Statistical methods for real-time forecasts of infectious disease: expanding dynamic time-series and machine learning approaches for pandemic scenarios	The emergence and global expansion of SARS-CoV-2 as a human pathogen over the last four monthsrepresents a nearly unprecedented challenge for the infectious disease modelling community. This pandemichas benefitted from huge volumes of data being generated, but the rate of dissemination of these data hasoften outpaced existing data pipelines. While the last decade has seen significant advances in real-timeinfectious disease forecasting — spurred by rapid growth in data and computational methods — thesemethods have primarily focused on seasonal endemic diseases based, are based on historical data, and sodo not apply easily to this novel pathogen, or to pandemic scenarios. New methods are needed to leveragethe wealth of surveillance data at fine spatial granularity, together with associated information about policyinterventions and environmental conditions over space and time, to reason directly about the mechanisms toforecast and understand the transmission dynamics of SARS-CoV-2 transmission. These methods must usesound statistical and epidemiological principles and be flexible and computationally efficient to provide real-time forecasts to guide public health decision-making and respond to changing aspects of this global crisis.The central research activities of this project are (1) to develop scalable, computationally efficient Bayesianhierarchical compartmental models to flexibly respond to state-level public health forecasting needs, and (2)to design models and conduct analyses to draw robust inference about the effectiveness of interventions inimpacting the reproductive rate of SARS-CoV-2 infections within the US to build an evidence-base forcontinued responses to COVID-19 and future pandemics.	National Institutes of Health	Research Grant	78507.0USD
478	Dr Villar Moreschi Sofia	MRC Biostatistics Unit	2020-09-01	2021-08-31	Innovative designs for trials: methods and implementation	The emergence and global expansion of SARS-CoV-2 as a human pathogen over the last four monthsrepresents a nearly unprecedented challenge for the infectious disease modelling community. This pandemichas benefitted from huge volumes of data being generated, but the rate of dissemination of these data hasoften outpaced existing data pipelines. While the last decade has seen significant advances in real-timeinfectious disease forecasting — spurred by rapid growth in data and computational methods — thesemethods have primarily focused on seasonal endemic diseases based, are based on historical data, and sodo not apply easily to this novel pathogen, or to pandemic scenarios. New methods are needed to leveragethe wealth of surveillance data at fine spatial granularity, together with associated information about policyinterventions and environmental conditions over space and time, to reason directly about the mechanisms toforecast and understand the transmission dynamics of SARS-CoV-2 transmission. These methods must usesound statistical and epidemiological principles and be flexible and computationally efficient to provide real-time forecasts to guide public health decision-making and respond to changing aspects of this global crisis.The central research activities of this project are (1) to develop scalable, computationally efficient Bayesianhierarchical compartmental models to flexibly respond to state-level public health forecasting needs, and (2)to design models and conduct analyses to draw robust inference about the effectiveness of interventions inimpacting the reproductive rate of SARS-CoV-2 infections within the US to build an evidence-base forcontinued responses to COVID-19 and future pandemics.	Medical Research Council	Research Grant	78507.0USD
479	Dr Schmuker Michael	University of Hertfordshire	2020-08-14	2023-08-13	2014217 NeuroNex: From Odor to Action - Discovering Principles of Olfactory-Guided Natural Behavior	The emergence and global expansion of SARS-CoV-2 as a human pathogen over the last four monthsrepresents a nearly unprecedented challenge for the infectious disease modelling community. This pandemichas benefitted from huge volumes of data being generated, but the rate of dissemination of these data hasoften outpaced existing data pipelines. While the last decade has seen significant advances in real-timeinfectious disease forecasting — spurred by rapid growth in data and computational methods — thesemethods have primarily focused on seasonal endemic diseases based, are based on historical data, and sodo not apply easily to this novel pathogen, or to pandemic scenarios. New methods are needed to leveragethe wealth of surveillance data at fine spatial granularity, together with associated information about policyinterventions and environmental conditions over space and time, to reason directly about the mechanisms toforecast and understand the transmission dynamics of SARS-CoV-2 transmission. These methods must usesound statistical and epidemiological principles and be flexible and computationally efficient to provide real-time forecasts to guide public health decision-making and respond to changing aspects of this global crisis.The central research activities of this project are (1) to develop scalable, computationally efficient Bayesianhierarchical compartmental models to flexibly respond to state-level public health forecasting needs, and (2)to design models and conduct analyses to draw robust inference about the effectiveness of interventions inimpacting the reproductive rate of SARS-CoV-2 infections within the US to build an evidence-base forcontinued responses to COVID-19 and future pandemics.	Medical Research Council	Research Grant	688656.0GBP
480	Prof Arlt Wiebke	University of Birmingham	2009-03-01	2012-02-29	Steroid profiling as a biomarker tool in the diagnosis and monitoring of adrenal tumours	The emergence and global expansion of SARS-CoV-2 as a human pathogen over the last four monthsrepresents a nearly unprecedented challenge for the infectious disease modelling community. This pandemichas benefitted from huge volumes of data being generated, but the rate of dissemination of these data hasoften outpaced existing data pipelines. While the last decade has seen significant advances in real-timeinfectious disease forecasting — spurred by rapid growth in data and computational methods — thesemethods have primarily focused on seasonal endemic diseases based, are based on historical data, and sodo not apply easily to this novel pathogen, or to pandemic scenarios. New methods are needed to leveragethe wealth of surveillance data at fine spatial granularity, together with associated information about policyinterventions and environmental conditions over space and time, to reason directly about the mechanisms toforecast and understand the transmission dynamics of SARS-CoV-2 transmission. These methods must usesound statistical and epidemiological principles and be flexible and computationally efficient to provide real-time forecasts to guide public health decision-making and respond to changing aspects of this global crisis.The central research activities of this project are (1) to develop scalable, computationally efficient Bayesianhierarchical compartmental models to flexibly respond to state-level public health forecasting needs, and (2)to design models and conduct analyses to draw robust inference about the effectiveness of interventions inimpacting the reproductive rate of SARS-CoV-2 infections within the US to build an evidence-base forcontinued responses to COVID-19 and future pandemics.	Medical Research Council	Research Grant	727149.0GBP
481	Prof Arlt Wiebke	University of Birmingham	2018-04-01	2023-04-01	Dissecting Androgen excess and metabolic dysfunction – an Integrated SYstems approach to PolyCystic Ovary Syndrome (DAISY-PCOS)	The emergence and global expansion of SARS-CoV-2 as a human pathogen over the last four monthsrepresents a nearly unprecedented challenge for the infectious disease modelling community. This pandemichas benefitted from huge volumes of data being generated, but the rate of dissemination of these data hasoften outpaced existing data pipelines. While the last decade has seen significant advances in real-timeinfectious disease forecasting — spurred by rapid growth in data and computational methods — thesemethods have primarily focused on seasonal endemic diseases based, are based on historical data, and sodo not apply easily to this novel pathogen, or to pandemic scenarios. New methods are needed to leveragethe wealth of surveillance data at fine spatial granularity, together with associated information about policyinterventions and environmental conditions over space and time, to reason directly about the mechanisms toforecast and understand the transmission dynamics of SARS-CoV-2 transmission. These methods must usesound statistical and epidemiological principles and be flexible and computationally efficient to provide real-time forecasts to guide public health decision-making and respond to changing aspects of this global crisis.The central research activities of this project are (1) to develop scalable, computationally efficient Bayesianhierarchical compartmental models to flexibly respond to state-level public health forecasting needs, and (2)to design models and conduct analyses to draw robust inference about the effectiveness of interventions inimpacting the reproductive rate of SARS-CoV-2 infections within the US to build an evidence-base forcontinued responses to COVID-19 and future pandemics.	Wellcome Trust	Investigator Award in Science	2389852.0GBP
482	Dr Shallcross Laura	University College London	2017-02-01	2022-01-31	Precision antibiotic prescribing for urinary tract infection in hospital	The emergence and global expansion of SARS-CoV-2 as a human pathogen over the last four monthsrepresents a nearly unprecedented challenge for the infectious disease modelling community. This pandemichas benefitted from huge volumes of data being generated, but the rate of dissemination of these data hasoften outpaced existing data pipelines. While the last decade has seen significant advances in real-timeinfectious disease forecasting — spurred by rapid growth in data and computational methods — thesemethods have primarily focused on seasonal endemic diseases based, are based on historical data, and sodo not apply easily to this novel pathogen, or to pandemic scenarios. New methods are needed to leveragethe wealth of surveillance data at fine spatial granularity, together with associated information about policyinterventions and environmental conditions over space and time, to reason directly about the mechanisms toforecast and understand the transmission dynamics of SARS-CoV-2 transmission. These methods must usesound statistical and epidemiological principles and be flexible and computationally efficient to provide real-time forecasts to guide public health decision-making and respond to changing aspects of this global crisis.The central research activities of this project are (1) to develop scalable, computationally efficient Bayesianhierarchical compartmental models to flexibly respond to state-level public health forecasting needs, and (2)to design models and conduct analyses to draw robust inference about the effectiveness of interventions inimpacting the reproductive rate of SARS-CoV-2 infections within the US to build an evidence-base forcontinued responses to COVID-19 and future pandemics.	National Institute for Health Research (Department of Health)	Full Award	1127093.0GBP
483	Dr Stivaros Stavros	The University of Manchester	2006-09-04	2009-09-03	Language Based Decision Support System For Treatment Planning In Patients With Sub-arachnoid Haemorrhage.	The emergence and global expansion of SARS-CoV-2 as a human pathogen over the last four monthsrepresents a nearly unprecedented challenge for the infectious disease modelling community. This pandemichas benefitted from huge volumes of data being generated, but the rate of dissemination of these data hasoften outpaced existing data pipelines. While the last decade has seen significant advances in real-timeinfectious disease forecasting — spurred by rapid growth in data and computational methods — thesemethods have primarily focused on seasonal endemic diseases based, are based on historical data, and sodo not apply easily to this novel pathogen, or to pandemic scenarios. New methods are needed to leveragethe wealth of surveillance data at fine spatial granularity, together with associated information about policyinterventions and environmental conditions over space and time, to reason directly about the mechanisms toforecast and understand the transmission dynamics of SARS-CoV-2 transmission. These methods must usesound statistical and epidemiological principles and be flexible and computationally efficient to provide real-time forecasts to guide public health decision-making and respond to changing aspects of this global crisis.The central research activities of this project are (1) to develop scalable, computationally efficient Bayesianhierarchical compartmental models to flexibly respond to state-level public health forecasting needs, and (2)to design models and conduct analyses to draw robust inference about the effectiveness of interventions inimpacting the reproductive rate of SARS-CoV-2 infections within the US to build an evidence-base forcontinued responses to COVID-19 and future pandemics.	Medical Research Council	Fellowship	192024.0GBP
