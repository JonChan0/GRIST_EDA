0	Professor Philip Bath	Nottingham,University of	Stroke Trials Unit, Division of Clinical Neuroscience	None	None	Assessment of modern machine learning methods and conventional statistical regression techniques in diagnosis and prediction of outcome after acute stroke using big data	Stroke is common, may be mimicked by other conditions, and often has a poor outcome. We will use high-fidelity big data from acute stroke trials (which will include those with stroke, TIA and mimics) involving up to 90,000 instances/patients with baseline clinical and imaging, on treatment, diagnostic and outcome information. We will assess the accuracy of state-of-the-art machine learning with artificial neural network (ANN) models, including deep neural networks (DNN), in the diagnosis of stroke versus mimics, and prediction of early complications (deterioration, stroke recurrence and re-bleeding) and late outcomes (functional outcome, cognition, mood, quality of life, disposition, death) after stroke. These will be compared with conventional statistical regression models (binary, Cox, ordinal and linear regression). Neither ANN/DNN nor statistical approaches are used routinely in stroke management in part because of poor accuracy and acceptability to clinicians. If diagnostic and prediction models are accurate and acceptable to patients and clinicians then their use could be rapidly introduced into clinical care to improve patient management, for example through the use of apps on phones/tablets/computers.	British Heart Foundation	Project Grant	145079.0 GBP
1	Professor David Stuart	Diamond Light Source Ltd	None	2018-10-01	2021-09-30	SuRVoS Workbench: Enhanced machine learning for segmentation across structural biology	Very high-resolution 3D biological images can be directly interpreted in terms of atomic structures, lower-resolution data however is interpreted by segmenting into separate volumes. Volumetric biological image data has many complexities which make segmentation difficult. These data are often low contrast, with low signal-to-noise and can be terabytes in size, so that segmentation is an error-prone and time-consuming bottleneck. SuRVoS uses machine learning, advanced image filters and over-segmentation methods to semi-automate the process, dramatically reducing the time to segment data from weeks to days. This project aims to further reduce that time and increase reliability using the latest developments in machine vision and enhanced training sets. The ultimate goal is to completely automate segmentation using advances in deep learning. Such methods require significant quantities of already segmented data to train the systems. To build segmented data for this development the segmentation tools in SuRVoS will be incorporated into a citizen science platform (Zooniverse), enabling the segmentation of many datasets from a wide range of techniques and model systems, without placing a large burden on individual researchers. This platform will then feed-back firstly to enhance segmentation across the bio-imaging facilities at Diamond enabling usersâ€™ research, and ultimately the broader user community.	Wellcome Trust	Biomedical Resources Grant	605412.0 GBP
2	Univ.Prof. Dr. Andreas UHL	University of Salzburg	None	2019-10-01	2022-09-30	Tools for the Generation of Synthetic Biometric Sample Data	Current day biometric recognition and digitized forensics research struggles with a problem severely impeding progress in these security relevant fields: Large scale datasets of biometric data would be required to allow for flexible and timely assessments, but these are missing due to various reasons, amongst them privacy concerns. The latter have increased with the EU GDPR to an extend that even well established standardization bodies like NIST in the USA removed a large part of their publically available datasets before the GDPR became effective in May 2018. To solve this problem and address the attached data quality dimensions (quantitative as well as qualitative concerns), we will research methods allowing for the generation of large-scale sets of plausible and realistic synthetic data to enable reproducible, flexible and timely biometric and forensic experimental assessments not only compliant with the hunger for data we see with modern day techniques but also with EU data protection legislation. To achieve our goals, the work in this project follows two distinct solution approaches: The first (data adaptation) takes existing biometric / forensic samples, adapts them to reflect certain acquisition conditions (sensorial, physiological as well as environmental variability), and (if required by the application context) conducts context sensitive control of privacy attributes. The second approach (synthesizing) creates completely artificial samples from scratch according to specified sensorial, physiological as well as environmental variability. The practical work in the project is focused on digitized forensic (latent) fingerprints as well as on the two biometric modalities fingerprint (FP) and vascular data of hand and fingers (i.e. hand- and finger-vein images) (HFV). The theoretical and methodological concepts and empirical findings will be generalized, to discuss the potential benefits of the research performed also for other modalities (esp. in face recognition). From a methodological viewpoint, adapting samples to specific acquisition conditions may be seen as a domain adaptation process. To achieve this, we employ recent supervised and unsupervised generative deep learning techniques for data synthesis on the one hand; on the other hand we establish more traditional, model-based image processing pipelines using interactive guided image manipulations that are generalized to an arbitrary sample number. Also, the control of privacy attributes for samples is interpreted as a domain shift operation, removing privacy relevant data attributes by shifting the sample data to a domain in which these attributes are changed. Back-to-back, also model-based techniques will be investigated to remove privacy carrying sample-attributes from the data while keeping the identity carrying ones. All investigated techniques will be exemplary applied to generating data for investigating (i) robustness of hand- and finger-vein recognition schemes (exemplary selected for biometric recognition), (ii) presentation attack artifacts (exemplary selected for biometric recognition and digitized forensics), and (iii) digital dactyloskopy (exemplary selected digitized forensics). The project will be organised as an international project conducted by two groups at Magdeburg (Germany) and Salzburg (Austria) Universities, respectively, which are lead by Prof. Jana Dittmann (for the German side) and Prof. Andreas Uhl (for the Austrian side).	Austrian Science Fund FWF	None	431951.73 EUR
3	Professor Mauricio L. Barreto	Fiocruz (Oswaldo Cruz Foundation)	None	2020-04-01	2021-09-30	The risk of a chronic clinical condition following a previous hospitalisation by a psychiatric disorder: a linkage nationwide study in Brazil	This project will analyse electronic data routinely collected within the Brazilian Public Health System (SUS). It involves enriching this type of data by linking them to other Brazilian governmental databases destined to national social protection programmes, which have important socio-economic variables of the individuals. A very large dataset with over 114 million people (The 100 Million Brazilian Cohort), which more than half of the Brazilian population, will be consolidated to the study of multimorbidity and its socio-economic determinants nationwide. Therefore, establishing foundations on the magnitude of the problem, in particular on the most impoverished population and for further studies on this subject in Brazil and other similar middle-income countries. We aim to study the risk of hospitalisation or death by diabetes mellitus, cardiovascular disease, stroke, or tuberculosis associated with previous hospitalisation by the following psychiatric disorders: depression, alcohol and substance use-related, and schizophrenia. We also intend to identify disease clusters and their related patterns, and how these patterns interact over time to influence the formation of such clusters. "Big data" is seen as having great potential to answer numerous questions and CIDACS collection of Brazilian data is unique in low-middle income countries. Our access to this collection of data allows for unprecedented study of morbidity and mortality in a nationwide scope.	Medical Research Council	P&Cs	161780.0 GBP
4	Professor Christopher Gale	Leeds, University of	Leeds Institute of Cardiovascular and Metabolic Medicine	None	2021-09-30	Predicting patient-level new onset atrial fibrillation from population-based nationwide electronic health records: A precision medicine investigation using artificial intelligence	Atrial fibrillation (AF) is a major cardiovascular health problem: it is common, chronic and incurs substantial health-care expenditure as a result of stroke, sudden death, heart failure and unplanned hospitalisation. There is a compelling argument for the early diagnosis of AF, before the first complication occurs, but population-based screening is not recommended. Strategies to identify individuals at higher risk of new onset AF are required. Previous risk scores have been limited by data and methodology. This PhD will use routinely collected hospital-linked primary care data and focus on the use of artificial intelligence methods to develop and validate a model for the prediction of incident AF. Specifically, the studentship will investigate how population-based data may be used for precision medicine using a deep neural networks learning model. Using clinical factors readily accessible in primary care, the studentship will provide a method for the identification of individuals in the community who are at risk of AF, as well as when incident AF will occur in those at risk, thus accelerating research assessing technologies for the improvement of risk prediction, and the targeting of high-risk individuals for preventive measures	British Heart Foundation	Fellowship	188229.0 GBP
5	Professor Andrew Lotery	University of Southampton	None	2019-01-01	2024-01-01	Deciphering AMD by deep phenotyping and machine learning	We will identify the structural changes leading to and associated with cell degeneration in the retina in patients with early age-related-macular-degeneration (AMD). This will pinpoint what makes AMD progress towards visual loss (late AMD). This will be achieved via machine learning, genotyping and high resolution phenotyping of early AMD patients using retrospective and prospective data. Patients will undergo state-of-the-art imaging of the major tissues; neurosensory retina, retinal pigment epithelium and choriocapillaris to identify the sequence of cell degeneration. We will control for genetic risk by genotyping to infer ancestry and to identify individuals with extreme polygenic risk scores. Thus accounting for confounder effects of cryptic genetic diversity. Machine learning will be utilized to generate a generalized model of AMD progression on a population basis, enabling us to assess individual deviation from normal ageing. Structural imaging biomarkers will be developed in already collected extensive imaging databases, and validated in a prospective cohort study. Biomarkers will: 1) detect conversion to late AMD earlier; 2) discriminate slow/fast progressors and 3) identify therapeutic targets. Results should improve clinical trial design by better characterizing study populations and result in novel therapies by identifying the underlying mechanisms of one of the largest unmet medical needs.	Wellcome Trust	Collaborative Award in Science	3980169.0 GBP
6	Professor Andrea Mechelli	King's College London	None	2018-04-01	2020-04-01	Using deep learning technology to make individualised inferences in brain-based disorders	Brain-based disorders, including psychiatric and neurological illnesses, represent 10.4% of global disease. At present, objective tools for detecting these disorders, monitoring their progression over time and optimising treatment, are not available. In this project, Dr Andrea Mechelli at King's College London and colleagues are planning to use an innovative approach that capitalises on the latest developments in an emerging area of artificial intelligence known as "deep learning technology". Inspired by how the human brain processes information, this technology allows detection of complex and distributed patterns in the data that are difficult to capture using existing approaches. First, the team will assemble a very large dataset comprising neuroimaging data from 12,000+ disease-free individuals and 2,000+ patients with psychosis. Deep learning technology will then be used to develop a model of the disease-free brain across the different ages and genders. Finally, the team will illustrate how this model can be used to detect neuroanatomical alterations and inform clinical assessment in individual patients. This project will lead to the development of a practical and flexible web-based tool for measuring neuroanatomical alterations in any brain-based disorders. This tool could help clinicians assess the presence of a disease, monitor its progression over time and optimise treatment in individual patients.	Wellcome Trust	Innovator Award	421871.0 GBP
7	Dr Yee-Haur Mah	King's College London	Imaging & Biomedical Engineering	2019-09-09	2022-09-08	Clinical outcome modelling of rapid dynamics in acute stroke with joint-detail, continuous, remote, body motion analysis	Stroke is characterised by rapid temporal dynamics and marked functional heterogeneity, with early changes in the dynamics of upper limb recovery shown to be predictive of longer-term functional outcomes. Optimal stroke care therefore requires both detailed characterisation of the functional deficit and a high-resolution index of its rate of change. While such high-definition clinical supervision could be delivered, it would require close to a 1:1 staff/patient ratio, and could not be standardised with formal, objective models of the relation between outcomes and clinical features. Recent advances in deep learning-assisted modelling of ordinary video data have enabled the real-world extraction of detailed body motion information, resolved down to major joints, without body-worn devices and robust to large variations in body characteristics, dress and environmental context. My clinical and engineering collaborators have developed an inexpensive miniaturised device-MoCat-that combines in analysing body motion the richness and anatomical detail of a clinician with the objectivity and automatic deployability of a machine, in real time, within real-world clinical environments. Crucially, MoCat does not store or transmit video data, preserving patient privacy, and does not require any alteration to current care pathways (e.g. addition of body markers) or major changes to a hospital's digital or estate infrastructure, rendering it readily deployable throughout the NHS and beyond. We propose to deploy MoCat on the Hyperacute Stroke Unit at King's College Hospital. We shall quantify (1) within high-dimensional predictive models, the relationship between trajectories of motor deficits at different time scales and clinical outcomes of interest and actionable care pathway deviations; and (2) within high-dimensional lesion-deficit anatomical inferential models, the relation between trajectories of motor deficits at different temporal scales and stroke lesion patterns.	Medical Research Council	Research Grant	219833.0 GBP
8	Dr Heba Sailem	University of Oxford	None	2017-06-01	2021-11-30	Knowledge-driven analysis of image-based genetic screens using deep learning	Characterising gene functions is important for understanding life at the molecular level, and has a great impact on pharmacological and biomedical studies. Genetic screens that utilise High Throughput Imaging (HTI) have proved to be a powerful tool for studying gene functions by monitoring phenotypic changes in genetically modified cells. Challenges in analysing HTI datasets have significantly hindered knowledge discovery from such rich datasets. As HTI datasets are now acquired on a routine basis, there is a great need for generalisable analysis methods. Deep learning has revolutionised computer vision as it can automatically extract features and classify raw images without the need for any image preprocessing. I will develop deep learning methods to automatically discover cellular phenotypes and infer gene functions based on phenotypic similarity. I will build an integrative framework that utilises Functional Annotations (FAs) from various biological databases to annotate and classify images. I will apply these methods to five genome-wide datasets to predict tissue-specific gene functions based on the phenotypes of various cellular structures. The outcome of this work will be robust and generalisable HTI analysis methods that associate phenotypes to gene functions using deep learning as well as the discovery of novel gene functions and associated phenotypes.	Wellcome Trust	Sir Henry Wellcome Postdoctoral Fellowship	250000.0 GBP
9	Prof. Dr. Philipp GROHS	University of Vienna	None	2017-08-01	2020-10-31	Stable Infinite Dimensional Phase Retrieval	Phase retrieval refers to the problem of restoring a signal from intensity measurements only. Such problems appear in a number of important applications, starting from the classical phase retrieval problem in X-ray diffraction imaging where a signal is to be reconstructed from its Fraunhofer diffraction pattern which mathematically corresponds to the modulus of its Fourier transform. Further areas where such problems arise range from X-ray crystallography, microscopy to audio processing and deep learning. Due to the substantial amount of missing information, the numerical solution of phase retrieval problems is typically extremely challenging, especially in the presence of noise, which unfortunately is typically unavoidable. In recent work we were able to partially explain these difficulties by showing that any phase retrieval problem which is posed on an infinite-dimensional Banach space is automatically unstable in the sense that the reconstruction procedure can never be globally Lipschitz continuous. This negative result also shows that, in order to have any hope of achieving stable phase retrieval for realistic problems, new stabilization and preconditioning strategies are needed. The present proposal develops a deep understanding of the (in)stability properties of phase retrieval problems to explore such stabilization or preconditioning strategies, both from a mathematical and an algorithmic point of view. One of our starting points will be a newly developed theory of multi-component Lipschitz stability which has recently been introduced by the proposer and coauthors and which constitutes a weaker stability concept than global Lipschitz stability that is still sufficient for several applications of interest. As a main deliverable we plan to establish a comprehensive mathematical theory for the stability analysis of phase retrieval problems. Based on these mathematical results, we will analyze several physically meaningful measurement scenarios in terms of their stability with respect to noise with the aim of constructing the first ever provably convergent and noise-stable algorithms for the numerical solution of the corresponding phase retrieval problem. In addition, we will use our new theoretical understanding to explore the design of new measurement scenarios achieving superior stability as compared to standard methods, for instance in coherent diffraction imaging.	Austrian Science Fund FWF	Stand-Alone Project	368985.75 EUR
10	Professor Gunter Schumann	King's College London	None	2016-10-01	2021-09-30	Brain network based stratification of mental illness	To reduce the burden of mental disorders it is a formidable aim to identify widely applicable disease markers based on neural processes, which predict psychopathology and allow for targeted interventions. We will generate a neurobehavioural framework for stratification of psychopathology by characterising links between network properties of brain function and structure and reinforcementâ€“related behaviours, which are fundamental components of some of the most prevalent mental disorders, major depression, alcohol use disorder and ADHD. We will assess if network configurations define subtypes within and if they correspond to comorbidity across these diagnoses. We will identify discriminative data modalities and characterize predictors of future psychopathology. To identify specific neurobehavioural clusters we will carry out precision phenotyping of 900 patients with major depression, ADHD and alcohol use disorders and 300 controls, which we will investigate with innovative deep machine learning methods derived from artifical intelligence research. Development of these methods will optimize exploitation of a wide range of assessment modalities, including functional and structural neuroimaging, cognitive, emotional as well as environmental measures. The neurobehavioural clusters resulting from this analysis will be validated in a longitudinal population-based imaging genomics cohort, the IMAGEN sample of over 2000 participants spanning the period from adolescence to adulthood and integrated with information generated from genomic and imaging-genomic meta-analyses of >300.000 individuals. By targeting specific neural processes the resulting stratification markers will serve as paradigmatic examples for a diagnostic classification, which is based upon quantifiable neurobiological measures, thus enabling targetted early intervention, identification of novel pharmaceutical targets and the establishment of neurobehaviourally informed endpoints for clinical trials.	European Research Council	Advanced Grant	3394215.0 GBP
11	Prof. Giacomo INDIVERI	University Of Zurich	None	2017-09-01	2022-08-31	Neuromorphic Electronic Agents: from sensory processing to autonomous cognitive behavior	Neural networks and deep learning algorithms are currently achieving impressive state-of-the-art results. In parallel computational neuroscience has made tremendous progress with both theories of neural computation and with hardware implementations of dedicated brain-inspired computing platforms. However, despite this remarkable progress, todayâ€™s artificial systems are still not able to compete with biological ones in tasks that involve processing of sensory data acquired in real-time, in complex and uncertain settings. One of the reasons is that neural computation in biological systems is very different from the way today's computers operate: it is tightly linked to the properties of their computational embodiment, to the physics of their computing elements and to their temporal dynamics. Conventional computers on the other hand operate with mainly serial and synchronous logic gates, with functions that are decoupled from their hardware implementation, and with discretized and virtual time. In this project we will combine the recent advancements in machine learning and neural computation with the latest developments in neuromorphic computing technology to design autonomous systems that can express robust cognitive behavior while interacting with the environment, through the physics of their computing substrate. To achieve this we will embed in robotic platforms microelectronic neuromorphic processors and sensors that implement biophysically realistic neural computational primitives and dynamics. We will adopt active-sensing and on-line spike-based learning strategies, context and state-dependent computation, and probabilistic inference methods for "programming" these neuromorphic cognitive agents to solve challenging tasks in real-time. Our results will lead to compact low-power intelligent sensory-motor systems that will have a large impact on service and consumer robotics, Internet of Things, as well as prosthetics and personalized medicine.	European Research Council	Consolidator Grant	1999090.0 EUR
12	Nicolas Battich	Hubrecht Institute Van Oudenaarden Group Hubrecht Institute	Hubrecht Institute Van Oudenaarden Group Hubrecht Institute	2017-01-01	2017-07-31	A single-cell multi-omics approach to study mouse pre-implantation development	Pre-implantation development in mammals involves complex processes at the level of genomic organization, DNA methylation and transcriptional activity, leading to embryonic genome activation. How different epigenetic markers interact in single mouse blastomeres at early stages of development to influence the transcription of genes, and to what extent these processes vary from cell to cell and between embryos, is not fully understood. Although, in recent years a number of methods that enable single-cell omics measurement have been described, their integration to measure more than one entity in a single cell has remained challenging. Here, I propose a novel strategy for the integrated measurement of different epigenetic markers in combination with the measurement of nascent transcripts in single cells. The strategy relies on the use of a combination of specific restriction enzymes, for the measurement of 5-methylcytosine (MspJI), 5-hydroxymethylcytosine (AbaSI) and accessible chromatin (MseI and/or NlaIII), as well as 2â€™-fluoro GTP analogues in combination with specific RNases for the measurement of nascent transcripts. The application of these technologies to pre-implantation mouse embryos will shed light into the relationships of different epigenetic markers and their influence on transcription at the single cell level, and how these processes result in cell differentiation during early development. I further propose to use new deep-learning libraries, such as TensorFlow or Caffe, to quantitatively model and gain understanding of these dynamic processes in single blastomere cells.	Swiss National Science Foundation	Early Postdoc.Mobility	None
13	Dr. Andre HOLZAPFEL	The Austrian Research Institute for Artificial Intelligence (OFAI)	None	2016-04-01	2016-09-30	A deeper understanding of common elements in musical rhythm	When listening to an unfamiliar style of music, we attempt to tap the beat and to synchronise with the rhythm, a process that enables us to interpret the structure of what we hear. This process is made possible by properties of music encountered in cultures throughout the whole world. In this project, we aim to identify such common properties in musical rhythm and their culturally dependent interpretation by applying a novel multidisciplinary methodology that combines the perspectives of music information retrieval (MIR) and ethnomusicology. Our insights into common elements of musical rhythm will be shaped into unitary models for the analysis of temporal structure in the musics of the world. Such models represent an important contribution to temper the existing bias towards Western music in MIR research, and can contribute to systems that can cope with a larger cultural diversity of music. In this very moment, the epoch-making development of deep learning gives us the tool to explore the borders and potentials of machine learning in application to music as a cultural expression. We will approach discovering common elements by answering important research questions from ethnomusicology with the help of innovative unitary models that combine deep learning and Bayesian modelling. Deep learning enables the discovery of low-level common signal properties, and Bayesian models enable for inclusion of expert knowledge and culturally dependent high-level interpretation. Our developed models will offer perspectives for a fair and balanced music recommendation and distribution in digital platforms and offer radically novel scientific perspectives on music analysis within engineering and humanities. Our project will promote a deeper understanding of music that suits the needs of a new digital age and indicates ways to connect musicians and listeners across cultural borders.	Austrian Science Fund FWF	Lise Meitner Programme	159620.0 EUR
14	Dr Bender	University of Cambridge	None	2018-10-01	2021-09-30	Moving the Adverse Outcome Pathways Framework towards Practical Utility by Integrating Compound Profiling Data and Using Deep Learning	Despite a significant investment in in vitro and in vivo screening, clinical safety concerns are still a major cause of compounds being withdrawn from the development process. A lack of mechanistic understanding of safety findings, combined with sparse data sets, restricts the development of approaches capable of predicting earlier potential safety concerns for new chemical entities. It is often unclear whether these are truly surprise events or, with hindsight, could actually have been predicted based on the available data at the time. This failure to detect early signals of safety issues may arise due to problems at the interface between three main components of predictive analysis: observer, data and technology. In this work we propose to employ novel mathematical approaches to data representation and modelling, such as deep learning, of the processes that lead to an adverse outcome in order to reduce or eliminate events of this type. Data will be compiled from within AZ and GSK and shared via Lhasa Limited Ltd, with the student having access to both data sources for model generation. Depending on the adverse event considered, we will either compile AOP frameworks for adverse events, and/or use deep learning approaches for cases where only observational data is available, and hence extend current AOP frameworks where indicated. In particular, nodes in deep networks can be interpreted with respect to their contribution to output toxicity, and hence information for AOPs can be derived from them. The current project will involve GSK and AstraZeneca as industrial partners, with the charity Lhasa Ltd. as an 'honest broker', to exchange and pool compound profiling data for model generation. This framework is currently already being established in the context of the Cambridge Alliance on Medicines Safety (CAMS), and it hence improves significantly upon previous analyses which were based either on public data, or data from only a single source. A pilot study on Structural Cardiotoxicity has already been completed and it is currently being written up for publication, proving the viability and benefit of exchanging data for safety prediction in this framework. We now would like to broaden the approach to other adverse events where data is available. This proposal directly puts into practice recommendations published recently, and it represents an advancement over previous approaches (such as eTox) in being able to have broader access to internal safety data from two major pharmaceutical companies in the UK, AstraZeneca and GSK, shared under confidentiality agreements via a Trusted Broker, Lhasa. This means that all biological data will be made available in the context of the project, including experimental protocols, quantitative data on exposure of compounds, and additional target-based and biological profiling information of compounds can be used in this project. Hence, we expect that the combination of access to data, and the utilization of state-of-the-art machine learning methods, will enhance our ability to predict drug safety and toxicity events considerably. Increasing our understanding of the molecular mechanisms by which compounds can cause adverse events will lead to the implementation of much improved in silico and in vitro screens to detect safety risks will avoid unnecessary investments in preclinical and clinical studies.	National Centre for the Replacement, Refinement and Reduction of Animals in Research	Studentship	90000.0 GBP
15	Dr Alastair Droop	Wellcome Trust Sanger Institute	Cancer Genetics and Genomics	2019-04-08	2021-02-13	Facilitating Deep Learning with Domain-Specific Knowledge	Although a good conceptual fit, current methods in deep learning do not perform particularly well on biological data. One reason for this is the paucity of samples: we simply do not have enough data to train a neural network of sufficient size or complexity to learn the intricacies of our input datasets. Although we have many fewer biological samples, we have a much richer understanding of the underlying structure of the observations, as we have extensive prior knowledge about the interactions between genes. I am planning to use this rich prior knowledge to enhance the efficiency of the training process in two ways: 1) Appropriate data embedding A common technique in ML is to transform the input data into a form that is easier for the neural network to train upon (embedding). For example, high-dimensional data could be reduced using PCA to a lower dimensional form before use, thus removing unnecessary noise before training. I propose to develop similar methods using the large body of expertise in transcriptomic data analysis in Leeds to define suitable embedding strategies for preprocessing the input datasets. 2) Biologically appropriate network topologies A standard neural network consists of several hidden layers of neurones connected (both inside and between layers) in a regular manner. I propose a similar approach to learning gene expression analysis in which we use the vast amount of prior knowledge available in online repositories (such as the Gene Ontology, KEGG and String) to build intermediate neural network layers that model the known biological pathways and interactions in the data. This way, the ML approach does not need to learn these already-known interactions.	Medical Research Council	Fellowship	185651.0 GBP
16	Dr Deepti Gurdasani	Wellcome Trust Sanger Institute	Human Genetics	2018-02-14	2019-06-25	Predictive analytics of integrated genomic and clinical data using machine learning and complex statistical approaches	Although the utility of complex statistical, machine and deep learning (ML and DL) approaches in the context of multi-dimensional data has been clearly demonstrated, these methods have not been widely utilised to improve novel drug discovery and clinical risk prediction. This proposal aims to harness the potential of large-scale integrated genetic and health data to spur innovation, and develop predictive algorithms to improve clinical decision making and patient health. Specifically, this will focus on the development and evaluation of ML and DL frameworks for GWAS, and clinical risk prediction using publicly available large-scale EHR and genomics biodata resources, including UK Biobank, Genomics England and INTERVAL studies. Transcriptomic and functional data will be integrated into these using predictive approaches, where this has not been directly measured. This will be implemented in three stages: 1) assessment of complex time-dependent statistical approaches for modelling of hazard; 2) optimisation and assessment of existing ML and DL approaches for modelling of clinical risk; 3) development of novel approaches, specifically using recurrent neural networks (RNNs) to incorporate temporality and missingness in clinical data, including time varying covariates to accurately model complex hazard functions; the objective of this project will be to develop approaches that appropriately leverage the rich longitudinal and time-dependent data on individuals shown by us and others to substantially improve clinical risk prediction. In addition to risk prediction, this proposal will also focus on improving our understanding of genetic aetiology of disease. In addition to standard GWAS approaches, hybrid ML and GWAS approaches for prioritisation of candidate genes, and genetic variants associated with disease will also be applied, potentially improving the power to identify novel associations, with important implications for prioritisation of therapeutic targets.	Medical Research Council	Fellowship	319508.0 GBP
17	Dr David Wright	Queen's University of Belfast	Centre for Public Health	2018-02-14	2021-02-13	Data driven public health approaches for diabetic retinopathy and age-related macular degeneration	This fellowship will focus on the use of EHR and retinal imaging to improve population eye health. Developing novel outcome measures for chronic eye disease: Management of age-related macular degeneration (AMD) requires regular monitoring and rapid treatment if the wet form of the disease develops. Changes in either ocular structures or visual function can signal progression to wet AMD. 3D retinal imaging based on Optical Coherence Tomography can resolve ocular structures in unprecedented detail and QUB researchers have expertise using the latest modalities. However, analytical methods are sub-optimal, especially when attempting to link structural and functional changes. In this project, novel statistical methods will be developed to integrate a large retinal imaging dataset of AMD patients with EHRs of visual function. The aim is to develop meaningful outcome measures of AMD progression for use in clinical trials. Optimising diabetic retinopathy screening: Diabetic retinopathy (DR) is one of the most common causes of sight loss among working-age people in the UK. Those at risk of DR are screened; retinal photographs are taken at regular intervals and images are manually graded for specific pathologies. The aim of this project is to explore the potential of integrating automated image analysis into the Northern Ireland DR screening programme to both target treatment more effectively and reduce costs. A key challenge is predicting DR progression. There may be subtle patterns of retinal changes predicting DR progression that can be detected only by integrating data from many thousands of patients. Using screening images drawn from the NI diabetic eye screening programme (c. 87,000 patients), the fellow will apply the latest deep learning techniques (convolutional neural networks) to detect novel features predictive of DR progression. Performance of the automated methods will be assessed along with the potential for improvements to the screening programme.	Medical Research Council	Fellowship	279158.0 GBP
18	Professor Sophia Ananiadou	The University of Manchester	None	2009-09-01	2012-08-31	Automated Biological Event Extraction from the Literature for Drug Discovery	In establishing drug target confidence, it is essential to have evidence of the type of relationship between the target and key protein-bioprocesses. However, the primary starting point for target choice, and the context for interpretation of all pre-clinical observations is the literature. Text mining (TM) is ideally suited to support the discovery of reliable drug targets. But for TM systems to help researchers understand the role proteins play in biological processes, they have to extract, normalise and identify the context of complex relationships between genes, diseases and their underlying bioprocesses. Our TM techniques will recognise diverse surface forms in text describing bioprocesses and will link them with events and the proteins associated with them. Our methods are based on a combination of advanced semantic text mining (deep parsing, named entity recognition) and machine learning techniques, as we shall automatically identify events (involving proteins) such as decrease [in concentration], phosphorylation, ubiquitination, etc. Bioprocesses such as angiogenesis are composed of individual events described in the literature. We propose to identify these bioprocesses automatically and to link them with the associated events. A combination of kernel methods with knowledge resources and annotated texts (evaluated by biologists) will be used to automatically learn how bioprocesses underlying higher level processes are linked with which events. We shall concentrate on angiogenesis as an example. We shall thereby produce and make available a text mining service for researchers working in drug discovery. Both the software tools used for event extraction as well as the annotated texts used for training purposes will be made available. Co-funded by EPSRC under the RCUK Cross-Council Funding Agreement.	Biotechnology and Biological Sciences Research Council	Industrial (IPA)	288468.0 GBP
19	Prof. Titus NEUPERT	University Of Zurich	None	2018-01-01	2022-12-31	New paradigms for correlated quantum matter: Hierarchical topology, Kondo topological metals, and deep learning	Discovering, classifying and understanding phases of quantum matter is a core goal of condensed matter physics. Next to the notion of symmetry breaking phases, the concept of topological phases of matter is a prevailing theme of recent research. Topological phases are envisioned for various applications due to their universal and robust properties, such as protected conducting boundary modes, and provoke fundamental questions about the nature of many-body quantum states by providing the basis for exotic quasiparticles. In this ERC research project, I propose several new topological phases and novel numerical approaches for studying and classifying the most sought-after topological phases of matter. Concretely, I propose the concept of three-dimensional hierarchical topological insulators, which, in contrast to the known topological phases, do not posses gapless surface, but protected gapless edge modes. Moreover, I plan to study topological metals arising in strongly correlated Kondo systems, going beyond the current paradigm of considering topological metals that arise in the absence of electronic correlations. Furthermore, I propose to make the analogous step for topological superconductors, which have been studied as free models to search for Majorana quasiparticles: For the first time, I want to explore strongly interacting systems that realize the more powerful parafermion quasiparticles with numerical techniques. Finally, in a cross-disciplinary and exploratory sub-project, I will employ methods of deep neural networks to classify strongly correlated quantum phases using supervised learning combined with a technique called deep dreaming. Each of these sub-projects has the potential to make a paradigm-changing contribution to the study of strongly correlated and topological states of quantum matter and the combination of them allows to take advantage of synergy effects and a balance between high-risk and definitely feasible key developments.	European Research Council	Starting Grant	1362401.0 EUR
20	Dr Watjana Lilaonitkul	University College London	Institute of Health Informatics	2018-02-14	2021-02-13	Artificial Intelligence in Medical Treatment Decisions and Diagnosis	The data set is made of anonymised EHRs from the Great Osmond Street Hospital. Over time, we look to engage the NIHR Health Informatics Collaborative and other hospitals that are part of HDR London and HDR UK to collaboratively expand the data set. In the diagnosis network, we exploit a deep recurrent neural network architecture. We model EHR data using multi-task Gaussian processes (MGP) - a probabilistic model used to handle missing values and irregularly spaced observation times - and employ Long-Short-Term Memory (LSTM) units to capture long-range dependencies and nonlinear dynamics. We implement a multi-level deeply supervised network by introducing companion objective functions for each hidden layer that we will combine with the final loss value. We will also explore the effect of attention mechanisms on performance. We will compare the network performance with several state-of-the-art models and consider timeliness along with other metrics that quantify discrimination and accuracy. In the network where the goal is to learn optimal treatment regimens, we exploit a deep reinforcement learning architecture. We adopt an MGP-LSTM module to impute and interpolate the discretised EHR inputs, and employ a duelling Q-network architecture to learn the Q-values. The expected loss and gradient in the MGP layer will be approximated using a fixed number of Monte Carlo sampling. A different action-reward training environment will be structured for each disease explored. To speed up learning, we will explore prioritised experience. We will compare the network and physician performance using the estimated expected total Q-values.	Medical Research Council	Fellowship	313016.0 GBP
21	Dr Theo Sanderson	The Francis Crick Institute	None	2017-06-01	2021-05-31	Revealing the systems biology of malaria with a super-resolution atlas of the Plasmodium parasite	We now know that a majority of the malaria parasites 5,000 genes are essential for normal growth in the asexual blood stage (Bushell*, Gomes*, Sanderson*, et al., 2017). What we donâ€™t know is why â€“ what roles do these proteins play, and how do they contribute to parasite virulence? Building a systems-level understanding of malaria biology will require better answering that question, and doing so at the scale of the genome. One key question is protein localisation: the malaria parasite has many parasite-specific organelles, yet just 15% of its proteins have been imaged by microscopy to date. I propose to build a model of the three-dimensional localisations of at least 70% of P. falciparum proteins across the parasite's 48 hour intraerythocytic development cycle. I will do this by combining a recently-developed automated culture system with cutting edge super-resolution techniques. Using deep learning approaches, I will extract statistical maps of proteins from thousands of tagged lines, and generate a unified model of each proteinâ€™s localisation in an archetypical parasite developing over time. This data will generate profound insights into parasite ultrastructure which will be followed up with live-cell microscopy and conditional genetic approaches to understand the function of selected novel structures.	Wellcome Trust	Sir Henry Wellcome Postdoctoral Fellowship	250000.0 GBP
22	MSc BSc Rafael REISENHOFER	University of Vienna	None	2018-10-15	2020-10-14	Depth and Discriminability in Deep Learning Architectures	Deep neural networks have recently provided astounding results in a wide range of classification and regression tasks. This has sparked a renewed interest in the rigorous mathematical analysis of deep neural network architectures with the goal of uncovering the underlying principles that facilitate their groundbreaking success. Recent fundamental results already provide a better understanding of the relationship between depth and expressive power as well as a detailed analysis of the approximation properties of deep neural networks. It was also shown that certain types of convolutional neural networks exhibit desirable invariance properties such as stability with respect to small deformations. The present proposal aims at a mathematical investigation of another important property of a deep learning architecture, namely its discriminatory behavior. In most classification tasks, different classes are intertwined in a complex manner in the input space. In order to succeed, the realization of a deep neural network needs to disentangle those classes such that they can be separated in the corresponding feature space. Our goal is to better understand how the depth and other characteristics of a neural network influence its discriminatory power in the sense that they facilitate a clear separation of signal classes. Eventually, we aim to prove statements that quantify the discriminative power of a deep learning architecture with respect to distinct classes of signals as a function of depth and properties of the corresponding signals. The classification behavior of a neural network is mostly determined by its invariance properties on one side and its discriminatory properties on the other side. We will thus focus our investigation on a special class of convolutional neural networks, so-called scattering networks, for which substantial results regarding invariance and stability have already been established. We furthermore aim to utilize that when considering the modulus squared as a non-linearity, the output of each layer in a scattering network can be explicitly written as a cascade of autocorrelations in the frequency domain. In the initial phase of the project, we will investigate the discriminatory properties of scattering architectures with respect to simple template signal classes, such as signals that are sparse in the time or frequency domain. We then aim to extend our analysis to signal classes that resemble the structure of practical machine learning tasks in the sense that they are defined by shifts and time-frequency deformations of single prototype signals. Eventually, we aim to translate our theoretical findings into applicable guidelines regarding the optimal design of deep learning architectures for specific classification tasks. The research will primarily be conducted by the applicant (Rafael Reisenhofer) on a full-time basis. The work of the applicant will be supported by the invaluable expertise of the co-applicant (Philipp Grohs).	Austrian Science Fund FWF	None	156140.0 EUR
23	Assoz. Prof. Dr. Klaus SCHÃ–FFMANN	University of Klagenfurt	None	2018-10-01	2021-09-30	Relevance Detection of Ophthalmic Surgery Videos	In this project, we want to investigate fundamental research questions in the field of postoperative analysis of ophthalmic surgery videos (OSVs). More precisely, three research objectives are covered: (1) Classification of OSV segments - is it possible to improve upon the state-of-the-art in automatic content classification and content segmentation of OSVs, focusing on regular and irregular operation phases? (2) Relevance prediction and relevance-driven compression - how accurately can the relevance of OSV segments be determined automatically for educational, scientific, and documentary purposes (as medical experts would do), and what compression efficiency can be achieved for OSVs when considering relevance as an additional modality? (3) Analysis of common irregularities in OSVs for medical research - we address three quantitative medical research questions related to cataract surgeries, such as: is there a statistically significant difference in duration or complication rate between cataract surgeries showing intraoperative pupil reactions and those showing no such pupil reactions? We plan to perform these investigations using data acquisition, data modelling, video content analysis, statistical analysis, and state-of-the-art machine learning methods - such as content classifiers based on deep learning. The proposed methods will be evaluated on annotated video datasets ("ground truth") created by medical field experts during the project. Beyond developing novel methods for solving the abovementioned research problems, project results are expected to have innovative effects in the emerging interdisciplinary field of automatic video-based analysis of ophthalmic surgeries. In particular, research results of this project will enable efficient permanent video documentation of ophthalmic surgeries, allowing to create OSV datasets relevant for medical education, training, and research. Moreover, archives of relevant OSVs will enable novel postoperative analysis methods for medical research questions - such as causes for irregular operation phases, for example. The research project will be a cooperation between computer scientists of AAU Klagenfurt (conducted by Prof. Klaus SchÃ¶ffmann, supported and advised by Dr. Mario Taschwer and Prof. Laszlo BÃ¶szÃ¶rmenyi) and ophthalmic surgeons and researchers at Klinikum Klagenfurt (Dr. Doris Putzgruber-Adamitsch, Dr. Stephanie Sarny, Prof. Yosuf El-Shabrawi).	Austrian Science Fund FWF	None	379635.38 EUR
24	Professor Mark Gabbay	NHS Liverpool CCG	None	2019-10-01	2024-09-30	NIHR Applied Research Collaboration North West Coast	<p>The North West Coast region faces stark health inequalities. Average life expectancy can vary across Local Authority areas/wards by up to 12 years; for average <em>healthy</em> life expectancy this can be up to 27 years. Simultaneously, there are significant barriers to translating discoveries in academic health research into practice which improves lives (Cooksey, 2006). The Applied Research Collaboration North West Coast (ARC NWC) brings together health and social care providers, NHS commissioners, local authorities, universities, public advisers, the Innovation Agency (Academic Health Science Network) and more to address these problems. It will support the development, delivery and implementation of research into practice, enabling real and positive change for patients and the public, with an overarching focus on reducing health inequalities.</p> <p>We build on the CLAHRCNWC legacy, a successful collaborative programme of applied health research and implementation which has run from 2014. CLAHRCNWC has established infrastructure, connections and novel methodologies to address major challenges in this area. ARC will be an evolution of CLAHRCNWC, strengthened by some of CLAHRCNWC&rsquo;s key outputs, and with a broadened consortium including Primary and Social Care, and the third sector.</p> <p>&lsquo;Co-production&rsquo; is key to ARC NWC; projects will involve and in some cases be driven by partners and the public throughout the research life-cycle. This ensures research is relevant to local communities&rsquo; needs, practical, and shortens the delay between generating research findings and putting them into practice. Our established Neighbourhoods for Learning, a novel approach established by CLAHRCNWC for deep involvement of communities &ndash; will ensure co-production. Health Inequalities will remain a core focus of our work, aided by the CLAHRC-developed Health Inequalities Assessment Tool (HIAT) which integrates public involvement and assesses whether activities could potentially deliver a reduction in health inequalities.</p> <p>The partnership has jointly chosen three research themes reflecting local needs. These are: &lsquo;Person Centred Complex Care&rsquo;, &lsquo;Improving Population Health&rsquo; and &lsquo;Equitable Place-based Health and Care&rsquo;. ARC NWC&rsquo;s cross-cutting themes will provide expertise and guidance to support research theme activity: &lsquo;Care and Health Informatics&rsquo;, &lsquo;Methodological Innovation Development Adaptation and Support&rsquo; and &lsquo;Health and Care across the Life-course&rsquo;.&nbsp; NIHR&rsquo;s investment, combined with significant matched resources pledged by partners, will grow infrastructure to support a varied portfolio of projects across each theme to support our aims.</p> <p>ARC NWC&rsquo;s aims are:</p> <ul> <li>Engage and involve organisations which have a role in, or connection to, health and care service delivery, across social, primary, secondary care, public health and third sectors.</li> <li>Embed public involvement and health inequalities in all our work, from idea generation to implementing findings.</li> <li>Further develop partners&rsquo; and local communities&rsquo; research and implementation capacities and skills.</li> <li>Develop a portfolio of high quality applied research which responds to health inequalities.</li> <li>Support and evaluate implementation of research findings that address health priorities, to increase the sustainability and resilience of health and social care systems, locally and nationally.&nbsp;</li> <li>Shorten delays between research needs being identified, studied, and findings implemented.</li> <li>Ensure broader economic gains through increased research investment regionally.</li> <li>Collaborate with other ARCs and NIHR infrastructure to address national priorities.<br> &nbsp;</li> </ul> <p>&nbsp;</p>	National Institute for Health Research (Department of Health)	None	9000000.0 GBP
25	Ao. Prof. Dr. Roland KWITT	University of Salzburg	None	2019-08-01	2022-07-31	Deep Homological Learning	Topological data analysis (TDA), implemented via persistent homology, has gained considerable research interest over the past years, not least as it allows to capture topological (and geometrical) characteristics of data that are not easily extractable by existing methods. However, aside from using persistent homology as an exploratory tool for data analysis, its use as an additional data source for learning, or as a means to study learning problems, is still limited. The goal of this project, Deep Homological Learning, is to develop novel and theoretically well-founded approaches to bridge the gap between TDA, persistent homology in particular, and recent advances in learning with deep neural networks. This not only includes (1) leveraging information offered by persistent homology as an additional data source for learning, but also (2) to learn filtrations for persistent homology from data and (3) to use concepts from algebraic topology to study neural network architectures, their capacity and learning progress. Advances along these directions (1) will contribute to the theoretical foundation of learning with persistent homology and (2) will eventually lead to stronger guidelines for neural network design, informed by topological measures of dataset and network complexity. The principal investigator (PI) responsible for this project is Roland Kwitt (Department of Computer Science, University of Salzburg). Collaborations are planned with Marc Niethammer (UNC Chapel Hill), Ulrich Bauer (TU Munich) and Peter Bubenik (UF Gainesville).	Austrian Science Fund FWF	Stand-Alone Projects	238512.75 EUR
26	Dr Konrad Wagstyl	University of Cambridge	None	2019-08-05	2023-08-05	Deep Learning of Cerebral Cortex Microstructure	My research aims to bridge the gap between in vivo neuroimaging and underlying brain microstructure. MRI can inform us of shape or signal intensity changes in neurological and psychiatric diseases but has limited interpretability with respect to underlying neurobiological disturbances. Currently such specific measurement of neurobiology is only accessible through ex vivo histological techniques. I wish to further our understanding of how in vivo MRI reflects microstructure through three interrelated projects. 1) I will map the distribution of microstructural properties, including cytoarchitecture, myeloarchitecture and receptor-architecture to whole-brain reference atlases, thus representing these features in the space of brain scans. 2) I will create deep learning frameworks for analysing cerebral cortical structure. 3) I will use computational tools to link in vivo MRI cortical structure to its underlying microstructure in health and disease. I will use high-resolution and multi-modal structural MRI data to link interregional and interlaminar MRI differences to their underlying microstructural source. I will apply deep learning to a large cohort of patients with a form of surgically remediable epilepsy, focal cortical dysplasia, which is characterised by abnormal microstructure. This work aims to solve the clinical challenge of identifying subtle epileptogenic structural abnormalities from a patientâ€™s presurgical MRI scan.	Wellcome Trust	Sir Henry Wellcome Postdoctoral Fellowship	300000.0 GBP
27	Ao. Prof. Dr. Andreas HOLZINGER	Medical University of Graz	None	2019-11-04	2022-11-03	A Reference Model of Explainable AI for the Medical Domain	Towards A Reference Model of Explainable AI for the Medical Domain Andreas Holzinger December, 27, 2018 The progress of statistical machine learning methods has made AI increasingly successful. Deep learning exceeds human performance even in the medical domain. However, their full potential is limited by the difficulty to generate the underlying explanatory structures. The central problem is that they are regarded as "black-boxes" and even if we understand the mathematical principles, they lack an explicit declarative knowledge representation. A motivation for this project are rising legal and privacy issues. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make results re-traceable on demand - understandable by a human domain expert. We learned of a variety of technical solutions which are currently in development, which could help explain AI/ML systems and their decisions. Transparent algorithms could appropriately enhance trust of medical professionals, thereby raising acceptance of machine learning. Recently, the Google AI team emphasized the significance of research in explainable AI, the importance of Human-Computer Interaction (HCI) research for Knowledge Discovery from Data (KDD), and the urgent need for a research framework around the field of interpretability. In our ongoing research collaboration "Machine learning for diagnosis of colorectal cancer" with the Google AI group and the Institute of Pathology (Ethics Vote Medical University Graz 30-184 ex17/18) a training data set for AI/ML in digital pathology has been generated. As agreed with our colleagues at the Google AI group, we have now the chance to use machine learning algorithms developed in this context for our test environment; in return we will research towards making their algorithms explainable for our domain experts, who have the benefits of using the results for their medical teaching. This project will focus on but is not limited to digital pathological data and context. The advantage of this project is working with real-world data (under full ethics/data protection regulations) together with medical experts. This project will provide important contributions to the international research community in the following ways: evidence in various methods of explainability and novel methods and urgently needed tools for benchmarking and evaluation; Moreover, the contributions gained in this project can be used generally for reverse-engineering human learning and cognitive development and specifically to engineer more human like machine learning systems. All outcomes of this project will be made openly available to the international research community.	Austrian Science Fund FWF	Stand-Alone Projects	392773.5 EUR
28	Dr Brown	University of Lincoln	None	2020-09-07	2023-09-06	Towards an open-source, equipment-agnostic framework for automated welfare monitoring in the home cage	The moral implications of using mice and other higher animals in biomedical research has been a topic of debate for many years, and has steered various efforts towards minimising animal suffering. As the number of mice used in biomedical research continues to rise, there is greater emphasis than ever on approaches to refine husbandry protocols and humane endpoints to ensure that welfare concerns are dealt with swiftly and efficiently. Home cage monitoring has been proposed as a means capturing animal behaviour through the light and dark phases so that welfare deficits might be captured without the need for human intervention. Several commercial and non-commercial systems have been designed to capture video footage (and other data) of mice that offer several advantages over conventional experimental designs: (1) behaviours can be captured in a familiar, enriched environment; (2) observations are not confounded by novel apparatus or human experimenters; (3) it serves as a permanent digital record; and (4) video data are amenable to automated image analysis techniques. The challenge that remains in the context of HCA is the ability to analyse video footage in a manner that is fully-automated, comprehensive, robust, and computationally efficient. Under this proposal, the successful studentship awardee will develop a solution to the problem of automated HCA based on deep learning; a state-of-the-art approach to computer vision problems that utilises computational models inspired by the human visual cortex. The technique will be based on the "anomaly detection", whereby a model is initially trained to capture the range of possible behaviours exhibited by mice under normal circumstances (i.e., without welfare deficits). The methods works by learning to produce a set of visual and motion features that describe the behaviours of the mice, and then having the same model attempt to "reconstruct" the input clip from the features. In doing so, the model is forced to capture only key information about normal behaviour. Given that video clips depicting welfare deficits are visually different normal behaviors by definition, the model will be less capable of representing those data. Dr James Brown will be the primary supervisor of this work, who has a strong record of interdisciplinary research involving mouse models. He has previously worked with the Mary Lyon Centre - a collaborator on this project - on development 3D image analysis techniques for mouse embryo phenotyping. He has recently published several high-impact articles on machine learning techniques for the diagnosis of retinal disease, and is currently seeking approval from the US Food & Drug Administration. Prof Xujiong Ye will co-supervise this project, bringing more than 20 years' experience in computer vision and medical image analysis in both industry and academia. The methods outlined in this proposal align well recent work published Prof Ye on the development of algorithms to track and assess the welfare of farmyard pigs from top-down video footage.	National Centre for the Replacement, Refinement and Reduction of Animals in Research	Studentship	90000.0 GBP
29	Professor Emily Jefferson	University of Dundee	Population Health and Genomics	2019-08-01	2024-07-31	MICA: InterdisciPlInary Collaboration for efficienT and effective Use of clinical images in big data health care RESearch: PICTURES	The core programme focuses on 3 key areas: (1) Complex cohort building from noisy, heterogeneous data: we will develop algorithms for text mining and standardising imaging metadata. We will use machine learning classification techniques to group images and natural language processing to pre-process and mine knowledge features from the free-text data and remove the identifiable data. We will utilise imaging processing algorithms to search for features within the core dataset to build new cohorts based upon pixel data. (2) Scaling and handling big data: we will develop algorithms and optimisation processes for handling petabytes of imaging data and investigate how to optimise the use of GPUs within a virtual environment. (3) Cybersecurity: we will ensure that our systems are secure but also meet the requirements of the research community. Exemplar 1: Using CT scans provisioned from the national resource, we will use Deep Learning to train an algorithm to detect lung nodules and coronary artery calcification. The algorithm will be implemented as a Medical Device and made available within NHS Clinical PACS reporting workstations for Clinical performance evaluation and validation. To determine the risk of lung cancer and cardiovascular events we will use the nationally available longitudinal health outcomes data linked to the results from the CT scans. Exemplar 2: To develop a risk score for dementia and an understanding of which data is most important for prediction, we will use voxel based feature selection and support vector machines within a cross-validation framework for MRI brain images. Non-imaging analyses will be primarily cross-sectional and longitudinal including time variable exposure clinical covariates as well as genomic covariates. Risk predictions using image, genetics and clinical data for individual patients will be combined using decision tree methods to create a best overall predictor. Algorithm validation will use the national resource.	Medical Research Council	Research Grant	2851878.0 GBP
30	Dr Simon Walsh	Imperial College of Science, Technology and Medicine	None	2019-02-01	2024-01-31	Fibrotic lung disease on high-resolution computed tomography: predicting disease behaviour using computer algorithms.	Background: Fibrotic lung disease represents a group of conditions characterised by scarring of the lung with fibrotic tissue and is a seriously under-recognised problem for the NHS. In 2016, 1% of all deaths in the UK were due to fibrotic lung disease. Idiopathic pulmonary fibrosis (IPF) is the most common fibrotic lung disease and has an inexorably progressive disease course regardless of treatment.Among the fibrotic lung diseases is a group of patients who, independent of their clinical diagnosis, develop progressive, relentless fibrosis which is unresponsive to treatment and has a 'malignant-like' prognosis, recently termed, 'the progressive fibrotic phenotype'. However, there is currently no way to predict from baseline data which patients will develop this phenotype which negatively impacts treatment decisions and increases patient exposure to risky investigations such surgical lung biopsy. Identifying how fibrotic lung disease will progress is the most significant challenge for effective patient management in these conditions.High-resolution computed tomography (HRCT) is central to diagnosis in fibrotic lung disease, and a pattern of usual interstitial pneumonia (UIP) on HRCT is associated with progressive fibrosis. However, the progressive fibrotic phenotype is not confined to this pattern, and other HRCT appearances (or radiologic phenotypes) with the same progressive disease course have not been defined. Deep learning (DL) is a subset of machine learning which has recently achieved impressive results in several medical image classification problems. DL can provide accurate, universally accessible algorithms for analysing HRCT data and predicting which patients will develop the progressive fibrotic phenotype based on their baseline HRCT. Aims: To develop DL algorithms to predict the development of the progressive fibrotic phenotype using baseline HRCT. To identify novel radiologic phenotypes associated with progressive and stable disease.Methods: A dataset of fibrotic lung disease-specific HRCTs together with linked clinical data will be generated from the ILD unit of the Royal Brompton Hospital. Data and an international repository called The Open Source Imaging Consortium. DL will be used to generate an algorithm which predicts, using HRCT, progressive fibrotic lung disease defined by all-cause mortality or >=10% forced vital capacity decline at 12 months from the date of the HRCT. Novel 'progressive fibrotic radiologic phenotypes' identified by the deep learning algorithm will also be investigated. Algorithm validation will be performed prospectively in two longitudinal fibrotic lung disease cohorts and a prospective trial of anti-fibrotic therapy. The clinical impact of the algorithm will be tested by incorporating it in a multicentre study of multidisciplinary team diagnosis and management in 100 cases of fibrotic lung disease involving expert and non-expert multidisciplinary team groups.Dissemination: The results of this research will be disseminated through peer-reviewed publication and academic congresses. Patient groups, including the Pulmonary Fibrosis Warriors (who were instrumental in the development of this proposal), will assist in raising public and patient awareness of this research and identifying further possible avenues of research. Impact: Improved disease behaviour prediction in fibrotic lung disease will reduce patient exposure to unnecessary, risky investigations and reduce healthcare provider cost associated with those investigations.	National Institute for Health Research (Department of Health)	Full Grant	1053348.0 GBP
31	Dr Kirsten Christensen-Jeffries	King's College London	Imaging & Biomedical Engineering	2019-11-01	2024-10-31	Development of 2D and 3D Ultrasound Super-Resolution (US-SR) Imaging for the Clinic	Non-invasive imaging of the microvasculature is crucial for the early detection and intervention of diseases such as cancer and other microvascular related diseases. This proposal addresses a crucial clinical challenge: the lack of a sensitive, safe, repeatable method for detecting, characterising and monitoring such diseases. Recently developed ultrasound super-resolution (US-SR) is able to resolve microvascular details far beyond the diffraction limit at depth (>10 cm) in vivo. To transfer this technology into society, clinical translation and 3D US-SR development is urgently needed. Aims:. This research is designed to provide advances in areas of maximal clinical relevance through systematically designed experiments, recent technological advances, and clinical guidance and support. The results will provide both validation, and a solid grounding in basic science for the fast developing field of US-SR. The core objectives are: 1)To develop novel advanced, acquisition strategies and real-time software using deep learning, advanced signal processing methods, and the activation of sparse contrast agent to provide improved detection accuracy, localisation rates, and automation. 2)To design, develop and test (in vitro and in vivo) the accuracy and clinical relevance of structural and functional parameters for disease characterisation. 3)To formalise clinical 2D US-SR and establish clinical acceptance. This involves the demonstration of the diagnostic power of US-SR parameters over large clinical datasets from existing studies. 4)To develop and implement clinical 3D US-SR with the aid of advances made in 1)-3), a healthy volunteer pilot study and existing patient studies. By making maximal use of planned or existing trials, I will avoid the need for significant additional human involvement. Throughout this translational project, methods will be evaluated by an extensive network of clinicians and researchers, allowing continuous feedback and ongoing knowledge	Medical Research Council	Fellowship	1077817.0 GBP
32	Dr Andrew Saxe	University of Oxford	None	2019-09-01	2024-08-31	Principles of Learning in Distributed Brain Networks	For biological organisms, practice improves performance on any given task. Ultimately these behavioral improvements must be traceable to some change in the neuronal networks of the brain. Crossing these scalesâ€”linking learning to its neural basisâ€”is a critical challenge facing theories of brain function. What changes at the neural level enable these behavioral improvements at the psychological level? And what principles govern the dynamics of the learning process? The goal of this project is to use mathematical analyses to develop a fundamental understanding of learning dynamics in artificial deep neural networks; to exploit this understanding to make testable predictions for experiments in psychology and neuroscience; and to critically test these hypotheses through close collaboration with experimental groups. This theory-experiment loop will help identify computational principles governing learning in the brain and mind. The proposal focuses on the dynamics of learning in complexly interconnected networks of brain areas; in settings where new knowledge must be incorporated alongside old (continual learning); and where there are nonlinear interactions between inputs and context. This research will furnish a new analytical toolkit for studying learning in neural networks, enabling a better understanding of how experience structures neural representations, and improved algorithms for artificial systems.	Wellcome Trust	Sir Henry Dale Fellowship	771226.0 GBP
33	Dr Alastair Droop	University of Leeds	School of Medicine	2018-02-14	2019-04-07	Facilitating Deep Learning with Domain-Specific Knowledge	Although a good conceptual fit, current methods in deep learning do not perform particularly well on biological data. One reason for this is the paucity of samples: we simply do not have enough data to train a neural network of sufficient size or complexity to learn the intricacies of our input datasets. Although we have many fewer biological samples, we have a much richer understanding of the underlying structure of the observations, as we have extensive prior knowledge about the interactions between genes. I am planning to use this rich prior knowledge to enhance the efficiency of the training process in two ways: 1) Appropriate data embedding A common technique in ML is to transform the input data into a form that is easier for the neural network to train upon (embedding). For example, high-dimensional data could be reduced using PCA to a lower dimensional form before use, thus removing unnecessary noise before training. I propose to develop similar methods using the large body of expertise in transcriptomic data analysis in Leeds to define suitable embedding strategies for preprocessing the input datasets. 2) Biologically appropriate network topologies A standard neural network consists of several hidden layers of neurones connected (both inside and between layers) in a regular manner. I propose a similar approach to learning gene expression analysis in which we use the vast amount of prior knowledge available in online repositories (such as the Gene Ontology, KEGG and String) to build intermediate neural network layers that model the known biological pathways and interactions in the data. This way, the ML approach does not need to learn these already-known interactions.	Medical Research Council	Fellowship	286050.0 GBP
34	Dr Yang Long	Newcastle University	Sch of Computing	2018-02-14	2019-06-30	Intelligent Healthcare Systems for Large-scale Populations	The first stage focuses on proof-of-concept studies. After getting access to UK Biobank and NE 85+, the inital step is Data Preprocessing, i.e. integration of complex data, such as MRI, accelerometer, and electronic health records. My previous AI model for infant stroke detection can be employed to explore some preliminary experiments, e.g. associations between human activities and specific diseases. Following this, utilising a multi-modal learning framework, a comprehensive model will be proposed to study the correlations between different health data sources. For example, what MRI pattern can correspond to a disease and how would that relate to the accelerometer data for human activity. Furthermore, attribute learning techniques can provide health feedback, e.g. how to implement a habit that can significantly reduce the risk of a disease. Finally, the model will be upgraded into an inference model and link the predictions to attributes, in a manner that is intuitive to understand.To further improve the interpretations, high-level visualisation technology, e.g. tSNE, Confusion Matrix, will provide furhter guidance about rationales behind the AI model. During the second stage, proof-of-concept studies will continue to the end of the project. Meanwhile, I plan to implement stable models on apps and smart sensors to encourage more participants to take parts in data collection. For example, stable models can be packaged into apps that can utilise mobile phone-embedded accelerometers as an approximate measurement for data collection and large-scale cohort clinical study. The expected outcomes of the project will be a series of high-quality publications both in AI and health data science fields. In addition, continued grant proposals will be submitted for the continued study of large-scale health data collection using the delivered system.	Medical Research Council	Fellowship	193284.0 GBP
35	Dr Emmanouela Repapi	University of Oxford	Weatherall Inst of Molecular Medicine	2018-04-01	2022-10-28	Novel methods for the integration of high dimensional single cell proteomic and RNA data to understand cell populations in development and disease.	A methodology will be developed for the integration of single cell RNA-Sequencing (scRNA-Seq) and CyTOF data using novel machine learning techniques. I will be using an autoencoder, a technique that is widely used in machine learning, to extract the structure of data from populations of cells analysed with both technologies. In more detail, I will be using the python library TensorFlow, which is one of the most commonly used tools for deep learning, to analyse in parallel the scRNA-Seq and CyTOF data and use the data structure from each to disassociate cell heterogeneity from technical variability. In addition, this approach will ensure that the right normalization has been performed and will take into account batch effects arising from the different technologies, and potentially also from the experimental design. It is important to note that cell populations observed in one of the two technologies will also be reported separately to account for biological variation between transcriptomics and proteomics data. This approach is subsequently going to be used 1) to identify cell subpopulations and 2) to study cell fate in differentiation processes. The data will be acquired from collaborations in the WIMM and I will be working closely with the groups that have produced it to ensure accurate interpretation. Finally, I will be creating a visualisation tool to facilitate the interpretation of the results by both computational and bench scientists. I plan to use WebGL, which allows the use of 3D graphics in web pages with increased speed for data heavy applications. This approach will enable the visualisation of millions of cells. This will be implemented using python packages such as Plotly and Vispy, which can help create a WebGL application from python objects.	Medical Research Council	Fellowship	326906.0 GBP
36	Ms Esra Suel	Imperial College London	School of Public Health	2018-02-14	2021-02-13	Application of deep learning to heterogeneous open data for measuring urban environment and health	Recent advances in deep learning methods have achieved unprecedented improvements in accuracy on multiple visual tasks such as object recognition and classification. While previously unused large-scale data is gaining importance in public health research, recent technical developments mostly focused on advanced spatial machine learning and statistics methods while integration of deep learning techniques have been largely unexplored. The overall goal of my fellowship research is to leverage these advanced methods to answer important questions in environment and health research. Specifically, convolutional neural networks will be trained using satellite and street view imagery to extract outcomes of health and its environmental/social determinants. Transfer learning and class specific saliency maps will be used for post-hoc model visualisation. A combined analysis of models trained for multiple outcomes will be conducted to study overlaps and deviations. Secondly, joint use of mobile phone and image data as predictors will be explored for improving model performance. New methods of data integration will be developed to enable the use of multiple big data sources as predictors in a unified modelling framework. Finally, transferability of deep learning models trained on data from one city to other geographies will be evaluated; adaptation techniques to facilitate transferability will be explored.	Medical Research Council	Fellowship	331573.0 GBP
37	Mr Keith Smith	University of Edinburgh	Centre of Population Health Sciences	2018-02-14	2021-02-13	Developing network methodologies for disease classification	For objective (1) I will develop an integrated framework of complex networks and convolutional neural networks for health data which will build upon geometric deep learning [3], building established connections with Prof Pierre Vandergheynst of the Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne (EPFL). Complex physiological networks will be derived from the multidimensional, complex datasets in the UK Biobank, which will then be utilised in this framework to gather important insights into health and disease and develop enhanced large-scale classification strategies. For objective (2) I will build on pilot work (unpublished) which applies novel complex network methods [4,5] to detect AD from task-related EEG signals. This will be extended for the following: (i) refinement and continued development of theoretical approaches to analyse functional connectivity (ii) adaptations for application to high spatial resolution fMRI and MRI data (iii) applied to resting-state fMRI in large datasets such as the Lothian Birth Cohort and UK Biobank (iv) employment of powerful classification and regression approaches such as support vector machines Such methods are also of great relevance to an array of different pathologies, including other forms of dementia, epilepsy, Parkinson's disease and schizophrenia, to which the strategy proposed will be generalised. This project is in alignment with HDR UK priorities, developing novel data analyses and classification strategies for key national health problems, establishing national leadership, developing international collaborations, and strengthening interdisciplinary skills. References: see Summary	Medical Research Council	Fellowship	276418.0 GBP
38	Dr Philipp Schneider	University of Southampton	None	2019-01-16	2022-01-16	Foundations for routine 3D X-ray histology	Whilst 3D medical imaging is commonplace, microscopic tissue structure analysis (i.e. histology) remains overwhelmingly wedded to ~200-year-old practices of microscopic 2D examination of tissue sections. Building on our first demonstrated assessment of 3D microstructural detail in standard wax-embedded soft tissue (Scott et al., 2015, http://dx.doi.org/10.1371/journal.pone.0126230) we will develop the foundations for routine 3D X-ray histology by: Providing: 1. Robust, non-destructive, high-throughput 3D microscopic imaging of routinely processed soft tissue samples at 5-10 Âµm resolution. 2. Access to orders of magnitude more tissue structure data per sample (cf. optical sections). 3. Tissue-level volumetric structure and connectivity insights (previously unavailable). 4. Correlative 3D data supplementing other routine or specialist imaging (e.g. immunostaining, laser microdissection). 5. Routes to novel medical and scientific discovery (Jones et al., 2016, http://dx.doi.org/10.1172/jci.insight.86375; Koo et al., 2018, in revision). Establishing: 6. Hardware approaches to increased sample throughput. 7. Standardised, automated workflows for image acquisition, calibration, CT reconstruction, data handling. 8. Long-term roadmap for emerging X-ray technology improvements targeted for biomedical applications. Raising awareness/driving uptake through: 9. Pilot studies (n~10 samples/study)/new applications validated by 2D histology and integrated with correlative imaging modalities. 10. Biomedical case studies (n~100) at high statistical power. 11. Data resources (n~100) for artificial intelligence and deep learning approaches in digital 3D histopathology.	Wellcome Trust	Technology Development Grant	1135071.0 GBP
39	Professor Sarah Barman	Kingston University	Faculty: Science Engineering & Computing	2018-11-15	2020-11-14	Classification of oral lesions using deep learning for early detection of oral cancer	About 300,000 individuals are diagnosed with oral cancer every year and the majority of these patients live in low- and middle-income countries. Up to 80% of oral cancers are preceded by early lesions called oral potentially malignant lesions (OPMD) and this presents an opportunity to identify and manage these lesions to reduce the chances of malignant transformation. However, due to the numerous types of lesions that present in the oral cavity, the identification of OPMD is not straightforward. Further, this is confounded by the limited number of trained oral medicine specialists in countries where the prevalence is at its highest. One way to overcome the lack of access to expertise for the clinical diagnosis of these lesions is to use deep learning approaches to develop an algorithm that could automatically classify images into different categories that require distinct clinical management. Such an approach has been successfully used for the classification of skin diseases. In this project, we will work within a multi-disciplinary team to use deep learning for the classification of oral lesions. In particular, we will employ two different deep learning architectures - VGG-16 and ResNet, pre-trained using ImageNet, and apply transfer learning to classify oral lesions. The outcome of this project will pave the way for further rigorous testing, development of an App incorporating this automated tool and clinical validation for the early detection of oral cancer. The development of an automated tool for the classification of oral lesions would facilitate the identification of patients most at risk of developing oral cancer so that these individuals can be managed appropriately.	Medical Research Council	Research Grant	146920.0 GBP
40	Assoz. Prof. Dr. Klaus SCHÃ–FFMANN	University of Klagenfurt	None	2019-04-01	2023-03-31	Surgical Quality Assessment in Gynecologic Laparoscopy	Endoscopic surgeries require specific psychomotor skills that are difficult to learn and teach, and typically result in prolonged learning curves. These psychomotor skills have direct impact on the performance of the surgery, especially in a field with complex operation techniques. In order to assess surgical quality objectively, medical experts currently record the entire surgery on video and inspect and analyze the unedited video footage in a post-operative session for the occurrence of technical errors, according to some standardized rating scheme. Several studies have shown that such post-operative analysis of errors and the reporting of them to the responsible surgeons can significantly improve their performance over time and lead to better surgical quality, especially for young surgeons. However, currently the surgical quality assessment (SQA) process is so tedious and time-consuming that many surgeons/clinicians cannot afford to perform such error ratings, which is very unfortunate since their application would improve surgical quality and patient outcome. The main reason for the high effort is the fact that it is performed without any special error rating software, but with common software video players and manually edited checklists, where surgeons enter timestamps of corresponding relevant scenes in the video. This renders SQA currently not only a very time-consuming process, but also a very error-prone one. In this research project we want to address this issue and find out how we can make surgical quality assessment (SQA) more efficient through automatic video content analysis and, hence, more feasible. More specifically, for the field of gynecologic laparoscopy we want to investigate to what extent current methods of machine learning and content-based video retrieval can support SQA (i.e., optimize the entire process through automatic classification and retrieval of technical errors). For that purpose, we will evaluate deep learning approaches as well as video content description and similarity search. We consider this research project as a pioneering work in the interdisciplinary overlap of computer- and medical science, which will investigate fundamental research questions that should provide the basis for future computer-aided SQA. We expect that our research results will help to significantly facilitate the currently cumbersome and error-prone SQA process, and hence enable more surgeons to actually perform error ratings. We expect this project even to contribute to improve surgical education in the long run (through higher penetration of the SQA process â€“ due to lower time effort), and thereby raise surgical quality itself. Project results and their later application in appropriate software tools could help surgeons to keep track of their surgical actions in a novel, highly efficient way, and thus help them to avoid technical errors. This will not only save valuable time of medical experts and increase the performance of quality assessment, but also contribute to surgical risk management and quality control.	Austrian Science Fund FWF	01 Stand-Alone Projects	388542.0 EUR
41	DEEP RENDER LTD	DEEP RENDER LTD	None	None	2023-03-31	AI-based Image-As-Video Streaming	Deep Render Ltd is a London based deep-tech start-up developing the next generation of AI-based media compression algorithms. Our proprietary and patented technology is at the forefront of machine learning research. Deep Render is combining the fields of artificial intelligence, statistics and information theory to unlock the fundamental limits of image and video compression: The human eye is the best data compressor known to humanity - with compression ratios at least 2,000 times better than everything developed to date. Our Biological Compression approach approximates the neurological processes of the human eye through a non-linear, learning-based approach, thereby creating a novel class of highly efficient compression algorithms. We are world leaders in this domain, and our unique, AI-based image compression technology is already providing a 75% efficiency gain over the best previous compression standards. As global data consumption is growing exponentially with more than 80% of traffic being Image/Video, Deep Render's AI-based compression technology is vital to bypass broadband constraints. The outbreak of COVID-19 has now accelerated this growth, as a result of the crisis, internet usage has increased significantly. In particular, the demand for live-streaming and video-chat services. Therefore we want to apply our already working image compression codec to live-video streaming. The outcome of the project is to extend our image compression codec to an image-as-video live-streaming codec, at least 75% more efficient than the current state-of-the-art. Our target customers are the live video chat services (Zoom, Skype, Microsoft Teams), as well as the entertainment industry, including live streaming platforms (Twitch, Facebook, Instagram, YouTube). Our value proposition is easy to understand. By making file sizes 75% smaller, we directly increase the bandwidth supply of the internet for live-streaming by a factor of 4\. Increasing the bandwidth supply by making file sizes smaller, is magnitudes more value- and time-efficient than increasing the bandwidth supply through rewiring the globe with progressively more fibre-cables. Deep Render is going to help create a new age in which bandwidth constraints are a problem of the past. As a result of COVID 19 solving this problem has gained more importance and Deep Render is determined to create a fast solution.	UK Research and Innovation	Research Grant	50000.0 GBP
42	Nicolas Battich	Hubrecht Institute Van Oudenaarden Group Hubrecht Institute	Hubrecht Institute Van Oudenaarden Group Hubrecht Institute	2017-01-01	2017-07-31	A single-cell multi-omics approach to study mouse pre-implantation development	Pre-implantation development in mammals involves complex processes at the level of genomic organization, DNA methylation and transcriptional activity, leading to embryonic genome activation. How different epigenetic markers interact in single mouse blastomeres at early stages of development to influence the transcription of genes, and to what extent these processes vary from cell to cell and between embryos, is not fully understood. Although, in recent years a number of methods that enable single-cell omics measurement have been described, their integration to measure more than one entity in a single cell has remained challenging. Here, I propose a novel strategy for the integrated measurement of different epigenetic markers in combination with the measurement of nascent transcripts in single cells. The strategy relies on the use of a combination of specific restriction enzymes, for the measurement of 5-methylcytosine (MspJI), 5-hydroxymethylcytosine (AbaSI) and accessible chromatin (MseI and/or NlaIII), as well as 2â€™-fluoro GTP analogues in combination with specific RNases for the measurement of nascent transcripts. The application of these technologies to pre-implantation mouse embryos will shed light into the relationships of different epigenetic markers and their influence on transcription at the single cell level, and how these processes result in cell differentiation during early development. I further propose to use new deep-learning libraries, such as TensorFlow or Caffe, to quantitatively model and gain understanding of these dynamic processes in single blastomere cells.	Swiss National Science Foundation	Early Postdoc.Mobility	None
43	Dr Alastair Droop	Wellcome Trust Sanger Institute	Cancer Genetics and Genomics	2019-04-08	2021-02-13	Facilitating Deep Learning with Domain-Specific Knowledge	Although a good conceptual fit, current methods in deep learning do not perform particularly well on biological data. One reason for this is the paucity of samples: we simply do not have enough data to train a neural network of sufficient size or complexity to learn the intricacies of our input datasets. Although we have many fewer biological samples, we have a much richer understanding of the underlying structure of the observations, as we have extensive prior knowledge about the interactions between genes. I am planning to use this rich prior knowledge to enhance the efficiency of the training process in two ways: 1) Appropriate data embedding A common technique in ML is to transform the input data into a form that is easier for the neural network to train upon (embedding). For example, high-dimensional data could be reduced using PCA to a lower dimensional form before use, thus removing unnecessary noise before training. I propose to develop similar methods using the large body of expertise in transcriptomic data analysis in Leeds to define suitable embedding strategies for preprocessing the input datasets. 2) Biologically appropriate network topologies A standard neural network consists of several hidden layers of neurones connected (both inside and between layers) in a regular manner. I propose a similar approach to learning gene expression analysis in which we use the vast amount of prior knowledge available in online repositories (such as the Gene Ontology, KEGG and String) to build intermediate neural network layers that model the known biological pathways and interactions in the data. This way, the ML approach does not need to learn these already-known interactions.	Medical Research Council	Fellowship	185651.0 GBP
44	Dr Deepti Gurdasani	Wellcome Trust Sanger Institute	Human Genetics	2018-02-14	2019-06-25	Predictive analytics of integrated genomic and clinical data using machine learning and complex statistical approaches	Although the utility of complex statistical, machine and deep learning (ML and DL) approaches in the context of multi-dimensional data has been clearly demonstrated, these methods have not been widely utilised to improve novel drug discovery and clinical risk prediction. This proposal aims to harness the potential of large-scale integrated genetic and health data to spur innovation, and develop predictive algorithms to improve clinical decision making and patient health. Specifically, this will focus on the development and evaluation of ML and DL frameworks for GWAS, and clinical risk prediction using publicly available large-scale EHR and genomics biodata resources, including UK Biobank, Genomics England and INTERVAL studies. Transcriptomic and functional data will be integrated into these using predictive approaches, where this has not been directly measured. This will be implemented in three stages: 1) assessment of complex time-dependent statistical approaches for modelling of hazard; 2) optimisation and assessment of existing ML and DL approaches for modelling of clinical risk; 3) development of novel approaches, specifically using recurrent neural networks (RNNs) to incorporate temporality and missingness in clinical data, including time varying covariates to accurately model complex hazard functions; the objective of this project will be to develop approaches that appropriately leverage the rich longitudinal and time-dependent data on individuals shown by us and others to substantially improve clinical risk prediction. In addition to risk prediction, this proposal will also focus on improving our understanding of genetic aetiology of disease. In addition to standard GWAS approaches, hybrid ML and GWAS approaches for prioritisation of candidate genes, and genetic variants associated with disease will also be applied, potentially improving the power to identify novel associations, with important implications for prioritisation of therapeutic targets.	Medical Research Council	Fellowship	319508.0 GBP
45	Dr David Wright	Queen's University of Belfast	Centre for Public Health	2018-02-14	2021-02-13	Data driven public health approaches for diabetic retinopathy and age-related macular degeneration	This fellowship will focus on the use of EHR and retinal imaging to improve population eye health. Developing novel outcome measures for chronic eye disease: Management of age-related macular degeneration (AMD) requires regular monitoring and rapid treatment if the wet form of the disease develops. Changes in either ocular structures or visual function can signal progression to wet AMD. 3D retinal imaging based on Optical Coherence Tomography can resolve ocular structures in unprecedented detail and QUB researchers have expertise using the latest modalities. However, analytical methods are sub-optimal, especially when attempting to link structural and functional changes. In this project, novel statistical methods will be developed to integrate a large retinal imaging dataset of AMD patients with EHRs of visual function. The aim is to develop meaningful outcome measures of AMD progression for use in clinical trials. Optimising diabetic retinopathy screening: Diabetic retinopathy (DR) is one of the most common causes of sight loss among working-age people in the UK. Those at risk of DR are screened; retinal photographs are taken at regular intervals and images are manually graded for specific pathologies. The aim of this project is to explore the potential of integrating automated image analysis into the Northern Ireland DR screening programme to both target treatment more effectively and reduce costs. A key challenge is predicting DR progression. There may be subtle patterns of retinal changes predicting DR progression that can be detected only by integrating data from many thousands of patients. Using screening images drawn from the NI diabetic eye screening programme (c. 87,000 patients), the fellow will apply the latest deep learning techniques (convolutional neural networks) to detect novel features predictive of DR progression. Performance of the automated methods will be assessed along with the potential for improvements to the screening programme.	Medical Research Council	Fellowship	279158.0 GBP
46	Prof. Titus NEUPERT	University Of Zurich	None	2018-01-01	2022-12-31	New paradigms for correlated quantum matter: Hierarchical topology, Kondo topological metals, and deep learning	Discovering, classifying and understanding phases of quantum matter is a core goal of condensed matter physics. Next to the notion of symmetry breaking phases, the concept of topological phases of matter is a prevailing theme of recent research. Topological phases are envisioned for various applications due to their universal and robust properties, such as protected conducting boundary modes, and provoke fundamental questions about the nature of many-body quantum states by providing the basis for exotic quasiparticles. In this ERC research project, I propose several new topological phases and novel numerical approaches for studying and classifying the most sought-after topological phases of matter. Concretely, I propose the concept of three-dimensional hierarchical topological insulators, which, in contrast to the known topological phases, do not posses gapless surface, but protected gapless edge modes. Moreover, I plan to study topological metals arising in strongly correlated Kondo systems, going beyond the current paradigm of considering topological metals that arise in the absence of electronic correlations. Furthermore, I propose to make the analogous step for topological superconductors, which have been studied as free models to search for Majorana quasiparticles: For the first time, I want to explore strongly interacting systems that realize the more powerful parafermion quasiparticles with numerical techniques. Finally, in a cross-disciplinary and exploratory sub-project, I will employ methods of deep neural networks to classify strongly correlated quantum phases using supervised learning combined with a technique called deep dreaming. Each of these sub-projects has the potential to make a paradigm-changing contribution to the study of strongly correlated and topological states of quantum matter and the combination of them allows to take advantage of synergy effects and a balance between high-risk and definitely feasible key developments.	European Research Council	Starting Grant	1362401.0 EUR
47	Dr Watjana Lilaonitkul	University College London	Institute of Health Informatics	2018-02-14	2021-02-13	Artificial Intelligence in Medical Treatment Decisions and Diagnosis	The data set is made of anonymised EHRs from the Great Osmond Street Hospital. Over time, we look to engage the NIHR Health Informatics Collaborative and other hospitals that are part of HDR London and HDR UK to collaboratively expand the data set. In the diagnosis network, we exploit a deep recurrent neural network architecture. We model EHR data using multi-task Gaussian processes (MGP) - a probabilistic model used to handle missing values and irregularly spaced observation times - and employ Long-Short-Term Memory (LSTM) units to capture long-range dependencies and nonlinear dynamics. We implement a multi-level deeply supervised network by introducing companion objective functions for each hidden layer that we will combine with the final loss value. We will also explore the effect of attention mechanisms on performance. We will compare the network performance with several state-of-the-art models and consider timeliness along with other metrics that quantify discrimination and accuracy. In the network where the goal is to learn optimal treatment regimens, we exploit a deep reinforcement learning architecture. We adopt an MGP-LSTM module to impute and interpolate the discretised EHR inputs, and employ a duelling Q-network architecture to learn the Q-values. The expected loss and gradient in the MGP layer will be approximated using a fixed number of Monte Carlo sampling. A different action-reward training environment will be structured for each disease explored. To speed up learning, we will explore prioritised experience. We will compare the network and physician performance using the estimated expected total Q-values.	Medical Research Council	Fellowship	313016.0 GBP
48	Dr Theo Sanderson	The Francis Crick Institute	None	2017-06-01	2021-05-31	Revealing the systems biology of malaria with a super-resolution atlas of the Plasmodium parasite	We now know that a majority of the malaria parasites 5,000 genes are essential for normal growth in the asexual blood stage (Bushell*, Gomes*, Sanderson*, et al., 2017). What we donâ€™t know is why â€“ what roles do these proteins play, and how do they contribute to parasite virulence? Building a systems-level understanding of malaria biology will require better answering that question, and doing so at the scale of the genome. One key question is protein localisation: the malaria parasite has many parasite-specific organelles, yet just 15% of its proteins have been imaged by microscopy to date. I propose to build a model of the three-dimensional localisations of at least 70% of P. falciparum proteins across the parasite's 48 hour intraerythocytic development cycle. I will do this by combining a recently-developed automated culture system with cutting edge super-resolution techniques. Using deep learning approaches, I will extract statistical maps of proteins from thousands of tagged lines, and generate a unified model of each proteinâ€™s localisation in an archetypical parasite developing over time. This data will generate profound insights into parasite ultrastructure which will be followed up with live-cell microscopy and conditional genetic approaches to understand the function of selected novel structures.	Wellcome Trust	Sir Henry Wellcome Postdoctoral Fellowship	250000.0 GBP
49	MSc BSc Rafael REISENHOFER	University of Vienna	None	2018-10-15	2020-10-14	Depth and Discriminability in Deep Learning Architectures	Deep neural networks have recently provided astounding results in a wide range of classification and regression tasks. This has sparked a renewed interest in the rigorous mathematical analysis of deep neural network architectures with the goal of uncovering the underlying principles that facilitate their groundbreaking success. Recent fundamental results already provide a better understanding of the relationship between depth and expressive power as well as a detailed analysis of the approximation properties of deep neural networks. It was also shown that certain types of convolutional neural networks exhibit desirable invariance properties such as stability with respect to small deformations. The present proposal aims at a mathematical investigation of another important property of a deep learning architecture, namely its discriminatory behavior. In most classification tasks, different classes are intertwined in a complex manner in the input space. In order to succeed, the realization of a deep neural network needs to disentangle those classes such that they can be separated in the corresponding feature space. Our goal is to better understand how the depth and other characteristics of a neural network influence its discriminatory power in the sense that they facilitate a clear separation of signal classes. Eventually, we aim to prove statements that quantify the discriminative power of a deep learning architecture with respect to distinct classes of signals as a function of depth and properties of the corresponding signals. The classification behavior of a neural network is mostly determined by its invariance properties on one side and its discriminatory properties on the other side. We will thus focus our investigation on a special class of convolutional neural networks, so-called scattering networks, for which substantial results regarding invariance and stability have already been established. We furthermore aim to utilize that when considering the modulus squared as a non-linearity, the output of each layer in a scattering network can be explicitly written as a cascade of autocorrelations in the frequency domain. In the initial phase of the project, we will investigate the discriminatory properties of scattering architectures with respect to simple template signal classes, such as signals that are sparse in the time or frequency domain. We then aim to extend our analysis to signal classes that resemble the structure of practical machine learning tasks in the sense that they are defined by shifts and time-frequency deformations of single prototype signals. Eventually, we aim to translate our theoretical findings into applicable guidelines regarding the optimal design of deep learning architectures for specific classification tasks. The research will primarily be conducted by the applicant (Rafael Reisenhofer) on a full-time basis. The work of the applicant will be supported by the invaluable expertise of the co-applicant (Philipp Grohs).	Austrian Science Fund FWF	None	156140.0 EUR
50	Dr Konrad Wagstyl	University of Cambridge	None	2019-08-05	2023-08-05	Deep Learning of Cerebral Cortex Microstructure	My research aims to bridge the gap between in vivo neuroimaging and underlying brain microstructure. MRI can inform us of shape or signal intensity changes in neurological and psychiatric diseases but has limited interpretability with respect to underlying neurobiological disturbances. Currently such specific measurement of neurobiology is only accessible through ex vivo histological techniques. I wish to further our understanding of how in vivo MRI reflects microstructure through three interrelated projects. 1) I will map the distribution of microstructural properties, including cytoarchitecture, myeloarchitecture and receptor-architecture to whole-brain reference atlases, thus representing these features in the space of brain scans. 2) I will create deep learning frameworks for analysing cerebral cortical structure. 3) I will use computational tools to link in vivo MRI cortical structure to its underlying microstructure in health and disease. I will use high-resolution and multi-modal structural MRI data to link interregional and interlaminar MRI differences to their underlying microstructural source. I will apply deep learning to a large cohort of patients with a form of surgically remediable epilepsy, focal cortical dysplasia, which is characterised by abnormal microstructure. This work aims to solve the clinical challenge of identifying subtle epileptogenic structural abnormalities from a patientâ€™s presurgical MRI scan.	Wellcome Trust	Sir Henry Wellcome Postdoctoral Fellowship	300000.0 GBP
51	Ao. Prof. Dr. Andreas HOLZINGER	Medical University of Graz	None	2019-11-04	2022-11-03	A Reference Model of Explainable AI for the Medical Domain	Towards A Reference Model of Explainable AI for the Medical Domain Andreas Holzinger December, 27, 2018 The progress of statistical machine learning methods has made AI increasingly successful. Deep learning exceeds human performance even in the medical domain. However, their full potential is limited by the difficulty to generate the underlying explanatory structures. The central problem is that they are regarded as "black-boxes" and even if we understand the mathematical principles, they lack an explicit declarative knowledge representation. A motivation for this project are rising legal and privacy issues. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make results re-traceable on demand - understandable by a human domain expert. We learned of a variety of technical solutions which are currently in development, which could help explain AI/ML systems and their decisions. Transparent algorithms could appropriately enhance trust of medical professionals, thereby raising acceptance of machine learning. Recently, the Google AI team emphasized the significance of research in explainable AI, the importance of Human-Computer Interaction (HCI) research for Knowledge Discovery from Data (KDD), and the urgent need for a research framework around the field of interpretability. In our ongoing research collaboration "Machine learning for diagnosis of colorectal cancer" with the Google AI group and the Institute of Pathology (Ethics Vote Medical University Graz 30-184 ex17/18) a training data set for AI/ML in digital pathology has been generated. As agreed with our colleagues at the Google AI group, we have now the chance to use machine learning algorithms developed in this context for our test environment; in return we will research towards making their algorithms explainable for our domain experts, who have the benefits of using the results for their medical teaching. This project will focus on but is not limited to digital pathological data and context. The advantage of this project is working with real-world data (under full ethics/data protection regulations) together with medical experts. This project will provide important contributions to the international research community in the following ways: evidence in various methods of explainability and novel methods and urgently needed tools for benchmarking and evaluation; Moreover, the contributions gained in this project can be used generally for reverse-engineering human learning and cognitive development and specifically to engineer more human like machine learning systems. All outcomes of this project will be made openly available to the international research community.	Austrian Science Fund FWF	Stand-Alone Projects	392773.5 EUR
52	Dr Brown	University of Lincoln	None	2020-09-07	2023-09-06	Towards an open-source, equipment-agnostic framework for automated welfare monitoring in the home cage	The moral implications of using mice and other higher animals in biomedical research has been a topic of debate for many years, and has steered various efforts towards minimising animal suffering. As the number of mice used in biomedical research continues to rise, there is greater emphasis than ever on approaches to refine husbandry protocols and humane endpoints to ensure that welfare concerns are dealt with swiftly and efficiently. Home cage monitoring has been proposed as a means capturing animal behaviour through the light and dark phases so that welfare deficits might be captured without the need for human intervention. Several commercial and non-commercial systems have been designed to capture video footage (and other data) of mice that offer several advantages over conventional experimental designs: (1) behaviours can be captured in a familiar, enriched environment; (2) observations are not confounded by novel apparatus or human experimenters; (3) it serves as a permanent digital record; and (4) video data are amenable to automated image analysis techniques. The challenge that remains in the context of HCA is the ability to analyse video footage in a manner that is fully-automated, comprehensive, robust, and computationally efficient. Under this proposal, the successful studentship awardee will develop a solution to the problem of automated HCA based on deep learning; a state-of-the-art approach to computer vision problems that utilises computational models inspired by the human visual cortex. The technique will be based on the "anomaly detection", whereby a model is initially trained to capture the range of possible behaviours exhibited by mice under normal circumstances (i.e., without welfare deficits). The methods works by learning to produce a set of visual and motion features that describe the behaviours of the mice, and then having the same model attempt to "reconstruct" the input clip from the features. In doing so, the model is forced to capture only key information about normal behaviour. Given that video clips depicting welfare deficits are visually different normal behaviors by definition, the model will be less capable of representing those data. Dr James Brown will be the primary supervisor of this work, who has a strong record of interdisciplinary research involving mouse models. He has previously worked with the Mary Lyon Centre - a collaborator on this project - on development 3D image analysis techniques for mouse embryo phenotyping. He has recently published several high-impact articles on machine learning techniques for the diagnosis of retinal disease, and is currently seeking approval from the US Food & Drug Administration. Prof Xujiong Ye will co-supervise this project, bringing more than 20 years' experience in computer vision and medical image analysis in both industry and academia. The methods outlined in this proposal align well recent work published Prof Ye on the development of algorithms to track and assess the welfare of farmyard pigs from top-down video footage.	National Centre for the Replacement, Refinement and Reduction of Animals in Research	Studentship	90000.0 GBP
53	Ilya Safro	Clemson University	None	2020-05-01	2021-04-30	RAPID: Automated discovery of COVID-19 related hypotheses using publicly available scientific literature	Computer and Information Science and Engineering - The vast amounts of biomedical information that accumulate in modern databases (such as MEDLINE of the National Library of Medicine) impose a great difficulty for efficient wide surveying by researchers who try to evaluate new information considering existing biomedical literature even when advanced information search engines are used. Automated hypotheses generation systems are designed to help scientists to overcome these difficulties and accelerate their research. The pandemic situation with COVID-19 is precisely one of the cases when such systems can play an extremely important role in coping with the coronavirus. Using two different AI approaches, we have developed two systems to discover plausible hypotheses in the biomedical domain. In this project, we will will deploy the COVID-19 customized hypothesis generation and knowledge discovery system, massively run it on any relevant to this research queries, and publish the results (including trained AI models, and discovered information) in the open domain for broad scientific community with the goal to accelerate the COVID-19 research. This work focuses heavily on addressing fundamental knowledge discovery questions by modeling and formulating scientific hypotheses using the publicly available information in the biomedical domain. However, in general, these methods are not restricted to any specific information domain, i.e., they can be broadly used to discover knowledge in texts. Although our experimental work will be related to COVID-19, the methods can be applied with some reservations to any literature-based analysis. For example, in the Materials Science Initiative, one of the goals is to establish a systematic understanding of the material properties and discover new materials which can be done by analyzing using the massive corpus of papers. In the legal world, identifying related patents can be done using a similar hypothesis modeling methodology.<br/><br/>In the heart of the proposed approach lies a big multi-modal and multi-relational semantic knowledge network of all biomedical objects extracted from a variety of heterogeneous databases of the National Library of Medicine. These objects include but are not limited to scientific papers, abstracts, keywords, phrases, elements of thesaurus, genes, proteins, mutations, pathways, diseases, and diagnoses. We will leverage two systems, namely MOLIERE and AGATHA, that are based on structural and deep learning, respectively. We will customize them using the rapidly updated dataset of new papers that has not been yet processed by the National Library of Medicine but already exists in the open domain such as in various preprint archives and reports. The MOLIERE system is based on the network analysis techniques applied on the graph constructed using the low-dimensional embedding of the papers with the result interpretation methods that are based on the probabilistic topic modeling. The AGATHA system processes texts at much finer granularity, and creates a semantic knowledge network using more accurate embedding techniques followed by the deep learning training for knowledge discovery. Two systems complement each other. While the AGATHA is of higher quality, the MOLIERE is more interpretable. A combination of both will be leveraged in this research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	104474.0 USD
54	Shahin Vassigh	Florida International University	None	2020-06-01	2021-05-31	RAPID: A Platform for Mitigating the Impacts of COVID-19 on the Healthcare System	Office of the Director - This COVID-19 RAPID research program will focus on addressing the shortage of essential medical supplies, Personal Protective Equipment, and crucial medical technologies. A surge in demand for specialized healthcare has also created a shortage of skilled medical professionals capable of operating highly technical machines while protecting themselves and others against this dangerous virus. Thus, providing rapid training for caregivers and increasing the supply of Personal Protective Equipment and medical technology is an immediate priority for combating this unprecedented pandemic. This project will leverage the research and technology currently under development in a National Science Foundation Convergence Accelerator project to provide an easy to use and access training application for caregivers, as well as networking capacity for problem-solving among stakeholders. This project will also build and foster partnerships among the healthcare community, medical equipment manufacturers and distributors, and grassroots efforts to rapidly deploy prototypes as it strives to create awareness of the project and its novel features.<br/><br/>The specific goals of this project are to provide training support, problem-solving resources, and professional networking for idea exchanges for healthcare professionals, caregivers, and technologists that are on the frontlines in the fight against COVID-19. Artificial Intelligence techniques such as deep learning and self-adaptive autonomous systems will be used to develop an intelligent knowledge network to support virtual communities, as well as, create Augmented Reality technologies for delivering efficient and engaging virtual training. Thus, the research objectives of this project include: 1) development of an Augmented Reality training mobile application for caregivers and technologists responding to the COVID-19 pandemic; 2) deployment of an online repository for equipment problem-solving resources; and 3) development of a professional networking application to disseminate research and best practices in the fight against COVID-19 for all stakeholders engaged in addressing the pandemic. The project's overarching focus is to create a web application with mobile Augmented Reality capability for training healthcare professionals at any location and time, in order to improve access to training with the medical equipment and increase the efficiency and reach of safety training. The project team will advance the knowledge base in computer science and spatial computing as it creates its Artificial Intelligence-powered Augmented Reality training. The team will disseminate its findings and technology through relevant scholarly conferences and journals.<br/><br/>This RAPID award is made by the Convergence Accelerator program in the Office of Integrative Activities using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act, and is associated with the Convergence Accelerator Track B: Artificial Intelligence and Future Jobs and National Talent Ecosystem.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	159300.0 USD
55	Yulia Gel	University of Texas at Dallas	None	2020-05-01	2021-04-30	RAPID: Collaborative Research: Operational COVID-19 Forecasting with Multi-Source Information	Mathematical and Physical Sciences - This project aims to develop a new deep learning predictive platform for COVID-19 transmission, integrating multi-source information under model and data uncertainties. In contrast to other viruses such as influenza, SARS, and MERS, COVID-19 differs in a number of ways, including uncertainties in response to weather conditions, history of the disease, as well as the effectiveness of responses from public health officials or from the general public. An important aspect is to integrate multi-source data such as official reports, atmospheric variables, and social media data into operational biosurveillance and real-time prediction of COVID-19. The proposed biosurveillance framework will be used to forecast COVID-19 dynamics and to enhance mitigation strategies. In addition, it could also be applicable to tracking many other infectious diseases, thereby contributing to security of our society as a whole. Furthermore, the project will build innovative connections within and across mathematical biology, statistics, and deep learning, with a strong focus on interdisciplinary graduate research training.<br/><br/>As the main forecasting framework, the widely used Susceptible-Exposed-Infected-Recovered (SEIR) dynamic models can accurately describe the disease dynamics, but only with precise knowledge of disease parameters, which can take a long time to accurately estimate. Deep learning algorithms can potentially have superior predictive ability, but they require extensive training. Another key challenge in the statistical modeling of these events is how to timely and systematically integrate multiple sources of surveillance, anecdotal, and other health-related information under uncertainty. The proposed new predictive approach is based on the interaction between multiple data sources, dynamical SEIR models, and deep learning algorithms. The key idea is to view simulation SEIR models as ?surrogate? pre-trainers for the deep learning models, resulting in less real data needed to retrain the predictive model to reflect ?real world? COVID-19 progression. Deep learning predictive models can then be used for making predictions about the future COVID-19 dynamics, which can be compared to the predictions made by the original SEIR model. Depending on which mathematical model makes better predictions, another model can be updated with the better prediction as inputs, thereby representing reinforcement learning from both data and the best mathematical model. As a result, the new predictive framework will allow one to assess impacts of the immediate responses such as declaration of a national emergency, a school closing, or a quarantine, and can be considered as a step toward interpretable AI for COVID-19 biosurveillance.<br/><br/>This grant is being awarded using funds made available by the Coronavirus Aid, Relief, and Economic Security (CARES) Act supplemental funds allocated to MPS.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	80169.0 USD
56	Georgiy Bobashev	Research Triangle Institute	None	2020-05-01	2021-04-30	RAPID: Collaborative Research: Operational COVID-19 Forecasting with Multi-Source Information	Mathematical and Physical Sciences - This project aims to develop a new deep learning predictive platform for COVID-19 transmission, integrating multi-source information under model and data uncertainties. In contrast to other viruses such as influenza, SARS, and MERS, COVID-19 differs in a number of ways, including uncertainties in response to weather conditions, history of the disease, as well as the effectiveness of responses from public health officials or from the general public. An important aspect is to integrate multi-source data such as official reports, atmospheric variables, and social media data into operational biosurveillance and real-time prediction of COVID-19. The proposed biosurveillance framework will be used to forecast COVID-19 dynamics and to enhance mitigation strategies. In addition, it could also be applicable to tracking many other infectious diseases, thereby contributing to security of our society as a whole. Furthermore, the project will build innovative connections within and across mathematical biology, statistics, and deep learning, with a strong focus on interdisciplinary graduate research training.<br/><br/>As the main forecasting framework, the widely used Susceptible-Exposed-Infected-Recovered (SEIR) dynamic models can accurately describe the disease dynamics, but only with precise knowledge of disease parameters, which can take a long time to accurately estimate. Deep learning algorithms can potentially have superior predictive ability, but they require extensive training. Another key challenge in the statistical modeling of these events is how to timely and systematically integrate multiple sources of surveillance, anecdotal, and other health-related information under uncertainty. The proposed new predictive approach is based on the interaction between multiple data sources, dynamical SEIR models, and deep learning algorithms. The key idea is to view simulation SEIR models as ?surrogate? pre-trainers for the deep learning models, resulting in less real data needed to retrain the predictive model to reflect ?real world? COVID-19 progression. Deep learning predictive models can then be used for making predictions about the future COVID-19 dynamics, which can be compared to the predictions made by the original SEIR model. Depending on which mathematical model makes better predictions, another model can be updated with the better prediction as inputs, thereby representing reinforcement learning from both data and the best mathematical model. As a result, the new predictive framework will allow one to assess impacts of the immediate responses such as declaration of a national emergency, a school closing, or a quarantine, and can be considered as a step toward interpretable AI for COVID-19 biosurveillance.<br/><br/>This grant is being awarded using funds made available by the Coronavirus Aid, Relief, and Economic Security (CARES) Act supplemental funds allocated to MPS.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	89755.0 USD
57	Arshad Kudrolli	Clark University	None	2020-05-15	2021-04-30	RAPID: Predicting Coronavirus Disease (COVID-19) Impact with Multiscale Contact and Transmission Mitigation	Mathematical and Physical Sciences - Nontechnical Abstract: <br/>The rapid spread of new coronavirus SARS-CoV-2, which causes Coronavirus Disease (covid-19), requires a multidisciplinary mitigation strategy from the clinical to physical host-to-host transmission modelling. Data is required on the transmission of pathogen carrying airborne mucosalivary droplets and aerosols generated during normal breathing, talking, sneezing, and coughing. Synthetic exhalations will be measured leveraging advanced prototyping to obtain data needed to model the spread of covid-19, and the efficacy of personal protection devices and face coverings fabricated with various weaves and materials will be tested. Physical data related to temperature, humidity, and airflow on survival and dispersion of exhalations will be obtained. The data will be integrated using supervised machine learning methods, mathematical network simulations, and epidemiological data to develop an individual-based method that can give pandemic management results. Physical data will be published on transmission rates, including wearing of personal protective equipment and face coverings with various weaves, to inform mitigation strategies to alleviate covid-19 pandemic. Interactive Web based resources will be used for immediate broad dissemination of data and learning outcomes on covid-19 to the public, in addition to peer reviewed publications and training post-doctoral and undergraduate researchers in methods leading to pandemic mitigation.<br/><br/>Technical Abstract:<br/>The mode of transmission and extent of environmental contaminations on the outbreak of the Coronavirus Disease 2019 (covid-19), while sharing features with severe acute respiratory syndrome and other infectious diseases, remains unknown. This project will address fundamental rheology-matched metrics of transport and survival of airborne exhalation droplets and aerosols that carry coronavirus and on surfaces needed as input parameters for modeling mitigation. Impact of personal protective equipment on individual prognosis, with physical data related to temperature, humidity, and airflow-dependent dispersion distance of pathogen bearing viscoelastic droplets corresponding to breathing, sneezing, and coughing, will be obtained. The impact of the measured transmission rates on the spread and recurrence will be investigated with epidemiological data integrated with deep learning to implement a scalable, individual-based, stochastic, spatial model. Resulting peer-reviewed publications will serve as trusted source for calculation of covid-19 transmissibility and personal protection strategies. Post-doctoral and undergraduate researchers versed in fluid dynamics, soft matter physics, and network simulations will be trained toward mitigating infectious disease spread.<br/><br/>This Rapid Response Research (RAPID) grant supports research that will result in spatiotemporal mucosalivary droplet transmission range data required to develop covid-19 mitigation network methods with funding from the CARES Act managed by the Condensed Matter Physics Program in the Division of Materials Research of the Mathematical and Physical Sciences Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	200000.0 USD
58	Gholamali Rahnavard	George Washington University	None	2020-05-01	2021-04-30	RAPID: A novel platform for data integration and deep learning on COVID-19	Biological Sciences - The COVID-19 pandemic, caused by the SARS-CoV-2 virus, has fundamentally changed the world, and yet its ultimate impact is unknown. While China has experienced a slowdown in new cases, infections in the US continue to rise and are threatening to exceed our health care system?s capacity. Tests capacities are limited compared to the need, hospital services are becoming overwhelmed, and critical supplies are in shortage. There is a diversity of efforts currently ongoing to develop both new treatments as well as vaccine strategies to combat COVID-19. Yet, we know from experience, the virus will evolve solutions to both host immune systems and intervention strategies. In order to diminish both the short-term and long-term impacts of COVID-19, it is essential to develop robust, repeatable, and accessible tools to integrate and analyze the diversity of data becoming available in the face of the COVID-19 pandemic. The development of a platform to characterize the dynamic nature of mutations in the virus and testing for associations with clinical variables and biomarkers is an essential broader impact and will help in making informed predictions of health outcomes such as the stage of the severity of the disease and efficacy of treatment. Additionally, this project provides professional development opportunities for early career researchers.<br/><br/>Advances in omics technologies provide a broad and deep range of genotypic and phenotypic data to integrate with clinical phenotypes. Machine learning techniques such as clustering using phylogenetic distance and Deep Neural Networks (DNNs) are suitable techniques to link these DNA level changes to clinical metadata for human disease prediction, diagnosis, and therapeutics. This project develops tools within an open-source platform for documented, repeatable analyses that can be conducted in real-time allowing integration of data from patients with new treatments/vaccines strategies. This deep learning bioinformatics platform will allow the prioritization of genes associated with outcome predictors, including health, therapeutic, and vaccine outcomes, as well as inform improved DNA tests for predicting disease status and severity. The computational tools developed in this study will provide the research community and health professionals with comprehensive and generic approaches for characterizing the dynamics of genotype/phenotype associations in viruses. Such tools allow healthcare professionals and researchers to address specific properties of viruses such as frequency and location of mutations across the viral genome. When added to other clinical and epidemiological data, such information could help pave the way to better treatments or a vaccine. The developed platform will provide a venue for robust, open, repeatable analyses of COVID-19 as more and more data become available.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	200000.0 USD
59	VAMSTAR LIMITED	VAMSTAR LIMITED	None	None	2021-04-30	AI-powered COVID-19 supplier risk index and demand planning toolkit	For health systems to better prepare and plan for the months ahead, Vamstar will create a risk-based framework to understand supply chain gaps and the evolving demand at a hospital-level in the UK and EU. The objective of the risk scoring matrix and the Demand-Planning-Toolkit is to focus on the most vulnerable parts of the health systems in Europe and facilitate decision making as quickly as possible. With the supply risk framework, we will be able to predict changes in the overall demand for various essential products and services needed to manage a crisis like COVID-19 and direct the focus of suppliers towards the most needed parts of the care delivery. Combining this with the Demand-Planning-Toolkit, Vamstar will be able to assess which suppliers have fulfilled a current order in the market and make predictions about when they will become fully production-ready to sell again. Additionally, the risk scoring framework and Demand-Planning-Toolkit will help stakeholders in the care delivery chain quickly assess countries that have developed or are developing preventative infrastructure and share the findings. This framework and connected real-time supply chain analytics will ensure that in the future such crises are managed with a more proactive strategy vs a reactive supply chain approach currently prevalent in our health systems. By analysing this dataset through artificial intelligence, we want to understand the Pandemic-Supply-Risk both from a macro (ability) and micro (willingness) levels needed to manage a pandemic like COVID-19\. This toolkit includes necessary items such as face masks but also digital platforms that will aid in patient and population health management across these countries as it undergoes a rapid transformation. Vamstar offers a data science powered platform for predicting and matching public contracts in healthcare and will create an automated "pandemic preparedness toolkit" specifically by using supply chain risk scoring matrix so as to focus on the most vulnerable parts of the health systems affected by the COVID-19 pandemic. It will leverage EU and UK data on public and private tendering, pricing sources and economic annual datasets to do this. Machine learning (ML) and deep learning will be used for tasks such as predicting ongoing list of suppliers with spare capacity, the date of shortages, and prices of key supplies. These predictions and analysis will form part of reports and an autonomous dashboard that will benefit the NHS hospitals and healthcare suppliers.	UK Research and Innovation	Research Grant	49794.0 GBP
60	Dr Raymond Carragher	University of Strathclyde	Inst of Pharmacy and Biomedical Sci	2018-02-14	2021-02-13	Precision Drug Theraputics: Risk Prediction in Pharmacoepidemiology	The project will develop statistical and machine learning algorithms to analyse NHS datasets to build predictive models to support precision therapeutics focusing on the following two areas: 1) Cancer chemotherapy 2) Cardiovascular therapies HDR UK will provide the data repositories for linkage of established patient cohorts to further phenotypic resources. This will provide deeper phenotyping and should allow more precise prediction algorithms to be developed. Advanced statistical modelling (logistic regression, least absolute shrinkage and selection operator (LASSO) regression, Bayesian modelling) and machine learning approaches (neural nets, deep learning algorithms) will be used as part of this approach. The precision and incremental value of additional datasets will be assessed as will automated approaches to optimising algorithms. Existing research in the use of Bayesian hierarchical point-mass models, based around body-systems, to detect safety signals in clinical trials may be extended for use in observational studies. This approach is designed to use the additional information given by the relationships within and between body-systems in a statistical analysis. The outcomes of the project, in this case the prediction models, will be translated into Clinical Decision Support (CDS) tools which will promote learning health systems. User and usability experience data will be captured during CDS design and development. The output will be a prototype intervention suitable for evaluation through the MRC Complex Intervention Framework to support clinicians in making informed choices and highlighting the risks in the different possible prescribing choices.	Medical Research Council	Fellowship	300445.0 GBP
61	Dr Deepti Gurdasani	Queen Mary University of London	William Harvey Research Institute	2019-08-05	2021-03-25	Predictive analytics of integrated genomic and clinical data using machine learning and complex statistical approaches	Although the utility of complex statistical, machine and deep learning (ML and DL) approaches in the context of multi-dimensional data has been clearly demonstrated, these methods have not been widely utilised to improve novel drug discovery and clinical risk prediction. This proposal aims to harness the potential of large-scale integrated genetic and health data to spur innovation, and develop predictive algorithms to improve clinical decision making and patient health. Specifically, this will focus on the development and evaluation of ML and DL frameworks for GWAS, and clinical risk prediction using publicly available large-scale EHR and genomics biodata resources, including UK Biobank, Genomics England and INTERVAL studies. Transcriptomic and functional data will be integrated into these using predictive approaches, where this has not been directly measured. This will be implemented in three stages: 1) assessment of complex time-dependent statistical approaches for modelling of hazard; 2) optimisation and assessment of existing ML and DL approaches for modelling of clinical risk; 3) development of novel approaches, specifically using recurrent neural networks (RNNs) to incorporate temporality and missingness in clinical data, including time varying covariates to accurately model complex hazard functions; the objective of this project will be to develop approaches that appropriately leverage the rich longitudinal and time-dependent data on individuals shown by us and others to substantially improve clinical risk prediction. In addition to risk prediction, this proposal will also focus on improving our understanding of genetic aetiology of disease. In addition to standard GWAS approaches, hybrid ML and GWAS approaches for prioritisation of candidate genes, and genetic variants associated with disease will also be applied, potentially improving the power to identify novel associations, with important implications for prioritisation of therapeutic targets.	Medical Research Council	Fellowship	177596.0 GBP
62	Dr Claire Niedzwiedz	University of Glasgow	College of Medical, Veterinary, Life Sci	2017-11-15	2021-01-13	A machine learning approach to understanding comorbidity between mental and physical health conditions	The key aim of the research is to enhance our understanding of the comorbidity between mental and physical health conditions by combining the disciplines of public health, computer science and psychiatry. The objectives of the research are to improve the prediction of the onset of co-morbidities between mental and physical health conditions (such as major depressive disorder and diabetes) and related adverse outcomes (e.g. hospitalisation and mortality). A range of data sources will be used to test the utility of implementing a machine learning approach to prediction. For example, using the UK Biobank and Generation Scotland cohorts will allow the exploration of a range of biological, environmental and lifestyle factors. Linked administrative health data (including the Scottish Morbidity Records, disease registers, Prescribing Information System, mortality records and the census) will facilitate the exploration of patients' complex medical histories and social characteristics (e.g. occupations). Machine learning algorithms (e.g. deep neural networks and random forest) will be used to learn from patterns in a range of big data by splitting the data into training and test datasets, assessing the algorithm performance and comparing the results with other methodological approaches. By identifying factors that are highly predictive of physical and mental health comorbidities and adverse outcomes, there is great potential to develop new approaches to patient stratification and novel precision medicine interventions. Collecting these data within medical settings may facilitate the development of improved diagnostic, treatment and preventative measures in clinical and public health practice.	Medical Research Council	Fellowship	290658.0 GBP
63	Mr Nicholas Byrne	Guy's and St Thomas' NHS Foundation Trust	None	2018-01-01	2020-12-31	Patient-specific care at the cutting-edge of paediatric cardiology practice: a 3D window on anatomy at the intersection of medical imaging, computer science and 3D printing	Background Despite advances in 3D cardiac magnetic resonance (CMR) imaging, structural appreciation of congenital heart disease remains hampered by presentation on a 2D computer screen. Advanced methods of medical image presentation, such as 3D printing, holography and virtual reality, have been proffered as solutions. In particular, many report that 3D printed models can provide a fully 3D, tactile appreciation of patient-specific anatomy and disease morphology. Understanding is quickly realised without image interpretation, and easily conveyed to others by inspection. Despite their potential for treatment planning, these techniques are yet to become part of routine care. We assert that their common dependence on image segmentation precludes their growth outside of the largest teaching hospitals. Current approaches rely on semi-automated procedures and normally include a significant manual component, demanding time and specialist expertise that are rarely available to clinical staff. Deep convolutional neural networks (DCNNs) are rapidly becoming the foundation to many tasks in medical image processing. In particular, DCNNs have outperformed conventional image processing approaches in the segmentation and measurement of ventricular volumes in CMR images. The ability of DCNNs to learn abstract representations from images makes this approach robust to the variation in anatomy and image quality that is observed in the clinic. Aims To explore the use of DCNNs to perform CMR image segmentation and achieve accurate, 3D presentation of congenital heart defectsTo assess the clinical feasibility of this approach Research Questions For what fraction of CMR scans can a DCNN derive patient-specific 3D geometries that are representative of congenital heart anatomy and disease morphology such that they are suitable for clinical presentation as a 3D model?How do segmentation by DCNN and manual operation differ in: Geometrical agreementDurationManual complexityCost Plan of Investigation Collection of training data CMR image data will be retrieved and segmented retrospectively for 300 patients with a range of conditions that reflect the mix of congenital heart disease seen at Evelina London Childrens Hospital. Image segmentation will be performed using: (i) a largely manual pipeline performed using Mimics Medical software by Materialise; (ii) a semi-automated pipeline using conventional image processing methods developed in house.DCNN development by deep learning 240 of the 3D cardiovascular geometries collected in phase 1 will be used to train a DCNN to perform image segmentation using deep learning algorithms.Experiment the DCNN (method (iii)) will be used to perform image segmentation in the remaining 60 patient data sets that were segmented using methods (i) and (ii) in phase 1. Methods (i), (ii) and (iii) will be statistically compared in terms of their geometrical agreement, duration, complexity and financial cost of image segmentation. Benefits to Patients Automated image segmentation may increase the use of 3D models to care for those with congenital heart disease, improving surgical and interventional planning, and providing safer access to bespoke treatment options. Furthermore, 3D models have been used as a consenting aid to help patients and their families make informed decisions about the direction of their care. Benefits to the NHS Automated segmentation will provide a low cost, clinically feasible and technologically based path to a branch of patient-specific care for a challenging disease cohort. More broadly, a deep learning approach to image segmentation could be applicable to other areas in which patient-specific 3D printed models have been employed. These include maxillofacial, orthopaedic, oncology, neuro and transplant surgeries to name but a few from a growing list. We hope that our findings will support the growth of bespoke treatment options that place the patient at the centre of care across a range of NHS services.	National Institute for Health Research (Department of Health)	Full award	184274.0 GBP
64	Dr Caroline Roney	King's College London	Imaging & Biomedical Engineering	2018-10-01	2021-09-30	Predicting Atrial Fibrillation Mechanisms Through Deep Learning	Persistent atrial fibrillation (AF) patients are a heterogeneous population: some patients require multiple procedures, with more extensive ablation strategies; while for others, isolation of the pulmonary veins using ablation (PVI) is sufficient. Identifying persistent AF patients where PVI will be a sufficient treatment remains a clinical challenge, which if solved could lead to improved safety, better patient selection, as well as decreased time and cost for procedures. Biophysical simulations personalised to cardiac imaging and electrical data may offer substantial insights into the mechanisms underlying AF, but run too slowly to be used during clinical procedures. My objective is to develop a combined biophysical simulation and deep learning network pipeline that accurately quantifies the likelihood of success of PVI for an individual patient quickly enough for use during a clinical procedure, to guide ablation therapy. Methodology: We will simulate a virtual patient cohort covering the range of observed electrical and anatomical properties. These biophysical simulations will use the cardiac monodomain equation and the Courtemanche-Ramirez-Nattel atrial cell model, solved on meshes constructed from MRI images, with different fibrosis distributions, and repolarisation and conduction properties. The deep learning convolutional neural network will be trained to large quantities of post-processed data from biophysical simulations to ensure that the network captures the physics and physiology of the system. The training will then be augmented with the complexity and reality of clinical data. Finally, the deep learning pipeline will be tested in a retrospective study. We hope this study will provide insights into the mechanisms sustaining AF. We hope that different research and clinical centres will contribute to and make use of the trained network to predict patient-specific AF mechanisms and PVI ablation outcomes.	Medical Research Council	Fellowship	311169.0 GBP
65	Dr Tobias Goehring	University of Cambridge	MRC Cognition and Brain Sciences Unit	2020-06-12	2025-06-11	Restoring the sense of sound: deep-learning based compensation strategies for the electro-neural transmission of sound by cochlear implants	Over 800,000 severe-to-profound hearing-impaired individuals use a cochlear implant (CI) worldwide. Compromised speech perception in noisy environments is a major problem for CI users and can have a negative impact on their quality of life and mental health. Noise-reduction algorithms, that process the corrupted speech signal before being presented, have produced some benefits for CI users but they still struggle in even moderate levels of noise and there remains considerable variability in the benefits across CI users. I will develop a compensation strategy to overcome the limitations of previous approaches by taking into account CI-specific and user-specific effects. The compensation strategy consists of a computational model to simulate the electro-neural transmission of CIs and a machine-learning algorithm trained to reduce the noise component of a speech-in-noise signal. I will develop the strategy in three stages: (I) by incorporating a computational model of the CI processing, (II) by incorporating a computational model of the electrode-nerve interface and (III) by adjusting these models with user-specific parameters measured with electro-physiological and psycho-physical tests. Each stage will be used to generate training data for a noise-reduction algorithm based on deep recurrent neural networks. Hereby, real-world speech and noise recordings will be processed by the computational model to generate labels for the supervised training of the neural networks. Listening experiments will be performed with CI users to evaluate the strategy and its effect on the perception of speech in noise by measuring speech reception thresholds and quality ratings. If successful, the strategy could be integrated into the external speech processor of a cochlear implant without the need for surgical re-implantation. This CI-specific approach has the potential to overcome the current limitations and to provide benefits for CI users.	Medical Research Council	Fellowship	1025336.0 GBP
66	Dr Siegfried Wagner	University College London	Institute of Ophthalmology	2020-01-20	2023-01-19	Retinal bioimaging for neurodegenerative and cardiovascular diseases	Aim To identify the most accurate retinal biomarkers predictive of of incident CVD events and AD Objectives 1) Examine the relationship between retinal lamellar parameters and incident myocardial infarction and cerebrovascular accidents using the AlzEye database 2) Validate the association between retinal nerve fibre layer and ganglion cell-inner plexiform layer thickness with incident AD 3) Leverage the UK Biobank cohort for adult life course epidemiological analysis of CVD and AD risk factors and external validation 4) Development of a convolutional neural network predicting risk of CVD events and AD to identify novel biomarkers Methodology Analysis will be conducted using two curated large datasets - the UK Biobank and AlzEye, a pseudonymised dataset of >2 million optical coherence tomography (OCT) scans of 250,000 patients from Moorfields Eye Hospital. Fundus photographs and OCT scans will be analysed using automated segmentation and image analysis software. Regression techniques and survival modelling will evaluate the hypothesis that specific retinal changes are indicative of incident AD and CVD events. Convolutional neural networks on outcome data with OCT and fundus photos as inputs from the AlzEye dataset will be developed and leveraged as tools to identify structural biomarkers through spatial soft attention maps. Models will be externally validated using the UK Biobank Cohort. Scientific and medical opportunities Improving our understanding of retinal manifestations of two major epidemics of the developed world, cardiovascular disease and AD, may potentially refine risk stratification techniques with in-vivo evaluation. Identification of any biomarkers would also allow non-invasive measures of efficacy in any future interventional clinical trials assessing treatments in CVD or AD. Broader implications of the feasibility of linked healthcare research and the impact of real-world studies on healthcare policy will also be explored.	Medical Research Council	Fellowship	227704.0 GBP
67	Dr. Bastiaan Elie Dutilh	UNIVERSITEIT UTRECHT	None	2020-05-01	2025-04-30	Predicting the evolution of complex phage-host interactions	What determines if a phage can infect a host? This question arises as we work to understand the ecological roles of the hundreds of thousands of unknown viruses that I and others have discovered around the world. Phages are the most abundant life forms on Earth with important applications in medicine and biotechnology and far-ranging effects on microbial community functioning in all environments. Phage-host interactions (PHI) are an emergent trait that depends on the complex integration of factors like their taxonomic identity, the environment, and phage- and host-encoded proteins. With DiversiPHI, I propose a research program to unravel PHI by 1) measuring, 2) modelling, and 3) experimentally testing these diverse factors to develop a predictive understanding of host-range evolution. I will first measure a range of evolutionary, ecological, and molecular factors contributing to PHI at high resolution using newly developed computational tools that exploit high-throughput datasets from thousands of natural environments around the world. Next, I will apply deep learning to integrate these measurements to simultaneously (i) quantify the relative importance and complex inter-dependencies of the different factors, and (ii) create a unique predictive model of host-range evolution. To complement these in silico predictions, I will develop an experimental evolution setup that tests the effect of the different PHI factors on host-range evolution in vitro. Little is known about the abundant phages and their role in shaping our microbial world. DiversiPHI will vastly elevate this understanding and contribute new fundamental knowledge on how species-species interactions evolve in complex environments. Moreover, I will provide valuable new analysis tools to the community and consolidate my strong international reputation as a pioneering researcher in the cross-disciplinary field encompassing microbial ecology, virology, metagenomics, bioinformatics, and computer learning.	European Research Council	Consolidator Grant	2000000.0 EUR
68	Michael Pazzani	University of California-San Diego	None	2020-05-01	2021-04-30	RAPID: Explainable Machine Learning for Analysis of COVID-19 Chest CT	Computer and Information Science and Engineering - In December 2019, it was discovered that a widely contagious pneumonia was caused by a new coronavirus infection now named COVID-19. The primary test for detection of the virus is real-time polymerase chain reaction (RT-PCR) with sensitivity of approximately 71% in some studies. However, this test may require several days to provide a result. Perhaps more importantly, imaging with x-ray or computed tomography (CT) are required to confirm pneumonia, which is the principal cause of death, as it leads to acute respiratory distress syndrome (ARDS). Recent studies have shown sensitivity of chest CT for approximately 98% for COVID-19 pneumonia and could provide immediate results but currently require human interpretation. Given the need for rapid, more accurate diagnosis, this project will use, adapt, and evaluate explainable machine learning techniques to diagnosis of COVID-19 pneumonia. This project will improve the understanding of mechanisms of COVID-19 and will help mitigate its impacts.<br/><br/>Viral nucleic acid detection using real-time polymerase chain reaction (RT-PCR) is the primary method for diagnosis of COVID-19 infection, which has rapidly spread worldwide as a global pandemic. Sensitivity of this test for COVID-19 infection has been estimated at approximately 71% in some studies and may require several days for a result. X-ray and CT imaging are complementary technologies that allow diagnosis of COVID-19 pneumonia, which can evolve to acute respiratory distress syndrome (ARDS) -- the principal cause of death in patients with COVID-19 infection. Especially early in the course of the disease, chest CT has multiple advantages over RT-PCR yielding results more quickly and is already widely deployed, but requires expert radiologist interpretation. The number of chest CTs may rapidly exceed the speed and capacity of already strained radiologists. An explainable machine learning algorithm may address this disadvantage to expedite the interpretation of chest CT and assist rapid triage of patients to the ICU, inpatient ward, monitoring unit, or home self-quarantine. Machine learning algorithms, specifically those leveraging deep convolutional neural networks (deep learning), have the potential for facilitating even more rapid diagnosis within minutes. This project seeks to validate the use of explainable deep learning methods to adjust diagnostic operating points for multiple applications, including (a) disease screening, (b) disease staging and prognostication, and (c) evaluation of treatment response.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	101643.0 USD
69	Haris Vikalo	University of Texas at Austin	None	2020-05-15	2021-04-30	RAPID: Methods for Reconstructing Disease Transmissions from Viral Genomic Data with Application to COVID-19	Computer and Information Science and Engineering - The coronavirus causing COVID-19 was first detected in humans in November 2019 and rapidly developed into a pandemic. There is an urgent need to enhance the ability to precisely track and predict spread of the disease. However, analysis of classical epidemiological data such as the time of testing and lengths of exposure provides limited insight. This Rapid Response Research (RAPID) project aims to enable discovery of disease transmission patterns based on analysis of genomic data, provide accurate identification of transmission clusters, and enable detection of critical nodes in a network of pathogen hosts while also providing insight into pathogen-mutation processes that occur during the spread of the disease.<br/><br/>The specific aims of this project are to: (1) Develop methods for the inference of a network of hosts based on genomic information about viral pathogens infecting them. In particular, this research thrust is focused on the reconstruction of a weighted directed graph whose nodes represent hosts and edge weights reflect evolutionary distance between corresponding pathogens. (2) Develop methods for the discovery of transmission clusters and identification of critical nodes in the host network. The focus of this research thrust is on deep-learning algorithms for the identification of transmission clusters, and discovery of the host network nodes that played a pivotal role in the disease outbreak. (3) Relying on the developed methods, analyze publicly available COVID-19 datasets. The results of the outlined work are expected to have an immediate impact on the understanding of the coronavirus transmission and spread.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	100000.0 USD
70	Wei Gao	University of Pittsburgh	None	2020-07-01	2021-06-30	RAPID: In-Home Automated and Non-Invasive Evaluation of COVID-19 Infection with Commodity Smartphones	Computer and Information Science and Engineering - A key to combat the Coronavirus Disease (COVID-19) pandemic is to prevent the pandemic from overloading the public healthcare system, so that sufficient medical resources could be available for hospitalized patients. This project will develop new mobile sensing and Artificial Intelligence (AI) techniques for in-home evaluation of COVID-19 infection in order to pursue automated and non-invasive screening of potential viral disease carriers. It aims to timely identify negative cases caused by other diseases with similar symptoms, and hence avoids unnecessary hospital visits as many as possible. <br/><br/>The proposed techniques will use commodity smartphones to measure the changes of humans? airway mechanics, which are uniquely correlated to COVID-19 infection. These measurements build on acoustic sensing with smartphones? built-in speakers and microphones. First, new acoustic waveforms will be designed to minimize acoustic signal distortion in human airways. Second, new signal processing techniques will be developed for accurate measurements. Third, deep learning techniques will be used to develop generic models that depict the core characteristics of airway mechanics. These techniques will be evaluated by lab testing and experiments with student volunteers. This research will enable identifying false positives of COVID-19 infection out of the clinic and could contribute to the containment of the virus spread and damage. The proposed technologies will be applicable to a wide variety of commodity smartphones and could also be used in daily practice with handmade mouthpieces. Broader impacts will also result from a variety of education and outreach activities. New courses will be developed to incorporate the outcome of this research, and the research outcome will be disseminated through technology transfer to industry. The outcome of this project, including source codes and collected data from student volunteers, will be maintained at the project repository (http://www.pitt.edu/~weigao/research_COVID19.html) for at least five years, and will be made available to the public community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	200000.0 USD
71	Elena Zheleva	University of Illinois at Chicago	None	2020-05-15	2021-04-30	III: RAPID: Stay-at-home attitudes and their impact on the COVID-19 pandemic	Computer and Information Science and Engineering - The rapid spread of the novel SARS-CoV-2 coronavirus has paralyzed societies and strained health care systems, with a rising death toll and severe economic consequences. Stay-at-home orders around the globe have been embraced by some and protested by others. At the same time, little is known about the spectrum of attitudes towards these orders and people's justifications for following or resisting them. This research will develop algorithms for analyzing stay-at-home attitudes on social media, connecting these attitudes to pandemic impact through a novel visual representation that takes geographical location and socioeconomic context into account. This research will bring greater awareness to the public about the role of values that protect life in public discourse and their influence on citizens' appraisal of policies that affect their own well-being. Shared values, beliefs, and understandings build the social cohesion and cooperation needed to build greater economic prosperity. The results of this project will help policy makers craft more persuasive public health directives to enhance public health.<br/><br/>Framing--highlighting certain aspects of an issue or event--can have a significant impact on the formation of perspective. To address the complexity of modern information networks, this project will develop algorithms that automatically detect frames propagated through social media. It focuses on value frames because people use those values to justify a position and issues can be re-framed accordingly to appeal to and change the opinions of target audiences. Creating the first dataset of its kind, this project will collect and annotate tweets that include stay-at-home attitudes and core value frames. Current state-of-the-art approaches to detecting attitudes in tweets are limited to binary or ternary classification of either sentiment (positive, negative, neutral) or language type (abusive versus "normal"). By bringing together state-of-the-art deep learning models with models that have more explanatory power, this project will devise a novel methodology for identifying values frames in microblogs that takes advantage of semantic and discourse structure information. By enabling statistical computing and analysis over geospatial data, for which few techniques exist currently, this research will make it possible to analyze datasets at multiple levels of spatial aggregation and to compare temporal and spatial differences to enhance participation and promote positive public health outcomes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	99338.0 USD
72	Philip Resnik	University of Maryland College Park	None	2020-05-15	2021-04-30	RAPID: Advanced Topic Modeling Methods to Analyze Text Responses in COVID-19 Survey Data	Computer and Information Science and Engineering - As the COVID-19 pandemic continues, public and private organizations are deploying surveys to inform responses and policy choices. Survey designs using multiple choice responses are by far the most common -- "open ended" questions, where survey participants provide a longer-form written response, are used far less. This is true despite the fact that when you allow people to provide unconstrained spoken or text responses, it is possible to obtain richer, fine-grained information clarifying the other responses, as well as useful ?bottom up? information that the survey designers did not know to ask for. A key problem is that analyzing the unstructured language in open-ended responses is a labor-intensive process, creating obstacles to using them especially when speedy analysis is needed and resources are limited. Computational methods can help, but they often fail to provide coherent, interpretable categories, or they can fail to do a good job connecting the text in the survey with the closed-end responses. This project will develop new computational methods for fast and effective analysis of survey data that includes text responses, and it will apply these methods to support organizations doing high-impact survey work related to COVID-19 response. This will improve these organizations? ability to understand and mitigate the impact of the COVID-19 pandemic.<br/><br/>This project?s technical approach builds on recent techniques bringing together deep learning and Bayesian topic models. Several key technical innovations will be introduced that are specifically geared toward improving the quality of information available in surveys that include both closed- and open-ended responses. A common element in these approaches is the extension of methods commonly used in supervised learning settings, such as task-based fine-tuning of embeddings and knowledge distillation, to unsupervised topic modeling, with a specific focus on producing diverse, human-interpretable topic categories that are well aligned with discrete attributes such as demographic characteristics, closed-end responses, and experimental condition. Project activities include assisting in the analysis of organizations' survey data, conducting independent surveys aligned with their needs to obtain additional relevant data, and the public release of a clean, easy to use computational toolkit facilitating more widespread adoption of these new methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.	National Science Foundation	Research Grant	176785.0 USD
73	Artem Cherkasov	University of British Columbia	None	None	2021-04-30	Augmented Discovery of Potential Inhibitors of SARS-CoV-2 3CL Protease	The COVID-19 epidemic is causing serious or even fatal respiratory tract infections around the city of Wuhan, China and other countries. The urgent situation is pressing the global community to respond rapidly together to develop a vaccine or small molecule drug to inhibit viral infection. We have recently established a powerful Deep-Learning accelerated Docking pipeline to virtually screen a commercial 1.3-billion-compound library in a matter of one week--compared to the three years with previous programs. We have applied this novel algorithm to identify 1000 quality "candidate" compounds to inhibit the SARS-CoV-2 main protease (3CLpro) which is uniquely critical for the viral life cycle. We will screen these compounds with a high throughput screening biochemical assay and then evaluate these hits using a cell-based SARS-CoV-2 viral replication assay in a Canadian Containment Level 3 facilty at University of British Columbia. In addition we will use X-ray crystallography to refine the protease 3D crystal structure to accelerate the development of COVID-19 therapeutic drug development. Our research program will significantly contribute to global response to the COVID-19 outbreak by rapidly identifying small anti-viral drug molecules in an extremely condensed timeframe. Our expertise, facilities, and capabilities in cutting-edge Artificial Intelligence, inhibitor modeling, X-ray crystallography, coronavirus protease inhibition, human viral pathogen research, and anti-viral therapeutics are world class. Our first application this month of "Deep Docking" enabled the screening of 1.3B commercially available compounds against the essential SARS-CoV-2 protease, in 1 week compared to the 3 years of conventional docking. This accomplishment coupled with fast tracking anti-viral assays at UBC and high resolution 3D structure characterization provide our team, Canadians and global colleagues an enormous head start on developing an anti-viral therapeutic for COVID-19	Canadian Institutes of Health Research	Research Grant	999000.0 CAD
74	Helmholtz Zentrum MÃ¼nchen	Helmholtz Zentrum MÃ¼nchen, Germany	None	None	2021-04-30	RiPCoN	Rapid interaction profiling of 2019-nCoV for network-based deep drug- repurpose learning (DDRL)	European Commission	Research Grant	None
75	Dr Samir Bhatt	Imperial College School of Medicine	None	2019-04-01	2021-03-31	Unravelling and predicting the spatiotemporal structure of infectious disease outbreaks	Developing effective methods to model outbreak disease dynamics that can accurately forecast and predict over various spatiotemporal scales is a fundamental challenge in infection control. Statistical methods in this area have been dominated by individual-based and mechanistic models that are often cumbersome and difficult to apply in predictive scenarios. In contrast, probabilistic frameworks that both model infectiousness and draw from highly predictive deep learning frameworks have been rarely explored. In this grant, we will apply, and further develop a class of inhomogeneous, self-exciting, point processes that jointly capture both the spatiotemporal risk of primary infection and the secondary infection network structure. These spatiotemporal models will incorporate the self-exciting, environmental/demographic and latently varying drivers, and make use of advanced approximate inference to allow for tractable computation. Using these processes, we will explore what optimal forecasting performance can be expected from state-of-the-art machine learning methods and investigate what parts of the predictive model account for this performance, i.e. can we apportion the variance explained to the self-exciting versus latent factors. We will further investigate whether it is possible to exploit genetic information and embed these approaches in the standard suite of phylodynamic methods. We will apply these developed methods to historical outbreak data across a range of diseases and both qualitatively and quantitatively examine the differences.	The Academy of Medical Sciences	Springboard Round 4	99994.63 GBP
